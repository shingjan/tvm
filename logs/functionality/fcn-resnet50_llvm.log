nohup: ignoring input
[15:58:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"
[15:58:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 256, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 256, 54, 54, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 54, 54, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 53 and 1 <= i3_1 and i3_1 < 53, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 52, 52, 4, 1024, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [64, 256, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[15:58:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 256, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 256, 54, 54, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1 in T.grid(1, 4, 1, 13, 1, 1, 2, 13):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 256, 6, 6, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(54, i2_1 * 4 + ax2)
                        i3 = T.axis.spatial(54, i3_0 * 4 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 53 and 1 <= i3 and i3 < 53, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 32, 1, 3, 1, 4, 4, 1, 1, 32, 3, 1, 1, 2, 1, 2, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 16 + i1_1 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(52, i2_1 * 4 + i2_2)
                        ow = T.axis.spatial(52, i3_0 * 4 + i3_1 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(1024, i5_0 * 32 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [64, 256, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 2, 4, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 4, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[13, 2, 1, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[32, 32])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[15:58:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 256, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 256, 54, 54, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 13, 1, 1, 2, 13, 2, 1):
                for i5_0 in T.serial(32):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 6, 4, 4):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(256, i5_0 * 8 + ax1)
                            i2 = T.axis.spatial(54, i2_1 * 4 + ax2)
                            i3 = T.axis.spatial(54, i3_0 * 4 + i3_1 * 2 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 53 and 1 <= i3 and i3 < 53, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 1, 4, 4, 1, 1, 32, 3, 1, 1, 2, 1, 2, 4):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i1_0 * 16 + i1_1 * 8 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(52, i2_1 * 4 + i2_2)
                            ow = T.axis.spatial(52, i3_0 * 4 + i3_1 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_3)
                            ic = T.axis.reduce(1024, i5_0 * 32 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [64, 256, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 4, 2, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 16 + i1_1 * 8 + ax1)
                        ax2_1 = T.axis.spatial(52, i2_1 * 4 + ax2)
                        ax3_1 = T.axis.spatial(52, i3_0 * 4 + i3_1 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 2, 4, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 4, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[13, 2, 1, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[32, 32])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[15:58:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 256, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 256, 54, 54, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0 in T.grid(1, 4, 1, 13):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 256, 54, 6, 4):
                    with T.block("data_pad"):
                        i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        i3 = T.axis.spatial(54, i3_0 * 4 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 53 and 1 <= i3 and i3 < 53, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_0 in T.serial(1):
                    for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 13, 2, 1, 32, 1, 3, 1, 4, 4, 1, 1, 32, 3, 1, 1, 2, 1, 2, 4):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i1_0 * 16 + i1_1 * 8 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(52, i2_1 * 4 + i2_2)
                            ow = T.axis.spatial(52, i3_0 * 4 + i3_1 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_3)
                            ic = T.axis.reduce(1024, i5_0 * 32 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [64, 256, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 52, 4, 4):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(64, i1_0 * 16 + ax1)
                            ax2_1 = T.axis.spatial(52, ax2)
                            ax3_1 = T.axis.spatial(52, i3_0 * 4 + ax3)
                            ax4_1 = T.axis.spatial(4, ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 2, 4, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 4, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[13, 2, 1, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[32, 32])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[15:58:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #1: "fused_layout_transform"
[15:58:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], T_layout_trans: T.Buffer[(1, 128, 52, 52, 2), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 2):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 2 + ax4 < 256 and ax2 < 52 and ax3 < 52, placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4], T.float32(0), dtype="float32")
    

[15:58:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[15:58:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], T_layout_trans: T.Buffer[(1, 128, 52, 52, 2), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 2):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 2 + ax4 < 256 and ax2 < 52 and ax3 < 52, placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[15:58:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #2: "fused_nn_contrib_conv2d_NCHWc_add"
[15:58:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 2), "float32"], placeholder_1: T.Buffer[(7, 128, 1, 1, 2, 3), "float32"], placeholder_2: T.Buffer[(1, 7, 1, 1, 3), "float32"], T_add: T.Buffer[(1, 7, 52, 52, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 7, 52, 52, 3], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 7, 52, 52, 3, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2], placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 2], "float32"], ["TENSOR", [7, 128, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2] * placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 7, 52, 52, 3):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[15:58:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 2), "float32"], placeholder_1: T.Buffer[(7, 128, 1, 1, 2, 3), "float32"], placeholder_2: T.Buffer[(1, 7, 1, 1, 3), "float32"], T_add: T.Buffer[(1, 7, 52, 52, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 7, 52, 52, 3], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 2, 1, 1, 7, 1, 26, 1, 8, 1, 1, 1, 1, 26, 1, 3, 32, 1, 1, 1, 1, 2, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(7, i1_1)
                    oh = T.axis.spatial(52, i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(52, i3_0 * 26 + i3_1)
                    oc_block = T.axis.spatial(3, i4_2)
                    ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2], placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 2], "float32"], ["TENSOR", [7, 128, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2] * placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 7, 52, 52, 3):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 26, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 26, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 3, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[8, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[15:58:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 2), "float32"], placeholder_1: T.Buffer[(7, 128, 1, 1, 2, 3), "float32"], placeholder_2: T.Buffer[(1, 7, 1, 1, 3), "float32"], T_add: T.Buffer[(1, 7, 52, 52, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 7, 52, 52, 3], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 2, 1, 1, 7, 1, 26, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 1, 26, 1, 3, 32, 1, 1, 1, 1, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(7, i1_1)
                        oh = T.axis.spatial(52, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(52, i3_0 * 26 + i3_1)
                        oc_block = T.axis.spatial(3, i4_2)
                        ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2], placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 2], "float32"], ["TENSOR", [7, 128, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2] * placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 52, 1, 3):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(7, i1_1 + ax1)
                        ax2_1 = T.axis.spatial(52, ax2)
                        ax3_1 = T.axis.spatial(52, i3_0 * 26 + i3_1 + ax3)
                        ax4_1 = T.axis.spatial(3, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 26, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 26, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 3, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[8, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 2), "float32"], placeholder_1: T.Buffer[(7, 128, 1, 1, 2, 3), "float32"], placeholder_2: T.Buffer[(1, 7, 1, 1, 3), "float32"], T_add: T.Buffer[(1, 7, 52, 52, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 7, 52, 52, 3], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 7, 1, 26, 1, 8, 1, 1, 1, 1, 26, 1, 3, 32, 1, 1, 1, 1, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(7, i1_1)
                        oh = T.axis.spatial(52, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(52, i3_0 * 26 + i3_1)
                        oc_block = T.axis.spatial(3, i4_2)
                        ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2], placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 2], "float32"], ["TENSOR", [7, 128, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2] * placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 7, 52, 26, 3):
                    with T.block("T_add"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(52, i3_0 * 26 + ax3)
                        ax4_1 = T.axis.spatial(3, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 26, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 26, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 3, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[8, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"
[15:58:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 512, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 512, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 512, 52, 52, 4, 1024, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [512, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 512, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[15:58:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 512, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 512, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 13, 2, 1, 1, 128, 4, 1, 1, 16, 1, 1, 1, 1, 1, 2, 2, 64, 1, 1, 1, 1, 1, 13, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(512, i1_0 * 128 + i1_1)
                    oh = T.axis.spatial(52, i2_0 * 4 + i2_1)
                    ow = T.axis.spatial(52, i3_0 * 26 + i3_2 * 13 + i3_3)
                    oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                    ic = T.axis.reduce(1024, i5_0 * 64 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [512, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 512, 52, 52, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 128, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[13, 4, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[16, 64])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[15:58:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 512, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 512, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 13, 2, 1, 1, 128, 4, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 1, 1, 2, 2, 64, 1, 1, 1, 1, 1, 13, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(512, i1_0 * 128 + i1_1)
                        oh = T.axis.spatial(52, i2_0 * 4 + i2_1)
                        ow = T.axis.spatial(52, i3_0 * 26 + i3_2 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(1024, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [512, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 26, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(512, i1_0 * 128 + i1_1 + ax1)
                        ax2_1 = T.axis.spatial(52, i2_0 * 4 + i2_1 + ax2)
                        ax3_1 = T.axis.spatial(52, i3_0 * 26 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 128, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[13, 4, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[16, 64])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 512, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 512, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 13, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 128, 4, 1, 1, 16, 1, 1, 1, 1, 1, 2, 2, 64, 1, 1, 1, 1, 1, 13, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(512, i1_0 * 128 + i1_1)
                        oh = T.axis.spatial(52, i2_0 * 4 + i2_1)
                        ow = T.axis.spatial(52, i3_0 * 26 + i3_2 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(1024, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [512, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 4, 26, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(512, i1_0 * 128 + ax1)
                        ax2_1 = T.axis.spatial(52, i2_0 * 4 + ax2)
                        ax3_1 = T.axis.spatial(52, i3_0 * 26 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 128, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[13, 4, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[16, 64])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"
[15:58:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 256, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 256, 52, 52, 4, 512, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 256, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 256, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 1, 4, 1, 13, 1, 256, 1, 1, 1, 8, 52, 2, 1, 2, 1, 1, 1, 8, 1, 2, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(256, i1_1 * 64 + i1_2 * 8 + i1_3)
                    oh = T.axis.spatial(52, i2_2)
                    ow = T.axis.spatial(52, i3_1 * 4 + i3_2 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                    ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 256, 52, 52, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 4, 8, 8])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 52, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 2, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[256, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 256, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 256, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 2, 1, 4, 1, 13, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(256, 1, 1, 1, 8, 52, 2, 1, 2, 1, 1, 1, 8, 1, 2, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i1_1 * 64 + i1_2 * 8 + i1_3)
                        oh = T.axis.spatial(52, i2_2)
                        ow = T.axis.spatial(52, i3_1 * 4 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 52, 4, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(256, i1_1 * 64 + ax1)
                        ax2_1 = T.axis.spatial(52, ax2)
                        ax3_1 = T.axis.spatial(52, i3_1 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 4, 8, 8])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 52, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 2, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[256, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 256, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 256, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 13, 1, 256, 1, 1, 1, 8, 52, 2, 1, 2, 1, 1, 1, 8, 1, 2, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i1_1 * 64 + i1_2 * 8 + i1_3)
                        oh = T.axis.spatial(52, i2_2)
                        ow = T.axis.spatial(52, i3_1 * 4 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 256, 52, 52, 2):
                    with T.block("T_add"):
                        ax0_1, ax1_1, ax2_1, ax3_1 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 4, 8, 8])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 52, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 2, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[256, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"
[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 52, 52, 4, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 104, 104, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 16, 1, 2, 2, 1, 1, 13, 13, 1, 128, 1, 1, 1, 8, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i1_0 * 8 + i1_2)
                    oh = T.axis.spatial(52, i2_1 * 4 + i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(52, i3_0 * 26 + i3_1 * 2 + i3_2)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                    ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 104, 104, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[16, 1, 8, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 13, 2, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 13, 2, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[128, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 16, 1, 2, 2, 1, 1, 13, 13, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 1, 1, 1, 8, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 8 + i1_2)
                        oh = T.axis.spatial(52, i2_1 * 4 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(52, i3_0 * 26 + i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 104, 104, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 4, 2, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(52, i2_1 * 4 + ax2)
                        ax3_1 = T.axis.spatial(52, i3_0 * 26 + i3_1 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[16, 1, 8, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 13, 2, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 13, 2, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[128, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 16, 1, 2, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 13, 13, 1, 128, 1, 1, 1, 8, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 8 + i1_2)
                        oh = T.axis.spatial(52, i2_1 * 4 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(52, i3_0 * 26 + i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 104, 104, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 52, 26, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(52, ax2)
                        ax3_1 = T.axis.spatial(52, i3_0 * 26 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[16, 1, 8, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 13, 2, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 13, 2, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[128, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"
[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 104, 104, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 104, 104, 4, 64, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 104, 104, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 2, 13, 4, 1, 1, 26, 2, 1, 64, 1, 1, 1, 32, 2, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_2)
                    oh = T.axis.spatial(104, i2_0 * 52 + i2_1 * 2 + i2_2)
                    ow = T.axis.spatial(104, i3_0 * 8 + i3_1 * 4 + i3_3)
                    oc_block, ic = T.axis.remap("SR", [i4_0, i5_0])
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 104, 104, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 32, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 26, 2, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[13, 2, 1, 4])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 2, 13, 4, 1, 1, 26, 2, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 32, 2, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_2)
                        oh = T.axis.spatial(104, i2_0 * 52 + i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(104, i3_0 * 8 + i3_1 * 4 + i3_3)
                        oc_block, ic = T.axis.remap("SR", [i4_0, i5_0])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 2, 4, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 32 + ax1)
                        ax2_1 = T.axis.spatial(104, i2_0 * 52 + i2_1 * 2 + ax2)
                        ax3_1 = T.axis.spatial(104, i3_0 * 8 + i3_1 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 32, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 26, 2, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[13, 2, 1, 4])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 2, 13, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 26, 2, 1, 64, 1, 1, 1, 32, 2, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_2)
                        oh = T.axis.spatial(104, i2_0 * 52 + i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(104, i3_0 * 8 + i3_1 * 4 + i3_3)
                        oc_block, ic = T.axis.remap("SR", [i4_0, i5_0])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 52, 8, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 32 + ax1)
                        ax2_1 = T.axis.spatial(104, i2_0 * 52 + ax2)
                        ax3_1 = T.axis.spatial(104, i3_0 * 8 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 32, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 26, 2, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[13, 2, 1, 4])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #7: "fused_layout_transform_1"
[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 416, 416), "float32"], T_layout_trans: T.Buffer[(1, 1, 416, 416, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 416, 416, 3):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1 * 3 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 416 and ax3 < 416, placeholder[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[15:58:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[15:58:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 416, 416), "float32"], T_layout_trans: T.Buffer[(1, 1, 416, 416, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 1, 416, 416, 3):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1 * 3 + ax4, ax2, ax3])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 416 and ax3 < 416, placeholder[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[15:58:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"
[15:58:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 416, 416, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 7, 7, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 422, 422, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 422, 422, 3):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 3, i3_1 - 3, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(3 <= i2_1 and i2_1 < 419 and 3 <= i3_1 and i3_1 < 419, placeholder[i0_1, i1_1, i2_1 - 3, i3_1 - 3, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 208, 208, 4, 3, 7, 7):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 416, 416, 3], "float32"], ["TENSOR", [16, 1, 7, 7, 3, 4], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[15:58:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 416, 416, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 7, 7, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 1, 422, 422, 3], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
            for i0_0, i1_0, i2_0 in T.grid(1, 4, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 213, 421, 3):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(422, i2_0 * 208 + ax2)
                        i3 = T.axis.spatial(422, ax3)
                        i4 = T.axis.spatial(3, ax4)
                        T.reads(placeholder[i0, i1, i2 - 3, i3 - 3, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(3 <= i2 and i2 < 419 and 3 <= i3 and i3 < 419, placeholder[i0, i1, i2 - 3, i3 - 3, i4], T.float32(0), dtype="float32")
                for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 2, 1, 4, 2, 26, 1, 3, 7, 1, 1, 1, 26, 1, 2, 1, 1, 7, 1, 1, 2, 4, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 4 + i1_1)
                        oh = T.axis.spatial(208, i2_0 * 104 + i2_1 * 52 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(208, i3_0 * 104 + i3_1 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_1])
                        T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 416, 416, 3], "float32"], ["TENSOR", [16, 1, 7, 7, 3, 4], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 4, 1, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 26, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 26, 1, 4])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[7, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 7])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[15:58:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 416, 416, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 7, 7, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 1, 422, 422, 3], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 1, 422, 422, 3):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 3, i3_1 - 3, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(3 <= i2_1 and i2_1 < 419 and 3 <= i3_1 and i3_1 < 419, placeholder[i0_1, i1_1, i2_1 - 3, i3_1 - 3, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1 in T.grid(1, 4, 2, 2, 2, 1, 4, 2, 26, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 7, 1, 1, 1, 26, 1, 2, 1, 1, 7, 1, 1, 2, 4, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 4 + i1_1_1)
                        oh = T.axis.spatial(208, i2_0 * 104 + i2_1_1 * 52 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(208, i3_0 * 104 + i3_1_1 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_1])
                        T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 416, 416, 3], "float32"], ["TENSOR", [16, 1, 7, 7, 3, 4], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 52, 4, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 4 + i1_1_1 + ax1)
                        ax2_1 = T.axis.spatial(208, i2_0 * 104 + i2_1_1 * 52 + ax2)
                        ax3_1 = T.axis.spatial(208, i3_0 * 104 + i3_1_1 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 4, 1, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 26, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 26, 1, 4])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[7, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 7])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[15:58:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 416, 416, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 7, 7, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 1, 422, 422, 3], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 2, 2, 2):
                for i0_1, i1_1 in T.grid(1, 4):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 213, 213, 3):
                        with T.block("data_pad"):
                            i0, i1 = T.axis.remap("SS", [ax0, ax1])
                            i2 = T.axis.spatial(422, i2_0 * 208 + ax2)
                            i3 = T.axis.spatial(422, i3_0 * 208 + ax3)
                            i4 = T.axis.spatial(3, ax4)
                            T.reads(placeholder[i0, i1, i2 - 3, i3 - 3, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(3 <= i2 and i2 < 419 and 3 <= i3 and i3 < 419, placeholder[i0, i1, i2 - 3, i3 - 3, i4], T.float32(0), dtype="float32")
                    for i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 26, 1, 3, 7, 1, 1, 1, 26, 1, 2, 1, 1, 7, 1, 1, 2, 4, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_0 * 4 + i1_1)
                            oh = T.axis.spatial(208, i2_0 * 104 + i2_1 * 52 + i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(208, i3_0 * 104 + i3_1 * 4 + i3_3)
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                            ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_1])
                            T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 416, 416, 3], "float32"], ["TENSOR", [16, 1, 7, 7, 3, 4], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 104, 104, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(208, i2_0 * 104 + ax2)
                        ax3_1 = T.axis.spatial(208, i3_0 * 104 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 4, 1, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 26, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 26, 1, 4])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[7, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 7])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[15:58:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #9: "fused_nn_max_pool2d"
[15:58:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], tensor: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 16, 210, 210, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 210, 210, 4):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1, ax4])
                T.writes(pad_temp[ax0, ax1, ax2, ax3, ax4])
                pad_temp[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(1 <= ax2 and ax2 < 209 and 1 <= ax3 and ax3 < 209, placeholder[ax0, ax1, ax2 - 1, ax3 - 1, ax4], T.float32(-3.4028234663852886e+38), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 16, 104, 104, 4, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
    

[15:58:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], tensor: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 16, 210, 210, 4], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 16, 104, 104, 4, 1], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 3, 3, 1):
                    with T.block("pad_temp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1 + ax1)
                        ax2_1 = T.axis.spatial(210, i2 * 2 + ax2)
                        ax3_1 = T.axis.spatial(210, i3 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4 + ax4)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 - 1, ax3_1 - 1, ax4_1])
                        T.writes(pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.if_then_else(1 <= ax2_1 and ax2_1 < 209 and 1 <= ax3_1 and ax3_1 < 209, placeholder[ax0_1, ax1_1, ax2_1 - 1, ax3_1 - 1, ax4_1], T.float32(-3.4028234663852886e+38), dtype="float32")
                for i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 9):
                    with T.block("tensor_rf"):
                        vi5_i6_fused_0 = T.axis.spatial(1, i5_i6_fused_0)
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                        rv0 = T.axis.reduce(3, i5_i6_fused_1 // 3)
                        rv1 = T.axis.reduce(3, i5_i6_fused_1 % 3)
                        T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                        with T.init():
                            tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.float32(-3.4028234663852886e+38)
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0], pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
            for i0, i1, i2, i3, i4, i5_i6_fused_0 in T.grid(1, 16, 104, 104, 4, 1):
                with T.block("tensor"):
                    vi5_i6_fused_0 = T.axis.reduce(1, i5_i6_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 9])
l13, l14 = sch.split(loop=l10, factors=[v11, v12])
b15 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
l17 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
[15:58:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], tensor: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 16, 210, 210, 4], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 16, 104, 104, 4, 9], dtype="float32")
            for i0, i1 in T.grid(1, 16):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 209, 209, 4):
                    with T.block("pad_temp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1 + ax1)
                        ax2_1 = T.axis.spatial(210, ax2)
                        ax3_1 = T.axis.spatial(210, ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 - 1, ax3_1 - 1, ax4_1])
                        T.writes(pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.if_then_else(1 <= ax2_1 and ax2_1 < 209 and 1 <= ax3_1 and ax3_1 < 209, placeholder[ax0_1, ax1_1, ax2_1 - 1, ax3_1 - 1, ax4_1], T.float32(-3.4028234663852886e+38), dtype="float32")
                for i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(104, 104, 4, 1, 9):
                    with T.block("tensor_rf"):
                        vi5_i6_fused_1 = T.axis.spatial(9, i5_i6_fused_1)
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                        T.reads(pad_temp[ax0, ax1, ax2 * 2 + vi5_i6_fused_1 // 3, ax3 * 2 + vi5_i6_fused_1 % 3, ax4])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = pad_temp[ax0, ax1, ax2 * 2 + vi5_i6_fused_1 // 3, ax3 * 2 + vi5_i6_fused_1 % 3, ax4]
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 16, 104, 104, 4, 1, 9):
                with T.block("tensor"):
                    vi5_i6_fused_1 = T.axis.reduce(9, i5_i6_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 9])
l13, l14 = sch.split(loop=l10, factors=[v11, v12])
b15 = sch.rfactor(loop=l14, factor_axis=5)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
l17 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
[15:58:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], tensor: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 16, 104, 104, 4, 3, 3):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0 - 1, ax3 * 2 + rv1 - 1, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], T.if_then_else(1 <= ax2 * 2 + rv0 and ax2 * 2 + rv0 < 209 and 1 <= ax3 * 2 + rv1 and ax3 * 2 + rv1 < 209, placeholder[ax0, ax1, ax2 * 2 + rv0 - 1, ax3 * 2 + rv1 - 1, ax4], T.float32(-3.4028234663852886e+38), dtype="float32"))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[15:58:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"
[15:58:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 104, 104, 4, 64, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [16, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 26, 1, 1, 1, 26, 2, 4, 8, 1, 1, 1, 4, 2, 1, 1, 8, 1, 1, 1, 1, 2, 2, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i1_0 * 4 + i1_2)
                    oh = T.axis.spatial(104, i2_1 * 4 + i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(104, i3_0 * 4 + i3_1 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_1)
                    ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [16, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 26, 2, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[26, 2, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 26, 1, 1, 1, 26, 2, 4):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 4, 2, 1, 1, 8, 1, 1, 1, 1, 2, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 4 + i1_2)
                        oh = T.axis.spatial(104, i2_1 * 4 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(104, i3_0 * 4 + i3_1 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [16, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 4, 2, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(104, i2_1 * 4 + ax2)
                        ax3_1 = T.axis.spatial(104, i3_0 * 4 + i3_1 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 26, 2, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[26, 2, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 26, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 26, 2, 4, 8, 1, 1, 1, 4, 2, 1, 1, 8, 1, 1, 1, 1, 2, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 4 + i1_2)
                        oh = T.axis.spatial(104, i2_1 * 4 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(104, i3_0 * 4 + i3_1 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [16, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 104, 4, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(104, ax2)
                        ax3_1 = T.axis.spatial(104, i3_0 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 26, 2, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[26, 2, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"
[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 104, 104, 4, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 104, 104, 4], "float32"], ["TENSOR", [16, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 4, 2, 1, 1, 2, 1, 4, 1, 128, 1, 1, 1, 1, 1, 1, 4, 2, 1, 1, 1, 8, 26, 13, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i1_1 * 8 + i1_3)
                    oh = T.axis.spatial(104, i2_0 * 26 + i2_3)
                    ow = T.axis.spatial(104, i3_0 * 52 + i3_1 * 13 + i3_3)
                    oc_block = T.axis.spatial(4, i4_2)
                    ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 104, 104, 4], "float32"], ["TENSOR", [16, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 1, 26])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 4, 1, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 4, 2, 1, 1, 2, 1, 4, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 1, 1, 1, 1, 1, 1, 4, 2, 1, 1, 1, 8, 26, 13, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_1 * 8 + i1_3)
                        oh = T.axis.spatial(104, i2_0 * 26 + i2_3)
                        ow = T.axis.spatial(104, i3_0 * 52 + i3_1 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 104, 104, 4], "float32"], ["TENSOR", [16, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 26, 13, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_1 * 8 + ax1)
                        ax2_1 = T.axis.spatial(104, i2_0 * 26 + ax2)
                        ax3_1 = T.axis.spatial(104, i3_0 * 52 + i3_1 * 13 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 1, 26])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 4, 1, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 4, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 4, 1, 128, 1, 1, 1, 1, 1, 1, 4, 2, 1, 1, 1, 8, 26, 13, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_1 * 8 + i1_3)
                        oh = T.axis.spatial(104, i2_0 * 26 + i2_3)
                        ow = T.axis.spatial(104, i3_0 * 52 + i3_1 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 104, 104, 4], "float32"], ["TENSOR", [16, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 26, 52, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(104, i2_0 * 26 + ax2)
                        ax3_1 = T.axis.spatial(104, i3_0 * 52 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 1, 26])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 4, 1, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"
[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 16, 106, 106, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 106, 106, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 105 and 1 <= i3_1 and i3_1 < 105, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 104, 104, 4, 64, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 16, 106, 106, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 106, 106, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 105 and 1 <= i3_1 and i3_1 < 105, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 2, 1, 1, 2, 2, 13, 1, 16, 1, 1, 1, 2, 26, 1, 4, 4, 3, 3, 1, 2, 2, 4, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1_1 * 4 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(104, i2_1_1 * 52 + i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(104, i3_0 * 52 + i3_1_1 * 4 + i3_3)
                    oc_block = T.axis.spatial(4, i4_2)
                    ic = T.axis.reduce(64, i5_0 * 4 + i5_1)
                    kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                    T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 2, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 26, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 13, 1, 4])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 16, 106, 106, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 2, 1, 1, 2, 2, 13, 1):
                for i5_0 in T.serial(16):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 54, 6, 4):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(16, i5_0 + ax1)
                            i2 = T.axis.spatial(106, i2_1 * 52 + ax2)
                            i3 = T.axis.spatial(106, i3_0 * 52 + i3_1 * 4 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 105 and 1 <= i3 and i3 < 105, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 2, 26, 1, 4, 4, 3, 3, 1, 2, 2, 4, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(104, i2_1 * 52 + i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(104, i3_0 * 52 + i3_1 * 4 + i3_3)
                            oc_block = T.axis.spatial(4, i4_2)
                            ic = T.axis.reduce(64, i5_0 * 4 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 52, 4, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(104, i2_1 * 52 + ax2)
                        ax3_1 = T.axis.spatial(104, i3_0 * 52 + i3_1 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 2, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 26, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 13, 1, 4])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 16, 106, 106, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 106, 106, 4):
                    with T.block("data_pad"):
                        i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 105 and 1 <= i3 and i3 < 105, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0 in T.grid(1, 2, 1):
                    for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 2, 13, 1, 16, 1, 1, 1, 2, 26, 1, 4, 4, 3, 3, 1, 2, 2, 4, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(104, i2_1 * 52 + i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(104, i3_0 * 52 + i3_1 * 4 + i3_3)
                            oc_block = T.axis.spatial(4, i4_2)
                            ic = T.axis.reduce(64, i5_0 * 4 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 104, 52, 4):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                            ax2_1 = T.axis.spatial(104, ax2)
                            ax3_1 = T.axis.spatial(104, i3_0 * 52 + ax3)
                            ax4_1 = T.axis.spatial(4, ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 2, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 26, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 13, 1, 4])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 64, 104, 104, 4), "float32"], T_relu: T.Buffer[(1, 64, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 104, 104, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 104, 104, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 64, 104, 104, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 104, 104, 4, 64, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 104, 104, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 104, 104, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 104, 104, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 64, 104, 104, 4), "float32"], T_relu: T.Buffer[(1, 64, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 26, 52, 1, 1, 4, 1, 2, 1, 16, 1, 1, 1, 2, 2, 1, 4, 4, 1, 1, 1, 2, 2, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i1_0 * 16 + i1_1 * 4 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(104, i2_0 * 4 + i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(104, i3_0 * 2 + i3_1)
                    oc_block = T.axis.spatial(4, i4_2)
                    ic = T.axis.reduce(64, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 104, 104, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b0)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 4, 2, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[26, 1, 2, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[52, 2, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 64, 104, 104, 4), "float32"], T_relu: T.Buffer[(1, 64, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 26, 52, 1, 1, 4, 1, 2, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 2, 2, 1, 4, 4, 1, 1, 1, 2, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 16 + i1_1 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(104, i2_0 * 4 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(104, i3_0 * 2 + i3_1)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(64, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 4, 1, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 16 + i1_1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(104, i2_0 * 4 + ax2)
                        ax3_1 = T.axis.spatial(104, i3_0 * 2 + i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b0)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 4, 2, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[26, 1, 2, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[52, 2, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 64, 104, 104, 4), "float32"], T_relu: T.Buffer[(1, 64, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 26, 52, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 2, 1, 16, 1, 1, 1, 2, 2, 1, 4, 4, 1, 1, 1, 2, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 16 + i1_1 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(104, i2_0 * 4 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(104, i3_0 * 2 + i3_1)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(64, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 4, 2, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 16 + ax1)
                        ax2_1 = T.axis.spatial(104, i2_0 * 4 + ax2)
                        ax3_1 = T.axis.spatial(104, i3_0 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b0)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 4, 2, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[26, 1, 2, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[52, 2, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 104, 104, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 104, 104, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 104, 104, 4, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 104, 104, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 104, 104, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 104, 104, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[15:58:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 2, 4, 1, 1, 4, 1, 1, 2, 64, 1, 1, 1, 4, 2, 2, 2, 4, 1, 1, 1, 1, 26, 13, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 4 + i1_2)
                    oh = T.axis.spatial(104, i2_0 * 52 + i2_2 * 26 + i2_3)
                    ow = T.axis.spatial(104, i3_0 * 26 + i3_2 * 13 + i3_3)
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                    ic = T.axis.reduce(256, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 104, 104, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 104, 104, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 26])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 2, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 2, 4, 1, 1, 4, 1, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 4, 2, 2, 2, 4, 1, 1, 1, 1, 26, 13, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 4 + i1_2)
                        oh = T.axis.spatial(104, i2_0 * 52 + i2_2 * 26 + i2_3)
                        ow = T.axis.spatial(104, i3_0 * 26 + i3_2 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(256, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 104, 104, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 52, 26, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 16 + i1_1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(104, i2_0 * 52 + ax2)
                        ax3_1 = T.axis.spatial(104, i3_0 * 26 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 26])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 2, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 2, 4, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 1, 2, 64, 1, 1, 1, 4, 2, 2, 2, 4, 1, 1, 1, 1, 26, 13, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 4 + i1_2)
                        oh = T.axis.spatial(104, i2_0 * 52 + i2_2 * 26 + i2_3)
                        ow = T.axis.spatial(104, i3_0 * 26 + i3_2 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(256, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 104, 104, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 52, 26, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 16 + ax1)
                        ax2_1 = T.axis.spatial(104, i2_0 * 52 + ax2)
                        ax3_1 = T.axis.spatial(104, i3_0 * 26 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 26])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 2, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #15: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"
[15:58:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 106, 106, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 106, 106, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 105 and 1 <= i3_1 and i3_1 < 105, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 52, 52, 4, 128, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 104, 104, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[15:58:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 106, 106, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 106, 106, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 105 and 1 <= i3_1 and i3_1 < 105, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 13, 2, 1, 1, 13, 2, 1, 128, 1, 1, 1, 32, 2, 2, 1, 1, 3, 3, 1, 1, 2, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_2)
                    oh = T.axis.spatial(52, i2_1_1 * 4 + i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(52, i3_0 * 4 + i3_1_1 * 2 + i3_2)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                    ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_1, i7_1])
                    T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 104, 104, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 32, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 2, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[13, 2, 2, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[128, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[15:58:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 106, 106, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 106, 106, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 105 and 1 <= i3_1 and i3_1 < 105, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1 in T.grid(1, 1, 1, 13, 2, 1, 1, 13, 2, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 1, 1, 1, 32, 2, 2, 1, 1, 3, 3, 1, 1, 2, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_2)
                        oh = T.axis.spatial(52, i2_1_1 * 4 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(52, i3_0 * 4 + i3_1_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_1, i7_1])
                        T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 104, 104, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 4, 2, 2):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(52, i2_1_1 * 4 + ax2)
                        ax3_1 = T.axis.spatial(52, i3_0 * 4 + i3_1_1 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 32, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 2, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[13, 2, 2, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[128, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[15:58:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 106, 106, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0 in T.grid(1, 1, 1, 13):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 105, 9, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(106, ax2)
                        i3 = T.axis.spatial(106, i3_0 * 8 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 105 and 1 <= i3 and i3 < 105, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_0 in T.serial(2):
                    for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 13, 2, 1, 128, 1, 1, 1, 32, 2, 2, 1, 1, 3, 3, 1, 1, 2, 1, 2):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i1_2)
                            oh = T.axis.spatial(52, i2_1 * 4 + i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(52, i3_0 * 4 + i3_1 * 2 + i3_2)
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                            ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_1, i7_1])
                            T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 104, 104, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 52, 4, 2):
                        with T.block("T_relu"):
                            ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                            ax3_1 = T.axis.spatial(52, i3_0 * 4 + ax3)
                            ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 32, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 2, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[13, 2, 2, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[128, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[15:58:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #16: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"
[15:58:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 52, 52, 4, 512, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [32, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[15:58:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 2, 1, 1, 16, 26, 2, 2, 256, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 13, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1 * 2 + i1_2)
                    oh = T.axis.spatial(52, i2_1 * 2 + i2_2)
                    ow = T.axis.spatial(52, i3_0 * 26 + i3_1 * 13 + i3_3)
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                    ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [32, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 16, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 26, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 2, 1, 1, 16, 26, 2, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(256, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 13, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(52, i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(52, i3_0 * 26 + i3_1 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [32, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 2, 13, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(52, i2_1 * 2 + ax2)
                        ax3_1 = T.axis.spatial(52, i3_0 * 26 + i3_1 * 13 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 16, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 26, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 16, 26, 2, 2, 256, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 13, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(52, i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(52, i3_0 * 26 + i3_1 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [32, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 52, 26, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(52, i3_0 * 26 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 16, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 26, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"
[15:58:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 54, 54, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 54, 54, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 53 and 1 <= i3_1 and i3_1 < 53, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 52, 52, 4, 128, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[15:58:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 1, 16, 2, 1, 1, 8, 3, 3, 1, 2, 1, 26, 2, 16, 1, 1, 1, 1, 26, 2, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1 * 2 + i1_2)
                    oh = T.axis.spatial(52, i2_1 * 26 + i2_3)
                    ow = T.axis.spatial(52, i3_2 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                    ic = T.axis.reduce(128, i5_0 * 16 + i5_1)
                    kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                    T.reads(placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh + kh and oh + kh < 53 and 1 <= ow + kw and ow + kw < 53, placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 16, 2, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 26])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 26, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[8, 16])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[15:58:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 2, 1, 16, 2, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 3, 3, 1, 2, 1, 26, 2, 16, 1, 1, 1, 1, 26, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(52, i2_1 * 26 + i2_3)
                        ow = T.axis.spatial(52, i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(128, i5_0 * 16 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                        T.reads(placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh + kh and oh + kh < 53 and 1 <= ow + kw and ow + kw < 53, placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 26, 52, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(52, i2_1 * 26 + ax2)
                        ax3_1 = T.axis.spatial(52, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 16, 2, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 26])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 26, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[8, 16])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[15:58:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 16, 2, 1, 1, 8, 3, 3, 1, 2, 1, 26, 2, 16, 1, 1, 1, 1, 26, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(52, i2_1 * 26 + i2_3)
                        ow = T.axis.spatial(52, i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(128, i5_0 * 16 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                        T.reads(placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh + kh and oh + kh < 53 and 1 <= ow + kw and ow + kw < 53, placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 52, 52, 2):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 16, 2, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 26])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 26, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[8, 16])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1"
[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 52, 52, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 52, 52, 4, 128, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [128, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 52, 52, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 1, 1, 1, 4, 26, 1, 2, 32, 1, 1, 1, 1, 2, 4, 2, 4, 1, 1, 1, 8, 1, 13, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i1_0 * 32 + i1_1 * 8 + i1_3)
                    oh = T.axis.spatial(52, i2_1 * 2 + i2_2)
                    ow = T.axis.spatial(52, i3_2 * 13 + i3_3)
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                    ic = T.axis.reduce(128, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [128, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b0)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 4, 1, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 26, 2, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 13])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[32, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 52, 52, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 1, 1, 1, 4, 26, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 1, 2, 4, 2, 4, 1, 1, 1, 8, 1, 13, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 32 + i1_1 * 8 + i1_3)
                        oh = T.axis.spatial(52, i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(52, i3_2 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(128, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [128, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 2, 52, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 32 + i1_1 * 8 + ax1)
                        ax2_1 = T.axis.spatial(52, i2_1 * 2 + ax2)
                        ax3_1 = T.axis.spatial(52, ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b0)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 4, 1, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 26, 2, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 13])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[32, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 52, 52, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 26, 1, 2, 32, 1, 1, 1, 1, 2, 4, 2, 4, 1, 1, 1, 8, 1, 13, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 32 + i1_1 * 8 + i1_3)
                        oh = T.axis.spatial(52, i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(52, i3_2 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(128, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [128, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 52, 52, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 32 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b0)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 4, 1, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 26, 2, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 13])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[32, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"
[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 52, 52, 4, 512, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 4, 1, 32, 1, 2, 1, 128, 1, 1, 1, 1, 2, 2, 1, 4, 1, 1, 1, 1, 26, 13, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_1)
                    oh = T.axis.spatial(52, i2_2 * 26 + i2_3)
                    ow = T.axis.spatial(52, i3_1 * 26 + i3_2 * 13 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0)
                    ic = T.axis.reduce(512, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 32, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 26])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 4, 1, 32, 1, 2, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 1, 1, 1, 1, 2, 2, 1, 4, 1, 1, 1, 1, 26, 13, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_1)
                        oh = T.axis.spatial(52, i2_2 * 26 + i2_3)
                        ow = T.axis.spatial(52, i3_1 * 26 + i3_2 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(512, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 52, 26, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 32 + i1_1 + ax1)
                        ax2_1 = T.axis.spatial(52, ax2)
                        ax3_1 = T.axis.spatial(52, i3_1 * 26 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 32, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 26])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 32, 1, 2, 1, 128, 1, 1, 1, 1, 2, 2, 1, 4, 1, 1, 1, 1, 26, 13, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_1)
                        oh = T.axis.spatial(52, i2_2 * 26 + i2_3)
                        ow = T.axis.spatial(52, i3_1 * 26 + i3_2 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(512, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 52, 52, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 32 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 32, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 26])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #20: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"
[15:58:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 64, 54, 54, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 54, 54, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 53 and 1 <= i3_1 and i3_1 < 53, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 52, 52, 4, 256, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[15:58:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 54, 54, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 128):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 54, 54, 2):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(64, i5_0 // 2 + ax1)
                        i2, i3 = T.axis.remap("SS", [ax2, ax3])
                        i4 = T.axis.spatial(4, i5_0 % 2 * 2 + ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 53 and 1 <= i3 and i3 < 53, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 16, 26, 4, 2, 2, 3, 3, 1, 4, 2, 13, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(52, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(52, i3_2 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 16, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 26, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 13])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[128, 2])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[15:58:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 54, 54, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                for i5_0 in T.serial(128):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 54, 54, 2):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(64, i5_0 // 2 + ax1)
                            i2, i3 = T.axis.remap("SS", [ax2, ax3])
                            i4 = T.axis.spatial(4, i5_0 % 2 * 2 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 53 and 1 <= i3 and i3 < 53, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 16, 26, 4, 2, 2, 3, 3, 1, 4, 2, 13, 2):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i1_2 * 4 + i1_3)
                            oh = T.axis.spatial(52, i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(52, i3_2 * 13 + i3_3)
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                            ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 52, 52, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 16, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 26, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 13])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[128, 2])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[15:58:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 128, 1, 1, 1, 16, 26, 4, 2, 2, 3, 3, 1, 4, 2, 13, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(52, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(52, i3_2 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                        T.reads(placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh + kh and oh + kh < 53 and 1 <= ow + kw and ow + kw < 53, placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 52, 52, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 16, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 26, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 13])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[128, 2])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[15:58:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"
[15:58:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 52, 52, 4, 1024, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [64, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[15:58:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 1, 4, 1, 1, 4, 4, 1, 512, 1, 1, 1, 8, 1, 1, 1, 2, 1, 1, 1, 2, 13, 13, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i1_0 * 16 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(52, i2_1 * 13 + i2_3)
                    ow = T.axis.spatial(52, i3_1 * 13 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0)
                    ic = T.axis.reduce(1024, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [64, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 8, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[512, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 1, 4, 1, 1, 4, 4, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(512, 1, 1, 1, 8, 1, 1, 1, 2, 1, 1, 1, 2, 13, 13, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 16 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(52, i2_1 * 13 + i2_3)
                        ow = T.axis.spatial(52, i3_1 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(1024, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [64, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 13, 13, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 16 + ax1)
                        ax2_1 = T.axis.spatial(52, i2_1 * 13 + ax2)
                        ax3_1 = T.axis.spatial(52, i3_1 * 13 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 8, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[512, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 1, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 4, 4, 1, 512, 1, 1, 1, 8, 1, 1, 1, 2, 1, 1, 1, 2, 13, 13, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 16 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(52, i2_1 * 13 + i2_3)
                        ow = T.axis.spatial(52, i3_1 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(1024, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [64, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 52, 52, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 16 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 8, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[512, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"
[15:58:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 64, 56, 56, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 56, 56, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(2 <= i2_1 and i2_1 < 54 and 2 <= i3_1 and i3_1 < 54, placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 52, 52, 4, 256, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, kh * 2 + oh, kw * 2 + ow, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [2, 2, 2, 2], [2, 2], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, kh * 2 + oh, kw * 2 + ow, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 56, 56, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 8, 1, 1, 1, 1, 1, 26, 13, 1, 128):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 6, 8, 2):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(64, i5_0 // 2 + ax1)
                        i2 = T.axis.spatial(56, i2_1 * 2 + ax2)
                        i3 = T.axis.spatial(56, i3_1 * 4 + ax3)
                        i4 = T.axis.spatial(4, i5_0 % 2 * 2 + ax4)
                        T.reads(placeholder[i0, i1, i2 - 2, i3 - 2, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(2 <= i2 and i2 < 54 and 2 <= i3 and i3 < 54, placeholder[i0, i1, i2 - 2, i3 - 2, i4], T.float32(0), dtype="float32")
                for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 1, 8, 1, 1, 4, 2, 3, 1, 1, 1, 2, 4, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 8 + i1_2)
                        oh = T.axis.spatial(52, i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(52, i3_1 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(data_pad[n, ic // 4, kh * 2 + oh, kw * 2 + ow, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [2, 2, 2, 2], [2, 2], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, kh * 2 + oh, kw * 2 + ow, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 8, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 26, 1, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 13, 1, 4])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[128, 2])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 56, 56, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 8):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 56, 56, 4):
                    with T.block("data_pad"):
                        i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 2, i3 - 2, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(2 <= i2 and i2 < 54 and 2 <= i3 and i3 < 54, placeholder[i0, i1, i2 - 2, i3 - 2, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 26, 13, 1):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 1, 3, 1, 8, 1, 1, 4, 2, 3, 1, 1, 1, 2, 4, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i1_0 * 8 + i1_2)
                            oh = T.axis.spatial(52, i2_1 * 2 + i2_3)
                            ow = T.axis.spatial(52, i3_1 * 4 + i3_3)
                            oc_block = T.axis.spatial(4, i4_2)
                            ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                            T.reads(data_pad[n, ic // 4, kh * 2 + oh, kw * 2 + ow, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [2, 2, 2, 2], [2, 2], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, kh * 2 + oh, kw * 2 + ow, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 2, 4, 4):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(64, i1_0 * 8 + ax1)
                            ax2_1 = T.axis.spatial(52, i2_1 * 2 + ax2)
                            ax3_1 = T.axis.spatial(52, i3_1 * 4 + ax3)
                            ax4_1 = T.axis.spatial(4, ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 8, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 26, 1, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 13, 1, 4])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[128, 2])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 8, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 26, 13, 1, 128, 1, 3, 1, 8, 1, 1, 4, 2, 3, 1, 1, 1, 2, 4, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 8 + i1_2)
                        oh = T.axis.spatial(52, i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(52, i3_1 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(placeholder[n, ic // 4, kh * 2 + oh - 2, kw * 2 + ow - 2, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [2, 2, 2, 2], [2, 2], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(2 <= kh * 2 + oh and kh * 2 + oh < 54 and 2 <= kw * 2 + ow and kw * 2 + ow < 54, placeholder[n, ic // 4, kh * 2 + oh - 2, kw * 2 + ow - 2, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 52, 52, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 8 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 8, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 26, 1, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 13, 1, 4])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[128, 2])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2"
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(256, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 256, 52, 52, 4), "float32"], T_relu: T.Buffer[(1, 256, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 52, 52, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 52, 52, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 256, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 256, 52, 52, 4, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [256, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 52, 52, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 52, 52, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(256, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 256, 52, 52, 4), "float32"], T_relu: T.Buffer[(1, 256, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 256, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 16, 1, 1, 2, 1, 2, 1, 13, 2, 64, 1, 1, 1, 1, 2, 2, 1, 4, 1, 1, 1, 8, 26, 2, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(256, i1_0 * 16 + i1_1 * 8 + i1_3)
                    oh = T.axis.spatial(52, i2_2 * 26 + i2_3)
                    ow = T.axis.spatial(52, i3_1 * 4 + i3_2 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                    ic = T.axis.reduce(256, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [256, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 256, 52, 52, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b0)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[16, 2, 1, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 26])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 13, 2, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[64, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(256, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 256, 52, 52, 4), "float32"], T_relu: T.Buffer[(1, 256, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 256, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 16, 1, 1, 2, 1, 2, 1, 13, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 1, 2, 2, 1, 4, 1, 1, 1, 8, 26, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i1_0 * 16 + i1_1 * 8 + i1_3)
                        oh = T.axis.spatial(52, i2_2 * 26 + i2_3)
                        ow = T.axis.spatial(52, i3_1 * 4 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(256, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [256, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 52, 4, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(256, i1_0 * 16 + i1_1 * 8 + ax1)
                        ax2_1 = T.axis.spatial(52, ax2)
                        ax3_1 = T.axis.spatial(52, i3_1 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b0)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[16, 2, 1, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 26])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 13, 2, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[64, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(256, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 256, 52, 52, 4), "float32"], T_relu: T.Buffer[(1, 256, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 256, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 16, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 13, 2, 64, 1, 1, 1, 1, 2, 2, 1, 4, 1, 1, 1, 8, 26, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i1_0 * 16 + i1_1 * 8 + i1_3)
                        oh = T.axis.spatial(52, i2_2 * 26 + i2_3)
                        ow = T.axis.spatial(52, i3_1 * 4 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(256, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [256, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 52, 52, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(256, i1_0 * 16 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b0)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[16, 2, 1, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 26])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 13, 2, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[64, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 52, 52, 4, 1024, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [128, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[15:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 2, 1, 2, 26, 13, 1, 32, 1, 1, 1, 8, 1, 2, 1, 32, 1, 1, 1, 4, 2, 2, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_1 * 32 + i1_2 * 4 + i1_3)
                    oh = T.axis.spatial(52, i2_1 * 2 + i2_3)
                    ow = T.axis.spatial(52, i3_1 * 4 + i3_2 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                    ic = T.axis.reduce(1024, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [128, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 8, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 26, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 2, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 2, 1, 2, 26, 13, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 8, 1, 2, 1, 32, 1, 1, 1, 4, 2, 2, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_1 * 32 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(52, i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(52, i3_1 * 4 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(1024, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [128, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 2, 4, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 64 + i1_1 * 32 + ax1)
                        ax2_1 = T.axis.spatial(52, i2_1 * 2 + ax2)
                        ax3_1 = T.axis.spatial(52, i3_1 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 8, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 26, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 2, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 26, 13, 1, 32, 1, 1, 1, 8, 1, 2, 1, 32, 1, 1, 1, 4, 2, 2, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_1 * 32 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(52, i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(52, i3_1 * 4 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(1024, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [128, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 52, 52, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 64 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 8, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 26, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 2, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"
[15:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 128, 56, 56, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 56, 56, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(2 <= i2_1 and i2_1 < 54 and 2 <= i3_1 and i3_1 < 54, placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 52, 52, 4, 512, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, kh * 2 + oh, kw * 2 + ow, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [2, 2, 2, 2], [2, 2], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, kh * 2 + oh, kw * 2 + ow, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 128, 56, 56, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 2, 2, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 30, 30, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(56, i2_0 * 26 + ax2)
                        i3 = T.axis.spatial(56, i3_0 * 26 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 2, i3 - 2, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(2 <= i2 and i2 < 54 and 2 <= i3 and i3 < 54, placeholder[i0, i1, i2 - 2, i3 - 2, i4], T.float32(0), dtype="float32")
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 13, 1, 1, 16, 1, 1, 1, 2, 1, 1, 1, 32, 3, 3, 1, 4, 2, 26, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 32 + i1_1 * 8 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(52, i2_0 * 26 + i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(52, i3_0 * 26 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                        T.reads(data_pad[n, ic // 4, kh * 2 + oh, kw * 2 + ow, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [2, 2, 2, 2], [2, 2], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, kh * 2 + oh, kw * 2 + ow, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 4, 2, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 13, 1, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 26])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 32])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 2, 2, 2, 1, 4, 13, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 2, 1, 1, 1, 32, 3, 3, 1, 4, 2, 26, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 32 + i1_1 * 8 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(52, i2_0 * 26 + i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(52, i3_0 * 26 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                        T.reads(placeholder[n, ic // 4, kh * 2 + oh - 2, kw * 2 + ow - 2, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [2, 2, 2, 2], [2, 2], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(2 <= kh * 2 + oh and kh * 2 + oh < 54 and 2 <= kw * 2 + ow and kw * 2 + ow < 54, placeholder[n, ic // 4, kh * 2 + oh - 2, kw * 2 + ow - 2, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 2, 26, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 32 + i1_1 * 8 + ax1)
                        ax2_1 = T.axis.spatial(52, i2_0 * 26 + i2_1 * 2 + ax2)
                        ax3_1 = T.axis.spatial(52, i3_0 * 26 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 4, 2, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 13, 1, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 26])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 32])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 128, 56, 56, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 2, 2, 2):
                for i0_1, i1_1 in T.grid(1, 4):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 30, 30, 4):
                        with T.block("data_pad"):
                            i0, i1 = T.axis.remap("SS", [ax0, ax1])
                            i2 = T.axis.spatial(56, i2_0 * 26 + ax2)
                            i3 = T.axis.spatial(56, i3_0 * 26 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 2, i3 - 2, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(2 <= i2 and i2 < 54 and 2 <= i3 and i3 < 54, placeholder[i0, i1, i2 - 2, i3 - 2, i4], T.float32(0), dtype="float32")
                    for i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(13, 1, 1, 16, 1, 1, 1, 2, 1, 1, 1, 32, 3, 3, 1, 4, 2, 26, 2):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i1_0 * 32 + i1_1 * 8 + i1_2 * 4 + i1_3)
                            oh = T.axis.spatial(52, i2_0 * 26 + i2_1 * 2 + i2_3)
                            ow = T.axis.spatial(52, i3_0 * 26 + i3_3)
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                            ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                            T.reads(data_pad[n, ic // 4, kh * 2 + oh, kw * 2 + ow, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [2, 2, 2, 2], [2, 2], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, kh * 2 + oh, kw * 2 + ow, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 26, 26, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 32 + ax1)
                        ax2_1 = T.axis.spatial(52, i2_0 * 26 + ax2)
                        ax3_1 = T.axis.spatial(52, i3_0 * 26 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 4, 2, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 13, 1, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 26])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 32])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"
[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 512, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 52, 52, 4, 2048, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 512, 52, 52, 4], "float32"], ["TENSOR", [128, 512, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 512, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 2, 1, 8, 4, 2, 1, 256, 1, 1, 1, 2, 13, 26, 1, 8, 1, 1, 1, 4, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_1 * 8 + i1_2 * 4 + i1_3)
                    oh = T.axis.spatial(52, i2_1 * 13 + i2_2)
                    ow = T.axis.spatial(52, i3_1 * 26 + i3_2)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                    ic = T.axis.reduce(2048, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 512, 52, 52, 4], "float32"], ["TENSOR", [128, 512, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 8, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 13, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 26, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 512, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 2, 1, 8, 4, 2, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(256, 1, 1, 1, 2, 13, 26, 1, 8, 1, 1, 1, 4, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_1 * 8 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(52, i2_1 * 13 + i2_2)
                        ow = T.axis.spatial(52, i3_1 * 26 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(2048, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 512, 52, 52, 4], "float32"], ["TENSOR", [128, 512, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 13, 26, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 64 + i1_1 * 8 + ax1)
                        ax2_1 = T.axis.spatial(52, i2_1 * 13 + ax2)
                        ax3_1 = T.axis.spatial(52, i3_1 * 26 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 8, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 13, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 26, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 512, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 4, 2, 1, 256, 1, 1, 1, 2, 13, 26, 1, 8, 1, 1, 1, 4, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_1 * 8 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(52, i2_1 * 13 + i2_2)
                        ow = T.axis.spatial(52, i3_1 * 26 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(2048, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 512, 52, 52, 4], "float32"], ["TENSOR", [128, 512, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 52, 52, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 64 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 8, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 13, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 26, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #27: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"
[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 128, 60, 60, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 60, 60, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 4, i3_1 - 4, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(4 <= i2_1 and i2_1 < 56 and 4 <= i3_1 and i3_1 < 56, placeholder[i0_1, i1_1, i2_1 - 4, i3_1 - 4, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 52, 52, 4, 512, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, kh * 4 + oh, kw * 4 + ow, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [4, 4, 4, 4], [4, 4], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, kh * 4 + oh, kw * 4 + ow, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[15:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 128, 60, 60, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0 in T.grid(1, 1, 1, 2, 2, 1, 2, 1, 26, 1, 16, 1, 3):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 60, 1, 4):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(128, i5_0 * 8 + ax1)
                        i2 = T.axis.spatial(60, ax2)
                        i3 = T.axis.spatial(60, i3_0 * 26 + i7_0 * 4 + i3_1 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 4, i3 - 4, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(4 <= i2 and i2 < 56 and 4 <= i3 and i3 < 56, placeholder[i0, i1, i2 - 4, i3 - 4, i4], T.float32(0), dtype="float32")
                for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 16, 2, 1, 1, 32, 3, 1, 1, 4, 26, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_1 * 64 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(52, i2_2 * 26 + i2_3)
                        ow = T.axis.spatial(52, i3_0 * 26 + i3_1)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(data_pad[n, ic // 4, kh * 4 + oh, kw * 4 + ow, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [4, 4, 4, 4], [4, 4], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, kh * 4 + oh, kw * 4 + ow, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 16, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 26])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 26, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 32])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=12)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[15:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 2, 2, 1, 2, 1, 26, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 3, 1, 16, 2, 1, 1, 32, 3, 1, 1, 4, 26, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_1 * 64 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(52, i2_2 * 26 + i2_3)
                        ow = T.axis.spatial(52, i3_0 * 26 + i3_1)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(placeholder[n, ic // 4, kh * 4 + oh - 4, kw * 4 + ow - 4, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [4, 4, 4, 4], [4, 4], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(4 <= kh * 4 + oh and kh * 4 + oh < 56 and 4 <= kw * 4 + ow and kw * 4 + ow < 56, placeholder[n, ic // 4, kh * 4 + oh - 4, kw * 4 + ow - 4, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 52, 1, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_1 * 64 + ax1)
                        ax2_1 = T.axis.spatial(52, ax2)
                        ax3_1 = T.axis.spatial(52, i3_0 * 26 + i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 16, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 26])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 26, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 32])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[15:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 128, 60, 60, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 2, 2):
                for i0_1, i1_1, i2_1, i3_1 in T.grid(1, 2, 1, 26):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 60, 9, 4):
                        with T.block("data_pad"):
                            i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                            i3 = T.axis.spatial(60, i3_0 * 26 + i3_1 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 4, i3 - 4, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(4 <= i2 and i2 < 56 and 4 <= i3 and i3 < 56, placeholder[i0, i1, i2 - 4, i3 - 4, i4], T.float32(0), dtype="float32")
                    for i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 16, 1, 3, 1, 16, 2, 1, 1, 32, 3, 1, 1, 4, 26, 1, 2):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i1_1 * 64 + i1_2 * 4 + i1_3)
                            oh = T.axis.spatial(52, i2_2 * 26 + i2_3)
                            ow = T.axis.spatial(52, i3_0 * 26 + i3_1)
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                            ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                            T.reads(data_pad[n, ic // 4, kh * 4 + oh, kw * 4 + ow, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [4, 4, 4, 4], [4, 4], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, kh * 4 + oh, kw * 4 + ow, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 52, 26, 2):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(52, i3_0 * 26 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 16, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 26])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 26, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 32])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[15:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3"
[15:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(512, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 512, 52, 52, 4), "float32"], T_relu: T.Buffer[(1, 512, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 512, 52, 52, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 512, 52, 52, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 512, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 512, 52, 52, 4, 512, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [512, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 512, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 512, 52, 52, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 512, 52, 52, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(512, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 512, 52, 52, 4), "float32"], T_relu: T.Buffer[(1, 512, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 512, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 2, 1, 1, 1, 2, 1, 13, 1, 512, 1, 1, 1, 1, 13, 4, 4, 1, 1, 1, 1, 64, 2, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(512, i1_0 * 128 + i1_1 * 64 + i1_3)
                    oh = T.axis.spatial(52, i2_0 * 26 + i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(52, i3_1 * 4 + i3_2)
                    oc_block, ic = T.axis.remap("SR", [i4_2, i5_0])
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [512, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 512, 52, 52, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b0)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 2, 1, 64])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 13, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 13, 4, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[512, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(512, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 512, 52, 52, 4), "float32"], T_relu: T.Buffer[(1, 512, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 512, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 2, 1, 1, 1, 2, 1, 13, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(512, 1, 1, 1, 1, 13, 4, 4, 1, 1, 1, 1, 64, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(512, i1_0 * 128 + i1_1 * 64 + i1_3)
                        oh = T.axis.spatial(52, i2_0 * 26 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(52, i3_1 * 4 + i3_2)
                        oc_block, ic = T.axis.remap("SR", [i4_2, i5_0])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [512, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 26, 4, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(512, i1_0 * 128 + i1_1 * 64 + ax1)
                        ax2_1 = T.axis.spatial(52, i2_0 * 26 + ax2)
                        ax3_1 = T.axis.spatial(52, i3_1 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b0)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 2, 1, 64])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 13, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 13, 4, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[512, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(512, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 512, 52, 52, 4), "float32"], T_relu: T.Buffer[(1, 512, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 512, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 2, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 13, 1, 512, 1, 1, 1, 1, 13, 4, 4, 1, 1, 1, 1, 64, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(512, i1_0 * 128 + i1_1 * 64 + i1_3)
                        oh = T.axis.spatial(52, i2_0 * 26 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(52, i3_1 * 4 + i3_2)
                        oc_block, ic = T.axis.remap("SR", [i4_2, i5_0])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [512, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 26, 52, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(512, i1_0 * 128 + ax1)
                        ax2_1 = T.axis.spatial(52, i2_0 * 26 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b0)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 2, 1, 64])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 13, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 13, 4, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[512, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #29: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 512, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 512, 54, 54, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 512, 54, 54, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 53 and 1 <= i3_1 and i3_1 < 53, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 52, 52, 4, 2048, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 512, 52, 52, 4], "float32"], ["TENSOR", [128, 512, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 512, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 512, 54, 54, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 16, 13, 1, 1, 1, 2, 2, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 512, 4, 54, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(54, i2_0 * 4 + i2_1 * 2 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 53 and 1 <= i3 and i3 < 53, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2048, 3, 3, 1, 2, 1, 13, 2, 1, 1, 1, 1, 2, 2, 4, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 8 + i1_1 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(52, i2_0 * 4 + i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(52, i3_2 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_0])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 512, 52, 52, 4], "float32"], ["TENSOR", [128, 512, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[16, 2, 2, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 2, 1, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 13, 4])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[2048, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 512, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 512, 54, 54, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 16):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 512, 54, 54, 4):
                    with T.block("data_pad"):
                        i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 53 and 1 <= i3 and i3 < 53, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(13, 1, 1, 1, 2, 2, 1, 2):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2048, 3, 3, 1, 2, 1, 13, 2, 1, 1, 1, 1, 2, 2, 4, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i1_0 * 8 + i1_1 * 4 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(52, i2_0 * 4 + i2_1 * 2 + i2_3)
                            ow = T.axis.spatial(52, i3_2 * 4 + i3_3)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 512, 52, 52, 4], "float32"], ["TENSOR", [128, 512, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 2, 52, 2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(128, i1_0 * 8 + i1_1 * 4 + ax1)
                            ax2_1 = T.axis.spatial(52, i2_0 * 4 + i2_1 * 2 + ax2)
                            ax3_1 = T.axis.spatial(52, ax3)
                            ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[16, 2, 2, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 2, 1, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 13, 4])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[2048, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 512, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 512, 54, 54, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0 in T.grid(1, 16, 13):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 512, 6, 54, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(54, i2_0 * 4 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 53 and 1 <= i3 and i3 < 53, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_0, i4_0 in T.grid(1, 1):
                    for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 2, 1, 2, 2048, 3, 3, 1, 2, 1, 13, 2, 1, 1, 1, 1, 2, 2, 4, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i1_0 * 8 + i1_1 * 4 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(52, i2_0 * 4 + i2_1 * 2 + i2_3)
                            ow = T.axis.spatial(52, i3_2 * 4 + i3_3)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 512, 52, 52, 4], "float32"], ["TENSOR", [128, 512, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 4, 52, 4):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(128, i1_0 * 8 + ax1)
                            ax2_1 = T.axis.spatial(52, i2_0 * 4 + ax2)
                            ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[16, 2, 2, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 2, 1, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 13, 4])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[2048, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #30: "fused_layout_transform_2"
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], T_layout_trans: T.Buffer[(1, 256, 52, 52, 2), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 52, 52, 2):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 2 + ax4 < 512 and ax2 < 52 and ax3 < 52, placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4], T.float32(0), dtype="float32")
    

[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], T_layout_trans: T.Buffer[(1, 256, 52, 52, 2), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 256, 52, 52, 2):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 2 + ax4 < 512 and ax2 < 52 and ax3 < 52, placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #31: "fused_nn_contrib_conv2d_NCHWc_add_5"
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 2), "float32"], placeholder_1: T.Buffer[(7, 256, 1, 1, 2, 3), "float32"], placeholder_2: T.Buffer[(1, 7, 1, 1, 3), "float32"], T_add: T.Buffer[(1, 7, 52, 52, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 7, 52, 52, 3], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 7, 52, 52, 3, 512, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2], placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 2], "float32"], ["TENSOR", [7, 256, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2] * placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 7, 52, 52, 3):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[15:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 2), "float32"], placeholder_1: T.Buffer[(7, 256, 1, 1, 2, 3), "float32"], placeholder_2: T.Buffer[(1, 7, 1, 1, 3), "float32"], T_add: T.Buffer[(1, 7, 52, 52, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 7, 52, 52, 3], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 512, 1, 1, 1, 7, 2, 13, 3, 1, 1, 1, 1, 1, 26, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(7, i1_2)
                    oh = T.axis.spatial(52, i2_2 * 26 + i2_3)
                    ow = T.axis.spatial(52, i3_0 * 13 + i3_2)
                    oc_block, ic = T.axis.remap("SR", [i4_2, i5_0])
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2], placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 2], "float32"], ["TENSOR", [7, 256, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2] * placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 7, 52, 52, 3):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 2, 26])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 13, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 3, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[512, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[15:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 2), "float32"], placeholder_1: T.Buffer[(7, 256, 1, 1, 2, 3), "float32"], placeholder_2: T.Buffer[(1, 7, 1, 1, 3), "float32"], T_add: T.Buffer[(1, 7, 52, 52, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 7, 52, 52, 3], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 4, 1, 1, 1, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(512, 1, 1, 1, 7, 2, 13, 3, 1, 1, 1, 1, 1, 26, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(7, i1_2)
                        oh = T.axis.spatial(52, i2_2 * 26 + i2_3)
                        ow = T.axis.spatial(52, i3_0 * 13 + i3_2)
                        oc_block, ic = T.axis.remap("SR", [i4_2, i5_0])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2], placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 2], "float32"], ["TENSOR", [7, 256, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2] * placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 7, 52, 13, 3):
                    with T.block("T_add"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(52, i3_0 * 13 + ax3)
                        ax4_1 = T.axis.spatial(3, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 2, 26])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 13, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 3, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[512, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 2), "float32"], placeholder_1: T.Buffer[(7, 256, 1, 1, 2, 3), "float32"], placeholder_2: T.Buffer[(1, 7, 1, 1, 3), "float32"], T_add: T.Buffer[(1, 7, 52, 52, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 7, 52, 52, 3], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 4, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 512, 1, 1, 1, 7, 2, 13, 3, 1, 1, 1, 1, 1, 26, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(7, i1_2)
                        oh = T.axis.spatial(52, i2_2 * 26 + i2_3)
                        ow = T.axis.spatial(52, i3_0 * 13 + i3_2)
                        oc_block, ic = T.axis.remap("SR", [i4_2, i5_0])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2], placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 2], "float32"], ["TENSOR", [7, 256, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2] * placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 7, 52, 13, 3):
                    with T.block("T_add"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(52, i3_0 * 13 + ax3)
                        ax4_1 = T.axis.spatial(3, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 2, 26])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 13, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 3, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[512, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #32: "fused_layout_transform_image_resize2d"
[15:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 7, 52, 52, 3), "float32"], resize: T.Buffer[(1, 21, 416, 416), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 21, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 21, 52, 52):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 3, ax2, ax3, ax1 % 3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 21 and ax2 < 52 and ax3 < 52, placeholder[ax0, ax1 // 3, ax2, ax3, ax1 % 3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 21, 416, 416):
            with T.block("resize"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_layout_trans[i0_1, i1_1, 0 : 52, 0 : 52])
                T.writes(resize[i0_1, i1_1, i2_1, i3_1])
                resize[i0_1, i1_1, i2_1, i3_1] = (T_layout_trans[i0_1, i1_1, T.max(T.min(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), 51), 0), T.max(T.min(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), 51), 0)] * (T.float32(1) - ((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))) + T_layout_trans[i0_1, i1_1, T.max(T.min(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), 51), 0), T.max(T.min(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32") + 1, 51), 0)] * ((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))) * (T.float32(1) - ((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))) + (T_layout_trans[i0_1, i1_1, T.max(T.min(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32") + 1, 51), 0), T.max(T.min(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), 51), 0)] * (T.float32(1) - ((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))) + T_layout_trans[i0_1, i1_1, T.max(T.min(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32") + 1, 51), 0), T.max(T.min(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32") + 1, 51), 0)] * ((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))) * ((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))
    

[15:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[15:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 7, 52, 52, 3), "float32"], resize: T.Buffer[(1, 21, 416, 416), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_layout_trans = T.alloc_buffer([1, 21, 52, 52], dtype="float32")
            for i0, i1 in T.grid(1, 21):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 52, 52):
                    with T.block("T_layout_trans"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(21, i1 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        T.reads(placeholder[ax0_1, ax1_1 // 3, ax2_1, ax3_1, ax1_1 % 3])
                        T.writes(T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1])
                        T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 21 and ax2_1 < 52 and ax3_1 < 52, placeholder[ax0_1, ax1_1 // 3, ax2_1, ax3_1, ax1_1 % 3], T.float32(0), dtype="float32")
                for i2, i3 in T.grid(416, 416):
                    with T.block("resize"):
                        i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(T_layout_trans[i0_1, i1_1, 0 : 52, 0 : 52])
                        T.writes(resize[i0_1, i1_1, i2_1, i3_1])
                        resize[i0_1, i1_1, i2_1, i3_1] = (T_layout_trans[i0_1, i1_1, T.max(T.min(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), 51), 0), T.max(T.min(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), 51), 0)] * (T.float32(1) - ((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))) + T_layout_trans[i0_1, i1_1, T.max(T.min(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), 51), 0), T.max(T.min(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32") + 1, 51), 0)] * ((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))) * (T.float32(1) - ((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))) + (T_layout_trans[i0_1, i1_1, T.max(T.min(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32") + 1, 51), 0), T.max(T.min(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), 51), 0)] * (T.float32(1) - ((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))) + T_layout_trans[i0_1, i1_1, T.max(T.min(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32") + 1, 51), 0), T.max(T.min(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32") + 1, 51), 0)] * ((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))) * ((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[15:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
 ID |                                            Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |       fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 12760457216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  1 |                          fused_layout_transform |           1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |               fused_nn_contrib_conv2d_NCHWc_add |    29130192 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |             fused_nn_contrib_conv2d_NCHWc_add_1 | 11346935808 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |             fused_nn_contrib_conv2d_NCHWc_add_2 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |             fused_nn_contrib_conv2d_NCHWc_add_3 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |             fused_nn_contrib_conv2d_NCHWc_add_4 |   357187584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                        fused_layout_transform_1 |           1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |   819593216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                             fused_nn_max_pool2d |     6230016 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |    89989120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |   355803136 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |   798826496 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |   362725376 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |   798134272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   355110912 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   798134272 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |   358572032 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |  3191152640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |  1419059200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 22 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |  3191152640 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |  1425981440 |      6 |            N/A |          N/A |                   N/A |      0 |            
 24 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 | 12761841664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  5673467904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 | 12761841664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |  5687312384 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 51039059968 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_layout_transform_2 |           1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_contrib_conv2d_NCHWc_add_5 |    58203600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |           fused_layout_transform_image_resize2d |   196245504 |      2 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[15:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"
[15:58:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[15:58:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[15:59:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cbad6698)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cb4b5d28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cb4b5008)]: 0 failure(s)
[15:59:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[15:59:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cbad6698)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cb4b5d28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cb4b5008)]: 0 failure(s)
[16:00:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cbad6698)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cb4b5d28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cb4b5008)]: 0 failure(s)
[16:01:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cbad6698)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cb4b5d28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cb4b5008)]: 0 failure(s)
[16:01:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cbad6698)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cb4b5d28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cb4b5008)]: 0 failure(s)
[16:01:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9997  0.9997  0.9996  0.9995  0.9994  0.9991  0.9991  0.9988  0.9988  0.9986  0.9986  0.9985  0.9985  0.9982  0.9981
[17 : 32]:	0.9981  0.9980  0.9979  0.9979  0.9976  0.9976  0.9975  0.9974  0.9964  0.9964  0.9963  0.9961  0.9960  0.9960  0.9958  0.9958
[16:01:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:01:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:01:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:02:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:03:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_layout_transform"
[16:03:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:03:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:04:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cb5d0fd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6ccfd8718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cc26acf8)]: 0 failure(s)
[16:04:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:04:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cb5d0fd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6ccfd8718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cc26acf8)]: 0 failure(s)
[16:04:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cb5d0fd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6ccfd8718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cc26acf8)]: 0 failure(s)
[16:04:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cb5d0fd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6ccfd8718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cc26acf8)]: 0 failure(s)
[16:04:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cb5d0fd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6ccfd8718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cc26acf8)]: 0 failure(s)
[16:04:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 8 candidates:
[1 : 8]:	0.9515  0.9378  0.8216  0.4479  0.4420  0.3002  0.1246  0.0400
[16:04:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 8 candidate(s) with evolutionary search
[16:04:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 8 candidates(s) for measurement
[16:04:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 8 sample(s) to builder
[16:04:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 8 sample(s) to runner
[16:04:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_contrib_conv2d_NCHWc_add"
[16:04:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:04:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:04:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cd46a028)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6ccb5cf38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cc7121d8)]: 0 failure(s)
[16:04:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:04:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cd46a028)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6ccb5cf38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cc7121d8)]: 0 failure(s)
[16:04:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cd46a028)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6ccb5cf38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cc7121d8)]: 0 failure(s)
[16:05:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cd46a028)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6ccb5cf38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cc7121d8)]: 0 failure(s)
[16:05:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cd46a028)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6ccb5cf38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cc7121d8)]: 0 failure(s)
[16:05:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9998  0.9997  0.9997  0.9996  0.9996  0.9995  0.9993  0.9992  0.9991  0.9990  0.9990  0.9988  0.9987  0.9987  0.9987
[17 : 32]:	0.9986  0.9984  0.9984  0.9983  0.9982  0.9980  0.9979  0.9979  0.9978  0.9977  0.9976  0.9975  0.9973  0.9973  0.9972  0.9971
[16:05:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:05:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:05:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:05:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:05:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"
[16:05:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:05:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:06:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6ccfed868)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6ca805ac8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cb727538)]: 0 failure(s)
[16:06:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:06:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6ccfed868)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6ca805ac8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cb727538)]: 0 failure(s)
[16:06:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6ccfed868)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6ca805ac8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cb727538)]: 0 failure(s)
[16:06:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6ccfed868)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6ca805ac8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cb727538)]: 0 failure(s)
[16:06:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6ccfed868)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6ca805ac8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cb727538)]: 0 failure(s)
[16:06:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9997  0.9993  0.9989  0.9989  0.9987  0.9987  0.9985  0.9985  0.9984  0.9978  0.9978  0.9976  0.9976  0.9976
[17 : 32]:	0.9975  0.9974  0.9973  0.9972  0.9972  0.9971  0.9969  0.9969  0.9968  0.9967  0.9967  0.9966  0.9965  0.9958  0.9958  0.9957
[16:06:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:06:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:06:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:07:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:09:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"
[16:09:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:09:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:09:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6ce64dcd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cb77a0f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cb381e88)]: 0 failure(s)
[16:09:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:09:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6ce64dcd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cb77a0f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cb381e88)]: 0 failure(s)
[16:09:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6ce64dcd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cb77a0f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cb381e88)]: 0 failure(s)
[16:10:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6ce64dcd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cb77a0f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cb381e88)]: 0 failure(s)
[16:10:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6ce64dcd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cb77a0f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6cb381e88)]: 0 failure(s)
[16:10:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9996  0.9996  0.9996  0.9993  0.9993  0.9993  0.9992  0.9992  0.9991  0.9990  0.9987  0.9987  0.9987  0.9987  0.9986
[17 : 32]:	0.9986  0.9986  0.9986  0.9985  0.9985  0.9984  0.9983  0.9983  0.9981  0.9980  0.9979  0.9979  0.9978  0.9978  0.9978  0.9977
[16:10:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:10:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:10:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:11:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:11:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"
[16:11:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:11:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:12:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cbaae708)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6ccb7e7c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6ce649818)]: 0 failure(s)
[16:12:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:12:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cbaae708)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6ccb7e7c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6ce649818)]: 0 failure(s)
[16:12:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cbaae708)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6ccb7e7c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6ce649818)]: 0 failure(s)
[16:13:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cbaae708)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6ccb7e7c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6ce649818)]: 0 failure(s)
[16:13:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cbaae708)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6ccb7e7c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6ce649818)]: 0 failure(s)
[16:13:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9998  0.9994  0.9992  0.9991  0.9990  0.9990  0.9988  0.9988  0.9987  0.9984  0.9983  0.9981  0.9980  0.9979
[17 : 32]:	0.9977  0.9977  0.9976  0.9975  0.9973  0.9972  0.9971  0.9969  0.9966  0.9965  0.9964  0.9960  0.9960  0.9959  0.9959  0.9958
[16:13:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:13:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:13:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:14:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:14:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"
[16:14:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:14:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:14:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cb1210b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cb7349f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6d084bc78)]: 0 failure(s)
[16:14:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:14:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cb1210b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cb7349f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6d084bc78)]: 0 failure(s)
[16:15:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cb1210b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cb7349f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6d084bc78)]: 0 failure(s)
[16:15:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cb1210b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cb7349f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6d084bc78)]: 0 failure(s)
[16:15:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cb1210b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cb7349f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6d084bc78)]: 0 failure(s)
[16:15:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9997  0.9997  0.9996  0.9995  0.9994  0.9993  0.9993  0.9992  0.9992  0.9991  0.9985  0.9983  0.9982  0.9982  0.9981
[17 : 32]:	0.9980  0.9980  0.9978  0.9977  0.9977  0.9976  0.9976  0.9974  0.9973  0.9973  0.9973  0.9973  0.9972  0.9971  0.9968  0.9966
[16:15:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:15:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:15:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:16:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:16:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_layout_transform_1"
[16:16:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:16:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:16:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cb5d5688)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6c9f26eb8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6ca7fa6c8)]: 0 failure(s)
[16:16:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:16:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cb5d5688)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6c9f26eb8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6ca7fa6c8)]: 0 failure(s)
[16:16:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cb5d5688)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6c9f26eb8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6ca7fa6c8)]: 0 failure(s)
[16:16:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cb5d5688)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6c9f26eb8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6ca7fa6c8)]: 0 failure(s)
[16:16:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cb5d5688)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6c9f26eb8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6ca7fa6c8)]: 0 failure(s)
[16:16:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 4 candidates:
[1 : 4]:	0.7847  0.7129  0.7075  0.6624
[16:16:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 4 candidate(s) with evolutionary search
[16:16:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 4 candidates(s) for measurement
[16:16:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 4 sample(s) to builder
[16:16:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 4 sample(s) to runner
[16:16:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"
[16:16:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:16:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:17:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cb780138)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cc6f77f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6d07e2c98)]: 0 failure(s)
[16:17:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:18:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cb780138)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cc6f77f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6d07e2c98)]: 0 failure(s)
[16:18:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cb780138)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cc6f77f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6d07e2c98)]: 0 failure(s)
[16:19:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cb780138)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cc6f77f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6d07e2c98)]: 0 failure(s)
[16:20:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cb780138)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cc6f77f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6d07e2c98)]: 0 failure(s)
[16:20:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9996  0.9995  0.9995  0.9993  0.9993  0.9993  0.9992  0.9989  0.9986  0.9985  0.9981  0.9980  0.9980  0.9978
[17 : 32]:	0.9978  0.9978  0.9975  0.9971  0.9970  0.9968  0.9968  0.9967  0.9966  0.9965  0.9965  0.9964  0.9964  0.9964  0.9961  0.9960
[16:20:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:20:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:20:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:20:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:21:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #9: "fused_nn_max_pool2d"
[16:21:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:21:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:21:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cbac7368)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cc6ecc58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6ce6513e8)]: 0 failure(s)
[16:21:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:22:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cbac7368)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cc6ecc58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6ce6513e8)]: 0 failure(s)
[16:22:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cbac7368)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cc6ecc58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6ce6513e8)]: 0 failure(s)
[16:23:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cbac7368)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cc6ecc58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6ce6513e8)]: 0 failure(s)
[16:23:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55f6cbac7368)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55f6cc6ecc58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55f6ce6513e8)]: 0 failure(s)
[16:23:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9953  0.9929  0.9908  0.9861  0.9828  0.9775  0.9750  0.9730  0.9705  0.9701  0.9670  0.9620  0.9579  0.9515  0.9514  0.9502
[17 : 32]:	0.9447  0.9403  0.9402  0.9345  0.9309  0.9260  0.9201  0.9192  0.9020  0.9016  0.9003  0.8932  0.8929  0.8885  0.8857  0.8845
[16:23:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:23:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:23:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:23:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #0: GFLOPs: 32.8066. Time: 388.9598 ms. Best GFLOPs: 32.8066
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #1: GFLOPs: 15.1740. Time: 840.9405 ms. Best GFLOPs: 32.8066
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #2: GFLOPs: 29.2324. Time: 436.5183 ms. Best GFLOPs: 32.8066
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #3: GFLOPs: 18.8427. Time: 677.2095 ms. Best GFLOPs: 32.8066
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #4: GFLOPs: 16.9116. Time: 754.5394 ms. Best GFLOPs: 32.8066
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #5: GFLOPs: 10.8643. Time: 1174.5270 ms. Best GFLOPs: 32.8066
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #6: GFLOPs: 13.7075. Time: 930.9127 ms. Best GFLOPs: 32.8066
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #7: GFLOPs: 115.7276. Time: 110.2629 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #8: GFLOPs: 13.1903. Time: 967.4138 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #9: GFLOPs: 6.9470. Time: 1836.8354 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #10: GFLOPs: 23.3424. Time: 546.6634 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #11: GFLOPs: 15.2674. Time: 835.7986 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #12: GFLOPs: 13.1776. Time: 968.3448 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #13: GFLOPs: 19.1991. Time: 664.6394 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #14: GFLOPs: 8.4104. Time: 1517.2160 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #15: GFLOPs: 39.1058. Time: 326.3058 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #16: GFLOPs: 27.4527. Time: 464.8158 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #17: GFLOPs: 62.1489. Time: 205.3207 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #18: GFLOPs: 35.3161. Time: 361.3208 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #19: GFLOPs: 25.9067. Time: 492.5549 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #20: GFLOPs: 34.6124. Time: 368.6671 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #21: GFLOPs: 45.3587. Time: 281.3235 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #22: GFLOPs: 6.4778. Time: 1969.8672 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #23: GFLOPs: 29.8685. Time: 427.2214 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #24: GFLOPs: 24.9617. Time: 511.2008 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #25: GFLOPs: 34.6758. Time: 367.9933 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #26: GFLOPs: 21.3709. Time: 597.0944 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #27: GFLOPs: 16.8658. Time: 756.5881 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #28: GFLOPs: 32.6161. Time: 391.2316 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #29: GFLOPs: 18.0187. Time: 708.1789 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #30: GFLOPs: 47.3997. Time: 269.2099 ms. Best GFLOPs: 115.7276
[16:24:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #31: GFLOPs: 44.6602. Time: 285.7235 ms. Best GFLOPs: 115.7276
[16:24:22] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"
 ID |                                            Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |       fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 12760457216 |      1 |       115.7276 |  110262.8867 |           110262.8867 |     32 |            
  1 |                          fused_layout_transform |           1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |               fused_nn_contrib_conv2d_NCHWc_add |    29130192 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |             fused_nn_contrib_conv2d_NCHWc_add_1 | 11346935808 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |             fused_nn_contrib_conv2d_NCHWc_add_2 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |             fused_nn_contrib_conv2d_NCHWc_add_3 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |             fused_nn_contrib_conv2d_NCHWc_add_4 |   357187584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                        fused_layout_transform_1 |           1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |   819593216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                             fused_nn_max_pool2d |     6230016 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |    89989120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |   355803136 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |   798826496 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |   362725376 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |   798134272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   355110912 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   798134272 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |   358572032 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |  3191152640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |  1419059200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 22 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |  3191152640 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |  1425981440 |      6 |            N/A |          N/A |                   N/A |      0 |            
 24 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 | 12761841664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  5673467904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 | 12761841664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |  5687312384 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 51039059968 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_layout_transform_2 |           1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_contrib_conv2d_NCHWc_add_5 |    58203600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |           fused_layout_transform_image_resize2d |   196245504 |      2 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 32
Total latency (us): 110263

[16:24:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #0 has finished. Remaining task(s): 32
[16:24:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_layout_transform"] Trial #0: GFLOPs: 0.0000. Time: 5.3331 ms. Best GFLOPs: 0.0000
[16:24:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_layout_transform"] Trial #1: GFLOPs: 0.0000. Time: 4.7099 ms. Best GFLOPs: 0.0000
[16:24:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_layout_transform"] Trial #2: GFLOPs: 0.0000. Time: 3.2731 ms. Best GFLOPs: 0.0000
[16:24:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_layout_transform"] Trial #3: GFLOPs: 0.0000. Time: 4.4615 ms. Best GFLOPs: 0.0000
[16:24:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_layout_transform"] Trial #4: GFLOPs: 0.0000. Time: 4.4317 ms. Best GFLOPs: 0.0000
[16:24:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_layout_transform"] Trial #5: GFLOPs: 0.0000. Time: 5.7135 ms. Best GFLOPs: 0.0000
[16:24:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_layout_transform"] Trial #6: GFLOPs: 0.0000. Time: 4.2955 ms. Best GFLOPs: 0.0000
[16:24:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_layout_transform"] Trial #7: GFLOPs: 0.0000. Time: 2.3371 ms. Best GFLOPs: 0.0000
[16:24:22] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_layout_transform"
 ID |                                            Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |       fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 12760457216 |      1 |       115.7276 |  110262.8867 |           110262.8867 |     32 |          Y 
  1 |                          fused_layout_transform |           1 |      1 |         0.0000 |    2337.0640 |             2337.0640 |      8 |            
  2 |               fused_nn_contrib_conv2d_NCHWc_add |    29130192 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |             fused_nn_contrib_conv2d_NCHWc_add_1 | 11346935808 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |             fused_nn_contrib_conv2d_NCHWc_add_2 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |             fused_nn_contrib_conv2d_NCHWc_add_3 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |             fused_nn_contrib_conv2d_NCHWc_add_4 |   357187584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                        fused_layout_transform_1 |           1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |   819593216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                             fused_nn_max_pool2d |     6230016 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |    89989120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |   355803136 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |   798826496 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |   362725376 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |   798134272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   355110912 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   798134272 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |   358572032 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |  3191152640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |  1419059200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 22 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |  3191152640 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |  1425981440 |      6 |            N/A |          N/A |                   N/A |      0 |            
 24 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 | 12761841664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  5673467904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 | 12761841664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |  5687312384 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 51039059968 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_layout_transform_2 |           1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_contrib_conv2d_NCHWc_add_5 |    58203600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |           fused_layout_transform_image_resize2d |   196245504 |      2 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 40
Total latency (us): 112600

[16:24:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #1 has finished. Remaining task(s): 31
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #0: GFLOPs: 2.0409. Time: 14.2734 ms. Best GFLOPs: 2.0409
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #1: GFLOPs: 6.7164. Time: 4.3372 ms. Best GFLOPs: 6.7164
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #2: GFLOPs: 6.8293. Time: 4.2655 ms. Best GFLOPs: 6.8293
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #3: GFLOPs: 4.6239. Time: 6.2999 ms. Best GFLOPs: 6.8293
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #4: GFLOPs: 5.0466. Time: 5.7722 ms. Best GFLOPs: 6.8293
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #5: GFLOPs: 5.3562. Time: 5.4386 ms. Best GFLOPs: 6.8293
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #6: GFLOPs: 3.8070. Time: 7.6518 ms. Best GFLOPs: 6.8293
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #7: GFLOPs: 2.1423. Time: 13.5979 ms. Best GFLOPs: 6.8293
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #8: GFLOPs: 3.8018. Time: 7.6623 ms. Best GFLOPs: 6.8293
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #9: GFLOPs: 5.8260. Time: 5.0000 ms. Best GFLOPs: 6.8293
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #10: GFLOPs: 5.1605. Time: 5.6448 ms. Best GFLOPs: 6.8293
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #11: GFLOPs: 5.1980. Time: 5.6041 ms. Best GFLOPs: 6.8293
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #12: GFLOPs: 5.1096. Time: 5.7011 ms. Best GFLOPs: 6.8293
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #13: GFLOPs: 3.0950. Time: 9.4119 ms. Best GFLOPs: 6.8293
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #14: GFLOPs: 4.4991. Time: 6.4747 ms. Best GFLOPs: 6.8293
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #15: GFLOPs: 6.9157. Time: 4.2122 ms. Best GFLOPs: 6.9157
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #16: GFLOPs: 5.9595. Time: 4.8880 ms. Best GFLOPs: 6.9157
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #17: GFLOPs: 2.8110. Time: 10.3630 ms. Best GFLOPs: 6.9157
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #18: GFLOPs: 2.7635. Time: 10.5411 ms. Best GFLOPs: 6.9157
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #19: GFLOPs: 2.8464. Time: 10.2340 ms. Best GFLOPs: 6.9157
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #20: GFLOPs: 2.8993. Time: 10.0474 ms. Best GFLOPs: 6.9157
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #21: GFLOPs: 4.6104. Time: 6.3184 ms. Best GFLOPs: 6.9157
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #22: GFLOPs: 3.1223. Time: 9.3298 ms. Best GFLOPs: 6.9157
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #23: GFLOPs: 5.8268. Time: 4.9994 ms. Best GFLOPs: 6.9157
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #24: GFLOPs: 5.9268. Time: 4.9150 ms. Best GFLOPs: 6.9157
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #25: GFLOPs: 2.9775. Time: 9.7835 ms. Best GFLOPs: 6.9157
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #26: GFLOPs: 5.3289. Time: 5.4664 ms. Best GFLOPs: 6.9157
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #27: GFLOPs: 3.5693. Time: 8.1613 ms. Best GFLOPs: 6.9157
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #28: GFLOPs: 8.5206. Time: 3.4188 ms. Best GFLOPs: 8.5206
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #29: GFLOPs: 7.6880. Time: 3.7891 ms. Best GFLOPs: 8.5206
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #30: GFLOPs: 20.1363. Time: 1.4466 ms. Best GFLOPs: 20.1363
[16:24:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #31: GFLOPs: 18.1770. Time: 1.6026 ms. Best GFLOPs: 20.1363
[16:24:24] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_contrib_conv2d_NCHWc_add"
 ID |                                            Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |       fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 12760457216 |      1 |       115.7276 |  110262.8867 |           110262.8867 |     32 |          Y 
  1 |                          fused_layout_transform |           1 |      1 |         0.0000 |    2337.0640 |             2337.0640 |      8 |          Y 
  2 |               fused_nn_contrib_conv2d_NCHWc_add |    29130192 |      1 |        20.1363 |    1446.6491 |             1446.6491 |     32 |            
  3 |             fused_nn_contrib_conv2d_NCHWc_add_1 | 11346935808 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |             fused_nn_contrib_conv2d_NCHWc_add_2 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |             fused_nn_contrib_conv2d_NCHWc_add_3 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |             fused_nn_contrib_conv2d_NCHWc_add_4 |   357187584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                        fused_layout_transform_1 |           1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |   819593216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                             fused_nn_max_pool2d |     6230016 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |    89989120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |   355803136 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |   798826496 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |   362725376 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |   798134272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   355110912 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   798134272 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |   358572032 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |  3191152640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |  1419059200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 22 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |  3191152640 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |  1425981440 |      6 |            N/A |          N/A |                   N/A |      0 |            
 24 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 | 12761841664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  5673467904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 | 12761841664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |  5687312384 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 51039059968 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_layout_transform_2 |           1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_contrib_conv2d_NCHWc_add_5 |    58203600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |           fused_layout_transform_image_resize2d |   196245504 |      2 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 72
Total latency (us): 114047

[16:24:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #2 has finished. Remaining task(s): 30
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #0: GFLOPs: 27.9288. Time: 406.2805 ms. Best GFLOPs: 27.9288
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #1: GFLOPs: 11.3410. Time: 1000.5235 ms. Best GFLOPs: 27.9288
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #2: GFLOPs: 31.1458. Time: 364.3164 ms. Best GFLOPs: 31.1458
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #3: GFLOPs: 104.1450. Time: 108.9532 ms. Best GFLOPs: 104.1450
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #4: GFLOPs: 16.5640. Time: 685.0380 ms. Best GFLOPs: 104.1450
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #5: GFLOPs: 9.0588. Time: 1252.5842 ms. Best GFLOPs: 104.1450
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 512, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 512, 52, 52, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(13, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 64, 1, 1, 4):
                for i1_2_init, i2_2_init, i3_2_init, i2_3_init in T.grid(8, 2, 52, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(512, i1_1 * 8 + i1_2_init)
                        oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 4 + i2_2_init * 2 + i2_3_init)
                        ow, oc_block = T.axis.remap("SS", [i3_2_init, i4_1])
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [512, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1024, 1, 1, 1, 8, 2, 52, 1, 1, 1, 1, 1, 1, 2, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(512, i1_1 * 8 + i1_2)
                        oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 4 + i2_2 * 2 + i2_3)
                        ow, oc_block, ic = T.axis.remap("SSR", [i3_2, i4_1, i5_0])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [512, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 512, 4, 52):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(512, ax1)
                        ax2_1 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 4 + ax2)
                        ax3_1, ax4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 64, 8, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[13, 1, 2, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 52, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[1024, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l99)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b101)
b124 = sch.decompose_reduction(block=b101, loop=l108)
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 512, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 512, 52, 52, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 4, 1, 1):
                for i1_2_init, i2_2_init, i3_2_init, i1_3_init, i3_3_init in T.grid(4, 13, 2, 16, 26):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 64 + i1_2_init * 16 + i1_3_init)
                        oh = T.axis.spatial(52, i2_1 * 13 + i2_2_init)
                        ow = T.axis.spatial(52, i3_2_init * 26 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [512, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 4, 13, 2, 1, 32, 1, 1, 1, 16, 1, 26, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 64 + i1_2 * 16 + i1_3)
                        oh = T.axis.spatial(52, i2_1 * 13 + i2_2)
                        ow = T.axis.spatial(52, i3_2 * 26 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                        ic = T.axis.reduce(1024, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [512, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 52, 52, 1):
                with T.block("T_add"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 64 + ax1)
                    ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                    ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                    T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                    T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[8, 1, 4, 16])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 13, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 26])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b100 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b100)
b123 = sch.decompose_reduction(block=b100, loop=l107)
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #8: GFLOPs: 23.4637. Time: 483.5960 ms. Best GFLOPs: 104.1450
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #9: GFLOPs: 38.0173. Time: 298.4680 ms. Best GFLOPs: 104.1450
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #10: GFLOPs: 5.4934. Time: 2065.5504 ms. Best GFLOPs: 104.1450
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #11: GFLOPs: 13.3521. Time: 849.8244 ms. Best GFLOPs: 104.1450
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #12: GFLOPs: 14.1107. Time: 804.1367 ms. Best GFLOPs: 104.1450
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #13: GFLOPs: 32.3384. Time: 350.8812 ms. Best GFLOPs: 104.1450
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #14: GFLOPs: 11.7025. Time: 969.6183 ms. Best GFLOPs: 104.1450
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #15: GFLOPs: 34.5992. Time: 327.9535 ms. Best GFLOPs: 104.1450
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #16: GFLOPs: 104.0511. Time: 109.0516 ms. Best GFLOPs: 104.1450
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #17: GFLOPs: 28.8395. Time: 393.4512 ms. Best GFLOPs: 104.1450
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #18: GFLOPs: 14.7165. Time: 771.0353 ms. Best GFLOPs: 104.1450
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #19: GFLOPs: 66.8618. Time: 169.7073 ms. Best GFLOPs: 104.1450
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #20: GFLOPs: 78.5407. Time: 144.4721 ms. Best GFLOPs: 104.1450
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #21: GFLOPs: 61.8167. Time: 183.5578 ms. Best GFLOPs: 104.1450
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #22: GFLOPs: 58.6793. Time: 193.3721 ms. Best GFLOPs: 104.1450
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #23: GFLOPs: 16.4987. Time: 687.7466 ms. Best GFLOPs: 104.1450
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #24: GFLOPs: 20.5010. Time: 553.4816 ms. Best GFLOPs: 104.1450
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #25: GFLOPs: 119.9731. Time: 94.5790 ms. Best GFLOPs: 119.9731
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #26: GFLOPs: 63.4259. Time: 178.9006 ms. Best GFLOPs: 119.9731
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #27: GFLOPs: 27.7051. Time: 409.5613 ms. Best GFLOPs: 119.9731
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #28: GFLOPs: 4.8125. Time: 2357.8059 ms. Best GFLOPs: 119.9731
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #29: GFLOPs: 17.5237. Time: 647.5212 ms. Best GFLOPs: 119.9731
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #30: GFLOPs: 103.4835. Time: 109.6497 ms. Best GFLOPs: 119.9731
[16:24:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #31: GFLOPs: 24.7674. Time: 458.1391 ms. Best GFLOPs: 119.9731
[16:24:25] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_contrib_conv2d_NCHWc_add_1"
 ID |                                            Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |       fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 12760457216 |      1 |       115.7276 |  110262.8867 |           110262.8867 |     32 |          Y 
  1 |                          fused_layout_transform |           1 |      1 |         0.0000 |    2337.0640 |             2337.0640 |      8 |          Y 
  2 |               fused_nn_contrib_conv2d_NCHWc_add |    29130192 |      1 |        20.1363 |    1446.6491 |             1446.6491 |     32 |          Y 
  3 |             fused_nn_contrib_conv2d_NCHWc_add_1 | 11346935808 |      1 |       119.9731 |   94578.9817 |            94578.9817 |     32 |            
  4 |             fused_nn_contrib_conv2d_NCHWc_add_2 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |             fused_nn_contrib_conv2d_NCHWc_add_3 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |             fused_nn_contrib_conv2d_NCHWc_add_4 |   357187584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                        fused_layout_transform_1 |           1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |   819593216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                             fused_nn_max_pool2d |     6230016 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |    89989120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |   355803136 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |   798826496 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |   362725376 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |   798134272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   355110912 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   798134272 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |   358572032 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |  3191152640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |  1419059200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 22 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |  3191152640 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |  1425981440 |      6 |            N/A |          N/A |                   N/A |      0 |            
 24 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 | 12761841664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  5673467904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 | 12761841664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |  5687312384 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 51039059968 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_layout_transform_2 |           1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_contrib_conv2d_NCHWc_add_5 |    58203600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |           fused_layout_transform_image_resize2d |   196245504 |      2 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 104
Total latency (us): 208626

[16:24:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #3 has finished. Remaining task(s): 29
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #0: GFLOPs: 28.4907. Time: 99.6155 ms. Best GFLOPs: 28.4907
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #1: GFLOPs: 16.8965. Time: 167.9711 ms. Best GFLOPs: 28.4907
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #2: GFLOPs: 45.9288. Time: 61.7938 ms. Best GFLOPs: 45.9288
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #3: GFLOPs: 32.8746. Time: 86.3317 ms. Best GFLOPs: 45.9288
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #4: GFLOPs: 32.4013. Time: 87.5927 ms. Best GFLOPs: 45.9288
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #5: GFLOPs: 22.1640. Time: 128.0506 ms. Best GFLOPs: 45.9288
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #6: GFLOPs: 29.6900. Time: 95.5916 ms. Best GFLOPs: 45.9288
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 256, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 52, 52, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(52, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i1_3_init, i3_3_init in T.grid(4, 26, 64, 2):
                for i4_3_fused_init in T.vectorized(4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i1_2_init * 64 + i1_3_init)
                        oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 26 * 26 + i2_2_init)
                        ow = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 26 * 2 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(128, 1, 1, 1, 4, 26, 1, 1, 4, 1, 1, 1, 64, 1, 2):
                for i4_3_fused in T.vectorized(4):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i1_2 * 64 + i1_3)
                        oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 26 * 26 + i2_2)
                        ow = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 26 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3_fused)
                        ic = T.axis.reduce(512, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 256, 26):
                for ax3_ax4_fused in T.vectorized(8):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(256, ax1)
                        ax2_1 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 26 * 26 + ax2)
                        ax3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 26 * 2 + ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 4, 64])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 26, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 26, 1, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[128, 4])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b66)
l101 = sch.fuse(l99, l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l104)
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #8: GFLOPs: 38.4799. Time: 73.7558 ms. Best GFLOPs: 45.9288
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #9: GFLOPs: 9.0230. Time: 314.5424 ms. Best GFLOPs: 45.9288
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #10: GFLOPs: 60.3867. Time: 46.9991 ms. Best GFLOPs: 60.3867
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #11: GFLOPs: 34.4428. Time: 82.4008 ms. Best GFLOPs: 60.3867
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #12: GFLOPs: 14.9639. Time: 189.6640 ms. Best GFLOPs: 60.3867
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #13: GFLOPs: 2.0519. Time: 1383.1497 ms. Best GFLOPs: 60.3867
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #14: GFLOPs: 70.7342. Time: 40.1237 ms. Best GFLOPs: 70.7342
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #15: GFLOPs: 4.9045. Time: 578.6784 ms. Best GFLOPs: 70.7342
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #16: GFLOPs: 49.9947. Time: 56.7684 ms. Best GFLOPs: 70.7342
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #17: GFLOPs: 23.6134. Time: 120.1911 ms. Best GFLOPs: 70.7342
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #18: GFLOPs: 14.7801. Time: 192.0227 ms. Best GFLOPs: 70.7342
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #19: GFLOPs: 16.3896. Time: 173.1654 ms. Best GFLOPs: 70.7342
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 256, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 52, 52, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_1, i3_1, i4_1 in T.grid(1, 4, 1):
                for i1_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(2, 4, 26, 13):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 128 * 128 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 64 * 2 + i1_2_init)
                        oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 128 // 64 * 26 + i2_3_init)
                        ow = T.axis.spatial(52, i3_1 * 13 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 1, 1, 1, 2, 1, 1, 4, 4, 1, 1, 1, 1, 26, 13, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 128 * 128 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 64 * 2 + i1_2)
                        oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 128 // 64 * 26 + i2_3)
                        ow = T.axis.spatial(52, i3_1 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(512, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2, i3 in T.grid(52, 52):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 64, 2, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 26])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 1, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[128, 4])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72)
sch.parallel(loop=l92)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l93, l94, l95, l96, l97 = sch.get_loops(block=b65)
l98 = sch.fuse(l93, l94)
sch.parallel(loop=l98)
l99 = sch.fuse(l97)
sch.vectorize(loop=l99)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b100 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b100)
b121 = sch.decompose_reduction(block=b100, loop=l105)
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #21: GFLOPs: 16.1038. Time: 176.2386 ms. Best GFLOPs: 70.7342
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #22: GFLOPs: 32.5357. Time: 87.2310 ms. Best GFLOPs: 70.7342
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #23: GFLOPs: 32.5656. Time: 87.1509 ms. Best GFLOPs: 70.7342
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #24: GFLOPs: 37.1560. Time: 76.3839 ms. Best GFLOPs: 70.7342
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #25: GFLOPs: 18.1976. Time: 155.9611 ms. Best GFLOPs: 70.7342
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 256, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 52, 52, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(8, 2, 2, 2, 13, 13):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 32 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 16 + i1_2_init * 2 + i1_3_init)
                    oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 26 + i2_2_init * 13 + i2_3_init)
                    ow = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 16 * 26 + i3_2_init * 13 + i3_3_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 8, 2, 2, 1, 8, 1, 1, 1, 2, 13, 13, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 32 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 16 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 26 + i2_2 * 13 + i2_3)
                    ow = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 16 * 26 + i3_2 * 13 + i3_3)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    ic = T.axis.reduce(512, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2, i3 in T.grid(52, 52):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[8, 2, 8, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 2, 13])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l92)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l93, l94, l95, l96, l97 = sch.get_loops(block=b65)
l98 = sch.fuse(l93, l94)
sch.parallel(loop=l98)
l99 = sch.fuse(l97)
sch.vectorize(loop=l99)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b100 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b100)
b118 = sch.decompose_reduction(block=b100, loop=l102)
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #27: GFLOPs: 6.5699. Time: 431.9888 ms. Best GFLOPs: 70.7342
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #28: GFLOPs: 34.5985. Time: 82.0302 ms. Best GFLOPs: 70.7342
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 256, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 52, 52, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(338, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0 in T.serial(1):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 16, 2, 2, 4):
                    for i1_2_init, i1_3_init, i3_3_init in T.grid(8, 2, 2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(256, i1_1 * 16 + i1_2_init * 2 + i1_3_init)
                            oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 13 * 2 + i2_1)
                            ow = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i3_1 * 2 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_1)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 8, 1, 1, 1, 8, 1, 1, 1, 2, 1, 2, 1):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(256, i1_1 * 16 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 13 * 2 + i2_1)
                            ow = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i3_1 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_1)
                            ic = T.axis.reduce(512, i5_0 * 8 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2 in T.grid(1, 256, 2):
                    for ax3_ax4_fused in T.vectorized(16):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(256, ax1)
                            ax2_1 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 13 * 2 + ax2)
                            ax3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + ax3_ax4_fused // 4)
                            ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 16, 8, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[26, 2, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[13, 2, 1, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b66)
l101 = sch.fuse(l99, l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b102)
b126 = sch.decompose_reduction(block=b102, loop=l110)
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #30: GFLOPs: 13.6492. Time: 207.9323 ms. Best GFLOPs: 70.7342
[16:24:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #31: GFLOPs: 5.2075. Time: 545.0051 ms. Best GFLOPs: 70.7342
[16:24:33] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_nn_contrib_conv2d_NCHWc_add_2"
 ID |                                            Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |       fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 12760457216 |      1 |       115.7276 |  110262.8867 |           110262.8867 |     32 |          Y 
  1 |                          fused_layout_transform |           1 |      1 |         0.0000 |    2337.0640 |             2337.0640 |      8 |          Y 
  2 |               fused_nn_contrib_conv2d_NCHWc_add |    29130192 |      1 |        20.1363 |    1446.6491 |             1446.6491 |     32 |          Y 
  3 |             fused_nn_contrib_conv2d_NCHWc_add_1 | 11346935808 |      1 |       119.9731 |   94578.9817 |            94578.9817 |     32 |          Y 
  4 |             fused_nn_contrib_conv2d_NCHWc_add_2 |  2838118400 |      1 |        70.7342 |   40123.6917 |            40123.6917 |     32 |            
  5 |             fused_nn_contrib_conv2d_NCHWc_add_3 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |             fused_nn_contrib_conv2d_NCHWc_add_4 |   357187584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                        fused_layout_transform_1 |           1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |   819593216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                             fused_nn_max_pool2d |     6230016 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |    89989120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |   355803136 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |   798826496 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |   362725376 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |   798134272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   355110912 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   798134272 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |   358572032 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |  3191152640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |  1419059200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 22 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |  3191152640 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |  1425981440 |      6 |            N/A |          N/A |                   N/A |      0 |            
 24 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 | 12761841664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  5673467904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 | 12761841664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |  5687312384 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 51039059968 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_layout_transform_2 |           1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_contrib_conv2d_NCHWc_add_5 |    58203600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |           fused_layout_transform_image_resize2d |   196245504 |      2 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 136
Total latency (us): 248749

[16:24:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #4 has finished. Remaining task(s): 28
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #0: GFLOPs: 8.5924. Time: 82.6569 ms. Best GFLOPs: 8.5924
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #1: GFLOPs: 121.1281. Time: 5.8634 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #2: GFLOPs: 63.3280. Time: 11.2150 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #3: GFLOPs: 112.5812. Time: 6.3085 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #4: GFLOPs: 59.1479. Time: 12.0076 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #5: GFLOPs: 22.2435. Time: 31.9295 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(208, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(13, 1, 1, 4, 1, 1, 2):
                for i1_2_init, i2_2_init, i4_2_init, i1_3_init, i3_3_init in T.grid(2, 2, 2, 2, 4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_fused // 26 * 16 + i1_1 * 4 + i1_2_init * 2 + i1_3_init)
                        oh = T.axis.spatial(52, i0_0_i1_0_i2_0_fused % 26 * 2 + i2_2_init)
                        ow = T.axis.spatial(52, i3_0 * 4 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 104, 104, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 1, 1, 2, 2, 1, 2, 64, 1, 1, 1, 2, 1, 4, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_fused // 26 * 16 + i1_1 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(52, i0_0_i1_0_i2_0_fused % 26 * 2 + i2_2)
                        ow = T.axis.spatial(52, i3_0 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(256, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 104, 104, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(6656, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(52):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(128, i0_i1_i2_fused // 52)
                        ax2 = T.axis.spatial(52, i0_i1_i2_fused % 52)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[8, 4, 2, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[26, 1, 2, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[13, 1, 1, 4])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[4, 64])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l92)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l93, l94, l95, l96, l97 = sch.get_loops(block=b65)
l98 = sch.fuse(l93, l94, l95)
sch.parallel(loop=l98)
l99 = sch.fuse(l97)
sch.vectorize(loop=l99)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b100 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b100)
b125 = sch.decompose_reduction(block=b100, loop=l109)
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #7: GFLOPs: 32.3976. Time: 21.9221 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #8: GFLOPs: 40.7494. Time: 17.4290 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #9: GFLOPs: 5.8149. Time: 122.1387 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #10: GFLOPs: 39.5260. Time: 17.9684 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #11: GFLOPs: 19.5324. Time: 36.3613 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #12: GFLOPs: 12.9919. Time: 54.6665 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #13: GFLOPs: 6.0183. Time: 118.0113 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #14: GFLOPs: 6.9534. Time: 102.1398 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #15: GFLOPs: 73.0348. Time: 9.7244 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #16: GFLOPs: 36.5292. Time: 19.4426 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #17: GFLOPs: 8.8630. Time: 80.1331 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #18: GFLOPs: 21.7968. Time: 32.5838 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #19: GFLOPs: 8.2859. Time: 85.7141 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #20: GFLOPs: 5.5172. Time: 128.7276 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #21: GFLOPs: 20.2124. Time: 35.1379 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #22: GFLOPs: 5.2559. Time: 135.1275 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #23: GFLOPs: 15.1079. Time: 47.0099 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #24: GFLOPs: 3.0567. Time: 232.3455 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #25: GFLOPs: 8.8614. Time: 80.1475 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #26: GFLOPs: 30.2361. Time: 23.4892 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #27: GFLOPs: 18.2063. Time: 39.0097 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #28: GFLOPs: 8.4654. Time: 83.8974 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #29: GFLOPs: 18.6307. Time: 38.1210 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #30: GFLOPs: 18.6829. Time: 38.0146 ms. Best GFLOPs: 121.1281
[16:24:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #31: GFLOPs: 21.2745. Time: 33.3837 ms. Best GFLOPs: 121.1281
[16:24:39] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_contrib_conv2d_NCHWc_add_3"
 ID |                                            Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |       fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 12760457216 |      1 |       115.7276 |  110262.8867 |           110262.8867 |     32 |          Y 
  1 |                          fused_layout_transform |           1 |      1 |         0.0000 |    2337.0640 |             2337.0640 |      8 |          Y 
  2 |               fused_nn_contrib_conv2d_NCHWc_add |    29130192 |      1 |        20.1363 |    1446.6491 |             1446.6491 |     32 |          Y 
  3 |             fused_nn_contrib_conv2d_NCHWc_add_1 | 11346935808 |      1 |       119.9731 |   94578.9817 |            94578.9817 |     32 |          Y 
  4 |             fused_nn_contrib_conv2d_NCHWc_add_2 |  2838118400 |      1 |        70.7342 |   40123.6917 |            40123.6917 |     32 |          Y 
  5 |             fused_nn_contrib_conv2d_NCHWc_add_3 |   710221824 |      1 |       121.1281 |    5863.3961 |             5863.3961 |     32 |            
  6 |             fused_nn_contrib_conv2d_NCHWc_add_4 |   357187584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                        fused_layout_transform_1 |           1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |   819593216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                             fused_nn_max_pool2d |     6230016 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |    89989120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |   355803136 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |   798826496 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |   362725376 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |   798134272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   355110912 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   798134272 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |   358572032 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |  3191152640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |  1419059200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 22 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |  3191152640 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |  1425981440 |      6 |            N/A |          N/A |                   N/A |      0 |            
 24 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 | 12761841664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  5673467904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 | 12761841664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |  5687312384 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 51039059968 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_layout_transform_2 |           1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_contrib_conv2d_NCHWc_add_5 |    58203600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |           fused_layout_transform_image_resize2d |   196245504 |      2 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 168
Total latency (us): 254613

[16:24:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #5 has finished. Remaining task(s): 27
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 104, 104, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 2, 1, 1, 4):
                for i3_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 2, 26, 13):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 16 * 4 + i1_1 * 2 + i1_3_init)
                        oh = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 16 // 4 * 26 + i2_3_init)
                        ow = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 4 * 26 + i3_2_init * 13 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 1, 1, 2, 1, 32, 1, 1, 1, 2, 26, 13, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 16 * 4 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 16 // 4 * 26 + i2_3)
                        ow = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 4 * 26 + i3_2 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(64, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 26, 26, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 16 * 4 + i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 16 // 4 * 26 + ax2)
                        ax3_1 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 4 * 26 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[16, 2, 1, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 1, 26])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 2, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[2, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b66)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b106)
b130 = sch.decompose_reduction(block=b106, loop=l114)
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #1: GFLOPs: 8.9679. Time: 39.8294 ms. Best GFLOPs: 8.9679
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #2: GFLOPs: 12.8219. Time: 27.8575 ms. Best GFLOPs: 12.8219
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #3: GFLOPs: 23.7438. Time: 15.0434 ms. Best GFLOPs: 23.7438
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #4: GFLOPs: 25.4601. Time: 14.0293 ms. Best GFLOPs: 25.4601
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #5: GFLOPs: 20.3660. Time: 17.5384 ms. Best GFLOPs: 25.4601
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #6: GFLOPs: 18.4563. Time: 19.3532 ms. Best GFLOPs: 25.4601
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #7: GFLOPs: 28.8944. Time: 12.3618 ms. Best GFLOPs: 28.8944
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 104, 104, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 52, 1, 2):
                for i1_2_init, i3_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(4, 52, 2, 2, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 4 + i1_2_init)
                        oh = T.axis.spatial(104, i2_1 * 2 + i2_3_init)
                        ow = T.axis.spatial(104, i3_2_init * 2 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 4, 1, 52, 2, 1, 1, 1, 1, 1, 2, 2, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 4 + i1_2)
                        oh = T.axis.spatial(104, i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(104, i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(64, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 104, 104):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 4 + ax1)
                        ax2_1, ax3_1, ax4 = T.axis.remap("SSS", [ax2, ax3, ax4_fused])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[16, 1, 4, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 52, 1, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 52, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l99)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b101)
b124 = sch.decompose_reduction(block=b101, loop=l108)
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #9: GFLOPs: 20.6973. Time: 17.2577 ms. Best GFLOPs: 28.8944
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #10: GFLOPs: 2.5348. Time: 140.9124 ms. Best GFLOPs: 28.8944
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #11: GFLOPs: 8.1207. Time: 43.9846 ms. Best GFLOPs: 28.8944
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #12: GFLOPs: 26.9254. Time: 13.2658 ms. Best GFLOPs: 28.8944
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #13: GFLOPs: 48.2996. Time: 7.3953 ms. Best GFLOPs: 48.2996
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #14: GFLOPs: 58.0140. Time: 6.1569 ms. Best GFLOPs: 58.0140
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #15: GFLOPs: 59.0531. Time: 6.0486 ms. Best GFLOPs: 59.0531
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #16: GFLOPs: 17.3014. Time: 20.6450 ms. Best GFLOPs: 59.0531
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #17: GFLOPs: 14.1278. Time: 25.2826 ms. Best GFLOPs: 59.0531
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #18: GFLOPs: 27.5155. Time: 12.9813 ms. Best GFLOPs: 59.0531
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #19: GFLOPs: 17.9266. Time: 19.9250 ms. Best GFLOPs: 59.0531
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #20: GFLOPs: 6.5715. Time: 54.3540 ms. Best GFLOPs: 59.0531
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #21: GFLOPs: 50.6493. Time: 7.0522 ms. Best GFLOPs: 59.0531
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #22: GFLOPs: 31.3703. Time: 11.3862 ms. Best GFLOPs: 59.0531
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #23: GFLOPs: 12.7510. Time: 28.0125 ms. Best GFLOPs: 59.0531
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #24: GFLOPs: 10.5458. Time: 33.8702 ms. Best GFLOPs: 59.0531
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 104, 104, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(1352, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(2, 1, 2, 1, 2, 1):
                for i1_2_init, i3_2_init, i1_3_init, i2_3_init in T.grid(8, 4, 2, 2):
                    for i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 676 * 32 + i1_1 * 16 + i1_2_init * 2 + i1_3_init)
                            oh = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 676 // 13 * 2 + i2_3_init)
                            ow = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + i3_1 * 4 + i3_2_init)
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(2, 1, 1, 1, 8, 1, 4, 1, 32, 1, 1, 1, 2, 2):
                    for i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 676 * 32 + i1_1 * 16 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 676 // 13 * 2 + i2_3)
                            ow = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + i3_1 * 4 + i3_2)
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i3_3_i4_3_fused)
                            ic = T.axis.reduce(64, i5_0 * 32 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(6656, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(104):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(64, i0_i1_i2_fused // 104)
                        ax2 = T.axis.spatial(104, i0_i1_i2_fused % 104)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 2, 8, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[52, 1, 1, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[13, 2, 4, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[2, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69)
sch.parallel(loop=l92)
l93 = sch.fuse(l90, l91)
sch.vectorize(loop=l93)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b65)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b101)
b124 = sch.decompose_reduction(block=b101, loop=l109)
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #26: GFLOPs: 9.5455. Time: 37.4194 ms. Best GFLOPs: 59.0531
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 104, 104, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(1664, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_1 in T.serial(2):
                for i1_2_init, i2_2_init, i3_2_init, i2_3_init in T.grid(4, 2, 4, 26):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 832 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 208 // 26 * 4 + i1_2_init)
                        oh = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 26 // 13 * 52 + i2_2_init * 26 + i2_3_init)
                        ow = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 832 // 416 * 52 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + i3_2_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 416 // 208 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 4, 2, 4, 1, 1, 1, 1, 1, 1, 26, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 832 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 208 // 26 * 4 + i1_2)
                        oh = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 26 // 13 * 52 + i2_2 * 26 + i2_3)
                        ow = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 832 // 416 * 52 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + i3_2)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 416 // 208 * 2 + i4_1)
                        ic = T.axis.reduce(64, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 52, 4, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 832 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 208 // 26 * 4 + ax1)
                        ax2_1 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 26 // 13 * 52 + ax2)
                        ax3_1 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 832 // 416 * 52 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 416 // 208 * 2 + i4_1)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 8, 4, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 2, 26])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 13, 4, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b66)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b101)
b120 = sch.decompose_reduction(block=b101, loop=l104)
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #28: GFLOPs: 39.1791. Time: 9.1168 ms. Best GFLOPs: 59.0531
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #29: GFLOPs: 30.9327. Time: 11.5472 ms. Best GFLOPs: 59.0531
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #30: GFLOPs: 50.7739. Time: 7.0349 ms. Best GFLOPs: 59.0531
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #31: GFLOPs: 22.5160. Time: 15.8637 ms. Best GFLOPs: 59.0531
[16:24:40] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"
 ID |                                            Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |       fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 12760457216 |      1 |       115.7276 |  110262.8867 |           110262.8867 |     32 |          Y 
  1 |                          fused_layout_transform |           1 |      1 |         0.0000 |    2337.0640 |             2337.0640 |      8 |          Y 
  2 |               fused_nn_contrib_conv2d_NCHWc_add |    29130192 |      1 |        20.1363 |    1446.6491 |             1446.6491 |     32 |          Y 
  3 |             fused_nn_contrib_conv2d_NCHWc_add_1 | 11346935808 |      1 |       119.9731 |   94578.9817 |            94578.9817 |     32 |          Y 
  4 |             fused_nn_contrib_conv2d_NCHWc_add_2 |  2838118400 |      1 |        70.7342 |   40123.6917 |            40123.6917 |     32 |          Y 
  5 |             fused_nn_contrib_conv2d_NCHWc_add_3 |   710221824 |      1 |       121.1281 |    5863.3961 |             5863.3961 |     32 |          Y 
  6 |             fused_nn_contrib_conv2d_NCHWc_add_4 |   357187584 |      1 |        59.0531 |    6048.5864 |             6048.5864 |     32 |            
  7 |                        fused_layout_transform_1 |           1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |   819593216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                             fused_nn_max_pool2d |     6230016 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |    89989120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |   355803136 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |   798826496 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |   362725376 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |   798134272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   355110912 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   798134272 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |   358572032 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |  3191152640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |  1419059200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 22 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |  3191152640 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |  1425981440 |      6 |            N/A |          N/A |                   N/A |      0 |            
 24 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 | 12761841664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  5673467904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 | 12761841664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |  5687312384 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 51039059968 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_layout_transform_2 |           1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_contrib_conv2d_NCHWc_add_5 |    58203600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |           fused_layout_transform_image_resize2d |   196245504 |      2 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 200
Total latency (us): 260661

[16:24:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #6 has finished. Remaining task(s): 26
[16:24:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #0: GFLOPs: 0.0000. Time: 1.1705 ms. Best GFLOPs: 0.0000
[16:24:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #1: GFLOPs: 0.0000. Time: 3.8721 ms. Best GFLOPs: 0.0000
[16:24:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #2: GFLOPs: 0.0000. Time: 3.5898 ms. Best GFLOPs: 0.0000
[16:24:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #3: GFLOPs: 0.0000. Time: 1.2965 ms. Best GFLOPs: 0.0000
[16:24:42] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_layout_transform_1"
 ID |                                            Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |       fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 12760457216 |      1 |       115.7276 |  110262.8867 |           110262.8867 |     32 |          Y 
  1 |                          fused_layout_transform |           1 |      1 |         0.0000 |    2337.0640 |             2337.0640 |      8 |          Y 
  2 |               fused_nn_contrib_conv2d_NCHWc_add |    29130192 |      1 |        20.1363 |    1446.6491 |             1446.6491 |     32 |          Y 
  3 |             fused_nn_contrib_conv2d_NCHWc_add_1 | 11346935808 |      1 |       119.9731 |   94578.9817 |            94578.9817 |     32 |          Y 
  4 |             fused_nn_contrib_conv2d_NCHWc_add_2 |  2838118400 |      1 |        70.7342 |   40123.6917 |            40123.6917 |     32 |          Y 
  5 |             fused_nn_contrib_conv2d_NCHWc_add_3 |   710221824 |      1 |       121.1281 |    5863.3961 |             5863.3961 |     32 |          Y 
  6 |             fused_nn_contrib_conv2d_NCHWc_add_4 |   357187584 |      1 |        59.0531 |    6048.5864 |             6048.5864 |     32 |          Y 
  7 |                        fused_layout_transform_1 |           1 |      1 |         0.0000 |    1170.4562 |             1170.4562 |      4 |            
  8 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |   819593216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                             fused_nn_max_pool2d |     6230016 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |    89989120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |   355803136 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |   798826496 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |   362725376 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |   798134272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   355110912 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   798134272 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |   358572032 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |  3191152640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |  1419059200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 22 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |  3191152640 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |  1425981440 |      6 |            N/A |          N/A |                   N/A |      0 |            
 24 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 | 12761841664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  5673467904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 | 12761841664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |  5687312384 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 51039059968 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_layout_transform_2 |           1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_contrib_conv2d_NCHWc_add_5 |    58203600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |           fused_layout_transform_image_resize2d |   196245504 |      2 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 204
Total latency (us): 261832

[16:24:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #7 has finished. Remaining task(s): 25
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #0: GFLOPs: 1.5334. Time: 534.5050 ms. Best GFLOPs: 1.5334
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #1: GFLOPs: 25.6156. Time: 31.9958 ms. Best GFLOPs: 25.6156
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #2: GFLOPs: 39.8085. Time: 20.5884 ms. Best GFLOPs: 39.8085
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #3: GFLOPs: 16.3910. Time: 50.0027 ms. Best GFLOPs: 39.8085
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 416, 416, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 7, 7, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 2, 1):
                for i5_0 in T.serial(1):
                    for i3_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(13, 2, 13, 8):
                        for i4_3_fused_init in T.vectorized(2):
                            with T.block("conv2d_NCHWc_init"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 32 * 2 + i1_3_init)
                                oh = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 32 // 2 * 13 + i2_3_init)
                                ow = T.axis.spatial(208, i3_1 * 104 + i3_2_init * 8 + i3_3_init)
                                oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_3_fused_init)
                                T.reads()
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 416, 416, 3], "float32"], ["TENSOR", [16, 1, 7, 7, 3, 4], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(7, 1, 1, 1, 1, 13, 1, 3, 1, 7, 1, 2, 13, 8):
                        for i4_3_fused in T.vectorized(2):
                            with T.block("conv2d_NCHWc_update"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 32 * 2 + i1_3)
                                oh = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 32 // 2 * 13 + i2_3)
                                ow = T.axis.spatial(208, i3_1 * 104 + i3_2 * 8 + i3_3)
                                oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_3_fused)
                                ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_0, i7_1])
                                T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 3, oh * 2 + kh - 3, ow * 2 + kw - 3, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 416, 416, 3], "float32"], ["TENSOR", [16, 1, 7, 7, 3, 4], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(3 <= oh * 2 + kh and oh * 2 + kh < 419 and 3 <= ow * 2 + kw and ow * 2 + kw < 419, placeholder[n, ic // 3, oh * 2 + kh - 3, ow * 2 + kw - 3, ic % 3], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                for ax0, ax1, ax2, ax3 in T.grid(1, 2, 13, 104):
                    for ax4_fused in T.vectorized(2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 32 * 2 + ax1)
                            ax2_1 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 32 // 2 * 13 + ax2)
                            ax3_1 = T.axis.spatial(208, i3_1 * 104 + ax3)
                            ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 1, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[16, 1, 1, 13])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 13, 8])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[7, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 7])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69 = sch.get_child_blocks(b67)
l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b68)
l96 = sch.fuse(l70, l71, l72, l73, l74)
sch.parallel(loop=l96)
l97 = sch.fuse(l95)
sch.vectorize(loop=l97)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b69)
l109 = sch.fuse(l108)
sch.vectorize(loop=l109)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b110 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b110)
b133 = sch.decompose_reduction(block=b110, loop=l118)
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #5: GFLOPs: 3.2186. Time: 254.6394 ms. Best GFLOPs: 39.8085
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #6: GFLOPs: 25.6244. Time: 31.9849 ms. Best GFLOPs: 39.8085
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #7: GFLOPs: 5.6110. Time: 146.0697 ms. Best GFLOPs: 39.8085
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #8: GFLOPs: 24.1502. Time: 33.9374 ms. Best GFLOPs: 39.8085
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #9: GFLOPs: 18.9924. Time: 43.1537 ms. Best GFLOPs: 39.8085
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #10: GFLOPs: 36.9907. Time: 22.1567 ms. Best GFLOPs: 39.8085
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #11: GFLOPs: 33.3321. Time: 24.5887 ms. Best GFLOPs: 39.8085
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #12: GFLOPs: 24.2508. Time: 33.7966 ms. Best GFLOPs: 39.8085
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #13: GFLOPs: 16.1016. Time: 50.9013 ms. Best GFLOPs: 39.8085
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #14: GFLOPs: 6.1917. Time: 132.3694 ms. Best GFLOPs: 39.8085
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #15: GFLOPs: 5.2712. Time: 155.4840 ms. Best GFLOPs: 39.8085
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #16: GFLOPs: 19.6388. Time: 41.7333 ms. Best GFLOPs: 39.8085
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #17: GFLOPs: 7.3834. Time: 111.0042 ms. Best GFLOPs: 39.8085
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #18: GFLOPs: 49.4808. Time: 16.5639 ms. Best GFLOPs: 49.4808
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #19: GFLOPs: 11.1598. Time: 73.4417 ms. Best GFLOPs: 49.4808
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #20: GFLOPs: 28.8750. Time: 28.3842 ms. Best GFLOPs: 49.4808
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #21: GFLOPs: 3.6953. Time: 221.7957 ms. Best GFLOPs: 49.4808
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #22: GFLOPs: 6.0723. Time: 134.9733 ms. Best GFLOPs: 49.4808
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #23: GFLOPs: 54.2585. Time: 15.1053 ms. Best GFLOPs: 54.2585
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #24: GFLOPs: 46.6470. Time: 17.5701 ms. Best GFLOPs: 54.2585
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #25: GFLOPs: 39.9765. Time: 20.5019 ms. Best GFLOPs: 54.2585
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #26: GFLOPs: 20.4921. Time: 39.9955 ms. Best GFLOPs: 54.2585
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #27: GFLOPs: 17.9208. Time: 45.7343 ms. Best GFLOPs: 54.2585
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #28: GFLOPs: 18.9449. Time: 43.2619 ms. Best GFLOPs: 54.2585
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #29: GFLOPs: 74.2734. Time: 11.0348 ms. Best GFLOPs: 74.2734
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #30: GFLOPs: 51.2378. Time: 15.9959 ms. Best GFLOPs: 74.2734
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #31: GFLOPs: 26.4393. Time: 30.9991 ms. Best GFLOPs: 74.2734
[16:24:49] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"
 ID |                                            Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |       fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 12760457216 |      1 |       115.7276 |  110262.8867 |           110262.8867 |     32 |          Y 
  1 |                          fused_layout_transform |           1 |      1 |         0.0000 |    2337.0640 |             2337.0640 |      8 |          Y 
  2 |               fused_nn_contrib_conv2d_NCHWc_add |    29130192 |      1 |        20.1363 |    1446.6491 |             1446.6491 |     32 |          Y 
  3 |             fused_nn_contrib_conv2d_NCHWc_add_1 | 11346935808 |      1 |       119.9731 |   94578.9817 |            94578.9817 |     32 |          Y 
  4 |             fused_nn_contrib_conv2d_NCHWc_add_2 |  2838118400 |      1 |        70.7342 |   40123.6917 |            40123.6917 |     32 |          Y 
  5 |             fused_nn_contrib_conv2d_NCHWc_add_3 |   710221824 |      1 |       121.1281 |    5863.3961 |             5863.3961 |     32 |          Y 
  6 |             fused_nn_contrib_conv2d_NCHWc_add_4 |   357187584 |      1 |        59.0531 |    6048.5864 |             6048.5864 |     32 |          Y 
  7 |                        fused_layout_transform_1 |           1 |      1 |         0.0000 |    1170.4562 |             1170.4562 |      4 |          Y 
  8 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |   819593216 |      1 |        74.2734 |   11034.8151 |            11034.8151 |     32 |            
  9 |                             fused_nn_max_pool2d |     6230016 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |    89989120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |   355803136 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |   798826496 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |   362725376 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |   798134272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   355110912 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   798134272 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |   358572032 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |  3191152640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |  1419059200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 22 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |  3191152640 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |  1425981440 |      6 |            N/A |          N/A |                   N/A |      0 |            
 24 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 | 12761841664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  5673467904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 | 12761841664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |  5687312384 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 51039059968 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_layout_transform_2 |           1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_contrib_conv2d_NCHWc_add_5 |    58203600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |           fused_layout_transform_image_resize2d |   196245504 |      2 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 236
Total latency (us): 272867

[16:24:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #8 has finished. Remaining task(s): 24
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #0: GFLOPs: 0.7980. Time: 7.8069 ms. Best GFLOPs: 0.7980
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #1: GFLOPs: 2.7772. Time: 2.2432 ms. Best GFLOPs: 2.7772
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #2: GFLOPs: 6.5999. Time: 0.9440 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #3: GFLOPs: 1.0425. Time: 5.9762 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #4: GFLOPs: 0.6120. Time: 10.1790 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #5: GFLOPs: 0.5411. Time: 11.5128 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #6: GFLOPs: 0.7936. Time: 7.8507 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #7: GFLOPs: 0.6884. Time: 9.0499 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #8: GFLOPs: 0.5153. Time: 12.0894 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #9: GFLOPs: 1.0758. Time: 5.7908 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #10: GFLOPs: 0.7642. Time: 8.1528 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #11: GFLOPs: 1.3532. Time: 4.6038 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #12: GFLOPs: 4.0238. Time: 1.5483 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #13: GFLOPs: 1.6712. Time: 3.7279 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #14: GFLOPs: 0.7350. Time: 8.4764 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #15: GFLOPs: 3.0283. Time: 2.0572 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #16: GFLOPs: 0.5769. Time: 10.7989 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #17: GFLOPs: 0.5842. Time: 10.6640 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #18: GFLOPs: 0.4480. Time: 13.9072 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #19: GFLOPs: 0.3465. Time: 17.9808 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #20: GFLOPs: 0.6784. Time: 9.1839 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #21: GFLOPs: 0.4641. Time: 13.4241 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #22: GFLOPs: 0.8634. Time: 7.2160 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #23: GFLOPs: 0.7844. Time: 7.9426 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #24: GFLOPs: 1.3064. Time: 4.7688 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #25: GFLOPs: 0.5591. Time: 11.1430 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #26: GFLOPs: 0.4174. Time: 14.9252 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #27: GFLOPs: 2.4253. Time: 2.5687 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #28: GFLOPs: 5.7382. Time: 1.0857 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #29: GFLOPs: 2.7299. Time: 2.2821 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #30: GFLOPs: 2.1513. Time: 2.8959 ms. Best GFLOPs: 6.5999
[16:24:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d"] Trial #31: GFLOPs: 5.8108. Time: 1.0721 ms. Best GFLOPs: 6.5999
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #9: "fused_nn_max_pool2d"
 ID |                                            Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |       fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 12760457216 |      1 |       115.7276 |  110262.8867 |           110262.8867 |     32 |          Y 
  1 |                          fused_layout_transform |           1 |      1 |         0.0000 |    2337.0640 |             2337.0640 |      8 |          Y 
  2 |               fused_nn_contrib_conv2d_NCHWc_add |    29130192 |      1 |        20.1363 |    1446.6491 |             1446.6491 |     32 |          Y 
  3 |             fused_nn_contrib_conv2d_NCHWc_add_1 | 11346935808 |      1 |       119.9731 |   94578.9817 |            94578.9817 |     32 |          Y 
  4 |             fused_nn_contrib_conv2d_NCHWc_add_2 |  2838118400 |      1 |        70.7342 |   40123.6917 |            40123.6917 |     32 |          Y 
  5 |             fused_nn_contrib_conv2d_NCHWc_add_3 |   710221824 |      1 |       121.1281 |    5863.3961 |             5863.3961 |     32 |          Y 
  6 |             fused_nn_contrib_conv2d_NCHWc_add_4 |   357187584 |      1 |        59.0531 |    6048.5864 |             6048.5864 |     32 |          Y 
  7 |                        fused_layout_transform_1 |           1 |      1 |         0.0000 |    1170.4562 |             1170.4562 |      4 |          Y 
  8 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |   819593216 |      1 |        74.2734 |   11034.8151 |            11034.8151 |     32 |          Y 
  9 |                             fused_nn_max_pool2d |     6230016 |      1 |         6.5999 |     943.9629 |              943.9629 |     32 |            
 10 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |    89989120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |   355803136 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |   798826496 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu |   362725376 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |   798134272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   355110912 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   798134272 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1 |   358572032 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |  3191152640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |  1419059200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 22 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |  3191152640 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2 |  1425981440 |      6 |            N/A |          N/A |                   N/A |      0 |            
 24 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 | 12761841664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  5673467904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 | 12761841664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3 |  5687312384 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 51039059968 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_layout_transform_2 |           1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_contrib_conv2d_NCHWc_add_5 |    58203600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |           fused_layout_transform_image_resize2d |   196245504 |      2 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 268
Total latency (us): 273810

[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #9 has finished. Remaining task(s): 23
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #10 has finished. Remaining task(s): 22
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #11 has finished. Remaining task(s): 21
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #12 has finished. Remaining task(s): 20
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #13 has finished. Remaining task(s): 19
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #14 has finished. Remaining task(s): 18
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #15 has finished. Remaining task(s): 17
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #16 has finished. Remaining task(s): 16
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #17 has finished. Remaining task(s): 15
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #18 has finished. Remaining task(s): 14
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #19 has finished. Remaining task(s): 13
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #20 has finished. Remaining task(s): 12
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #21 has finished. Remaining task(s): 11
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #22 has finished. Remaining task(s): 10
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #23 has finished. Remaining task(s): 9
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #24 has finished. Remaining task(s): 8
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #25 has finished. Remaining task(s): 7
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #26 has finished. Remaining task(s): 6
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #27 has finished. Remaining task(s): 5
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #28 has finished. Remaining task(s): 4
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #29 has finished. Remaining task(s): 3
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #30 has finished. Remaining task(s): 2
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #31 has finished. Remaining task(s): 1
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #32 has finished. Remaining task(s): 0
[16:25:39] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 416, 416), "float32"], T_layout_trans: T.Buffer[(1, 1, 416, 416, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 416, 416, 3):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1 * 3 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 416 and ax3 < 416, placeholder[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[16:25:39] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:25:39] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_layout_transform(placeholder: T.Buffer[(1, 3, 416, 416), "float32"], T_layout_trans: T.Buffer[(1, 1, 416, 416, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0_i1_i2_fused in T.parallel(416, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3, i4 in T.grid(416, 3):
                with T.block("T_layout_trans"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(1, 0)
                    ax2, ax3, ax4 = T.axis.remap("SSS", [i0_i1_i2_fused, i3, i4])
                    T.reads(placeholder[ax0, ax1 * 3 + ax4, ax2, ax3])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 416 and ax3 < 416, placeholder[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[16:25:39] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:25:39] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v1)", "sch.enter_postproc()", "b2 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b2, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b2, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b2, ann_key=\"meta_schedule.unroll_explicit\")", "b3, = sch.get_child_blocks(b2)", "l4, l5, l6, l7, l8 = sch.get_loops(block=b3)", "l9 = sch.fuse(l4, l5, l6)", "sch.parallel(loop=l9)", "sch.annotate(block_or_loop=l9, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l9, ann_key=\"pragma_unroll_explicit\", ann_val=1)"]
[16:25:39] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 416, 416, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 7, 7, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 422, 422, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 422, 422, 3):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 3, i3_1 - 3, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(3 <= i2_1 and i2_1 < 419 and 3 <= i3_1 and i3_1 < 419, placeholder[i0_1, i1_1, i2_1 - 3, i3_1 - 3, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 208, 208, 4, 3, 7, 7):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 416, 416, 3], "float32"], ["TENSOR", [16, 1, 7, 7, 3, 4], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:25:39] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:25:39] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu(placeholder: T.Buffer[(1, 1, 416, 416, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 7, 7, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 422, 422, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1 in T.grid(1, 2):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 57, 421):
                    for ax4_fused in T.vectorized(3):
                        with T.block("data_pad"):
                            i0, i1 = T.axis.remap("SS", [ax0, ax1])
                            i2 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 * 52 + ax2)
                            i3 = T.axis.spatial(422, ax3)
                            i4 = T.axis.spatial(3, ax4_fused)
                            T.reads(placeholder[i0, i1, i2 - 3, i3 - 3, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(3 <= i2 and i2 < 419 and 3 <= i3 and i3 < 419, placeholder[i0, i1, i2 - 3, i3 - 3, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1 in T.grid(1, 104, 1):
                    for i1_2_init, i2_2_init, i1_3_init, i3_3_init in T.grid(2, 26, 4, 2):
                        for i4_3_fused_init in T.vectorized(4):
                            with T.block("conv2d_NCHWc_init"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(16, i1_1 * 8 + i1_2_init * 4 + i1_3_init)
                                oh = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 26 + i2_2_init)
                                ow = T.axis.spatial(208, i3_1 * 2 + i3_3_init)
                                oc_block = T.axis.spatial(4, i4_3_fused_init)
                                T.reads()
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 416, 416, 3], "float32"], ["TENSOR", [16, 1, 7, 7, 3, 4], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 7, 1, 1, 2, 26, 1, 1, 1, 1, 7, 1, 4, 1, 2):
                        for i4_3_fused in T.vectorized(4):
                            with T.block("conv2d_NCHWc_update"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(16, i1_1 * 8 + i1_2 * 4 + i1_3)
                                oh = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 26 + i2_2)
                                ow = T.axis.spatial(208, i3_1 * 2 + i3_3)
                                oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_3_fused, i5_0, i6_0, i7_1])
                                T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 416, 416, 3], "float32"], ["TENSOR", [16, 1, 7, 7, 3, 4], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 16, 26, 208):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(16, ax1)
                        ax2_1 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 26 + ax2)
                        ax3_1, ax4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

[16:25:39] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:25:39] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"data_pad\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b2)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSRSRS\")", "l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)", "v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])", "l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])", "v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 2, 4])", "l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])", "v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[8, 1, 26, 1])", "l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])", "v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 104, 1, 2])", "l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])", "v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])", "l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])", "v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])", "l54, l55 = sch.split(loop=l9, factors=[v52, v53])", "v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[7, 1])", "l58, l59 = sch.split(loop=l10, factors=[v56, v57])", "v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 7])", "l62, l63 = sch.split(loop=l11, factors=[v60, v61])", "sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)", "b64, = sch.get_consumers(block=b1)", "sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v65)", "l66 = sch.sample_compute_location(block=b0, decision=6)", "sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)", "sch.enter_postproc()", "b67 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b67, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b67, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b67, ann_key=\"meta_schedule.unroll_explicit\")", "b68, b69, b70 = sch.get_child_blocks(b67)", "l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b68)", "l83 = sch.fuse(l71, l72, l73, l74, l75)", "sch.parallel(loop=l83)", "l84 = sch.fuse(l82)", "sch.vectorize(loop=l84)", "sch.annotate(block_or_loop=l83, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l83, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b69)", "l107 = sch.fuse(l106)", "sch.vectorize(loop=l107)", "sch.annotate(block_or_loop=l85, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l85, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b70)", "l114 = sch.fuse(l113)", "sch.vectorize(loop=l114)", "sch.annotate(block_or_loop=l108, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l108, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b115 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137 = sch.get_loops(block=b115)", "b138 = sch.decompose_reduction(block=b115, loop=l122)"]
[16:25:48] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], tensor: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 16, 210, 210, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 210, 210, 4):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1, ax4])
                T.writes(pad_temp[ax0, ax1, ax2, ax3, ax4])
                pad_temp[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(1 <= ax2 and ax2 < 209 and 1 <= ax3 and ax3 < 209, placeholder[ax0, ax1, ax2 - 1, ax3 - 1, ax4], T.float32(-3.4028234663852886e+38), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 16, 104, 104, 4, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
    

[16:25:48] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:25:48] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_max_pool2d(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], tensor: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 16, 210, 210, 4], dtype="float32")
        tensor_rf = T.alloc_buffer([1, 16, 104, 104, 4, 3], dtype="float32")
        for i0_i1_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 209, 209):
                for ax4_fused in T.vectorized(4):
                    with T.block("pad_temp"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(16, i0_i1_fused)
                        ax2_1 = T.axis.spatial(210, ax2)
                        ax3_1 = T.axis.spatial(210, ax3)
                        ax4 = T.axis.spatial(4, ax4_fused)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 - 1, ax3_1 - 1, ax4])
                        T.writes(pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.if_then_else(1 <= ax2_1 and ax2_1 < 209 and 1 <= ax3_1 and ax3_1 < 209, placeholder[ax0_1, ax1_1, ax2_1 - 1, ax3_1 - 1, ax4], T.float32(-3.4028234663852886e+38), dtype="float32")
            for i2, i3, i4, i5_i6_fused_0 in T.grid(104, 104, 4, 3):
                with T.block("tensor_rf_init"):
                    vi5_i6_fused_0 = T.axis.spatial(3, i5_i6_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                    T.reads()
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.float32(-3.4028234663852886e+38)
                for i5_i6_fused_1 in T.serial(3):
                    with T.block("tensor_rf_update"):
                        vi5_i6_fused_0 = T.axis.spatial(3, i5_i6_fused_0)
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4, vi5_i6_fused_1 = T.axis.remap("SSSSR", [i0_i1_fused, i2, i3, i4, i5_i6_fused_1])
                        T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0], pad_temp[ax0, ax1, ax2 * 2 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) // 3, ax3 * 2 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) % 3, ax4])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0], pad_temp[ax0, ax1, ax2 * 2 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) // 3, ax3 * 2 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) % 3, ax4])
        for i0_i1_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i2, i3, i4 in T.grid(104, 104, 4):
                with T.block("tensor_init"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                    T.reads()
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                for i5_i6_fused_0 in T.serial(3):
                    with T.block("tensor_update"):
                        vi5_i6_fused_0 = T.axis.reduce(3, i5_i6_fused_0)
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                        T.reads(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                        T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                        T.block_attr({"meta_schedule.random_compute_producer":True})
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
    

[16:25:48] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:25:48] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"tensor\", func_name=\"main\")", "b2 = sch.get_block(name=\"root\", func_name=\"main\")", "l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)", "l10 = sch.fuse(l8, l9)", "v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 3])", "l13, l14 = sch.split(loop=l10, factors=[v11, v12])", "b15 = sch.rfactor(loop=l13, factor_axis=5)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.random_compute_producer\", ann_val=1)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.parallel\", ann_val=1)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v16)", "l17 = sch.sample_compute_location(block=b0, decision=1)", "sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)", "sch.enter_postproc()", "b18 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b18, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b18, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b18, ann_key=\"meta_schedule.unroll_explicit\")", "b19, b20, b21 = sch.get_child_blocks(b18)", "l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b19)", "l29 = sch.fuse(l22, l23)", "sch.parallel(loop=l29)", "l30 = sch.fuse(l28)", "sch.vectorize(loop=l30)", "sch.annotate(block_or_loop=l29, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l29, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b20)", "sch.annotate(block_or_loop=l31, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l31, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b21)", "l43 = sch.fuse(l37, l38)", "sch.parallel(loop=l43)", "sch.annotate(block_or_loop=l43, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l43, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b44 = sch.get_block(name=\"tensor_rf\", func_name=\"main\")", "l45, l46, l47, l48, l49, l50 = sch.get_loops(block=b44)", "b51 = sch.decompose_reduction(block=b44, loop=l50)", "b52 = sch.get_block(name=\"tensor\", func_name=\"main\")", "l53, l54, l55, l56, l57 = sch.get_loops(block=b52)", "b58 = sch.decompose_reduction(block=b52, loop=l57)"]
[16:25:48] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1
[16:25:48] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2
[16:25:48] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 104, 104, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 104, 104, 4, 64, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 104, 104, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[16:25:48] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:25:48] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 104, 104, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(3328, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1, i4_1 in T.grid(2, 2):
                for i1_3_init, i3_3_init in T.grid(8, 26):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 208 // 26 * 8 + i1_3_init)
                        oh = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 832 * 26 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 26)
                        ow = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 832 // 416 * 52 + i3_1 * 26 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 416 // 208 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 1, 1, 1, 1, 1, 1, 16, 1, 1, 1, 8, 1, 26, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 208 // 26 * 8 + i1_3)
                        oh = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 832 * 26 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 26)
                        ow = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 832 // 416 * 52 + i3_1 * 26 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 416 // 208 * 2 + i4_1)
                        ic = T.axis.reduce(64, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 1, 26, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 208 // 26 * 8 + ax1)
                        ax2_1 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 832 * 26 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 26)
                        ax3_1 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 832 // 416 * 52 + i3_1 * 26 + ax3)
                        ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 416 // 208 * 2 + i4_1)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

[16:25:48] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:25:48] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "b1 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSRSRS\")", "l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)", "v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])", "l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])", "v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 8, 1, 8])", "l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])", "v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 26, 1, 1])", "l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])", "v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 1, 26])", "l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])", "v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])", "l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])", "v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[4, 16])", "l52, l53 = sch.split(loop=l7, factors=[v50, v51])", "v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])", "l56, l57 = sch.split(loop=l8, factors=[v54, v55])", "v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])", "l60, l61 = sch.split(loop=l9, factors=[v58, v59])", "sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)", "b62, = sch.get_consumers(block=b0)", "sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v63)", "sch.enter_postproc()", "b64 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b64, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b64, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b64, ann_key=\"meta_schedule.unroll_explicit\")", "b65, b66 = sch.get_child_blocks(b64)", "l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)", "l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74)", "sch.parallel(loop=l93)", "sch.annotate(block_or_loop=l93, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l93, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l94, l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b66)", "sch.annotate(block_or_loop=l94, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l94, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b102 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b102)", "b122 = sch.decompose_reduction(block=b102, loop=l106)"]
[16:25:51] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu
[16:25:52] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3
[16:25:52] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4
[16:25:52] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5
[16:25:52] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 52, 52, 4, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 104, 104, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[16:25:52] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:25:52] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_1(placeholder: T.Buffer[(1, 64, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 52, 52, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(4, 13, 52, 4, 2, 2):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 8 + i1_2_init * 2 + i1_3_init)
                    oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 26 + i2_2_init * 2 + i2_3_init)
                    ow, oc_block = T.axis.remap("SS", [i3_2_init, i4_2_init])
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 104, 104, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 4, 13, 52, 4, 8, 1, 1, 1, 2, 2, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 8 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 26 + i2_2 * 2 + i2_3)
                    ow, oc_block = T.axis.remap("SS", [i3_2, i4_2])
                    ic = T.axis.reduce(256, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 104, 104, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 26, 52):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 8 + ax1)
                        ax2_1 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 26 + ax2)
                        ax3_1, ax4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

[16:25:52] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:25:52] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "b1 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSRSRS\")", "l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)", "v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])", "l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])", "v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[8, 2, 4, 2])", "l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])", "v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 13, 2])", "l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])", "v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 52, 1])", "l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])", "v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])", "l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])", "v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 8])", "l52, l53 = sch.split(loop=l7, factors=[v50, v51])", "v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])", "l56, l57 = sch.split(loop=l8, factors=[v54, v55])", "v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])", "l60, l61 = sch.split(loop=l9, factors=[v58, v59])", "sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)", "b62, = sch.get_consumers(block=b0)", "sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v63)", "sch.enter_postproc()", "b64 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b64, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b64, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b64, ann_key=\"meta_schedule.unroll_explicit\")", "b65, b66 = sch.get_child_blocks(b64)", "l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)", "l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)", "sch.parallel(loop=l93)", "sch.annotate(block_or_loop=l93, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l93, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b66)", "l100 = sch.fuse(l99)", "sch.vectorize(loop=l100)", "sch.annotate(block_or_loop=l94, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l94, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b101 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)", "b119 = sch.decompose_reduction(block=b101, loop=l103)"]
[16:25:53] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1
[16:25:53] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6
[16:25:53] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7
[16:25:53] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8
[16:25:53] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9
[16:25:54] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 256, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 256, 52, 52, 4, 512, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[16:25:54] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:25:54] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_2(placeholder: T.Buffer[(1, 128, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 256, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 52, 52, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(1664, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 2, 1, 13, 1):
                for i1_3_init, i3_3_init in T.grid(4, 4):
                    for i4_3_fused_init in T.vectorized(4):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_fused // 52 * 8 + i1_1 * 4 + i1_3_init)
                            oh = T.axis.spatial(52, i0_0_i1_0_i2_0_fused % 52)
                            ow = T.axis.spatial(52, i3_1 * 4 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(32, 1, 1, 1, 1, 1, 1, 1, 16, 1, 1, 1, 4, 1, 4):
                    for i4_3_fused in T.vectorized(4):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_fused // 52 * 8 + i1_1 * 4 + i1_3)
                            oh = T.axis.spatial(52, i0_0_i1_0_i2_0_fused % 52)
                            ow = T.axis.spatial(52, i3_1 * 4 + i3_3)
                            oc_block = T.axis.spatial(4, i4_3_fused)
                            ic = T.axis.reduce(512, i5_0 * 16 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2, i3 in T.grid(52, 52):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[16:25:54] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:25:54] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "b1 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSRSRS\")", "l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)", "v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])", "l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])", "v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[32, 2, 1, 4])", "l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])", "v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[52, 1, 1, 1])", "l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])", "v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 4])", "l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])", "v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])", "l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])", "v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 16])", "l52, l53 = sch.split(loop=l7, factors=[v50, v51])", "v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])", "l56, l57 = sch.split(loop=l8, factors=[v54, v55])", "v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])", "l60, l61 = sch.split(loop=l9, factors=[v58, v59])", "sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v62)", "sch.enter_postproc()", "b63 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b63, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b63, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b63, ann_key=\"meta_schedule.unroll_explicit\")", "b64, b65 = sch.get_child_blocks(b63)", "l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)", "l92 = sch.fuse(l66, l67, l68)", "sch.parallel(loop=l92)", "l93 = sch.fuse(l91)", "sch.vectorize(loop=l93)", "sch.annotate(block_or_loop=l92, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l92, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l94, l95, l96, l97, l98 = sch.get_loops(block=b65)", "l99 = sch.fuse(l94, l95)", "sch.parallel(loop=l99)", "l100 = sch.fuse(l98)", "sch.vectorize(loop=l100)", "sch.annotate(block_or_loop=l99, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l99, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b101 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b101)", "b126 = sch.decompose_reduction(block=b101, loop=l110)"]
[16:25:58] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2
[16:25:58] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10
[16:25:58] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11
[16:25:59] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12
[16:26:00] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13
[16:26:00] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 512, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 512, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 512, 52, 52, 4, 1024, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [512, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 512, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[16:26:00] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:26:00] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_3(placeholder: T.Buffer[(1, 256, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 512, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 512, 52, 52, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(832, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i4_1 in T.serial(1):
                for i2_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(26, 16, 2, 2):
                    for i4_3_fused_init in T.vectorized(4):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 104 * 64 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 104 // 26 * 16 + i1_3_init)
                            oh = T.axis.spatial(52, i2_2_init * 2 + i2_3_init)
                            ow = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 26 * 2 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [512, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(512, 1, 1, 1, 1, 26, 1, 1, 2, 1, 1, 1, 16, 2, 2):
                    for i4_3_fused in T.vectorized(4):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 104 * 64 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 104 // 26 * 16 + i1_3)
                            oh = T.axis.spatial(52, i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 26 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_3_fused)
                            ic = T.axis.reduce(1024, i5_0 * 2 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [512, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2 in T.grid(1, 16, 52):
                    for ax3_ax4_fused in T.vectorized(8):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 104 * 64 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 104 // 26 * 16 + ax1)
                            ax2_1 = T.axis.spatial(52, ax2)
                            ax3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 26 * 2 + ax3_ax4_fused // 4)
                            ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

[16:26:00] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:26:00] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "b1 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSRSRS\")", "l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)", "v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])", "l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])", "v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[8, 4, 1, 16])", "l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])", "v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 26, 2])", "l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])", "v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 26, 1, 2])", "l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])", "v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])", "l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])", "v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[512, 2])", "l52, l53 = sch.split(loop=l7, factors=[v50, v51])", "v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])", "l56, l57 = sch.split(loop=l8, factors=[v54, v55])", "v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])", "l60, l61 = sch.split(loop=l9, factors=[v58, v59])", "sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)", "b62, = sch.get_consumers(block=b0)", "sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v63)", "sch.enter_postproc()", "b64 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b64, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b64, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b64, ann_key=\"meta_schedule.unroll_explicit\")", "b65, b66 = sch.get_child_blocks(b64)", "l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)", "l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75)", "sch.parallel(loop=l93)", "l94 = sch.fuse(l92)", "sch.vectorize(loop=l94)", "sch.annotate(block_or_loop=l93, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l93, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b66)", "l102 = sch.fuse(l100, l101)", "sch.vectorize(loop=l102)", "sch.annotate(block_or_loop=l95, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l95, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b103 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b103)", "b122 = sch.decompose_reduction(block=b103, loop=l106)"]
[16:26:02] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3
[16:26:02] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14
[16:26:03] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15
[16:26:03] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16
[16:26:03] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_1
[16:26:03] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_4
[16:26:04] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_image_resize2d
[16:26:04] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 256, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 256, 54, 54, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 54, 54, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 53 and 1 <= i3_1 and i3_1 < 53, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 52, 52, 4, 1024, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [64, 256, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:26:04] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:26:04] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17(placeholder: T.Buffer[(1, 256, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 256, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 256, 54, 54, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(128, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_1 in T.serial(2):
                for i1_2_init, i2_2_init, i3_2_init, i4_2_init, i1_3_init in T.grid(2, 26, 13, 2, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 8 * 4 + i1_2_init * 2 + i1_3_init)
                        oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 8 // 4 * 26 + i2_2_init)
                        ow = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 4 * 13 + i3_2_init)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [64, 256, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0 in T.grid(1024, 1, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 28, 13, 1):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(256, i5_0 // 4 + ax1)
                            i2 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 8 // 4 * 26 + ax2)
                            i3 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 4 * 13 + i7_0 + ax3)
                            i4 = T.axis.spatial(4, i5_0 % 4 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 53 and 1 <= i3 and i3 < 53, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 26, 13, 2, 1, 3, 1, 1, 2, 1, 1, 1):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 8 * 4 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 8 // 4 * 26 + i2_2)
                            ow = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 4 * 13 + i3_2)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_1, i7_0])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 52, 52, 4], "float32"], ["TENSOR", [64, 256, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 26, 13):
                    for ax4_fused in T.vectorized(2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 8 * 4 + ax1)
                            ax2_1 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 8 // 4 * 26 + ax2)
                            ax3_1 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 4 * 13 + ax3)
                            ax4 = T.axis.spatial(4, i4_1 * 2 + ax4_fused)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

[16:26:04] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:26:04] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"data_pad\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b2)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSRSRS\")", "l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)", "v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])", "l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])", "v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 16, 2, 2])", "l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])", "v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 26, 1])", "l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])", "v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 13, 1])", "l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])", "v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])", "l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])", "v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1024, 1])", "l54, l55 = sch.split(loop=l9, factors=[v52, v53])", "v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])", "l58, l59 = sch.split(loop=l10, factors=[v56, v57])", "v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])", "l62, l63 = sch.split(loop=l11, factors=[v60, v61])", "sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)", "b64, = sch.get_consumers(block=b1)", "sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.parallel\", ann_val=32)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v65)", "l66 = sch.sample_compute_location(block=b0, decision=12)", "sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)", "sch.enter_postproc()", "b67 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b67, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b67, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b67, ann_key=\"meta_schedule.unroll_explicit\")", "b68, b69, b70 = sch.get_child_blocks(b67)", "l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b68)", "l89 = sch.fuse(l71, l72, l73, l74, l75, l76, l77, l78, l79)", "sch.parallel(loop=l89)", "sch.annotate(block_or_loop=l89, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l89, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b69)", "sch.annotate(block_or_loop=l90, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l90, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b70)", "l115 = sch.fuse(l114)", "sch.vectorize(loop=l115)", "sch.annotate(block_or_loop=l108, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l108, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b116 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b116)", "b135 = sch.decompose_reduction(block=b116, loop=l119)"]
[16:26:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], T_layout_trans: T.Buffer[(1, 128, 52, 52, 2), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 2):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 2 + ax4 < 256 and ax2 < 52 and ax3 < 52, placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4], T.float32(0), dtype="float32")
    

[16:26:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:26:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_layout_transform_2(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], T_layout_trans: T.Buffer[(1, 128, 52, 52, 2), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0_i1_i2_fused in T.parallel(6656):
            for i3 in T.serial(52):
                for i4_fused in T.vectorized(2):
                    with T.block("T_layout_trans"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(128, i0_i1_i2_fused // 52)
                        ax2 = T.axis.spatial(52, i0_i1_i2_fused % 52)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4])
                        T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                        T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 2 + ax4 < 256 and ax2 < 52 and ax3 < 52, placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4], T.float32(0), dtype="float32")
    

[16:26:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:26:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v1)", "sch.enter_postproc()", "b2 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b2, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b2, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b2, ann_key=\"meta_schedule.unroll_explicit\")", "b3, = sch.get_child_blocks(b2)", "l4, l5, l6, l7, l8 = sch.get_loops(block=b3)", "l9 = sch.fuse(l4, l5, l6)", "sch.parallel(loop=l9)", "l10 = sch.fuse(l8)", "sch.vectorize(loop=l10)"]
[16:26:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 2), "float32"], placeholder_1: T.Buffer[(7, 128, 1, 1, 2, 3), "float32"], placeholder_2: T.Buffer[(1, 7, 1, 1, 3), "float32"], T_add: T.Buffer[(1, 7, 52, 52, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 7, 52, 52, 3], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 7, 52, 52, 3, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2], placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 2], "float32"], ["TENSOR", [7, 128, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2] * placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 7, 52, 52, 3):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[16:26:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:26:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_5(placeholder: T.Buffer[(1, 128, 52, 52, 2), "float32"], placeholder_1: T.Buffer[(7, 128, 1, 1, 2, 3), "float32"], placeholder_2: T.Buffer[(1, 7, 1, 1, 3), "float32"], T_add: T.Buffer[(1, 7, 52, 52, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 7, 52, 52, 3], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused in T.parallel(21):
            for i2_3_init, i3_3_init in T.grid(52, 52):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused // 3)
                    oh, ow = T.axis.remap("SS", [i2_3_init, i3_3_init])
                    oc_block = T.axis.spatial(3, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused % 3)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 2], "float32"], ["TENSOR", [7, 128, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 52, 52, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused // 3)
                    oh, ow = T.axis.remap("SS", [i2_3, i3_3])
                    oc_block = T.axis.spatial(3, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused % 3)
                    ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2], placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 2], "float32"], ["TENSOR", [7, 128, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2] * placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 52, 52, 1):
                with T.block("T_add"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused // 3)
                    ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                    ax4_1 = T.axis.spatial(3, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused % 3)
                    T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                    T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

[16:26:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:26:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "b1 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSRSRS\")", "l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)", "v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])", "l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])", "v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])", "l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])", "v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 52])", "l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])", "v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 52])", "l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])", "v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 3, 1, 1])", "l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])", "v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[128, 2])", "l52, l53 = sch.split(loop=l7, factors=[v50, v51])", "v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])", "l56, l57 = sch.split(loop=l8, factors=[v54, v55])", "v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])", "l60, l61 = sch.split(loop=l9, factors=[v58, v59])", "sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)", "b62, = sch.get_consumers(block=b0)", "sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v63)", "sch.enter_postproc()", "b64 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b64, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b64, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b64, ann_key=\"meta_schedule.unroll_explicit\")", "b65, b66 = sch.get_child_blocks(b64)", "l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)", "l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)", "sch.parallel(loop=l93)", "l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b66)", "l100 = sch.fuse(l94)", "sch.parallel(loop=l100)", "b101 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)", "b119 = sch.decompose_reduction(block=b101, loop=l103)"]
https://storage.cloud.google.com/octoml-aquarium-models/onnx_model_zoo/vision_object_detection_segmentation_fcn-resnet50-11.onnx
file existed. Skipping downloading.
/home/yj/models/fcn-resnet50.onnx
Starting to build with relay.
/home/yj/anaconda3/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
The result is correct!
