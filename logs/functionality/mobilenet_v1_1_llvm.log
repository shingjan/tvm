nohup: ignoring input
[23:06:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #0: "fused_layout_transform"
[23:06:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], T_layout_trans: T.Buffer[(1, 1, 224, 224, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 224, 224, 3):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1 * 3 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 224 and ax3 < 224, placeholder[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[23:06:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:06:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], T_layout_trans: T.Buffer[(1, 1, 224, 224, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 1, 224, 224, 3):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1 * 3 + ax4, ax2, ax3])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 224 and ax3 < 224, placeholder[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:06:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"
[23:06:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 225, 225, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 225, 225, 3):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(0 <= i2_1 and i2_1 < 224 and 0 <= i3_1 and i3_1 < 224, placeholder[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 8, 112, 112, 4, 3, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[23:06:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:06:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 1, 225, 225, 3], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1 in T.grid(1, 2, 8, 7, 2, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 2, 8, 2, 3, 3):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 5, 1):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(225, i2_0 * 28 + i2_1 * 4 + i2_2 * 2 + i6_1 + ax2)
                        i3 = T.axis.spatial(225, i3_0 * 32 + i3_2 * 4 + ax3)
                        i4 = T.axis.spatial(3, i5_1 + ax4)
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 224 and 0 <= i3 and i3 < 224, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                for i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 4, 1, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0 * 4 + i1_3)
                        oh = T.axis.spatial(112, i2_0 * 14 + i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(112, i3_0 * 16 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_1, i7_1])
                        T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 1, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[8, 7, 2, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 8, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=19)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[23:06:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 1, 225, 225, 3], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 8, 7, 2, 1, 1, 7, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2 in T.grid(1, 1, 1, 1, 1, 2):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 3, 33, 3):
                        with T.block("data_pad"):
                            i0, i1 = T.axis.remap("SS", [ax0, ax1])
                            i2 = T.axis.spatial(225, i2_0 * 28 + i2_1 * 4 + i2_2 * 2 + ax2)
                            i3 = T.axis.spatial(225, i3_0 * 32 + ax3)
                            i4 = T.axis.spatial(3, ax4)
                            T.reads(placeholder[i0, i1, i2, i3, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 224 and 0 <= i3 and i3 < 224, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                    for i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 2, 3, 3, 3, 1, 4, 1, 2, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i1_0 * 4 + i1_3)
                            oh = T.axis.spatial(112, i2_0 * 14 + i2_1 * 2 + i2_2)
                            ow = T.axis.spatial(112, i3_0 * 16 + i3_2 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                            ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_1, i7_1])
                            T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 2, 16, 2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(8, i1_0 * 4 + ax1)
                        i2 = T.axis.spatial(112, i2_0 * 14 + i2_1 * 2 + ax2)
                        i3 = T.axis.spatial(112, i3_0 * 16 + ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 1, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[8, 7, 2, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 8, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=15)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[23:06:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 1, 225, 225, 3], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 8, 7, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 2, 8, 2):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 3, 5, 3):
                        with T.block("data_pad"):
                            i0, i1 = T.axis.remap("SS", [ax0, ax1])
                            i2 = T.axis.spatial(225, i2_0 * 28 + i2_1 * 4 + i2_2 * 2 + ax2)
                            i3 = T.axis.spatial(225, i3_0 * 32 + i3_2 * 4 + ax3)
                            i4 = T.axis.spatial(3, ax4)
                            T.reads(placeholder[i0, i1, i2, i3, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 224 and 0 <= i3 and i3 < 224, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                    for i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 3, 1, 4, 1, 2, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i1_0 * 4 + i1_3)
                            oh = T.axis.spatial(112, i2_0 * 14 + i2_1 * 2 + i2_2)
                            ow = T.axis.spatial(112, i3_0 * 16 + i3_2 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                            ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_1, i7_1])
                            T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 14, 16, 2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(8, i1_0 * 4 + ax1)
                        i2 = T.axis.spatial(112, i2_0 * 14 + ax2)
                        i3 = T.axis.spatial(112, i3_0 * 16 + ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 1, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[8, 7, 2, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 8, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=17)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[23:06:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"
[23:06:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 114, 114, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 113 and 1 <= i3_1 and i3_1 < 113, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 8, 112, 112, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[23:06:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:06:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1 in T.grid(1, 2, 1, 4, 1, 1, 4, 8, 28):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 16, 3, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(8, i1_0 * 4 + i1_1 + ax1)
                        i2 = T.axis.spatial(114, i2_1 * 14 + ax2)
                        i3 = T.axis.spatial(114, i3_0 * 28 + i3_1 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 3, 1, 1, 14, 1, 1, 3, 1, 1, 1, 1, 1, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(8, i1_0 * 4 + i1_1)
                        oh = T.axis.spatial(112, i2_1 * 14 + i2_2)
                        ow = T.axis.spatial(112, i3_0 * 28 + i3_1)
                        oci, kh, kw = T.axis.remap("SRR", [i4_1, i5_1, i6_0])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 8, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 28, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[23:06:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 4, 1, 1, 4, 8, 28, 4):
                for i5_0, i6_0 in T.grid(1, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 16, 1, 1):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(8, i1_0 * 4 + i1_1 + ax1)
                            i2 = T.axis.spatial(114, i2_1 * 14 + ax2)
                            i3 = T.axis.spatial(114, i3_0 * 28 + i3_1 + i6_0 + ax3)
                            i4 = T.axis.spatial(4, i4_1 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 14, 1, 1, 3, 1, 1, 1, 1, 1, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(8, i1_0 * 4 + i1_1)
                            oh = T.axis.spatial(112, i2_1 * 14 + i2_2)
                            ow = T.axis.spatial(112, i3_0 * 28 + i3_1)
                            oci, kh, kw = T.axis.remap("SRR", [i4_1, i5_1, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 14, 1, 1):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(8, i1_0 * 4 + i1_1 + ax1)
                        i2 = T.axis.spatial(112, i2_1 * 14 + ax2)
                        i3 = T.axis.spatial(112, i3_0 * 28 + i3_1 + ax3)
                        i4 = T.axis.spatial(4, i4_1 + ax4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 8, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 28, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:06:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 4, 1):
                for i0_1, i1_1, i2_1 in T.grid(1, 4, 8):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 16, 30, 4):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(8, i1_0 * 4 + i1_1 + ax1)
                            i2 = T.axis.spatial(114, i2_1 * 14 + ax2)
                            i3 = T.axis.spatial(114, i3_0 * 28 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(28, 4, 1, 3, 1, 1, 14, 1, 1, 3, 1, 1, 1, 1, 1, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(8, i1_0 * 4 + i1_1)
                            oh = T.axis.spatial(112, i2_1 * 14 + i2_2)
                            ow = T.axis.spatial(112, i3_0 * 28 + i3_1)
                            oci, kh, kw = T.axis.remap("SRR", [i4_1, i5_1, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 112, 28, 4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(8, i1_0 * 4 + ax1)
                        i2 = T.axis.spatial(112, ax2)
                        i3 = T.axis.spatial(112, i3_0 * 28 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 8, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 28, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:06:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"
[23:06:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 112, 112, 4, 32, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 112, 112, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 112, 112, 4):
            with T.block("compute"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(compute[i0_1, i1_1, i2_1, i3_1, i4_1])
                compute[i0_1, i1_1, i2_1, i3_1, i4_1] = T.max(T.min(T_add[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(6)), T.float32(0))
    

[23:06:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:06:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 2, 8, 4, 1, 2, 4, 7, 1, 2, 1, 1, 1, 2, 1, 2, 1, 16, 1, 1, 1, 1, 14, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i1_0 * 4 + i1_1 * 2 + i1_2)
                    oh = T.axis.spatial(112, i2_0 * 56 + i2_1 * 14 + i2_3)
                    ow = T.axis.spatial(112, i3_0 * 14 + i3_1 * 2 + i3_2)
                    oc_block = T.axis.spatial(4, i4_0)
                    ic = T.axis.reduce(32, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 112, 112, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 1, 14])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[8, 7, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:06:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 2, 8, 4, 1, 2, 4, 7, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 2, 1, 2, 1, 16, 1, 1, 1, 1, 14, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 4 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(112, i2_0 * 56 + i2_1 * 14 + i2_3)
                        ow = T.axis.spatial(112, i3_0 * 14 + i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(32, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 14, 2, 1):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(16, i1_0 * 4 + i1_1 * 2 + ax1)
                        i2 = T.axis.spatial(112, i2_0 * 56 + i2_1 * 14 + ax2)
                        i3 = T.axis.spatial(112, i3_0 * 14 + i3_1 * 2 + ax3)
                        i4 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 1, 14])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[8, 7, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:06:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 2, 8, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 4, 7, 1, 2, 1, 1, 1, 2, 1, 2, 1, 16, 1, 1, 1, 1, 14, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 4 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(112, i2_0 * 56 + i2_1 * 14 + i2_3)
                        ow = T.axis.spatial(112, i3_0 * 14 + i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(32, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 56, 14, 1):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(16, i1_0 * 4 + ax1)
                        i2 = T.axis.spatial(112, i2_0 * 56 + ax2)
                        i3 = T.axis.spatial(112, i3_0 * 14 + ax3)
                        i4 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 1, 14])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[8, 7, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:06:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"
[23:06:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 16, 113, 113, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 113, 113, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(0 <= i2_1 and i2_1 < 112 and 0 <= i3_1 and i3_1 < 112, placeholder[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 16, 56, 56, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[23:06:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:06:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 16, 113, 113, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0 in T.grid(1, 1, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 29, 113, 4):
                    with T.block("PaddedInput"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(113, i2_0 * 28 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 112 and 0 <= i3 and i3 < 112, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 2, 1, 16, 1, 2, 1, 3, 3, 1, 1, 7, 1, 1, 1, 1, 1, 1, 2, 14, 2):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(16, i1_1)
                        oh = T.axis.spatial(56, i2_0 * 14 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + i3_3)
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 16, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[23:06:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 16, 113, 113, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1 in T.grid(1, 1, 4, 2, 2, 1, 16):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 29, 57, 2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(16, i1_1 + ax1)
                        i2 = T.axis.spatial(113, i2_0 * 28 + ax2)
                        i3 = T.axis.spatial(113, i3_0 * 56 + ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 112 and 0 <= i3 and i3 < 112, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1 in T.grid(1, 2, 1):
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 7, 1, 1, 1, 1, 1, 1, 2, 14, 2):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(16, i1_1)
                            oh = T.axis.spatial(56, i2_0 * 14 + i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + i3_3)
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_3)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 14, 14, 2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(16, i1_1 + ax1)
                            i2 = T.axis.spatial(56, i2_0 * 14 + ax2)
                            i3 = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 16, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:06:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 16, 113, 113, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0 in T.grid(1, 1, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 29, 113, 4):
                    with T.block("PaddedInput"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(113, i2_0 * 28 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 112 and 0 <= i3 and i3 < 112, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                for i3_0, i4_0 in T.grid(2, 2):
                    for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 16, 1, 2, 1, 3, 3, 1, 1, 7, 1, 1, 1, 1, 1, 1, 2, 14, 2):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(16, i1_1)
                            oh = T.axis.spatial(56, i2_0 * 14 + i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + i3_3)
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_3)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 14, 28, 2):
                        with T.block("compute"):
                            i0, i1 = T.axis.remap("SS", [ax0, ax1])
                            i2 = T.axis.spatial(56, i2_0 * 14 + ax2)
                            i3 = T.axis.spatial(56, i3_0 * 28 + ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 16, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:06:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"
[23:06:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 56, 56, 4, 64, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("compute"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(compute[i0_1, i1_1, i2_1, i3_1, i4_1])
                compute[i0_1, i1_1, i2_1, i3_1, i4_1] = T.max(T.min(T_add[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(6)), T.float32(0))
    

[23:06:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:06:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 4, 2, 1, 1, 1, 2, 2, 1, 16, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 16, 7, 14, 4):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_3)
                    oh = T.axis.spatial(56, i2_0 * 14 + i2_1 * 7 + i2_3)
                    ow = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + i3_3)
                    oc_block = T.axis.spatial(4, i4_3)
                    ic = T.axis.reduce(64, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 2, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:06:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 4, 2, 1, 1, 1, 2, 2, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 16, 7, 14, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_3)
                        oh = T.axis.spatial(56, i2_0 * 14 + i2_1 * 7 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(64, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 7, 14, 4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(32, i1_0 * 16 + ax1)
                        i2 = T.axis.spatial(56, i2_0 * 14 + i2_1 * 7 + ax2)
                        i3 = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 2, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:06:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 4, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 2, 1, 16, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 16, 7, 14, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_3)
                        oh = T.axis.spatial(56, i2_0 * 14 + i2_1 * 7 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(64, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 14, 28, 4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(32, i1_0 * 16 + ax1)
                        i2 = T.axis.spatial(56, i2_0 * 14 + ax2)
                        i3 = T.axis.spatial(56, i3_0 * 28 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 2, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:06:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"
[23:06:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 32, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 58, 58, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 56, 56, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[23:06:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:06:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            DepthwiseConv2d = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 2, 1, 2, 1, 2, 4, 7, 2, 1, 3, 1, 1, 1, 1, 1, 3, 1, 1, 2, 7, 8, 1):
                with T.block("DepthwiseConv2d"):
                    b = T.axis.spatial(1, 0)
                    oco = T.axis.spatial(32, i1_0 * 4 + i1_1 * 2 + i1_3)
                    oh = T.axis.spatial(56, i2_0 * 28 + i2_1 * 7 + i2_3)
                    ow = T.axis.spatial(56, i3_1 * 8 + i3_3)
                    oci = T.axis.spatial(4, i4_0 * 2 + i4_1)
                    kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                    T.reads(placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh + kh and oh + kh < 57 and 1 <= ow + kw and ow + kw < 57, placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 4, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 1, 8])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[23:06:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            DepthwiseConv2d = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 2, 1, 2, 1, 2, 4, 7, 2):
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 1, 1, 1, 1, 1, 3, 1, 1, 2, 7, 8, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(32, i1_0 * 4 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(56, i2_0 * 28 + i2_1 * 7 + i2_3)
                        ow = T.axis.spatial(56, i3_1 * 8 + i3_3)
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh + kh and oh + kh < 57 and 1 <= ow + kw and ow + kw < 57, placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 7, 8, 1):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(32, i1_0 * 4 + i1_1 * 2 + ax1)
                        i2 = T.axis.spatial(56, i2_0 * 28 + i2_1 * 7 + ax2)
                        i3 = T.axis.spatial(56, i3_1 * 8 + ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 4, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 1, 8])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:06:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 32, 58, 58, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 8, 2, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0 in T.grid(1, 2, 4, 7, 2, 1, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 9, 8, 1):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(32, i1_0 * 4 + i1_1 * 2 + ax1)
                            i2 = T.axis.spatial(58, i2_0 * 28 + i2_1 * 7 + ax2)
                            i3 = T.axis.spatial(58, i3_1 * 8 + i6_0 + ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 3, 1, 1, 2, 7, 8, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i1_0 * 4 + i1_1 * 2 + i1_3)
                            oh = T.axis.spatial(56, i2_0 * 28 + i2_1 * 7 + i2_3)
                            ow = T.axis.spatial(56, i3_1 * 8 + i3_3)
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_1)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 28, 56, 2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(32, i1_0 * 4 + ax1)
                        i2 = T.axis.spatial(56, i2_0 * 28 + ax2)
                        i3 = T.axis.spatial(56, ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 4, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 1, 8])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:06:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"
[23:06:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 56, 56, 4, 128, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("compute"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(compute[i0_1, i1_1, i2_1, i3_1, i4_1])
                compute[i0_1, i1_1, i2_1, i3_1, i4_1] = T.max(T.min(T_add[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(6)), T.float32(0))
    

[23:06:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:06:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 2, 1, 1, 8, 8, 1, 1, 2, 1, 1, 1, 1, 1, 7, 1, 64, 1, 1, 1, 4, 7, 4, 4):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1 * 4 + i1_3)
                    oh = T.axis.spatial(56, i2_1 * 7 + i2_3)
                    ow = T.axis.spatial(56, i3_0 * 28 + i3_2 * 4 + i3_3)
                    oc_block = T.axis.spatial(4, i4_3)
                    ic = T.axis.reduce(128, i5_0 * 64 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 8, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:06:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 2, 1, 1, 8, 8, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 1, 1, 7, 1, 64, 1, 1, 1, 4, 7, 4, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 4 + i1_3)
                        oh = T.axis.spatial(56, i2_1 * 7 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 28 + i3_2 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(128, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 7, 28, 4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(32, i1_1 * 4 + ax1)
                        i2 = T.axis.spatial(56, i2_1 * 7 + ax2)
                        i3 = T.axis.spatial(56, i3_0 * 28 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 8, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:06:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 8, 1, 1, 2, 1, 1, 1, 1, 1, 7, 1, 64, 1, 1, 1, 4, 7, 4, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 4 + i1_3)
                        oh = T.axis.spatial(56, i2_1 * 7 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 28 + i3_2 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(128, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 56, 28, 4):
                    with T.block("compute"):
                        i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        i3 = T.axis.spatial(56, i3_0 * 28 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 8, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:06:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"
[23:06:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 32, 57, 57, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 57, 57, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(0 <= i2_1 and i2_1 < 56 and 0 <= i3_1 and i3_1 < 56, placeholder[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 28, 28, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[23:06:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:06:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 32, 57, 57, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1 in T.grid(1, 4, 1, 1, 2, 1, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 57, 57, 2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + ax1)
                        i2, i3 = T.axis.remap("SS", [ax2, ax3])
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 56 and 0 <= i3 and i3 < 56, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 1, 1, 1, 2, 7, 2, 3, 3, 1, 2, 7, 4, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_1 * 14 + i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(28, i3_2 * 4 + i3_3)
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 4, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 4])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[23:06:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 32, 57, 57, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1 in T.grid(1, 4, 1, 1, 2, 1, 4, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 29, 57, 2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + ax1)
                        i2 = T.axis.spatial(57, i2_1 * 28 + ax2)
                        i3 = T.axis.spatial(57, ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 56 and 0 <= i3 and i3 < 56, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                for i3_1, i4_1 in T.grid(1, 1):
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 7, 2, 3, 3, 1, 2, 7, 4, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + i1_3)
                            oh = T.axis.spatial(28, i2_1 * 14 + i2_2 * 7 + i2_3)
                            ow = T.axis.spatial(28, i3_2 * 4 + i3_3)
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 14, 28, 2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + ax1)
                            i2 = T.axis.spatial(28, i2_1 * 14 + ax2)
                            i3 = T.axis.spatial(28, ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 4, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 4])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:06:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 32, 57, 57, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 4, 2, 1, 1, 1, 1, 1, 1, 2, 7, 2):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 15, 9, 1):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + ax1)
                            i2 = T.axis.spatial(57, i2_1 * 28 + i2_2 * 14 + ax2)
                            i3 = T.axis.spatial(57, i3_2 * 8 + ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + i4_2 + ax4)
                            T.reads(placeholder[i0, i1, i2, i3, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 56 and 0 <= i3 and i3 < 56, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                    for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 2, 7, 4, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + i1_3)
                            oh = T.axis.spatial(28, i2_1 * 14 + i2_2 * 7 + i2_3)
                            ow = T.axis.spatial(28, i3_2 * 4 + i3_3)
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 28, 28, 2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        i2, i3 = T.axis.remap("SS", [ax2, ax3])
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 4, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 4])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=16)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:06:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"
[23:06:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 28, 28, 4, 128, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [64, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
            with T.block("compute"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(compute[i0_1, i1_1, i2_1, i3_1, i4_1])
                compute[i0_1, i1_1, i2_1, i3_1, i4_1] = T.max(T.min(T_add[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(6)), T.float32(0))
    

[23:06:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:06:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 28, 1, 4, 1, 2, 1, 7, 1, 4, 1, 1, 1, 8, 1, 4, 1, 32, 1, 1, 1, 1, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i1_0 * 16 + i1_1 * 8 + i1_2)
                    oh = T.axis.spatial(28, i2_0)
                    ow = T.axis.spatial(28, i3_1 * 4 + i3_2)
                    oc_block = T.axis.spatial(4, i4_0)
                    ic = T.axis.reduce(128, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [64, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[28, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 4, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:06:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 28, 1, 4, 1, 2, 1, 7, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 1, 1, 8, 1, 4, 1, 32, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 16 + i1_1 * 8 + i1_2)
                        oh = T.axis.spatial(28, i2_0)
                        ow = T.axis.spatial(28, i3_1 * 4 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(128, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [64, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 1, 4, 1):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(64, i1_0 * 16 + i1_1 * 8 + ax1)
                        i2 = T.axis.spatial(28, i2_0 + ax2)
                        i3 = T.axis.spatial(28, i3_1 * 4 + ax3)
                        i4 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[28, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 4, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:06:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 28, 1, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 7, 1, 4, 1, 1, 1, 8, 1, 4, 1, 32, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 16 + i1_1 * 8 + i1_2)
                        oh = T.axis.spatial(28, i2_0)
                        ow = T.axis.spatial(28, i3_1 * 4 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(128, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [64, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 1, 28, 1):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(64, i1_0 * 16 + ax1)
                        i2 = T.axis.spatial(28, i2_0 + ax2)
                        i3 = T.axis.spatial(28, ax3)
                        i4 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[28, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 4, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:06:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"
[23:06:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 64, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 30, 30, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 28, 28, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[23:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 64, 30, 30, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1 in T.grid(1, 16, 1, 1, 1, 1, 2, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 16, 30, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(64, i1_0 * 4 + i1_1 * 2 + ax1)
                        i2 = T.axis.spatial(30, i2_1 * 14 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 1, 1, 7, 7, 2, 3, 3, 1, 2, 2, 4, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(64, i1_0 * 4 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_1 * 14 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(28, i3_2 * 4 + i3_3)
                        oci = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[16, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 4])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[23:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 64, 30, 30, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1 in T.grid(1, 16, 1, 1, 1, 1, 2, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 16, 30, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(64, i1_0 * 4 + i1_1 * 2 + ax1)
                        i2 = T.axis.spatial(30, i2_1 * 14 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_1, i4_1 in T.grid(1, 2):
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 7, 7, 2, 3, 3, 1, 2, 2, 4, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(64, i1_0 * 4 + i1_1 * 2 + i1_3)
                            oh = T.axis.spatial(28, i2_1 * 14 + i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(28, i3_2 * 4 + i3_3)
                            oci = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 14, 28, 2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(64, i1_0 * 4 + i1_1 * 2 + ax1)
                            i2 = T.axis.spatial(28, i2_1 * 14 + ax2)
                            i3 = T.axis.spatial(28, ax3)
                            i4 = T.axis.spatial(4, i4_1 * 2 + ax4)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[16, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 4])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 64, 30, 30, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 16, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1 in T.grid(1, 2, 2, 1, 2, 1, 1, 1, 1, 7, 7, 2, 3, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 2, 4, 1):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(64, i1_0 * 4 + i1_1 * 2 + ax1)
                            i2 = T.axis.spatial(30, i2_1 * 14 + i2_2 * 2 + i5_1 + ax2)
                            i3 = T.axis.spatial(30, i3_2 * 4 + i6_1 + ax3)
                            i4 = T.axis.spatial(4, i4_1 * 2 + i4_2 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 2, 4, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(64, i1_0 * 4 + i1_1 * 2 + i1_3)
                            oh = T.axis.spatial(28, i2_1 * 14 + i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(28, i3_2 * 4 + i3_3)
                            oci = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 28, 28, 4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(64, i1_0 * 4 + ax1)
                        i2, i3, i4 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[16, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 4])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=18)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"
[23:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 28, 28, 4, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
            with T.block("compute"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(compute[i0_1, i1_1, i2_1, i3_1, i4_1])
                compute[i0_1, i1_1, i2_1, i3_1, i4_1] = T.max(T.min(T_add[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(6)), T.float32(0))
    

[23:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 14, 1, 1, 32, 2, 1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 32, 1, 1, 1, 2, 2, 2, 4):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i1_1 * 2 + i1_3)
                    oh = T.axis.spatial(28, i2_0 * 4 + i2_1 * 2 + i2_3)
                    ow = T.axis.spatial(28, i3_0 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_3)
                    ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 32, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 2, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 7, 14, 1, 1, 32, 2, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 1, 1, 1, 1, 32, 1, 1, 1, 2, 2, 2, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_0 * 4 + i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 2, 2, 4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(64, i1_1 * 2 + ax1)
                        i2 = T.axis.spatial(28, i2_0 * 4 + i2_1 * 2 + ax2)
                        i3 = T.axis.spatial(28, i3_0 * 2 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 32, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 2, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 7, 14, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 32, 2, 1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 32, 1, 1, 1, 2, 2, 2, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_0 * 4 + i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 4, 2, 4):
                    with T.block("compute"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(28, i2_0 * 4 + ax2)
                        i3 = T.axis.spatial(28, i3_0 * 2 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 32, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 2, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"
[23:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 64, 29, 29, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 29, 29, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(0 <= i2_1 and i2_1 < 28 and 0 <= i3_1 and i3_1 < 28, placeholder[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 14, 14, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 14, 14, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[23:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 64, 29, 29, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1 in T.grid(1, 8, 2, 1, 1, 1, 1, 1, 14):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 15, 3, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(64, i1_0 * 8 + ax1)
                        i2 = T.axis.spatial(29, i2_0 * 14 + ax2)
                        i3 = T.axis.spatial(29, i3_1 * 2 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 28 and 0 <= i3 and i3 < 28, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                for i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 1, 1, 1, 1, 1, 4, 1, 3, 1, 8, 7, 1, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(64, i1_0 * 8 + i1_3)
                        oh = T.axis.spatial(14, i2_0 * 7 + i2_3)
                        ow, oci, kh, kw = T.axis.remap("SSRR", [i3_1, i4_2, i5_0, i6_1])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 14, 14, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[23:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 64, 29, 29, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 2, 1, 1, 1, 1, 1, 14, 1):
                for i5_0 in T.serial(3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 13, 3, 4):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(64, i1_0 * 8 + ax1)
                            i2 = T.axis.spatial(29, i2_0 * 14 + i5_0 + ax2)
                            i3 = T.axis.spatial(29, i3_1 * 2 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2, i3, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 28 and 0 <= i3 and i3 < 28, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 4, 1, 3, 1, 8, 7, 1, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(64, i1_0 * 8 + i1_3)
                            oh = T.axis.spatial(14, i2_0 * 7 + i2_3)
                            ow, oci, kh, kw = T.axis.remap("SSRR", [i3_1, i4_2, i5_0, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 7, 1, 4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(64, i1_0 * 8 + ax1)
                        i2 = T.axis.spatial(14, i2_0 * 7 + ax2)
                        i3 = T.axis.spatial(14, i3_1 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 64, 29, 29, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 8):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 29, 29, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(64, i1_0 * 8 + ax1)
                        i2, i3, i4 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 28 and 0 <= i3 and i3 < 28, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0 in T.grid(2, 1, 1):
                    for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 14, 1, 3, 1, 1, 1, 1, 1, 4, 1, 3, 1, 8, 7, 1, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(64, i1_0 * 8 + i1_3)
                            oh = T.axis.spatial(14, i2_0 * 7 + i2_3)
                            ow, oci, kh, kw = T.axis.remap("SSRR", [i3_1, i4_2, i5_0, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 7, 14, 4):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(64, i1_0 * 8 + ax1)
                            i2 = T.axis.spatial(14, i2_0 * 7 + ax2)
                            i3, i4 = T.axis.remap("SS", [ax3, ax4])
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"
[23:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 14, 14, 4, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 14, 14, 4):
            with T.block("compute"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(compute[i0_1, i1_1, i2_1, i3_1, i4_1])
                compute[i0_1, i1_1, i2_1, i3_1, i4_1] = T.max(T.min(T_add[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(6)), T.float32(0))
    

[23:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 7, 1, 1, 1, 8, 1, 7, 4, 8, 1, 1, 1, 4, 1, 2, 1, 32, 1, 1, 1, 1, 2, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i1_0 * 32 + i1_1 * 4 + i1_2)
                    oh = T.axis.spatial(14, i2_0 * 2 + i2_3)
                    ow = T.axis.spatial(14, i3_1 * 2 + i3_2)
                    oc_block = T.axis.spatial(4, i4_1)
                    ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 14, 14, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 8, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 7, 1, 1, 1, 8, 1, 7, 4):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 4, 1, 2, 1, 32, 1, 1, 1, 1, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 32 + i1_1 * 4 + i1_2)
                        oh = T.axis.spatial(14, i2_0 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 2, 2, 1):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(128, i1_0 * 32 + i1_1 * 4 + ax1)
                        i2 = T.axis.spatial(14, i2_0 * 2 + ax2)
                        i3 = T.axis.spatial(14, i3_1 * 2 + ax3)
                        i4 = T.axis.spatial(4, i4_1 + ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 8, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 7, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 1, 7, 4, 8, 1, 1, 1, 4, 1, 2, 1, 32, 1, 1, 1, 1, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 32 + i1_1 * 4 + i1_2)
                        oh = T.axis.spatial(14, i2_0 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 2, 14, 4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(128, i1_0 * 32 + ax1)
                        i2 = T.axis.spatial(14, i2_0 * 2 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 8, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"
[23:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 128, 16, 16, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 16, 16, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 14, 14, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[23:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            DepthwiseConv2d = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 16, 2, 7, 1, 3, 3, 1, 4, 1, 1, 4):
                with T.block("DepthwiseConv2d"):
                    b = T.axis.spatial(1, 0)
                    oco = T.axis.spatial(128, i1_1 * 64 + i1_2 * 4 + i1_3)
                    oh = T.axis.spatial(14, i2_0 * 2 + i2_2)
                    ow = T.axis.spatial(14, i3_1 * 7 + i3_2)
                    oci, kh, kw = T.axis.remap("SRR", [i4_3, i5_1, i6_1])
                    T.reads(placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh + kh and oh + kh < 15 and 1 <= ow + kw and ow + kw < 15, placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 14, 14, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 16, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[23:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 128, 16, 16, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1 in T.grid(1, 1, 7, 1, 1, 1, 2, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 4, 9, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(128, i1_1 * 64 + ax1)
                        i2 = T.axis.spatial(16, i2_0 * 2 + ax2)
                        i3 = T.axis.spatial(16, i3_1 * 7 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_1 in T.serial(1):
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 16, 2, 7, 1, 3, 3, 1, 4, 1, 1, 4):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(128, i1_1 * 64 + i1_2 * 4 + i1_3)
                            oh = T.axis.spatial(14, i2_0 * 2 + i2_2)
                            ow = T.axis.spatial(14, i3_1 * 7 + i3_2)
                            oci, kh, kw = T.axis.remap("SRR", [i4_3, i5_1, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 2, 7, 4):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(128, i1_1 * 64 + ax1)
                            i2 = T.axis.spatial(14, i2_0 * 2 + ax2)
                            i3 = T.axis.spatial(14, i3_1 * 7 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 16, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 128, 16, 16, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0 in T.grid(1, 1, 7):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 4, 16, 4):
                    with T.block("PaddedInput"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(16, i2_0 * 2 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_0, i4_0 in T.grid(1, 1):
                    for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 2, 1, 1, 1, 1, 16, 2, 7, 1, 3, 3, 1, 4, 1, 1, 4):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(128, i1_1 * 64 + i1_2 * 4 + i1_3)
                            oh = T.axis.spatial(14, i2_0 * 2 + i2_2)
                            ow = T.axis.spatial(14, i3_1 * 7 + i3_2)
                            oci, kh, kw = T.axis.remap("SRR", [i4_3, i5_1, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 2, 14, 4):
                        with T.block("compute"):
                            i0, i1 = T.axis.remap("SS", [ax0, ax1])
                            i2 = T.axis.spatial(14, i2_0 * 2 + ax2)
                            i3, i4 = T.axis.remap("SS", [ax3, ax4])
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 16, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"
[23:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 14, 14, 4, 512, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 14, 14, 4):
            with T.block("compute"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(compute[i0_1, i1_1, i2_1, i3_1, i4_1])
                compute[i0_1, i1_1, i2_1, i3_1, i4_1] = T.max(T.min(T_add[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(6)), T.float32(0))
    

[23:06:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:06:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 16, 1, 7, 1, 1, 1, 7, 1, 2, 16, 1, 1, 1, 2, 1, 2, 2, 32, 1, 1, 1, 4, 2, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i1_0 * 8 + i1_2 * 4 + i1_3)
                    oh = T.axis.spatial(14, i2_1 * 2 + i2_3)
                    ow = T.axis.spatial(14, i3_0 * 2 + i3_2)
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                    ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 14, 14, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 1, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:06:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 16, 1, 7, 1, 1, 1, 7, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 2, 1, 2, 2, 32, 1, 1, 1, 4, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 8 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_0 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 2, 2, 2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(128, i1_0 * 8 + ax1)
                        i2 = T.axis.spatial(14, i2_1 * 2 + ax2)
                        i3 = T.axis.spatial(14, i3_0 * 2 + ax3)
                        i4 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 1, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:06:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 16, 1, 7, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 1, 2, 16, 1, 1, 1, 2, 1, 2, 2, 32, 1, 1, 1, 4, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 8 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_0 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 14, 2, 4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(128, i1_0 * 8 + ax1)
                        i2 = T.axis.spatial(14, ax2)
                        i3 = T.axis.spatial(14, i3_0 * 2 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 1, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:06:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"
[23:06:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 128, 15, 15, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 15, 15, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(0 <= i2_1 and i2_1 < 14 and 0 <= i3_1 and i3_1 < 14, placeholder[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[23:06:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:06:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 128, 15, 15, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1 in T.grid(1, 16, 7, 1, 2, 1, 1, 1, 7):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 3, 3, 2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(128, i1_0 * 8 + ax1)
                        i2 = T.axis.spatial(15, i2_0 * 2 + ax2)
                        i3 = T.axis.spatial(15, i3_1 * 2 + ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 14 and 0 <= i3 and i3 < 14, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                for i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 1, 1, 4, 1, 1, 1, 1, 3, 1, 2, 1, 1, 2):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(128, i1_0 * 8 + i1_2 * 2 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_0, i3_1])
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[16, 1, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[23:06:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 128, 15, 15, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 16, 7, 1, 2, 1, 1, 1, 7, 1):
                for i5_0 in T.serial(3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 1, 3, 2):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(128, i1_0 * 8 + ax1)
                            i2 = T.axis.spatial(15, i2_0 * 2 + i5_0 + ax2)
                            i3 = T.axis.spatial(15, i3_1 * 2 + ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                            T.reads(placeholder[i0, i1, i2, i3, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 14 and 0 <= i3 and i3 < 14, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 4, 1, 1, 1, 1, 3, 1, 2, 1, 1, 2):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(128, i1_0 * 8 + i1_2 * 2 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_0, i3_1])
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_3)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 1, 1, 2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(128, i1_0 * 8 + ax1)
                        i2 = T.axis.spatial(7, i2_0 + ax2)
                        i3 = T.axis.spatial(7, i3_1 + ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[16, 1, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:06:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 128, 15, 15, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0 in T.grid(1, 16, 7):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 3, 15, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(128, i1_0 * 8 + ax1)
                        i2 = T.axis.spatial(15, i2_0 * 2 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 14 and 0 <= i3 and i3 < 14, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                for i3_0, i4_0 in T.grid(1, 2):
                    for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 7, 1, 3, 1, 1, 4, 1, 1, 1, 1, 3, 1, 2, 1, 1, 2):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(128, i1_0 * 8 + i1_2 * 2 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_0, i3_1])
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_3)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 1, 7, 2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(128, i1_0 * 8 + ax1)
                            i2 = T.axis.spatial(7, i2_0 + ax2)
                            i3 = T.axis.spatial(7, ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[16, 1, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:06:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"
[23:06:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 256, 7, 7, 4, 512, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 7, 7, 4):
            with T.block("compute"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(compute[i0_1, i1_1, i2_1, i3_1, i4_1])
                compute[i0_1, i1_1, i2_1, i3_1, i4_1] = T.max(T.min(T_add[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(6)), T.float32(0))
    

[23:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 1, 4, 7, 1, 4, 32, 1, 1, 1, 2, 1, 7, 1, 16, 1, 1, 1, 32, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(256, i1_1 * 64 + i1_2 * 32 + i1_3)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_2, i4_1])
                    ic = T.axis.reduce(512, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 256, 7, 7, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 2, 32])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 1, 4, 7, 1, 4):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 2, 1, 7, 1, 16, 1, 1, 1, 32, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i1_1 * 64 + i1_2 * 32 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_2, i4_1])
                        ic = T.axis.reduce(512, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 1, 7, 1):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(256, i1_1 * 64 + ax1)
                        i2 = T.axis.spatial(7, i2_1 + ax2)
                        i3 = T.axis.spatial(7, ax3)
                        i4 = T.axis.spatial(4, i4_1 + ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 2, 32])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 7, 1, 4, 32, 1, 1, 1, 2, 1, 7, 1, 16, 1, 1, 1, 32, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i1_1 * 64 + i1_2 * 32 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_2, i4_1])
                        ic = T.axis.reduce(512, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 256, 7, 7, 4):
                    with T.block("compute"):
                        i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 2, 32])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"
[23:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 256, 9, 9, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 9, 9, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3_1 and i3_1 < 8, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 7, 7, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 7, 7, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[23:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 256, 9, 9, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1 in T.grid(1, 2, 1, 7, 1, 1, 4, 1, 1, 4, 1, 1, 1, 2, 1, 1, 1, 3):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 7, 3, 1):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(256, i1_0 * 128 + i1_1 * 32 + i1_2 * 16 + ax1)
                        i2 = T.axis.spatial(9, i5_1 + ax2)
                        i3 = T.axis.spatial(9, i3_0 + ax3)
                        i4 = T.axis.spatial(4, i4_1 + ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 16, 7, 1, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(256, i1_0 * 128 + i1_1 * 32 + i1_2 * 16 + i1_3)
                        oh, ow, oci, kh, kw = T.axis.remap("SSSRR", [i2_3, i3_0, i4_1, i5_1, i6_1])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 256, 7, 7, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 2, 16])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=17)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[23:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 256, 9, 9, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 256, 9, 9, 4):
                with T.block("PaddedInput"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                    PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3_1 and i3_1 < 8, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1 in T.grid(1, 2, 1, 7, 1, 1, 4, 1, 1, 4):
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 2, 1, 1, 1, 3, 3, 1, 16, 7, 1, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(256, i1_0 * 128 + i1_1_1 * 32 + i1_2 * 16 + i1_3)
                        oh, ow, oci, kh, kw = T.axis.remap("SSSRR", [i2_3, i3_0, i4_1_1, i5_1, i6_1])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 7, 1, 1):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(256, i1_0 * 128 + i1_1_1 * 32 + ax1)
                        i2 = T.axis.spatial(7, ax2)
                        i3 = T.axis.spatial(7, i3_0 + ax3)
                        i4 = T.axis.spatial(4, i4_1_1 + ax4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 2, 16])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 256, 9, 9, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 7, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1 in T.grid(1, 4, 1, 1, 4, 1, 1, 1, 2, 1, 1, 1, 3, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 7, 1, 1):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(256, i1_0 * 128 + i1_1 * 32 + i1_2 * 16 + ax1)
                            i2 = T.axis.spatial(9, i5_1 + ax2)
                            i3 = T.axis.spatial(9, i3_0 + i6_1 + ax3)
                            i4 = T.axis.spatial(4, i4_1 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 16, 7, 1, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(256, i1_0 * 128 + i1_1 * 32 + i1_2 * 16 + i1_3)
                            oh, ow, oci, kh, kw = T.axis.remap("SSSRR", [i2_3, i3_0, i4_1, i5_1, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 7, 1, 4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(256, i1_0 * 128 + ax1)
                        i2 = T.axis.spatial(7, ax2)
                        i3 = T.axis.spatial(7, i3_0 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 2, 16])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=18)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"
[23:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 256, 7, 7, 4, 1024, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 7, 7, 4):
            with T.block("compute"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(compute[i0_1, i1_1, i2_1, i3_1, i4_1])
                compute[i0_1, i1_1, i2_1, i3_1, i4_1] = T.max(T.min(T_add[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(6)), T.float32(0))
    

[23:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 1, 4, 1, 1, 1, 256, 1, 1, 1, 4, 1, 7, 1, 4, 1, 1, 1, 16, 7, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(256, i1_1 * 64 + i1_2 * 16 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_3, i3_2])
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                    ic = T.axis.reduce(1024, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 256, 7, 7, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 4, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 2, 1, 4, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(256, 1, 1, 1, 4, 1, 7, 1, 4, 1, 1, 1, 16, 7, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i1_1 * 64 + i1_2 * 16 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_2])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(1024, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 7, 7, 2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(256, i1_1 * 64 + ax1)
                        i2, i3 = T.axis.remap("SS", [ax2, ax3])
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 4, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 1, 1, 256, 1, 1, 1, 4, 1, 7, 1, 4, 1, 1, 1, 16, 7, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i1_1 * 64 + i1_2 * 16 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_2])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(1024, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 256, 7, 7, 2):
                    with T.block("compute"):
                        i0, i1, i2, i3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 4, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #20: "fused_nn_avg_pool2d"
[23:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 256, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 256, 1, 1, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 1, 1, 4, 7, 7):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                T.writes(tensor_1[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] + placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 1, 1, 4):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] / T.cast(T.max((T.min(ax2, 0) * 2 + T.min(ax2 * 2 + 6, 6) + 1 - ax2 * 2) * (T.min(ax3, 0) * 2 + T.min(ax3 * 2 + 6, 6) + 1 - ax3 * 2), 1), "float32")
    

[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 256, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            tensor_1 = T.alloc_buffer([1, 256, 1, 1, 4], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 256, 1, 1, 4, 7], dtype="float32")
            for i0, i1 in T.grid(1, 256):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(7, 1, 1, 1, 1, 4, 7):
                    with T.block("tensor_rf"):
                        vi5_i6_fused_0, ax0_1 = T.axis.remap("SS", [ax0, ax1])
                        ax1_1 = T.axis.spatial(256, i1 + ax2)
                        ax2_1, ax3_1, ax4_1, vi5_i6_fused_1 = T.axis.remap("SSSR", [ax3, ax4, ax5, ax6])
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 * 2 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) // 7, ax3_1 * 2 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) % 7, ax4_1])
                        T.writes(tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0])
                        with T.init():
                            tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0] = T.float32(0)
                        tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0] = tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0] + placeholder[ax0_1, ax1_1, ax2_1 * 2 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) // 7, ax3_1 * 2 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) % 7, ax4_1]
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(7, 1, 1, 1, 1, 4):
                    with T.block("tensor"):
                        vi5_i6_fused_0, ax0_2 = T.axis.remap("RS", [ax0, ax1])
                        ax1_2 = T.axis.spatial(256, i1 + ax2)
                        ax2_2, ax3_2, ax4_2 = T.axis.remap("SSS", [ax3, ax4, ax5])
                        T.reads(tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_0])
                        T.writes(tensor_1[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2])
                        with T.init():
                            tensor_1[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2] = T.float32(0)
                        tensor_1[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2] = tensor_1[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2] + tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_0]
                for i2, i3, i4 in T.grid(1, 1, 4):
                    with T.block("tensor_1"):
                        ax0_3, ax1_3, ax2_3, ax3_3, ax4_3 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(tensor_1[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3])
                        T.writes(tensor[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3])
                        tensor[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3] = tensor_1[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3] / T.cast(T.max((T.min(ax2_3, 0) * 2 + T.min(ax2_3 * 2 + 6, 6) + 1 - ax2_3 * 2) * (T.min(ax3_3, 0) * 2 + T.min(ax3_3 * 2 + 6, 6) + 1 - ax3_3 * 2), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[7, 7])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l12, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
b16, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l17 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b16, decision=1)
sch.compute_at(block=b16, loop=l18, preserve_unit_loops=True)
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 256, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            tensor_1 = T.alloc_buffer([1, 256, 1, 1, 4], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 256, 1, 1, 4, 7], dtype="float32")
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 256, 1, 1, 4, 7, 7):
                with T.block("tensor_rf"):
                    vi5_i6_fused_1 = T.axis.spatial(7, i5_i6_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(256, i1)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    ax4, vi5_i6_fused_0 = T.axis.remap("SR", [i4, i5_i6_fused_0])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) // 7, ax3 * 2 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) % 7, ax4])
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    with T.init():
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.float32(0)
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] + placeholder[ax0, ax1, ax2 * 2 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) // 7, ax3 * 2 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) % 7, ax4]
            for i0, i1, i2, i3, i4 in T.grid(1, 256, 1, 1, 4):
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(7, 1, 1, 1, 1, 1):
                    with T.block("tensor"):
                        vi5_i6_fused_1, ax0_1 = T.axis.remap("RS", [ax0, ax1])
                        ax1_1 = T.axis.spatial(256, i1 + ax2)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax3, ax4])
                        ax4_1 = T.axis.spatial(4, i4 + ax5)
                        T.reads(tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_1])
                        T.writes(tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        with T.init():
                            tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.float32(0)
                        tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_1]
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] / T.cast(T.max((T.min(ax2, 0) * 2 + T.min(ax2 * 2 + 6, 6) + 1 - ax2 * 2) * (T.min(ax3, 0) * 2 + T.min(ax3 * 2 + 6, 6) + 1 - ax3 * 2), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[7, 7])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
b16, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l17 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b16, decision=-1)
sch.compute_at(block=b16, loop=l18, preserve_unit_loops=True)
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 256, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            tensor_1 = T.alloc_buffer([1, 256, 1, 1, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 256, 1, 1, 4):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(1, 1, 1, 1, 1, 7, 7):
                    with T.block("tensor"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(256, i1 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4 + ax4)
                        rv0, rv1 = T.axis.remap("RR", [ax5, ax6])
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 * 2 + rv0, ax3_1 * 2 + rv1, ax4_1])
                        T.writes(tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        with T.init():
                            tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.float32(0)
                        tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder[ax0_1, ax1_1, ax2_1 * 2 + rv0, ax3_1 * 2 + rv1, ax4_1]
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] / T.cast(T.max((T.min(ax2, 0) * 2 + T.min(ax2 * 2 + 6, 6) + 1 - ax2 * 2) * (T.min(ax3, 0) * 2 + T.min(ax3 * 2 + 6, 6) + 1 - ax3 * 2), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #21: "fused_layout_transform_1"
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_layout_trans: T.Buffer[(1, 1024, 1, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 1024, 1, 1, 1):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, (ax1 + ax4) // 4, ax2, ax3, (ax1 + ax4) % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 + ax4 < 1024 and ax2 < 1 and ax3 < 1, placeholder[ax0, (ax1 + ax4) // 4, ax2, ax3, (ax1 + ax4) % 4], T.float32(0), dtype="float32")
    

[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_layout_trans: T.Buffer[(1, 1024, 1, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 1024, 1, 1, 1):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, (ax1 + ax4) // 4, ax2, ax3, (ax1 + ax4) % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 + ax4 < 1024 and ax2 < 1 and ax3 < 1, placeholder[ax0, (ax1 + ax4) // 4, ax2, ax3, (ax1 + ax4) % 4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #22: "fused_nn_contrib_conv2d_NCHWc_add"
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 1, 1, 1), "float32"], placeholder_1: T.Buffer[(1001, 1024, 1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1001, 1, 1, 1), "float32"], T_add: T.Buffer[(1, 1001, 1, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 1001, 1, 1, 1], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 1001, 1, 1, 1, 1024, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic, oh + kh, ow + kw, 0], placeholder_1[oc_chunk, ic, kh, kw, 0, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1024, 1, 1, 1], "float32"], ["TENSOR", [1001, 1024, 1, 1, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW1c", "NCHW1c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic, oh + kh, ow + kw, 0] * placeholder_1[oc_chunk, ic, kh, kw, 0, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 1001, 1, 1, 1):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, ax2, ax3, ax4]
    

[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 1, 1, 1), "float32"], placeholder_1: T.Buffer[(1001, 1024, 1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1001, 1, 1, 1), "float32"], T_add: T.Buffer[(1, 1001, 1, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 1001, 1, 1, 1], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 1, 143, 1, 1, 1, 128, 1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 7, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(1001, i1_1 * 7 + i1_3)
                    oh = T.axis.spatial(1, 0)
                    ow = T.axis.spatial(1, 0)
                    oc_block = T.axis.spatial(1, 0)
                    ic = T.axis.reduce(1024, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic, oh + kh, ow + kw, 0], placeholder_1[oc_chunk, ic, kh, kw, 0, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1024, 1, 1, 1], "float32"], ["TENSOR", [1001, 1024, 1, 1, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW1c", "NCHW1c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic, oh + kh, ow + kw, 0] * placeholder_1[oc_chunk, ic, kh, kw, 0, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 1001, 1, 1, 1):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 143, 1, 7])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[128, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 1, 1, 1), "float32"], placeholder_1: T.Buffer[(1001, 1024, 1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1001, 1, 1, 1), "float32"], T_add: T.Buffer[(1, 1001, 1, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 1001, 1, 1, 1], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 1, 143, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 7, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(1001, i1_1 * 7 + i1_3)
                        oh = T.axis.spatial(1, 0)
                        ow = T.axis.spatial(1, 0)
                        oc_block = T.axis.spatial(1, 0)
                        ic = T.axis.reduce(1024, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic, oh + kh, ow + kw, 0], placeholder_1[oc_chunk, ic, kh, kw, 0, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1024, 1, 1, 1], "float32"], ["TENSOR", [1001, 1024, 1, 1, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW1c", "NCHW1c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic, oh + kh, ow + kw, 0] * placeholder_1[oc_chunk, ic, kh, kw, 0, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 7, 1, 1, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(1001, i1_1 * 7 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 143, 1, 7])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[128, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 1, 1, 1), "float32"], placeholder_1: T.Buffer[(1001, 1024, 1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1001, 1, 1, 1), "float32"], T_add: T.Buffer[(1, 1001, 1, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 1001, 1, 1, 1], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 143, 1, 1, 1, 128, 1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 7, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(1001, i1_1 * 7 + i1_3)
                        oh = T.axis.spatial(1, 0)
                        ow = T.axis.spatial(1, 0)
                        oc_block = T.axis.spatial(1, 0)
                        ic = T.axis.reduce(1024, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic, oh + kh, ow + kw, 0], placeholder_1[oc_chunk, ic, kh, kw, 0, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1024, 1, 1, 1], "float32"], ["TENSOR", [1001, 1024, 1, 1, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW1c", "NCHW1c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic, oh + kh, ow + kw, 0] * placeholder_1[oc_chunk, ic, kh, kw, 0, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1001, 1, 1, 1):
                    with T.block("T_add"):
                        ax0_1, ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 143, 1, 7])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[128, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #23: "fused_layout_transform_reshape_squeeze_reshape"
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001, 1, 1, 1), "float32"], T_reshape: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 1001, 1, 1], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 1, 1, 1001], dtype="float32")
        T_squeeze = T.alloc_buffer([1, 1001], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1001, 1, 1):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3, 0])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 1001 and ax2 < 1 and ax3 < 1, placeholder[ax0, ax1, ax2, ax3, 0], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1, 1, 1001):
            with T.block("T_reshape"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_layout_trans[0, ax3 % 1001, 0, 0])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3])
                T_reshape_1[ax0, ax1, ax2, ax3] = T_layout_trans[0, ax3 % 1001, 0, 0]
        for i0, i1 in T.grid(1, 1001):
            with T.block("T_squeeze"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_reshape_1[ax0, 0, 0, ax1])
                T.writes(T_squeeze[ax0, ax1])
                T_squeeze[ax0, ax1] = T_reshape_1[ax0, 0, 0, ax1]
        for i0, i1 in T.grid(1, 1001):
            with T.block("T_reshape_1"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_squeeze[0, ax1 % 1001])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = T_squeeze[0, ax1 % 1001]
    

[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001, 1, 1, 1), "float32"], T_reshape: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_layout_trans = T.alloc_buffer([1, 1001, 1, 1], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 1001, 1, 1):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, 0])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 1001 and ax2 < 1 and ax3 < 1, placeholder[ax0, ax1, ax2, ax3, 0], T.float32(0), dtype="float32")
            for i0, i1 in T.grid(1, 1001):
                with T.block("T_reshape_1"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(T_layout_trans[0, ax1 % 1001, 0, 0])
                    T.writes(T_reshape[ax0, ax1])
                    T_reshape[ax0, ax1] = T_layout_trans[0, ax1 % 1001 % 1001, 0, 0]
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_squeeze", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l6, preserve_unit_loops=True)
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #24: "fused_nn_softmax"
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_softmax_norm: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
        T_softmax_exp = T.alloc_buffer([1, 1001], dtype="float32")
        T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
        for i0, i1 in T.grid(1, 1001):
            with T.block("T_softmax_maxelem"):
                i0_1, k = T.axis.remap("SR", [i0, i1])
                T.reads(placeholder[i0_1, k])
                T.writes(T_softmax_maxelem[i0_1])
                with T.init():
                    T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
        for i0, i1 in T.grid(1, 1001):
            with T.block("T_softmax_exp"):
                i0_2, i1_1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder[i0_2, i1_1], T_softmax_maxelem[i0_2])
                T.writes(T_softmax_exp[i0_2, i1_1])
                T_softmax_exp[i0_2, i1_1] = T.exp(placeholder[i0_2, i1_1] - T_softmax_maxelem[i0_2], dtype="float32")
        for i0_3, i1 in T.grid(1, 1001):
            with T.block("T_softmax_expsum"):
                i0_4, k = T.axis.remap("SR", [i0_3, i1])
                T.reads(T_softmax_exp[i0_4, k])
                T.writes(T_softmax_expsum[i0_4])
                with T.init():
                    T_softmax_expsum[i0_4] = T.float32(0)
                T_softmax_expsum[i0_4] = T_softmax_expsum[i0_4] + T_softmax_exp[i0_4, k]
        for i0_5, i1 in T.grid(1, 1001):
            with T.block("T_softmax_norm"):
                i0_6, i1_2 = T.axis.remap("SS", [i0_5, i1])
                T.reads(T_softmax_exp[i0_6, i1_2], T_softmax_expsum[i0_6])
                T.writes(T_softmax_norm[i0_6, i1_2])
                T.block_attr({"axis":1})
                T_softmax_norm[i0_6, i1_2] = T_softmax_exp[i0_6, i1_2] / T_softmax_expsum[i0_6]
    

[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 9 design space(s) generated
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_softmax_norm: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_exp = T.alloc_buffer([1, 1001], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum_rf = T.alloc_buffer([1, 91], dtype="float32")
            T_softmax_maxelem_rf = T.alloc_buffer([1, 143], dtype="float32")
            for i0, i1_0, i1_1 in T.grid(1, 143, 7):
                with T.block("T_softmax_maxelem_rf"):
                    vi1_0 = T.axis.spatial(143, i1_0)
                    i0_1 = T.axis.spatial(1, 0)
                    vi1_1 = T.axis.reduce(7, i1_1)
                    T.reads(placeholder[i0_1, vi1_0 * 7 + vi1_1])
                    T.writes(T_softmax_maxelem_rf[i0_1, vi1_0])
                    with T.init():
                        T_softmax_maxelem_rf[i0_1, vi1_0] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem_rf[i0_1, vi1_0] = T.max(T_softmax_maxelem_rf[i0_1, vi1_0], placeholder[i0_1, vi1_0 * 7 + vi1_1])
            for i0, i1_0 in T.grid(1, 143):
                with T.block("T_softmax_maxelem"):
                    vi1_0 = T.axis.reduce(143, i1_0)
                    i0_2 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_maxelem_rf[i0_2, vi1_0])
                    T.writes(T_softmax_maxelem[i0_2])
                    with T.init():
                        T_softmax_maxelem[i0_2] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_2] = T.max(T_softmax_maxelem[i0_2], T_softmax_maxelem_rf[i0_2, vi1_0])
            for i0_3, i1 in T.grid(1, 1001):
                with T.block("T_softmax_exp"):
                    i0_4, i1_2 = T.axis.remap("SS", [i0_3, i1])
                    T.reads(placeholder[i0_4, i1_2], T_softmax_maxelem[i0_4])
                    T.writes(T_softmax_exp[i0_4, i1_2])
                    T_softmax_exp[i0_4, i1_2] = T.exp(placeholder[i0_4, i1_2] - T_softmax_maxelem[i0_4], dtype="float32")
            for i0_5, i1_0, i1_1 in T.grid(1, 91, 11):
                with T.block("T_softmax_expsum_rf"):
                    vi1_0 = T.axis.spatial(91, i1_0)
                    i0_6 = T.axis.spatial(1, 0)
                    vi1_1 = T.axis.reduce(11, i1_1)
                    T.reads(T_softmax_exp[i0_6, vi1_0 * 11 + vi1_1])
                    T.writes(T_softmax_expsum_rf[i0_6, vi1_0])
                    with T.init():
                        T_softmax_expsum_rf[i0_6, vi1_0] = T.float32(0)
                    T_softmax_expsum_rf[i0_6, vi1_0] = T_softmax_expsum_rf[i0_6, vi1_0] + T_softmax_exp[i0_6, vi1_0 * 11 + vi1_1]
            for i0_7, i1_0 in T.grid(1, 91):
                with T.block("T_softmax_expsum"):
                    vi1_0 = T.axis.reduce(91, i1_0)
                    i0_8 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_expsum_rf[i0_8, vi1_0])
                    T.writes(T_softmax_expsum[i0_8])
                    with T.init():
                        T_softmax_expsum[i0_8] = T.float32(0)
                    T_softmax_expsum[i0_8] = T_softmax_expsum[i0_8] + T_softmax_expsum_rf[i0_8, vi1_0]
            for i0_9, i1_3 in T.grid(1, 1001):
                with T.block("T_softmax_norm"):
                    i0_10, i1_4 = T.axis.remap("SS", [i0_9, i1_3])
                    T.reads(T_softmax_exp[i0_10, i1_4], T_softmax_expsum[i0_10])
                    T.writes(T_softmax_norm[i0_10, i1_4])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_10, i1_4] = T_softmax_exp[i0_10, i1_4] / T_softmax_expsum[i0_10]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b2)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[91, 11])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l8, factor_axis=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer", ann_val=1)
l11, l12 = sch.get_loops(block=b0)
v13, v14 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[143, 7])
l15, l16 = sch.split(loop=l12, factors=[v13, v14])
b17 = sch.rfactor(loop=l15, factor_axis=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
b19, = sch.get_producers(block=b2)
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer")
l20 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l20, preserve_unit_loops=True)
l21 = sch.sample_compute_location(block=b19, decision=-1)
sch.compute_at(block=b19, loop=l21, preserve_unit_loops=True)
l22 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l22, preserve_unit_loops=True)
b23, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l24 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l24, preserve_unit_loops=True)
l25 = sch.sample_compute_location(block=b23, decision=-1)
sch.compute_at(block=b23, loop=l25, preserve_unit_loops=True)
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_softmax_norm: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum_rf = T.alloc_buffer([1, 91], dtype="float32")
            T_softmax_maxelem_rf = T.alloc_buffer([1, 7], dtype="float32")
            for i0, i1_0, i1_1 in T.grid(1, 143, 7):
                with T.block("T_softmax_maxelem_rf"):
                    vi1_1 = T.axis.spatial(7, i1_1)
                    i0_1 = T.axis.spatial(1, 0)
                    vi1_0 = T.axis.reduce(143, i1_0)
                    T.reads(placeholder[i0_1, vi1_0 * 7 + vi1_1])
                    T.writes(T_softmax_maxelem_rf[i0_1, vi1_1])
                    with T.init():
                        T_softmax_maxelem_rf[i0_1, vi1_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem_rf[i0_1, vi1_1] = T.max(T_softmax_maxelem_rf[i0_1, vi1_1], placeholder[i0_1, vi1_0 * 7 + vi1_1])
            for i0, i1_1 in T.grid(1, 7):
                with T.block("T_softmax_maxelem"):
                    vi1_1 = T.axis.reduce(7, i1_1)
                    i0_2 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_maxelem_rf[i0_2, vi1_1])
                    T.writes(T_softmax_maxelem[i0_2])
                    with T.init():
                        T_softmax_maxelem[i0_2] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_2] = T.max(T_softmax_maxelem[i0_2], T_softmax_maxelem_rf[i0_2, vi1_1])
            for i0_3, i1_0, i1_1 in T.grid(1, 91, 11):
                with T.block("T_softmax_expsum_rf"):
                    vi1_0 = T.axis.spatial(91, i1_0)
                    i0_4 = T.axis.spatial(1, 0)
                    vi1_1 = T.axis.reduce(11, i1_1)
                    T.reads(placeholder[i0_4, vi1_0 * 11 + vi1_1], T_softmax_maxelem[i0_4])
                    T.writes(T_softmax_expsum_rf[i0_4, vi1_0])
                    with T.init():
                        T_softmax_expsum_rf[i0_4, vi1_0] = T.float32(0)
                    T_softmax_expsum_rf[i0_4, vi1_0] = T_softmax_expsum_rf[i0_4, vi1_0] + T.exp(placeholder[i0_4, vi1_0 * 11 + vi1_1] - T_softmax_maxelem[i0_4], dtype="float32")
            for i0_5, i1 in T.grid(1, 1001):
                for ax0, ax1 in T.grid(91, 1):
                    with T.block("T_softmax_expsum"):
                        vi1_0, i0_6 = T.axis.remap("RS", [ax0, ax1])
                        T.reads(T_softmax_expsum_rf[i0_6, vi1_0])
                        T.writes(T_softmax_expsum[i0_6])
                        with T.init():
                            T_softmax_expsum[i0_6] = T.float32(0)
                        T_softmax_expsum[i0_6] = T_softmax_expsum[i0_6] + T_softmax_expsum_rf[i0_6, vi1_0]
                with T.block("T_softmax_norm"):
                    i0_7, i1_2 = T.axis.remap("SS", [i0_5, i1])
                    T.reads(placeholder[i0_7, i1_2], T_softmax_maxelem[i0_7], T_softmax_expsum[i0_7])
                    T.writes(T_softmax_norm[i0_7, i1_2])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_7, i1_2] = T.exp(placeholder[i0_7, i1_2] - T_softmax_maxelem[i0_7], dtype="float32") / T_softmax_expsum[i0_7]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b2)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[91, 11])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l8, factor_axis=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer", ann_val=1)
l11, l12 = sch.get_loops(block=b0)
v13, v14 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[143, 7])
l15, l16 = sch.split(loop=l12, factors=[v13, v14])
b17 = sch.rfactor(loop=l16, factor_axis=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
b19, = sch.get_producers(block=b2)
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer")
l20 = sch.sample_compute_location(block=b2, decision=1)
sch.compute_at(block=b2, loop=l20, preserve_unit_loops=True)
l21 = sch.sample_compute_location(block=b19, decision=-1)
sch.compute_at(block=b19, loop=l21, preserve_unit_loops=True)
l22 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l22, preserve_unit_loops=True)
b23, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l24 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l24, preserve_unit_loops=True)
l25 = sch.sample_compute_location(block=b23, decision=-1)
sch.compute_at(block=b23, loop=l25, preserve_unit_loops=True)
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_softmax_norm: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_exp = T.alloc_buffer([1, 1001], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum_rf = T.alloc_buffer([1, 91], dtype="float32")
            for i0, i1 in T.grid(1, 1001):
                with T.block("T_softmax_maxelem"):
                    i0_1, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_1, k])
                    T.writes(T_softmax_maxelem[i0_1])
                    with T.init():
                        T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
            for i0, i1 in T.grid(1, 1001):
                with T.block("T_softmax_exp"):
                    i0_2, i1_1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder[i0_2, i1_1], T_softmax_maxelem[i0_2])
                    T.writes(T_softmax_exp[i0_2, i1_1])
                    T_softmax_exp[i0_2, i1_1] = T.exp(placeholder[i0_2, i1_1] - T_softmax_maxelem[i0_2], dtype="float32")
            for i0_3, i1_0, i1_1_1 in T.grid(1, 91, 11):
                with T.block("T_softmax_expsum_rf"):
                    vi1_0 = T.axis.spatial(91, i1_0)
                    i0_4 = T.axis.spatial(1, 0)
                    vi1_1 = T.axis.reduce(11, i1_1_1)
                    T.reads(T_softmax_exp[i0_4, vi1_0 * 11 + vi1_1])
                    T.writes(T_softmax_expsum_rf[i0_4, vi1_0])
                    with T.init():
                        T_softmax_expsum_rf[i0_4, vi1_0] = T.float32(0)
                    T_softmax_expsum_rf[i0_4, vi1_0] = T_softmax_expsum_rf[i0_4, vi1_0] + T_softmax_exp[i0_4, vi1_0 * 11 + vi1_1]
            for i0_5, i1_0 in T.grid(1, 91):
                with T.block("T_softmax_expsum"):
                    vi1_0 = T.axis.reduce(91, i1_0)
                    i0_6 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_expsum_rf[i0_6, vi1_0])
                    T.writes(T_softmax_expsum[i0_6])
                    with T.init():
                        T_softmax_expsum[i0_6] = T.float32(0)
                    T_softmax_expsum[i0_6] = T_softmax_expsum[i0_6] + T_softmax_expsum_rf[i0_6, vi1_0]
            for i0_7, i1 in T.grid(1, 1001):
                with T.block("T_softmax_norm"):
                    i0_8, i1_2 = T.axis.remap("SS", [i0_7, i1])
                    T.reads(T_softmax_exp[i0_8, i1_2], T_softmax_expsum[i0_8])
                    T.writes(T_softmax_norm[i0_8, i1_2])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_8, i1_2] = T_softmax_exp[i0_8, i1_2] / T_softmax_expsum[i0_8]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b2)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[91, 11])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l8, factor_axis=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v11 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v11)
b12, = sch.get_producers(block=b2)
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer")
l13 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l13, preserve_unit_loops=True)
l14 = sch.sample_compute_location(block=b12, decision=-1)
sch.compute_at(block=b12, loop=l14, preserve_unit_loops=True)
l15 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l15, preserve_unit_loops=True)
l16 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l16, preserve_unit_loops=True)
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #3:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_softmax_norm: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_exp = T.alloc_buffer([1, 1001], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum_rf = T.alloc_buffer([1, 11], dtype="float32")
            T_softmax_maxelem_rf = T.alloc_buffer([1, 91], dtype="float32")
            for i0, i1_0, i1_1 in T.grid(1, 91, 11):
                with T.block("T_softmax_maxelem_rf"):
                    vi1_0 = T.axis.spatial(91, i1_0)
                    i0_1 = T.axis.spatial(1, 0)
                    vi1_1 = T.axis.reduce(11, i1_1)
                    T.reads(placeholder[i0_1, vi1_0 * 11 + vi1_1])
                    T.writes(T_softmax_maxelem_rf[i0_1, vi1_0])
                    with T.init():
                        T_softmax_maxelem_rf[i0_1, vi1_0] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem_rf[i0_1, vi1_0] = T.max(T_softmax_maxelem_rf[i0_1, vi1_0], placeholder[i0_1, vi1_0 * 11 + vi1_1])
            for i0, i1 in T.grid(1, 1001):
                for ax0, ax1 in T.grid(91, 1):
                    with T.block("T_softmax_maxelem"):
                        vi1_0, i0_2 = T.axis.remap("RS", [ax0, ax1])
                        T.reads(T_softmax_maxelem_rf[i0_2, vi1_0])
                        T.writes(T_softmax_maxelem[i0_2])
                        with T.init():
                            T_softmax_maxelem[i0_2] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem[i0_2] = T.max(T_softmax_maxelem[i0_2], T_softmax_maxelem_rf[i0_2, vi1_0])
                with T.block("T_softmax_exp"):
                    i0_3, i1_2 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder[i0_3, i1_2], T_softmax_maxelem[i0_3])
                    T.writes(T_softmax_exp[i0_3, i1_2])
                    T_softmax_exp[i0_3, i1_2] = T.exp(placeholder[i0_3, i1_2] - T_softmax_maxelem[i0_3], dtype="float32")
            for i0_4, i1_3 in T.grid(1, 1001):
                for ax0 in T.serial(11):
                    for ax0_1, ax1, ax2 in T.grid(1, 1, 91):
                        with T.block("T_softmax_expsum_rf"):
                            vi1_1 = T.axis.spatial(11, ax0 + ax0_1)
                            i0_5, vi1_0 = T.axis.remap("SR", [ax1, ax2])
                            T.reads(T_softmax_exp[i0_5, vi1_0 * 11 + vi1_1])
                            T.writes(T_softmax_expsum_rf[i0_5, vi1_1])
                            with T.init():
                                T_softmax_expsum_rf[i0_5, vi1_1] = T.float32(0)
                            T_softmax_expsum_rf[i0_5, vi1_1] = T_softmax_expsum_rf[i0_5, vi1_1] + T_softmax_exp[i0_5, vi1_0 * 11 + vi1_1]
                    for ax1 in T.serial(1):
                        with T.block("T_softmax_expsum"):
                            vi1_1, i0_6 = T.axis.remap("RS", [ax0, ax1])
                            T.reads(T_softmax_expsum_rf[i0_6, vi1_1])
                            T.writes(T_softmax_expsum[i0_6])
                            with T.init():
                                T_softmax_expsum[i0_6] = T.float32(0)
                            T_softmax_expsum[i0_6] = T_softmax_expsum[i0_6] + T_softmax_expsum_rf[i0_6, vi1_1]
                with T.block("T_softmax_norm"):
                    i0_7, i1_4 = T.axis.remap("SS", [i0_4, i1_3])
                    T.reads(T_softmax_exp[i0_7, i1_4], T_softmax_expsum[i0_7])
                    T.writes(T_softmax_norm[i0_7, i1_4])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_7, i1_4] = T_softmax_exp[i0_7, i1_4] / T_softmax_expsum[i0_7]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b2)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[91, 11])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l9, factor_axis=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer", ann_val=1)
l11, l12 = sch.get_loops(block=b0)
v13, v14 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[91, 11])
l15, l16 = sch.split(loop=l12, factors=[v13, v14])
b17 = sch.rfactor(loop=l15, factor_axis=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
b19, = sch.get_producers(block=b2)
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer")
l20 = sch.sample_compute_location(block=b2, decision=1)
sch.compute_at(block=b2, loop=l20, preserve_unit_loops=True)
l21 = sch.sample_compute_location(block=b19, decision=2)
sch.compute_at(block=b19, loop=l21, preserve_unit_loops=True)
l22 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l22, preserve_unit_loops=True)
b23, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l24 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l24, preserve_unit_loops=True)
l25 = sch.sample_compute_location(block=b23, decision=-1)
sch.compute_at(block=b23, loop=l25, preserve_unit_loops=True)
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #4:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_softmax_norm: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum_rf = T.alloc_buffer([1, 11], dtype="float32")
            T_softmax_maxelem_rf = T.alloc_buffer([1, 11], dtype="float32")
            for i0, i1_0, i1_1 in T.grid(1, 91, 11):
                with T.block("T_softmax_maxelem_rf"):
                    vi1_1 = T.axis.spatial(11, i1_1)
                    i0_1 = T.axis.spatial(1, 0)
                    vi1_0 = T.axis.reduce(91, i1_0)
                    T.reads(placeholder[i0_1, vi1_0 * 11 + vi1_1])
                    T.writes(T_softmax_maxelem_rf[i0_1, vi1_1])
                    with T.init():
                        T_softmax_maxelem_rf[i0_1, vi1_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem_rf[i0_1, vi1_1] = T.max(T_softmax_maxelem_rf[i0_1, vi1_1], placeholder[i0_1, vi1_0 * 11 + vi1_1])
            for i0, i1_1 in T.grid(1, 11):
                with T.block("T_softmax_maxelem"):
                    vi1_1 = T.axis.reduce(11, i1_1)
                    i0_2 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_maxelem_rf[i0_2, vi1_1])
                    T.writes(T_softmax_maxelem[i0_2])
                    with T.init():
                        T_softmax_maxelem[i0_2] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_2] = T.max(T_softmax_maxelem[i0_2], T_softmax_maxelem_rf[i0_2, vi1_1])
            for i0_3, i1_0, i1_1 in T.grid(1, 91, 11):
                with T.block("T_softmax_expsum_rf"):
                    vi1_1 = T.axis.spatial(11, i1_1)
                    i0_4 = T.axis.spatial(1, 0)
                    vi1_0 = T.axis.reduce(91, i1_0)
                    T.reads(placeholder[i0_4, vi1_0 * 11 + vi1_1], T_softmax_maxelem[i0_4])
                    T.writes(T_softmax_expsum_rf[i0_4, vi1_1])
                    with T.init():
                        T_softmax_expsum_rf[i0_4, vi1_1] = T.float32(0)
                    T_softmax_expsum_rf[i0_4, vi1_1] = T_softmax_expsum_rf[i0_4, vi1_1] + T.exp(placeholder[i0_4, vi1_0 * 11 + vi1_1] - T_softmax_maxelem[i0_4], dtype="float32")
            for i0_5, i1_1 in T.grid(1, 11):
                with T.block("T_softmax_expsum"):
                    vi1_1 = T.axis.reduce(11, i1_1)
                    i0_6 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_expsum_rf[i0_6, vi1_1])
                    T.writes(T_softmax_expsum[i0_6])
                    with T.init():
                        T_softmax_expsum[i0_6] = T.float32(0)
                    T_softmax_expsum[i0_6] = T_softmax_expsum[i0_6] + T_softmax_expsum_rf[i0_6, vi1_1]
            for i0_7, i1 in T.grid(1, 1001):
                with T.block("T_softmax_norm"):
                    i0_8, i1_2 = T.axis.remap("SS", [i0_7, i1])
                    T.reads(placeholder[i0_8, i1_2], T_softmax_maxelem[i0_8], T_softmax_expsum[i0_8])
                    T.writes(T_softmax_norm[i0_8, i1_2])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_8, i1_2] = T.exp(placeholder[i0_8, i1_2] - T_softmax_maxelem[i0_8], dtype="float32") / T_softmax_expsum[i0_8]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b2)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[91, 11])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l9, factor_axis=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer", ann_val=1)
l11, l12 = sch.get_loops(block=b0)
v13, v14 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[91, 11])
l15, l16 = sch.split(loop=l12, factors=[v13, v14])
b17 = sch.rfactor(loop=l16, factor_axis=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
b19, = sch.get_producers(block=b2)
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer")
l20 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l20, preserve_unit_loops=True)
l21 = sch.sample_compute_location(block=b19, decision=-1)
sch.compute_at(block=b19, loop=l21, preserve_unit_loops=True)
l22 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l22, preserve_unit_loops=True)
b23, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l24 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l24, preserve_unit_loops=True)
l25 = sch.sample_compute_location(block=b23, decision=-1)
sch.compute_at(block=b23, loop=l25, preserve_unit_loops=True)
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #5:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_softmax_norm: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum_rf = T.alloc_buffer([1, 11], dtype="float32")
            for i0, i1 in T.grid(1, 1001):
                with T.block("T_softmax_maxelem"):
                    i0_1, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_1, k])
                    T.writes(T_softmax_maxelem[i0_1])
                    with T.init():
                        T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
            for i0, i1_0, i1_1 in T.grid(1, 91, 11):
                with T.block("T_softmax_expsum_rf"):
                    vi1_1 = T.axis.spatial(11, i1_1)
                    i0_2 = T.axis.spatial(1, 0)
                    vi1_0 = T.axis.reduce(91, i1_0)
                    T.reads(placeholder[i0_2, vi1_0 * 11 + vi1_1], T_softmax_maxelem[i0_2])
                    T.writes(T_softmax_expsum_rf[i0_2, vi1_1])
                    with T.init():
                        T_softmax_expsum_rf[i0_2, vi1_1] = T.float32(0)
                    T_softmax_expsum_rf[i0_2, vi1_1] = T_softmax_expsum_rf[i0_2, vi1_1] + T.exp(placeholder[i0_2, vi1_0 * 11 + vi1_1] - T_softmax_maxelem[i0_2], dtype="float32")
            for i0_3, i1_1 in T.grid(1, 11):
                with T.block("T_softmax_expsum"):
                    vi1_1 = T.axis.reduce(11, i1_1)
                    i0_4 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_expsum_rf[i0_4, vi1_1])
                    T.writes(T_softmax_expsum[i0_4])
                    with T.init():
                        T_softmax_expsum[i0_4] = T.float32(0)
                    T_softmax_expsum[i0_4] = T_softmax_expsum[i0_4] + T_softmax_expsum_rf[i0_4, vi1_1]
            for i0_5, i1 in T.grid(1, 1001):
                with T.block("T_softmax_norm"):
                    i0_6, i1_2 = T.axis.remap("SS", [i0_5, i1])
                    T.reads(placeholder[i0_6, i1_2], T_softmax_maxelem[i0_6], T_softmax_expsum[i0_6])
                    T.writes(T_softmax_norm[i0_6, i1_2])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_6, i1_2] = T.exp(placeholder[i0_6, i1_2] - T_softmax_maxelem[i0_6], dtype="float32") / T_softmax_expsum[i0_6]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b2)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[91, 11])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l9, factor_axis=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v11 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v11)
b12, = sch.get_producers(block=b2)
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer")
l13 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l13, preserve_unit_loops=True)
l14 = sch.sample_compute_location(block=b12, decision=-1)
sch.compute_at(block=b12, loop=l14, preserve_unit_loops=True)
l15 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l15, preserve_unit_loops=True)
l16 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l16, preserve_unit_loops=True)
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #6:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_softmax_norm: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_exp = T.alloc_buffer([1, 1001], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_maxelem_rf = T.alloc_buffer([1, 77], dtype="float32")
            for i0, i1 in T.grid(1, 1001):
                for ax0 in T.serial(77):
                    for ax0_1, ax1, ax2 in T.grid(1, 1, 13):
                        with T.block("T_softmax_maxelem_rf"):
                            vi1_0 = T.axis.spatial(77, ax0 + ax0_1)
                            i0_1, vi1_1 = T.axis.remap("SR", [ax1, ax2])
                            T.reads(placeholder[i0_1, vi1_0 * 13 + vi1_1])
                            T.writes(T_softmax_maxelem_rf[i0_1, vi1_0])
                            with T.init():
                                T_softmax_maxelem_rf[i0_1, vi1_0] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_rf[i0_1, vi1_0] = T.max(T_softmax_maxelem_rf[i0_1, vi1_0], placeholder[i0_1, vi1_0 * 13 + vi1_1])
                    for ax1 in T.serial(1):
                        with T.block("T_softmax_maxelem"):
                            vi1_0, i0_2 = T.axis.remap("RS", [ax0, ax1])
                            T.reads(T_softmax_maxelem_rf[i0_2, vi1_0])
                            T.writes(T_softmax_maxelem[i0_2])
                            with T.init():
                                T_softmax_maxelem[i0_2] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem[i0_2] = T.max(T_softmax_maxelem[i0_2], T_softmax_maxelem_rf[i0_2, vi1_0])
                for ax0, ax1 in T.grid(1, 1001):
                    with T.block("T_softmax_exp"):
                        i0_3, i1_1 = T.axis.remap("SS", [ax0, ax1])
                        T.reads(placeholder[i0_3, i1_1], T_softmax_maxelem[i0_3])
                        T.writes(T_softmax_exp[i0_3, i1_1])
                        T_softmax_exp[i0_3, i1_1] = T.exp(placeholder[i0_3, i1_1] - T_softmax_maxelem[i0_3], dtype="float32")
                for ax0, ax1 in T.grid(1, 1001):
                    with T.block("T_softmax_expsum"):
                        i0_4, k = T.axis.remap("SR", [ax0, ax1])
                        T.reads(T_softmax_exp[i0_4, k])
                        T.writes(T_softmax_expsum[i0_4])
                        with T.init():
                            T_softmax_expsum[i0_4] = T.float32(0)
                        T_softmax_expsum[i0_4] = T_softmax_expsum[i0_4] + T_softmax_exp[i0_4, k]
                with T.block("T_softmax_norm"):
                    i0_5, i1_2 = T.axis.remap("SS", [i0, i1])
                    T.reads(T_softmax_exp[i0_5, i1_2], T_softmax_expsum[i0_5])
                    T.writes(T_softmax_norm[i0_5, i1_2])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_5, i1_2] = T_softmax_exp[i0_5, i1_2] / T_softmax_expsum[i0_5]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b0)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[77, 13])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l8, factor_axis=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v11 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v11)
l12 = sch.sample_compute_location(block=b2, decision=1)
sch.compute_at(block=b2, loop=l12, preserve_unit_loops=True)
l13 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l13, preserve_unit_loops=True)
b14, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l15 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l15, preserve_unit_loops=True)
l16 = sch.sample_compute_location(block=b14, decision=2)
sch.compute_at(block=b14, loop=l16, preserve_unit_loops=True)
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #7:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_softmax_norm: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_maxelem_rf = T.alloc_buffer([1, 13], dtype="float32")
            for i0, i1_0, i1_1 in T.grid(1, 77, 13):
                with T.block("T_softmax_maxelem_rf"):
                    vi1_1 = T.axis.spatial(13, i1_1)
                    i0_1 = T.axis.spatial(1, 0)
                    vi1_0 = T.axis.reduce(77, i1_0)
                    T.reads(placeholder[i0_1, vi1_0 * 13 + vi1_1])
                    T.writes(T_softmax_maxelem_rf[i0_1, vi1_1])
                    with T.init():
                        T_softmax_maxelem_rf[i0_1, vi1_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem_rf[i0_1, vi1_1] = T.max(T_softmax_maxelem_rf[i0_1, vi1_1], placeholder[i0_1, vi1_0 * 13 + vi1_1])
            for i0, i1_1 in T.grid(1, 13):
                with T.block("T_softmax_maxelem"):
                    vi1_1 = T.axis.reduce(13, i1_1)
                    i0_2 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_maxelem_rf[i0_2, vi1_1])
                    T.writes(T_softmax_maxelem[i0_2])
                    with T.init():
                        T_softmax_maxelem[i0_2] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_2] = T.max(T_softmax_maxelem[i0_2], T_softmax_maxelem_rf[i0_2, vi1_1])
            for i0_3, i1 in T.grid(1, 1001):
                with T.block("T_softmax_expsum"):
                    i0_4, k = T.axis.remap("SR", [i0_3, i1])
                    T.reads(placeholder[i0_4, k], T_softmax_maxelem[i0_4])
                    T.writes(T_softmax_expsum[i0_4])
                    with T.init():
                        T_softmax_expsum[i0_4] = T.float32(0)
                    T_softmax_expsum[i0_4] = T_softmax_expsum[i0_4] + T.exp(placeholder[i0_4, k] - T_softmax_maxelem[i0_4], dtype="float32")
            for i0_5, i1 in T.grid(1, 1001):
                with T.block("T_softmax_norm"):
                    i0_6, i1_2 = T.axis.remap("SS", [i0_5, i1])
                    T.reads(placeholder[i0_6, i1_2], T_softmax_maxelem[i0_6], T_softmax_expsum[i0_6])
                    T.writes(T_softmax_norm[i0_6, i1_2])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_6, i1_2] = T.exp(placeholder[i0_6, i1_2] - T_softmax_maxelem[i0_6], dtype="float32") / T_softmax_expsum[i0_6]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b0)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[77, 13])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l9, factor_axis=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v11 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v11)
l12 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l12, preserve_unit_loops=True)
l13 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l13, preserve_unit_loops=True)
b14, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l15 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l15, preserve_unit_loops=True)
l16 = sch.sample_compute_location(block=b14, decision=-1)
sch.compute_at(block=b14, loop=l16, preserve_unit_loops=True)
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #8:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_softmax_norm: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            for i0, i1 in T.grid(1, 1001):
                with T.block("T_softmax_maxelem"):
                    i0_1, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_1, k])
                    T.writes(T_softmax_maxelem[i0_1])
                    with T.init():
                        T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
            for i0, i1 in T.grid(1, 1001):
                with T.block("T_softmax_expsum"):
                    i0_2, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_2, k], T_softmax_maxelem[i0_2])
                    T.writes(T_softmax_expsum[i0_2])
                    with T.init():
                        T_softmax_expsum[i0_2] = T.float32(0)
                    T_softmax_expsum[i0_2] = T_softmax_expsum[i0_2] + T.exp(placeholder[i0_2, k] - T_softmax_maxelem[i0_2], dtype="float32")
            for i0_3, i1 in T.grid(1, 1001):
                with T.block("T_softmax_norm"):
                    i0_4, i1_1 = T.axis.remap("SS", [i0_3, i1])
                    T.reads(placeholder[i0_4, i1_1], T_softmax_maxelem[i0_4], T_softmax_expsum[i0_4])
                    T.writes(T_softmax_norm[i0_4, i1_1])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_4, i1_1] = T.exp(placeholder[i0_4, i1_1] - T_softmax_maxelem[i0_4], dtype="float32") / T_softmax_expsum[i0_4]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #25: "fused_reshape"
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_reshape: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1 in T.grid(1, 1001):
            with T.block("T_reshape"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder[0, ax1 % 1001])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = placeholder[0, ax1 % 1001]
    

[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_reshape: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1 in T.grid(1, 1001):
                with T.block("T_reshape"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder[0, ax1 % 1001])
                    T.writes(T_reshape[ax0, ax1])
                    T_reshape[ax0, ax1] = placeholder[0, ax1 % 1001]
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[23:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_layout_transform"
[23:06:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:06:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:06:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[23:06:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[23:06:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[23:06:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[23:06:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[23:06:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[23:06:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 4 candidates:
[1 : 4]:	0.7072  0.4178  0.3237  0.0426
[23:06:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 4 candidate(s) with evolutionary search
[23:06:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 4 candidates(s) for measurement
[23:06:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 4 sample(s) to builder
[23:06:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 4 sample(s) to runner
[23:06:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"
[23:06:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:06:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:07:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627061dd5d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562705a70818)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627056f6fd8)]: 0 failure(s)
[23:07:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[23:07:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627061dd5d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562705a70818)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627056f6fd8)]: 0 failure(s)
[23:07:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627061dd5d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562705a70818)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627056f6fd8)]: 0 failure(s)
[23:08:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627061dd5d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562705a70818)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627056f6fd8)]: 0 failure(s)
[23:09:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627061dd5d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562705a70818)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627056f6fd8)]: 0 failure(s)
[23:09:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9999  0.9997  0.9994  0.9993  0.9991  0.9991  0.9990  0.9989  0.9989  0.9989  0.9988  0.9986  0.9986  0.9984
[17 : 32]:	0.9982  0.9981  0.9981  0.9979  0.9979  0.9979  0.9979  0.9978  0.9978  0.9970  0.9965  0.9965  0.9959  0.9958  0.9958  0.9956
[23:09:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[23:09:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[23:09:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[23:09:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[23:10:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"
[23:10:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:10:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:10:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a828a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061a8c78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627061885e8)]: 0 failure(s)
[23:10:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[23:11:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a828a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061a8c78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627061885e8)]: 0 failure(s)
[23:12:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a828a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061a8c78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627061885e8)]: 0 failure(s)
[23:13:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a828a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061a8c78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627061885e8)]: 0 failure(s)
[23:14:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a828a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061a8c78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627061885e8)]: 0 failure(s)
[23:14:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9995  0.9994  0.9994  0.9993  0.9993  0.9992  0.9990  0.9989  0.9988  0.9987  0.9986  0.9985  0.9985  0.9982
[17 : 32]:	0.9981  0.9981  0.9981  0.9981  0.9979  0.9979  0.9977  0.9976  0.9974  0.9974  0.9972  0.9971  0.9971  0.9971  0.9968  0.9967
[23:14:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[23:14:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[23:14:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[23:14:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[23:14:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"
[23:14:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:14:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:15:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705148b88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706117da8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270625f258)]: 0 failure(s)
[23:15:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[23:15:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705148b88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706117da8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270625f258)]: 0 failure(s)
[23:16:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705148b88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706117da8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270625f258)]: 0 failure(s)
[23:16:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705148b88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706117da8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270625f258)]: 0 failure(s)
[23:17:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705148b88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706117da8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270625f258)]: 0 failure(s)
[23:17:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9999  0.9999  0.9998  0.9997  0.9997  0.9997  0.9994  0.9993  0.9992  0.9992  0.9992  0.9990  0.9989  0.9987
[17 : 32]:	0.9987  0.9986  0.9985  0.9984  0.9983  0.9981  0.9978  0.9977  0.9976  0.9975  0.9971  0.9970  0.9970  0.9968  0.9968  0.9968
[23:17:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[23:17:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[23:17:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[23:18:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[23:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"
[23:18:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:18:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:20:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627061deb88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706127cb8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705138c68)]: 0 failure(s)
[23:20:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[23:22:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627061deb88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706127cb8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705138c68)]: 0 failure(s)
[23:24:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627061deb88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706127cb8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705138c68)]: 0 failure(s)
[23:26:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627061deb88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706127cb8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705138c68)]: 0 failure(s)
[23:27:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627061deb88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706127cb8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705138c68)]: 0 failure(s)
[23:28:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9996  0.9996  0.9995  0.9993  0.9993  0.9992  0.9992  0.9991  0.9991  0.9982  0.9982  0.9980  0.9980  0.9979  0.9976
[17 : 32]:	0.9976  0.9973  0.9971  0.9970  0.9970  0.9970  0.9969  0.9969  0.9966  0.9966  0.9966  0.9964  0.9963  0.9963  0.9961  0.9959
[23:28:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[23:28:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[23:28:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[23:28:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[23:28:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"
[23:28:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:28:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:29:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705aa3148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627060f62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705776f38)]: 0 failure(s)
[23:29:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[23:30:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705aa3148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627060f62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705776f38)]: 0 failure(s)
[23:31:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705aa3148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627060f62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705776f38)]: 0 failure(s)
[23:32:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705aa3148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627060f62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705776f38)]: 0 failure(s)
[23:33:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705aa3148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627060f62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705776f38)]: 0 failure(s)
[23:34:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9998  0.9997  0.9997  0.9996  0.9994  0.9993  0.9993  0.9992  0.9990  0.9988  0.9983  0.9983  0.9980  0.9980
[17 : 32]:	0.9979  0.9978  0.9978  0.9977  0.9976  0.9976  0.9975  0.9975  0.9971  0.9971  0.9969  0.9968  0.9967  0.9967  0.9966  0.9960
[23:34:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[23:34:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[23:34:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[23:35:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[23:35:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"
[23:35:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:35:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:37:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270576d0e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627060ca3a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627061d7708)]: 0 failure(s)
[23:37:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[23:39:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270576d0e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627060ca3a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627061d7708)]: 0 failure(s)
[23:41:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270576d0e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627060ca3a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627061d7708)]: 0 failure(s)
[23:43:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270576d0e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627060ca3a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627061d7708)]: 0 failure(s)
[23:45:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270576d0e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627060ca3a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627061d7708)]: 0 failure(s)
[23:46:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9998  0.9997  0.9997  0.9996  0.9996  0.9993  0.9992  0.9991  0.9991  0.9990  0.9990  0.9989  0.9989  0.9989  0.9989
[17 : 32]:	0.9989  0.9988  0.9986  0.9980  0.9978  0.9977  0.9975  0.9973  0.9970  0.9968  0.9967  0.9966  0.9966  0.9966  0.9966  0.9963
[23:46:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[23:46:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[23:46:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[23:46:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[23:47:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"
[23:47:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:47:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:47:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706123a48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627060df798)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270627aaf8)]: 0 failure(s)
[23:47:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[23:49:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706123a48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627060df798)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270627aaf8)]: 0 failure(s)
[23:50:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706123a48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627060df798)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270627aaf8)]: 0 failure(s)
[23:51:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706123a48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627060df798)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270627aaf8)]: 0 failure(s)
[23:51:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706123a48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627060df798)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270627aaf8)]: 0 failure(s)
[23:52:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9998  0.9998  0.9996  0.9994  0.9994  0.9994  0.9993  0.9991  0.9990  0.9990  0.9989  0.9985  0.9981  0.9980
[17 : 32]:	0.9978  0.9977  0.9977  0.9977  0.9976  0.9976  0.9975  0.9975  0.9974  0.9972  0.9971  0.9971  0.9970  0.9969  0.9969  0.9968
[23:52:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[23:52:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[23:52:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[23:53:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[23:53:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"
[23:53:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:53:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:54:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706278748)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270611eee8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627057622a8)]: 0 failure(s)
[23:54:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[23:57:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706278748)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270611eee8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627057622a8)]: 0 failure(s)
[23:59:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706278748)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270611eee8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627057622a8)]: 0 failure(s)
[00:01:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706278748)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270611eee8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627057622a8)]: 0 failure(s)
[00:03:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706278748)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270611eee8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627057622a8)]: 0 failure(s)
[00:04:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9997  0.9996  0.9994  0.9990  0.9990  0.9988  0.9984  0.9983  0.9982  0.9981  0.9980  0.9977  0.9977  0.9977
[17 : 32]:	0.9976  0.9975  0.9974  0.9973  0.9973  0.9973  0.9972  0.9972  0.9972  0.9970  0.9968  0.9967  0.9967  0.9964  0.9964  0.9963
[00:04:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:04:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:04:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:04:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:05:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"
[00:05:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:05:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:05:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270514ab78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270571fb68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627057511f8)]: 0 failure(s)
[00:05:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:06:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270514ab78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270571fb68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627057511f8)]: 0 failure(s)
[00:07:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270514ab78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270571fb68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627057511f8)]: 0 failure(s)
[00:08:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270514ab78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270571fb68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627057511f8)]: 0 failure(s)
[00:09:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270514ab78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270571fb68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627057511f8)]: 0 failure(s)
[00:09:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9998  0.9997  0.9995  0.9994  0.9993  0.9989  0.9988  0.9988  0.9987  0.9987  0.9986  0.9985  0.9984  0.9984
[17 : 32]:	0.9983  0.9982  0.9981  0.9981  0.9981  0.9979  0.9979  0.9978  0.9974  0.9974  0.9972  0.9971  0.9970  0.9970  0.9968  0.9961
[00:09:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:09:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:09:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:10:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:10:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"
[00:10:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:10:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:12:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627056ed1b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627062fba88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270577ccf8)]: 0 failure(s)
[00:12:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:14:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627056ed1b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627062fba88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270577ccf8)]: 0 failure(s)
[00:16:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627056ed1b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627062fba88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270577ccf8)]: 0 failure(s)
[00:18:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627056ed1b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627062fba88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270577ccf8)]: 0 failure(s)
[00:20:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627056ed1b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627062fba88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270577ccf8)]: 0 failure(s)
[00:20:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9999  0.9999  0.9996  0.9995  0.9995  0.9994  0.9993  0.9993  0.9992  0.9991  0.9991  0.9990  0.9989  0.9986
[17 : 32]:	0.9986  0.9985  0.9984  0.9982  0.9980  0.9980  0.9980  0.9977  0.9976  0.9976  0.9975  0.9974  0.9972  0.9971  0.9970  0.9970
[00:20:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:20:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:21:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:21:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:21:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"
[00:21:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:21:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:22:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a918d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061eadf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562706284948)]: 0 failure(s)
[00:22:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:23:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a918d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061eadf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562706284948)]: 0 failure(s)
[00:24:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a918d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061eadf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562706284948)]: 0 failure(s)
[00:25:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a918d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061eadf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562706284948)]: 0 failure(s)
[00:25:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a918d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061eadf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562706284948)]: 0 failure(s)
[00:26:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9998  0.9997  0.9996  0.9992  0.9990  0.9988  0.9985  0.9985  0.9985  0.9985  0.9985  0.9981  0.9981  0.9981  0.9980
[17 : 32]:	0.9980  0.9979  0.9979  0.9978  0.9975  0.9974  0.9973  0.9972  0.9972  0.9972  0.9970  0.9969  0.9968  0.9966  0.9965  0.9964
[00:26:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:26:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:26:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:27:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:27:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"
[00:27:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:27:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:29:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627061ee358)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627059e7528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627051447b8)]: 0 failure(s)
[00:29:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:31:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627061ee358)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627059e7528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627051447b8)]: 0 failure(s)
[00:33:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627061ee358)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627059e7528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627051447b8)]: 0 failure(s)
[00:35:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627061ee358)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627059e7528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627051447b8)]: 0 failure(s)
[00:37:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627061ee358)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627059e7528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627051447b8)]: 0 failure(s)
[00:37:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9995  0.9995  0.9992  0.9990  0.9989  0.9987  0.9986  0.9985  0.9984  0.9983  0.9981  0.9981  0.9980  0.9977  0.9976
[17 : 32]:	0.9974  0.9973  0.9971  0.9971  0.9965  0.9965  0.9964  0.9964  0.9963  0.9963  0.9963  0.9960  0.9959  0.9959  0.9958  0.9957
[00:38:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:38:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:38:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:38:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:38:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"
[00:38:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:38:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:39:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a0bf28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061136e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627061957b8)]: 0 failure(s)
[00:39:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:40:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a0bf28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061136e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627061957b8)]: 0 failure(s)
[00:41:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a0bf28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061136e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627061957b8)]: 0 failure(s)
[00:42:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a0bf28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061136e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627061957b8)]: 0 failure(s)
[00:43:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a0bf28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061136e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627061957b8)]: 0 failure(s)
[00:43:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9995  0.9995  0.9995  0.9993  0.9991  0.9990  0.9990  0.9987  0.9987  0.9985  0.9984  0.9983  0.9982  0.9982
[17 : 32]:	0.9979  0.9978  0.9978  0.9977  0.9977  0.9977  0.9976  0.9974  0.9974  0.9973  0.9970  0.9970  0.9969  0.9968  0.9967  0.9967
[00:43:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:43:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:43:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:44:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:44:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"
[00:44:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:44:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:45:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627056ecee8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056ebb98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627060f5658)]: 0 failure(s)
[00:45:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:47:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627056ecee8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056ebb98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627060f5658)]: 0 failure(s)
[00:49:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627056ecee8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056ebb98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627060f5658)]: 0 failure(s)
[00:51:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627056ecee8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056ebb98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627060f5658)]: 0 failure(s)
[00:53:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627056ecee8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056ebb98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627060f5658)]: 0 failure(s)
[00:53:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9997  0.9995  0.9994  0.9992  0.9991  0.9990  0.9987  0.9986  0.9986  0.9984  0.9982  0.9981  0.9980  0.9978
[17 : 32]:	0.9977  0.9976  0.9974  0.9974  0.9970  0.9970  0.9968  0.9968  0.9967  0.9966  0.9965  0.9964  0.9964  0.9963  0.9961  0.9957
[00:53:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:53:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:53:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:54:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:54:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"
[00:54:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:54:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:55:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627059e17a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061ded68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270617d1f8)]: 0 failure(s)
[00:55:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:56:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627059e17a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061ded68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270617d1f8)]: 0 failure(s)
[00:57:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627059e17a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061ded68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270617d1f8)]: 0 failure(s)
[00:58:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627059e17a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061ded68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270617d1f8)]: 0 failure(s)
[00:59:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627059e17a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061ded68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270617d1f8)]: 0 failure(s)
[01:00:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9997  0.9994  0.9994  0.9993  0.9992  0.9992  0.9992  0.9990  0.9988  0.9987  0.9986  0.9985  0.9985  0.9983
[17 : 32]:	0.9982  0.9979  0.9979  0.9979  0.9977  0.9976  0.9976  0.9975  0.9973  0.9972  0.9970  0.9970  0.9969  0.9968  0.9966  0.9965
[01:00:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:00:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:00:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:01:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:01:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"
[01:01:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:01:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:03:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270619e378)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270573e3e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705739668)]: 0 failure(s)
[01:03:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:05:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270619e378)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270573e3e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705739668)]: 0 failure(s)
[01:07:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270619e378)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270573e3e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705739668)]: 0 failure(s)
[01:09:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270619e378)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270573e3e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705739668)]: 0 failure(s)
[01:11:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270619e378)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270573e3e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705739668)]: 0 failure(s)
[01:11:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9997  0.9996  0.9995  0.9994  0.9993  0.9992  0.9991  0.9991  0.9990  0.9987  0.9987  0.9985  0.9984  0.9984  0.9983
[17 : 32]:	0.9979  0.9978  0.9976  0.9975  0.9975  0.9975  0.9974  0.9972  0.9966  0.9965  0.9965  0.9964  0.9962  0.9961  0.9960  0.9957
[01:11:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:11:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:11:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:12:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:12:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"
[01:12:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:12:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:13:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627061de6a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270619c9a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270612cd28)]: 0 failure(s)
[01:13:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:14:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627061de6a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270619c9a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270612cd28)]: 0 failure(s)
[01:15:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627061de6a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270619c9a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270612cd28)]: 0 failure(s)
[01:16:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627061de6a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270619c9a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270612cd28)]: 0 failure(s)
[01:17:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627061de6a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270619c9a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270612cd28)]: 0 failure(s)
[01:17:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9997  0.9997  0.9994  0.9994  0.9992  0.9991  0.9990  0.9989  0.9989  0.9987  0.9986  0.9986  0.9985  0.9982  0.9981
[17 : 32]:	0.9980  0.9979  0.9978  0.9977  0.9976  0.9974  0.9970  0.9970  0.9967  0.9965  0.9964  0.9964  0.9963  0.9963  0.9963  0.9961
[01:17:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:17:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:17:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:18:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:18:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"
[01:18:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:18:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:19:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706158448)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270616e758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270579c038)]: 0 failure(s)
[01:19:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:21:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706158448)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270616e758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270579c038)]: 0 failure(s)
[01:23:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706158448)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270616e758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270579c038)]: 0 failure(s)
[01:25:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706158448)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270616e758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270579c038)]: 0 failure(s)
[01:27:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706158448)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56270616e758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270579c038)]: 0 failure(s)
[01:28:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9996  0.9996  0.9995  0.9992  0.9992  0.9990  0.9989  0.9982  0.9980  0.9976  0.9976  0.9975  0.9975  0.9974  0.9970  0.9969
[17 : 32]:	0.9968  0.9968  0.9966  0.9965  0.9965  0.9960  0.9959  0.9959  0.9957  0.9953  0.9953  0.9953  0.9952  0.9951  0.9951  0.9949
[01:28:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:28:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:28:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:28:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:29:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"
[01:29:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:29:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:30:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627057b3718)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061eb328)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705163678)]: 0 failure(s)
[01:30:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:31:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627057b3718)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061eb328)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705163678)]: 0 failure(s)
[01:32:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627057b3718)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061eb328)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705163678)]: 0 failure(s)
[01:33:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627057b3718)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061eb328)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705163678)]: 0 failure(s)
[01:34:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627057b3718)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061eb328)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705163678)]: 0 failure(s)
[01:35:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9997  0.9997  0.9995  0.9993  0.9991  0.9991  0.9991  0.9990  0.9990  0.9989  0.9988  0.9988  0.9986  0.9986  0.9986
[17 : 32]:	0.9986  0.9985  0.9983  0.9981  0.9980  0.9975  0.9974  0.9974  0.9973  0.9973  0.9972  0.9970  0.9970  0.9968  0.9966  0.9964
[01:35:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:35:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:35:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:35:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:35:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #20: "fused_nn_avg_pool2d"
[01:35:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:35:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:36:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705742e68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706de2c88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627060e29e8)]: 0 failure(s)
[01:36:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:36:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705742e68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706de2c88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627060e29e8)]: 0 failure(s)
[01:37:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705742e68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706de2c88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627060e29e8)]: 0 failure(s)
[01:38:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705742e68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706de2c88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627060e29e8)]: 0 failure(s)
[01:39:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705742e68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706de2c88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627060e29e8)]: 0 failure(s)
[01:40:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9970  0.9881  0.9808  0.9755  0.9749  0.9732  0.9724  0.9683  0.9671  0.9653  0.9622  0.9582  0.9567  0.9455  0.9411  0.9394
[17 : 32]:	0.9376  0.9333  0.9309  0.9301  0.9262  0.9202  0.9002  0.8992  0.8929  0.8898  0.8876  0.8644  0.8621  0.8556  0.8546  0.8480
[01:40:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:40:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:40:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:40:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:43:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #21: "fused_layout_transform_1"
[01:43:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:43:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:43:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[01:43:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:43:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[01:43:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[01:44:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[01:44:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[01:44:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 4 candidates:
[1 : 4]:	0.8847  0.7522  0.5735  0.0585
[01:44:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 4 candidate(s) with evolutionary search
[01:44:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 4 candidates(s) for measurement
[01:44:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 4 sample(s) to builder
[01:44:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 4 sample(s) to runner
[01:45:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #22: "fused_nn_contrib_conv2d_NCHWc_add"
[01:45:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:45:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:45:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060d9918)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562705754898)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270576f3f8)]: 0 failure(s)
[01:45:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:46:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060d9918)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562705754898)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270576f3f8)]: 0 failure(s)
[01:47:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060d9918)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562705754898)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270576f3f8)]: 0 failure(s)
[01:48:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060d9918)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562705754898)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270576f3f8)]: 0 failure(s)
[01:49:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060d9918)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562705754898)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270576f3f8)]: 0 failure(s)
[01:49:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9996  0.9995  0.9995  0.9993  0.9992  0.9989  0.9988  0.9987  0.9987  0.9981  0.9981  0.9976  0.9967  0.9963  0.9961  0.9956
[17 : 32]:	0.9952  0.9950  0.9945  0.9941  0.9939  0.9939  0.9931  0.9930  0.9922  0.9922  0.9916  0.9915  0.9912  0.9911  0.9911  0.9908
[01:49:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:49:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:49:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:49:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:50:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_layout_transform_reshape_squeeze_reshape"
[01:50:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:50:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:50:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706118818)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061e6698)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627057a84d8)]: 0 failure(s)
[01:50:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:50:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706118818)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061e6698)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627057a84d8)]: 0 failure(s)
[01:51:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706118818)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061e6698)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627057a84d8)]: 0 failure(s)
[01:51:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706118818)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061e6698)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627057a84d8)]: 0 failure(s)
[01:52:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706118818)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061e6698)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627057a84d8)]: 0 failure(s)
[01:52:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9983  0.9438  0.9200  0.9090  0.8660  0.8104  0.7807  0.6561  0.6540  0.6482  0.6334  0.6170  0.5687  0.5153  0.5015  0.4969
[17 : 32]:	0.4907  0.4865  0.4796  0.4765  0.4754  0.4685  0.4426  0.4335  0.3879  0.3784  0.3117  0.2576  0.2356  0.2030  0.0974  0.0881
[01:52:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:52:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 31 candidates(s) for measurement
[01:52:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 31 sample(s) to builder
[01:52:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 31 sample(s) to runner
[01:57:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #24: "fused_nn_softmax"
[01:57:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:57:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:57:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270619d958)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562705751048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270617df78)]: 0 failure(s)
[01:57:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:58:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270619d958)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562705751048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270617df78)]: 0 failure(s)
[01:58:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270619d958)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562705751048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270617df78)]: 0 failure(s)
[01:59:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270619d958)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562705751048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270617df78)]: 0 failure(s)
[02:00:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270619d958)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562705751048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270617df78)]: 0 failure(s)
[02:00:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9995  0.9986  0.9984  0.9984  0.9981  0.9980  0.9972  0.9972  0.9970  0.9969  0.9968  0.9964  0.9960  0.9960  0.9950
[17 : 32]:	0.9944  0.9935  0.9929  0.9929  0.9927  0.9926  0.9921  0.9917  0.9916  0.9914  0.9909  0.9907  0.9906  0.9904  0.9903  0.9902
[02:00:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:00:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:00:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:00:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:01:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #25: "fused_reshape"
[02:01:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:01:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:01:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:01:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[02:01:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:02:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:02:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:02:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:02:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 4 candidates:
[1 : 4]:	0.7918  0.3156  0.1629  0.0669
[02:02:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 4 candidate(s) with evolutionary search
[02:02:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 4 candidates(s) for measurement
[02:02:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 4 sample(s) to builder
[02:02:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 4 sample(s) to runner
[02:03:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #0: GFLOPs: 0.0000. Time: 0.0048 ms. Best GFLOPs: 0.0000
[02:03:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #1: GFLOPs: 0.0000. Time: 0.0077 ms. Best GFLOPs: 0.0000
[02:03:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #2: GFLOPs: 0.0000. Time: 0.0074 ms. Best GFLOPs: 0.0000
[02:03:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #3: GFLOPs: 0.0000. Time: 0.0073 ms. Best GFLOPs: 0.0000
[02:03:52] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_layout_transform"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 4
Total latency (us): 4.77153

[02:03:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #0: GFLOPs: 3.4262. Time: 6.6780 ms. Best GFLOPs: 3.4262
[02:03:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #1: GFLOPs: 2.4098. Time: 9.4946 ms. Best GFLOPs: 3.4262
[02:03:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #2: GFLOPs: 14.9609. Time: 1.5293 ms. Best GFLOPs: 14.9609
[02:03:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #3: GFLOPs: 10.6297. Time: 2.1525 ms. Best GFLOPs: 14.9609
[02:03:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #4: GFLOPs: 16.7925. Time: 1.3625 ms. Best GFLOPs: 16.7925
[02:03:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #5: GFLOPs: 13.7677. Time: 1.6619 ms. Best GFLOPs: 16.7925
[02:03:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #6: GFLOPs: 10.2104. Time: 2.2409 ms. Best GFLOPs: 16.7925
[02:03:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #7: GFLOPs: 13.3119. Time: 1.7188 ms. Best GFLOPs: 16.7925
[02:03:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #8: GFLOPs: 29.7439. Time: 0.7692 ms. Best GFLOPs: 29.7439
[02:03:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #9: GFLOPs: 7.2465. Time: 3.1574 ms. Best GFLOPs: 29.7439
[02:03:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #10: GFLOPs: 19.0461. Time: 1.2013 ms. Best GFLOPs: 29.7439
[02:03:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #11: GFLOPs: 1.7640. Time: 12.9710 ms. Best GFLOPs: 29.7439
[02:03:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #12: GFLOPs: 4.1803. Time: 5.4733 ms. Best GFLOPs: 29.7439
[02:03:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 225, 225, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(225, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(225):
                for i4_fused in T.vectorized(3):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(1, 0)
                        i2, i3_1, i4 = T.axis.remap("SSS", [i0_i1_i2_fused, i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2, i3_1, i4])
                        T.writes(data_pad[i0, i1, i2, i3_1, i4])
                        data_pad[i0, i1, i2, i3_1, i4] = T.if_then_else(0 <= i2 and i2 < 224 and 0 <= i3_1 and i3_1 < 224, placeholder[i0, i1, i2, i3_1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1_1, i4_1 in T.grid(1, 1, 7, 7, 1):
                for i2_2_init, i3_3_init in T.grid(16, 8):
                    for i4_3_fused_init in T.vectorized(4):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2)
                            oh = T.axis.spatial(112, i2_1 * 16 + i2_2_init)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 56 + i3_1_1 * 8 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 1, 3, 1, 1, 16, 1, 1, 1, 3, 1, 1, 1, 1, 8):
                    for i4_3_fused in T.vectorized(4):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2)
                            oh = T.axis.spatial(112, i2_1 * 16 + i2_2)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 56 + i3_1_1 * 8 + i3_3)
                            oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_3_fused, i5_0, i6_1, i7_0])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 112, 56):
                for ax4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2)
                        i2 = T.axis.spatial(112, ax2)
                        i3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 56 + ax3)
                        i4 = T.axis.spatial(4, ax4_fused)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 1, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 16, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 7, 1, 8])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75 = sch.get_loops(block=b68)
l76 = sch.fuse(l71, l72, l73)
sch.parallel(loop=l76)
l77 = sch.fuse(l75)
sch.vectorize(loop=l77)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b69)
l104 = sch.fuse(l78, l79, l80, l81, l82)
sch.parallel(loop=l104)
l105 = sch.fuse(l103)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l104, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l104, ann_key="pragma_unroll_explicit", ann_val=1)
l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b70)
l112 = sch.fuse(l111)
sch.vectorize(loop=l112)
sch.annotate(block_or_loop=l106, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l106, ann_key="pragma_unroll_explicit", ann_val=1)
b113 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135 = sch.get_loops(block=b113)
b136 = sch.decompose_reduction(block=b113, loop=l120)
[02:04:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #14: GFLOPs: 3.6986. Time: 6.1862 ms. Best GFLOPs: 29.7439
[02:04:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #15: GFLOPs: 4.3340. Time: 5.2792 ms. Best GFLOPs: 29.7439
[02:04:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #16: GFLOPs: 3.6834. Time: 6.2117 ms. Best GFLOPs: 29.7439
[02:04:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #17: GFLOPs: 3.4989. Time: 6.5393 ms. Best GFLOPs: 29.7439
[02:04:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #18: GFLOPs: 2.5043. Time: 9.1364 ms. Best GFLOPs: 29.7439
[02:04:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #19: GFLOPs: 5.5677. Time: 4.1094 ms. Best GFLOPs: 29.7439
[02:04:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #20: GFLOPs: 4.5453. Time: 5.0338 ms. Best GFLOPs: 29.7439
[02:04:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #21: GFLOPs: 6.7858. Time: 3.3718 ms. Best GFLOPs: 29.7439
[02:04:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #22: GFLOPs: 2.4088. Time: 9.4986 ms. Best GFLOPs: 29.7439
[02:04:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #23: GFLOPs: 4.1704. Time: 5.4863 ms. Best GFLOPs: 29.7439
[02:04:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #24: GFLOPs: 2.8413. Time: 8.0529 ms. Best GFLOPs: 29.7439
[02:04:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #25: GFLOPs: 4.7550. Time: 4.8118 ms. Best GFLOPs: 29.7439
[02:04:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #26: GFLOPs: 3.2628. Time: 7.0124 ms. Best GFLOPs: 29.7439
[02:04:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #27: GFLOPs: 5.4677. Time: 4.1846 ms. Best GFLOPs: 29.7439
[02:04:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 225, 225, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 14, 4, 4, 56):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_2_init * 4 + i1_3_init)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 56 + i2_2_init * 4 + i2_3_init)
                        ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 56 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0 in T.grid(3, 3, 3):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 111, 111, 1):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 112 + i6_0 + ax2)
                        i3 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 112 + i7_0 + ax3)
                        i4 = T.axis.spatial(3, i5_0 + ax4)
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 224 and 0 <= i3 and i3 < 224, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 2, 14, 1, 1, 1, 1, 1, 1, 4, 4, 56):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i1_2 * 4 + i1_3)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 56 + i2_2 * 4 + i2_3)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 56 + i3_3)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 2 + i4_3_fused)
                            ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_0])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0_i1_i2_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(112):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(8, i0_i1_i2_fused // 112)
                        i2 = T.axis.spatial(112, i0_i1_i2_fused % 112)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3_1, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_1, i4])
                        compute[i0, i1, i2, i3_1, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3_1, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 14, 4])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 56])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=12)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b67)
l88 = sch.fuse(l70, l71, l72, l73, l74, l75, l76, l77, l78, l79)
sch.parallel(loop=l88)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b68)
l106 = sch.fuse(l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l107, l108, l109, l110, l111 = sch.get_loops(block=b69)
l112 = sch.fuse(l107, l108, l109)
sch.parallel(loop=l112)
l113 = sch.fuse(l111)
sch.vectorize(loop=l113)
sch.annotate(block_or_loop=l112, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l112, ann_key="pragma_unroll_explicit", ann_val=1)
b114 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b114)
b132 = sch.decompose_reduction(block=b114, loop=l116)
[02:04:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #29: GFLOPs: 3.8245. Time: 5.9825 ms. Best GFLOPs: 29.7439
[02:04:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #30: GFLOPs: 6.3099. Time: 3.6261 ms. Best GFLOPs: 29.7439
[02:04:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #31: GFLOPs: 2.8195. Time: 8.1150 ms. Best GFLOPs: 29.7439
[02:04:17] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 36
Total latency (us): 774.015

[02:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #0: GFLOPs: 8.2429. Time: 1.0226 ms. Best GFLOPs: 8.2429
[02:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #1: GFLOPs: 2.4995. Time: 3.3726 ms. Best GFLOPs: 8.2429
[02:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #2: GFLOPs: 6.6282. Time: 1.2718 ms. Best GFLOPs: 8.2429
[02:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #3: GFLOPs: 5.4060. Time: 1.5593 ms. Best GFLOPs: 8.2429
[02:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #4: GFLOPs: 2.6238. Time: 3.2127 ms. Best GFLOPs: 8.2429
[02:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #5: GFLOPs: 4.6790. Time: 1.8016 ms. Best GFLOPs: 8.2429
[02:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #6: GFLOPs: 2.5017. Time: 3.3696 ms. Best GFLOPs: 8.2429
[02:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #7: GFLOPs: 7.6408. Time: 1.1032 ms. Best GFLOPs: 8.2429
[02:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #8: GFLOPs: 2.0674. Time: 4.0773 ms. Best GFLOPs: 8.2429
[02:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #9: GFLOPs: 0.7479. Time: 11.2705 ms. Best GFLOPs: 8.2429
[02:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 2, 1, 1):
                for i1_2_init, i2_2_init, i3_2_init, i4_2_init, i1_3_init, i3_3_init in T.grid(2, 56, 2, 4, 2, 28):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(8, i1_1 * 4 + i1_2_init * 2 + i1_3_init)
                        oh = T.axis.spatial(112, i2_1 * 56 + i2_2_init)
                        ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 56 + i3_2_init * 28 + i3_3_init)
                        oci = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0 in T.serial(3):
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 56, 58):
                        for ax4_fused in T.vectorized(4):
                            with T.block("PaddedInput"):
                                i0 = T.axis.spatial(1, ax0)
                                i1 = T.axis.spatial(8, i1_1 * 4 + ax1)
                                i2 = T.axis.spatial(114, i2_1 * 56 + i5_0 + ax2)
                                i3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 56 + ax3)
                                i4 = T.axis.spatial(4, ax4_fused)
                                T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                T.writes(PaddedInput[i0, i1, i2, i3, i4])
                                PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 2, 56, 2, 4, 1, 1, 1, 2, 1, 28, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(8, i1_1 * 4 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(112, i2_1 * 56 + i2_2)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 56 + i3_2 * 28 + i3_3)
                            oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_0, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 112, 56):
                for ax4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2 = T.axis.remap("SS", [ax1, ax2])
                        i3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 56 + ax3)
                        i4 = T.axis.spatial(4, ax4_fused)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 56, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 28])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b63)
l82 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l82)
l83 = sch.fuse(l81)
sch.vectorize(loop=l83)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l84, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l84, ann_key="pragma_unroll_explicit", ann_val=1)
l104, l105, l106, l107, l108, l109 = sch.get_loops(block=b65)
l110 = sch.fuse(l109)
sch.vectorize(loop=l110)
sch.annotate(block_or_loop=l104, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l104, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b111)
b132 = sch.decompose_reduction(block=b111, loop=l118)
[02:04:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 1):
                for i5_0 in T.serial(1):
                    for i1_2_init, i3_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 2, 2, 2, 28):
                        for i4_3_fused_init in T.vectorized(4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_fused // 112 * 4 + i1_2_init * 2 + i1_3_init)
                                oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 112 // 2 * 2 + i2_3_init)
                                ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i3_2_init * 28 + i3_3_init)
                                oci = T.axis.spatial(4, i4_3_fused_init)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0 in T.serial(3):
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 56):
                            for ax4_fused in T.vectorized(4):
                                with T.block("PaddedInput"):
                                    i0 = T.axis.spatial(1, ax0)
                                    i1 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_fused % 224 // 112 * 4 + ax1)
                                    i2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 112 // 2 * 2 + ax2)
                                    i3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i6_0 + ax3)
                                    i4 = T.axis.spatial(4, ax4_fused)
                                    T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                    T.writes(PaddedInput[i0, i1, i2, i3, i4])
                                    PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                        for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 2, 1, 2, 1, 3, 1, 1, 2, 2, 28):
                            for i4_3_fused in T.vectorized(4):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    oco = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_fused // 112 * 4 + i1_2 * 2 + i1_3)
                                    oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 112 // 2 * 2 + i2_3)
                                    ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i3_2 * 28 + i3_3)
                                    oci, kh, kw = T.axis.remap("SRR", [i4_3_fused, i5_1, i6_0])
                                    T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                    DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 56):
                    for ax4_fused in T.vectorized(4):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_fused // 112 * 4 + ax1)
                            i2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 112 // 2 * 2 + ax2)
                            i3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + ax3)
                            i4 = T.axis.spatial(4, ax4_fused)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[56, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 28])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b63)
l83 = sch.fuse(l66, l67, l68, l69)
sch.parallel(loop=l83)
l84 = sch.fuse(l82)
sch.vectorize(loop=l84)
sch.annotate(block_or_loop=l83, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l83, ann_key="pragma_unroll_explicit", ann_val=1)
l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b64)
l106 = sch.fuse(l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l85, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l85, ann_key="pragma_unroll_explicit", ann_val=1)
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b65)
l119 = sch.fuse(l118)
sch.vectorize(loop=l119)
sch.annotate(block_or_loop=l107, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l107, ann_key="pragma_unroll_explicit", ann_val=1)
b120 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b120)
b142 = sch.decompose_reduction(block=b120, loop=l129)
[02:04:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(28, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 6, 114):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(114, i0_0_i1_0_i2_0_fused % 28 * 4 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 2, 1, 1):
                for i1_2_init, i3_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(4, 4, 2, 2, 2, 28):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(8, i1_2_init * 2 + i1_3_init)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_fused * 4 + i2_1 * 2 + i2_3_init)
                        ow = T.axis.spatial(112, i3_2_init * 28 + i3_3_init)
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 4, 1, 4, 2, 1, 1, 1, 2, 2, 28, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(8, i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_fused * 4 + i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(112, i3_2 * 28 + i3_3)
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(112):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(8, i0_i1_i2_fused // 112)
                        i2 = T.axis.spatial(112, i0_i1_i2_fused % 112)
                        i3_4, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3_4, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_4, i4])
                        compute[i0, i1, i2, i3_4, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3_4, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[28, 2, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 28])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72 = sch.get_loops(block=b62)
l73 = sch.fuse(l65, l66, l67)
sch.parallel(loop=l73)
l74 = sch.fuse(l72)
sch.vectorize(loop=l74)
sch.annotate(block_or_loop=l73, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l73, ann_key="pragma_unroll_explicit", ann_val=1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b63)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b64)
l102 = sch.fuse(l97, l98, l99)
sch.parallel(loop=l102)
l103 = sch.fuse(l101)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b104)
b127 = sch.decompose_reduction(block=b104, loop=l113)
[02:04:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #13: GFLOPs: 2.7291. Time: 3.0888 ms. Best GFLOPs: 8.2429
[02:04:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #14: GFLOPs: 0.9084. Time: 9.2801 ms. Best GFLOPs: 8.2429
[02:04:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #15: GFLOPs: 1.7836. Time: 4.7262 ms. Best GFLOPs: 8.2429
[02:04:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #16: GFLOPs: 10.5664. Time: 0.7978 ms. Best GFLOPs: 10.5664
[02:04:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #17: GFLOPs: 11.3736. Time: 0.7412 ms. Best GFLOPs: 11.3736
[02:04:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #18: GFLOPs: 1.3353. Time: 6.3127 ms. Best GFLOPs: 11.3736
[02:04:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #19: GFLOPs: 1.2645. Time: 6.6661 ms. Best GFLOPs: 11.3736
[02:04:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #20: GFLOPs: 1.2345. Time: 6.8284 ms. Best GFLOPs: 11.3736
[02:04:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #21: GFLOPs: 1.3946. Time: 6.0444 ms. Best GFLOPs: 11.3736
[02:04:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #22: GFLOPs: 19.0070. Time: 0.4435 ms. Best GFLOPs: 19.0070
[02:04:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #23: GFLOPs: 1.8289. Time: 4.6091 ms. Best GFLOPs: 19.0070
[02:04:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #24: GFLOPs: 2.1222. Time: 3.9721 ms. Best GFLOPs: 19.0070
[02:04:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #25: GFLOPs: 1.0650. Time: 7.9151 ms. Best GFLOPs: 19.0070
[02:04:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(196, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0 in T.serial(1):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 1):
                    for i1_2_init, i2_2_init, i3_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(4, 2, 2, 4, 8, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(8, i1_1 * 4 + i1_2_init)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 28 * 16 + i2_2_init * 8 + i2_3_init)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + i3_2_init * 2 + i3_3_init)
                            oci = T.axis.spatial(4, i4_2_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_0 in T.serial(3):
                        for ax0, ax1, ax2 in T.grid(1, 4, 16):
                            for ax3_ax4_fused in T.vectorized(24):
                                with T.block("PaddedInput"):
                                    i0 = T.axis.spatial(1, ax0)
                                    i1 = T.axis.spatial(8, i1_1 * 4 + ax1)
                                    i2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 196 // 28 * 16 + i5_0 + ax2)
                                    i3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + ax3_ax4_fused // 4)
                                    i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                                    T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                    T.writes(PaddedInput[i0, i1, i2, i3, i4])
                                    PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                        for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 4, 2, 2, 4, 1, 1, 1, 1, 8, 2, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(8, i1_1 * 4 + i1_2)
                                oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 28 * 16 + i2_2 * 8 + i2_3)
                                ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + i3_2 * 2 + i3_3)
                                oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_0, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2 in T.grid(1, 8, 16):
                    for ax3_ax4_fused in T.vectorized(16):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(8, ax1)
                            i2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 28 * 16 + ax2)
                            i3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 8])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[28, 1, 2, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b63)
l82 = sch.fuse(l66, l67, l68, l69)
sch.parallel(loop=l82)
l83 = sch.fuse(l80, l81)
sch.vectorize(loop=l83)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l84, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l84, ann_key="pragma_unroll_explicit", ann_val=1)
l105, l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b65)
l112 = sch.fuse(l110, l111)
sch.vectorize(loop=l112)
sch.annotate(block_or_loop=l105, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l105, ann_key="pragma_unroll_explicit", ann_val=1)
b113 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b113)
b135 = sch.decompose_reduction(block=b113, loop=l121)
[02:04:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #27: GFLOPs: 1.1083. Time: 7.6060 ms. Best GFLOPs: 19.0070
[02:04:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #28: GFLOPs: 1.1040. Time: 7.6354 ms. Best GFLOPs: 19.0070
[02:04:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #29: GFLOPs: 1.2828. Time: 6.5714 ms. Best GFLOPs: 19.0070
[02:04:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #30: GFLOPs: 1.3132. Time: 6.4189 ms. Best GFLOPs: 19.0070
[02:04:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #31: GFLOPs: 0.8174. Time: 10.3128 ms. Best GFLOPs: 19.0070
[02:04:32] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 68
Total latency (us): 1217.51

[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #0: GFLOPs: 4.6618. Time: 11.5382 ms. Best GFLOPs: 4.6618
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #1: GFLOPs: 0.9208. Time: 58.4169 ms. Best GFLOPs: 4.6618
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #2: GFLOPs: 2.6882. Time: 20.0089 ms. Best GFLOPs: 4.6618
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 28, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 1, 1, 1, 2, 2):
                    for i1_3_init, i2_3_init in T.grid(2, 4):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 112 * 8 + i1_1 * 2 + i1_3_init)
                            oh = T.axis.spatial(112, i2_1 * 4 + i2_3_init)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 112 // 2 * 2 + i3_2)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_2)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 2, 4, 1, 1):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 112 * 8 + i1_1 * 2 + i1_3)
                            oh = T.axis.spatial(112, i2_1 * 4 + i2_3)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 112 // 2 * 2 + i3_2)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_2)
                            ic = T.axis.reduce(32, i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3 in T.grid(1, 2, 4, 2):
                    for ax4_fused in T.vectorized(2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 112 * 8 + i1_1 * 2 + ax1)
                            i2 = T.axis.spatial(112, i2_1 * 4 + ax2)
                            i3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 112 // 2 * 2 + ax3)
                            i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                            T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 28, 1, 4])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[56, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b67)
l106 = sch.fuse(l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b107 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b107)
b130 = sch.decompose_reduction(block=b107, loop=l122)
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #4: GFLOPs: 1.9779. Time: 27.1945 ms. Best GFLOPs: 4.6618
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #5: GFLOPs: 3.6521. Time: 14.7281 ms. Best GFLOPs: 4.6618
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #6: GFLOPs: 2.4095. Time: 22.3239 ms. Best GFLOPs: 4.6618
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #7: GFLOPs: 4.1120. Time: 13.0810 ms. Best GFLOPs: 4.6618
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #8: GFLOPs: 3.4445. Time: 15.6158 ms. Best GFLOPs: 4.6618
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #9: GFLOPs: 3.2098. Time: 16.7576 ms. Best GFLOPs: 4.6618
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #10: GFLOPs: 0.9039. Time: 59.5052 ms. Best GFLOPs: 4.6618
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 1, 1, 4, 14, 2):
                for i1_3_init, i2_3_init, i3_3_init in T.grid(4, 28, 2):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 4 + i1_3_init)
                            oh = T.axis.spatial(112, i2_2 * 28 + i2_3_init)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 28 + i3_2 * 2 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(32, 1, 1, 1, 4, 28, 2):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 4 + i1_3)
                            oh = T.axis.spatial(112, i2_2 * 28 + i2_3)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 28 + i3_2 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                            ic = T.axis.reduce(32, i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 112, 28):
                for ax4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 4 + ax1)
                        i2 = T.axis.spatial(112, ax2)
                        i3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 28 + ax3)
                        i4 = T.axis.spatial(4, ax4_fused)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 4, 28])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 14, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b103)
b121 = sch.decompose_reduction(block=b103, loop=l113)
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #12: GFLOPs: 6.1940. Time: 8.6840 ms. Best GFLOPs: 6.1940
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(448, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(4, 1, 1, 4, 2, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 1, 1, 7, 1, 1):
                    for i1_3_init, i2_3_init, i3_3_init in T.grid(2, 2, 2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 56 * 2 + i1_3_init)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 56 // 28 * 56 + i2_1 * 14 + i2_2 * 2 + i2_3_init)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + i3_1 * 2 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_0)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 2, 2, 2, 1):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 56 * 2 + i1_3)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 56 // 28 * 56 + i2_1 * 14 + i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + i3_1 * 2 + i3_3)
                            oc_block, ic = T.axis.remap("SR", [i4_0, i5_1])
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 14, 2, 1):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 56 * 2 + ax1)
                        i2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 56 // 28 * 56 + i2_1 * 14 + ax2)
                        i3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + i3_1 * 2 + ax3)
                        i4 = T.axis.spatial(4, i4_0)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[28, 2, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b67)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b107 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b107)
b131 = sch.decompose_reduction(block=b107, loop=l123)
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #14: GFLOPs: 5.6227. Time: 9.5664 ms. Best GFLOPs: 6.1940
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #15: GFLOPs: 6.1024. Time: 8.8144 ms. Best GFLOPs: 6.1940
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 2, 1):
                for i1_2_init, i2_2_init, i3_2_init, i1_3_init, i3_3_init in T.grid(2, 112, 4, 2, 14):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 8 + i1_1 * 4 + i1_2_init * 2 + i1_3_init)
                            oh = T.axis.spatial(112, i2_2_init)
                            ow = T.axis.spatial(112, i3_1 * 56 + i3_2_init * 14 + i3_3_init)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(8, 1, 1, 1, 2, 112, 4, 1, 4, 1, 1, 1, 2, 1, 14):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 8 + i1_1 * 4 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(112, i2_2)
                            ow = T.axis.spatial(112, i3_1 * 56 + i3_2 * 14 + i3_3)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_3_fused)
                            ic = T.axis.reduce(32, i5_0 * 4 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 112, 112):
                for ax4_fused in T.vectorized(2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 8 + ax1)
                        i2, i3 = T.axis.remap("SS", [ax2, ax3])
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 112, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 4, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b103)
b126 = sch.decompose_reduction(block=b103, loop=l110)
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #17: GFLOPs: 3.5990. Time: 14.9455 ms. Best GFLOPs: 6.1940
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_1 in T.serial(1):
                for i1_2_init, i2_2_init, i3_2_init, i4_2_init in T.grid(8, 14, 8, 2):
                    for i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 32 // 16 * 8 + i1_2_init)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 16 // 2 * 14 + i2_2_init)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 32 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + i3_2_init)
                            oc_block = T.axis.spatial(4, i4_2_init * 2 + i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1 in T.grid(16, 1, 1, 1, 8, 14, 8, 2, 2):
                    for i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 32 // 16 * 8 + i1_2)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 16 // 2 * 14 + i2_2)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 32 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + i3_2)
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused)
                            ic = T.axis.reduce(32, i5_0 * 2 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(112):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(16, i0_i1_i2_fused // 112)
                        i2 = T.axis.spatial(112, i0_i1_i2_fused % 112)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3_1, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_1, i4])
                        compute[i0, i1, i2, i3_1, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3_1, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 8, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 8, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l93)
l94 = sch.fuse(l86, l87, l88, l89, l90, l91, l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b102)
b115 = sch.decompose_reduction(block=b102, loop=l105)
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 56, 4, 2, 2, 28):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 4 + i1_2_init * 2 + i1_3_init)
                    oh = T.axis.spatial(112, i2_2_init * 2 + i2_3_init)
                    ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 28 + i3_3_init)
                    oc_block = T.axis.spatial(4, i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 2, 56, 1, 4, 4, 1, 1, 1, 2, 2, 28, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 4 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(112, i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 28 + i3_3)
                    oc_block = T.axis.spatial(4, i4_2)
                    ic = T.axis.reduce(32, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 112, 28):
                for ax4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 4 + ax1)
                        i2 = T.axis.spatial(112, ax2)
                        i3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 28 + ax3)
                        i4 = T.axis.spatial(4, ax4_fused)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 56, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 28])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l104)
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #20: GFLOPs: 2.2008. Time: 24.4408 ms. Best GFLOPs: 6.1940
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #21: GFLOPs: 6.7351. Time: 7.9863 ms. Best GFLOPs: 6.7351
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 1, 2, 28, 4, 2):
                for i2_3_init, i3_3_init in T.grid(4, 28):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i1_2)
                        oh = T.axis.spatial(112, i2_2 * 4 + i2_3_init)
                        ow = T.axis.spatial(112, i3_2 * 28 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 2 + i4_2)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 1, 4, 28, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i1_2)
                        oh = T.axis.spatial(112, i2_2 * 4 + i2_3)
                        ow = T.axis.spatial(112, i3_2 * 28 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 2 + i4_2)
                        ic = T.axis.reduce(32, i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 2, 112, 112):
                for ax4_fused in T.vectorized(2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + ax1)
                        i2, i3 = T.axis.remap("SS", [ax2, ax3])
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 28, 4])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 28])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l112)
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(448, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_1 in T.serial(1):
                for i1_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 4, 4, 2, 28):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 224 * 8 + i1_2_init * 4 + i1_3_init)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 224 // 112 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 112 // 4 * 2 + i2_3_init)
                        ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 4 * 28 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 2, 1, 1, 4, 2, 1, 1, 1, 4, 2, 28, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 224 * 8 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 224 // 112 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 112 // 4 * 2 + i2_3)
                        ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 4 * 28 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(32, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(112):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(16, i0_i1_i2_fused // 112)
                        i2 = T.axis.spatial(112, i0_i1_i2_fused % 112)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3_1, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_1, i4])
                        compute[i0, i1, i2, i3_1, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3_1, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 28, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 28])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b101)
b120 = sch.decompose_reduction(block=b101, loop=l104)
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #24: GFLOPs: 8.5692. Time: 6.2770 ms. Best GFLOPs: 8.5692
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #25: GFLOPs: 6.5920. Time: 8.1597 ms. Best GFLOPs: 8.5692
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #26: GFLOPs: 5.5622. Time: 9.6703 ms. Best GFLOPs: 8.5692
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #27: GFLOPs: 2.7140. Time: 19.8191 ms. Best GFLOPs: 8.5692
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #28: GFLOPs: 3.2609. Time: 16.4951 ms. Best GFLOPs: 8.5692
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(448, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1, i4_1 in T.grid(14, 2):
                for i1_2_init, i3_2_init, i4_2_init, i3_3_init in T.grid(4, 4, 2, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 64 // 16 * 4 + i1_2_init)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 64 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 16)
                        ow = T.axis.spatial(112, i3_1 * 8 + i3_2_init * 2 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 4, 1, 4, 2, 4, 1, 1, 1, 1, 1, 2, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 64 // 16 * 4 + i1_2)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 64 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 16)
                        ow = T.axis.spatial(112, i3_1 * 8 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(32, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(112):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(16, i0_i1_i2_fused // 112)
                        i2 = T.axis.spatial(112, i0_i1_i2_fused % 112)
                        i3_4, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3_4, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_4, i4])
                        compute[i0, i1, i2, i3_4, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3_4, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 16, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 4, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b101)
b121 = sch.decompose_reduction(block=b101, loop=l105)
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #30: GFLOPs: 2.3259. Time: 23.1259 ms. Best GFLOPs: 8.5692
[02:04:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(64, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 2, 2):
                for i2_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(4, 2, 8, 7, 7):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 32 * 8 + i1_3_init)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 32 // 8 * 28 + i2_2_init * 7 + i2_3_init)
                        ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 * 14 + i3_1 * 7 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 1, 4, 1, 2, 4, 1, 1, 1, 8, 7, 7, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 32 * 8 + i1_3)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 32 // 8 * 28 + i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 * 14 + i3_1 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(32, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [16, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 8, 28):
                for ax3_ax4_fused in T.vectorized(56):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 32 * 8 + ax1)
                        i2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 32 // 8 * 28 + ax2)
                        i3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 * 14 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 4, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[8, 2, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l99, l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[02:04:54] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 100
Total latency (us): 7494.49

[02:04:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #0: GFLOPs: 0.3293. Time: 12.7993 ms. Best GFLOPs: 0.3293
[02:04:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #1: GFLOPs: 0.2287. Time: 18.4270 ms. Best GFLOPs: 0.3293
[02:04:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #2: GFLOPs: 0.8918. Time: 4.7263 ms. Best GFLOPs: 0.8918
[02:04:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #3: GFLOPs: 0.4566. Time: 9.2307 ms. Best GFLOPs: 0.8918
[02:04:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 16, 113, 113, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 16, 57, 57):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 56 + ax2)
                        i3 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + ax3)
                        i4 = T.axis.spatial(4, ax4_fused)
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 112 and 0 <= i3 and i3 < 112, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
            for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 14, 1, 2):
                for i1_2_init, i3_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(2, 28, 2, 8, 2):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(16, i1_2_init * 8 + i1_3_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + i2_1 * 2 + i2_3_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i3_2_init)
                        oci = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 2, 1, 28, 2, 1, 3, 1, 8, 2, 1, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(16, i1_2 * 8 + i1_3)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i3_2)
                        oci = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 16, 2, 28):
                    for ax4_fused in T.vectorized(2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(16, ax1)
                            i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + i2_1 * 2 + ax2)
                            i3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ax3)
                            i4 = T.axis.spatial(4, i4_1 * 2 + ax4_fused)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 8])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 14, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 28, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74 = sch.get_loops(block=b63)
l75 = sch.fuse(l66, l67, l68, l69)
sch.parallel(loop=l75)
l76 = sch.fuse(l74)
sch.vectorize(loop=l76)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l77, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l77, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109 = sch.get_loops(block=b65)
l110 = sch.fuse(l109)
sch.vectorize(loop=l110)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b111)
b133 = sch.decompose_reduction(block=b111, loop=l119)
[02:04:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #5: GFLOPs: 0.5896. Time: 7.1487 ms. Best GFLOPs: 0.8918
[02:04:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 16, 113, 113, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 16, 29, 113):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(113, i0_0_i1_0_i2_0_fused % 4 * 28 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 112 and 0 <= i3 and i3 < 112, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
            for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 1, 2, 2, 1):
                for i5_0 in T.serial(1):
                    for i1_2_init, i2_2_init, i3_2_init, i3_3_init in T.grid(16, 7, 7, 4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(16, i1_2_init)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_fused * 14 + i2_1 * 7 + i2_2_init)
                            ow = T.axis.spatial(56, i3_1 * 28 + i3_2_init * 4 + i3_3_init)
                            oci = T.axis.spatial(4, i4_0)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 16, 7, 7, 1, 3, 1, 1, 1, 1, 4, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(16, i1_2)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_fused * 14 + i2_1 * 7 + i2_2)
                            ow = T.axis.spatial(56, i3_1 * 28 + i3_2 * 4 + i3_3)
                            oci, kh, kw = T.axis.remap("SRR", [i4_0, i5_1, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 7, 28, 1):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(16, ax1)
                        i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_fused * 14 + i2_1 * 7 + ax2)
                        i3 = T.axis.spatial(56, i3_1 * 28 + ax3)
                        i4 = T.axis.spatial(4, i4_0)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 16, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 7, 4])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73 = sch.get_loops(block=b63)
l74 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l74)
l75 = sch.fuse(l73)
sch.vectorize(loop=l75)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110 = sch.get_loops(block=b65)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b111)
b134 = sch.decompose_reduction(block=b111, loop=l121)
[02:04:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #7: GFLOPs: 0.3057. Time: 13.7871 ms. Best GFLOPs: 0.8918
[02:04:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 16, 113, 113, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 2, 29, 113):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(16, i0_0_i1_0_i2_0_fused % 32 // 4 * 2 + ax1)
                        i2 = T.axis.spatial(113, i0_0_i1_0_i2_0_fused % 4 * 28 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 112 and 0 <= i3 and i3 < 112, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
            for i3_0, i4_0 in T.grid(7, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 7, 1, 2):
                    for i1_2_init, i2_3_init, i3_3_init in T.grid(2, 2, 8):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(16, i0_0_i1_0_i2_0_fused // 4 * 2 + i1_2_init)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_fused % 4 * 14 + i2_1 * 2 + i2_3_init)
                            ow = T.axis.spatial(56, i3_0 * 8 + i3_3_init)
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_1)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 2, 8, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(16, i0_0_i1_0_i2_0_fused // 4 * 2 + i1_2)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_fused % 4 * 14 + i2_1 * 2 + i2_3)
                            ow = T.axis.spatial(56, i3_0 * 8 + i3_3)
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_1)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 2, 14, 8):
                    for ax4_fused in T.vectorized(2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(16, i0_0_i1_0_i2_0_fused // 4 * 2 + ax1)
                            i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_fused % 4 * 14 + ax2)
                            i3 = T.axis.spatial(56, i3_0 * 8 + ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + ax4_fused)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 7, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 1, 8])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73 = sch.get_loops(block=b63)
l74 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l74)
l75 = sch.fuse(l73)
sch.vectorize(loop=l75)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b65)
l106 = sch.fuse(l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b107 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b107)
b130 = sch.decompose_reduction(block=b107, loop=l116)
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #9: GFLOPs: 0.3178. Time: 13.2632 ms. Best GFLOPs: 0.8918
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 2, 7, 28, 2, 1):
                for i2_2_init, i3_2_init in T.grid(8, 2):
                    for i6_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + i1_1)
                            oh = T.axis.spatial(56, i2_1 * 8 + i2_2_init)
                            ow = T.axis.spatial(56, i3_1 * 2 + i3_2_init)
                            oci = T.axis.spatial(4, i4_1 * 2 + i6_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1 in T.grid(3, 1, 1, 8, 2, 1, 3):
                    for i6_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + i1_1)
                            oh = T.axis.spatial(56, i2_1 * 8 + i2_2)
                            ow = T.axis.spatial(56, i3_1 * 2 + i3_2)
                            oci = T.axis.spatial(4, i4_1 * 2 + i6_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(0 <= oh * 2 + kh and oh * 2 + kh < 112 and 0 <= ow * 2 + kw and ow * 2 + kw < 112, placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 2, 56, 56):
                for ax4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + ax1)
                        i2, i3, i4 = T.axis.remap("SSS", [ax2, ax3, ax4_fused])
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 2, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 8, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 28, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69)
sch.parallel(loop=l89)
l90 = sch.fuse(l83, l84, l85, l86, l87, l88)
sch.vectorize(loop=l90)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l96)
sch.vectorize(loop=l97)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
b98 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b98)
b114 = sch.decompose_reduction(block=b98, loop=l106)
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 16, 113, 113, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(14, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 16, 9, 113):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(113, i0_0_i1_0_i2_0_fused % 14 * 8 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 112 and 0 <= i3 and i3 < 112, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
            for i3_0, i4_0 in T.grid(2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 2, 2, 1):
                    for i1_2_init, i2_2_init, i3_2_init, i4_2_init, i1_3_init in T.grid(2, 2, 14, 2, 4):
                        for i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(16, i1_1 * 8 + i1_2_init * 4 + i1_3_init)
                                oh = T.axis.spatial(56, i0_0_i1_0_i2_0_fused * 4 + i2_1 * 2 + i2_2_init)
                                ow = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + i3_2_init)
                                oci = T.axis.spatial(4, i4_2_init * 2 + i2_3_i3_3_i4_3_fused_init)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3 in T.grid(3, 3, 1, 2, 2, 14, 2, 1, 1, 1, 4):
                        for i2_3_i3_3_i4_3_fused in T.vectorized(2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(16, i1_1 * 8 + i1_2 * 4 + i1_3)
                                oh = T.axis.spatial(56, i0_0_i1_0_i2_0_fused * 4 + i2_1 * 2 + i2_2)
                                ow = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + i3_2)
                                oci = T.axis.spatial(4, i4_2 * 2 + i2_3_i3_3_i4_3_fused)
                                kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 16, 4, 28):
                    for ax4_fused in T.vectorized(4):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(16, ax1)
                            i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_fused * 4 + ax2)
                            i3 = T.axis.spatial(56, i3_0 * 28 + ax3)
                            i4 = T.axis.spatial(4, ax4_fused)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 2, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 14, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73 = sch.get_loops(block=b63)
l74 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l74)
l75 = sch.fuse(l73)
sch.vectorize(loop=l75)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
l98 = sch.fuse(l95, l96, l97)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b65)
l107 = sch.fuse(l106)
sch.vectorize(loop=l107)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b108 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b108)
b129 = sch.decompose_reduction(block=b108, loop=l117)
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 16, 113, 113, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1 in T.grid(1, 2):
                for ax0, ax1, ax2, ax3 in T.grid(1, 8, 57, 113):
                    for ax4_fused in T.vectorized(2):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(16, i1_1 * 8 + ax1)
                            i2 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 56 + ax2)
                            i3 = T.axis.spatial(113, ax3)
                            i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                            T.reads(placeholder[i0, i1, i2, i3, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 112 and 0 <= i3 and i3 < 112, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(2, 1, 1, 1, 1, 1, 2, 1, 2, 2):
                    for i1_3_init, i2_3_init, i3_3_init in T.grid(4, 14, 28):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(16, i1_1 * 8 + i1_2 * 4 + i1_3_init)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 28 + i2_1 * 14 + i2_3_init)
                            ow = T.axis.spatial(56, i3_2 * 28 + i3_3_init)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_2)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 4, 14, 28, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(16, i1_1 * 8 + i1_2 * 4 + i1_3)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 28 + i2_1 * 14 + i2_3)
                            ow = T.axis.spatial(56, i3_2 * 28 + i3_3)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 16, 28, 56):
                for ax4_fused in T.vectorized(2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(16, ax1)
                        i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 28 + ax2)
                        i3 = T.axis.spatial(56, ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 14])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 28])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77 = sch.get_loops(block=b63)
l78 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l78)
l79 = sch.fuse(l77)
sch.vectorize(loop=l79)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b65)
l106 = sch.fuse(l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b107 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b107)
b128 = sch.decompose_reduction(block=b107, loop=l121)
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #13: GFLOPs: 0.3529. Time: 11.9438 ms. Best GFLOPs: 0.8918
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #14: GFLOPs: 0.2451. Time: 17.1982 ms. Best GFLOPs: 0.8918
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #15: GFLOPs: 0.3184. Time: 13.2389 ms. Best GFLOPs: 0.8918
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_1 in T.serial(4):
                for i5_0 in T.serial(1):
                    for i1_2_init, i2_2_init, i3_2_init, i2_3_init in T.grid(2, 14, 2, 4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 28 * 2 + i1_2_init)
                            oh = T.axis.spatial(56, i2_2_init * 4 + i2_3_init)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 28 // 14 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 * 2 + i3_2_init)
                            oci = T.axis.spatial(4, i4_1)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 2, 14, 2, 1, 3, 1, 1, 1, 4, 1, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 28 * 2 + i1_2)
                            oh = T.axis.spatial(56, i2_2 * 4 + i2_3)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 28 // 14 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 * 2 + i3_2)
                            oci, kh, kw = T.axis.remap("SRR", [i4_1, i5_1, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(0 <= oh * 2 + kh and oh * 2 + kh < 112 and 0 <= ow * 2 + kw and ow * 2 + kw < 112, placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 56, 2, 1):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 28 * 2 + ax1)
                        i2 = T.axis.spatial(56, ax2)
                        i3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 28 // 14 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 * 2 + ax3)
                        i4 = T.axis.spatial(4, i4_1)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 14, 4])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 14, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69, l70, l71, l72, l73)
sch.parallel(loop=l89)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
b97 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b97)
b114 = sch.decompose_reduction(block=b97, loop=l101)
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #17: GFLOPs: 0.4029. Time: 10.4622 ms. Best GFLOPs: 0.8918
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 16, 113, 113, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 2, 57, 113):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(16, i0_0_i1_0_i2_0_fused % 16 // 2 * 2 + ax1)
                        i2 = T.axis.spatial(113, i0_0_i1_0_i2_0_fused % 2 * 56 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 112 and 0 <= i3 and i3 < 112, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
            for i3_0, i4_0 in T.grid(1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 14, 2, 1):
                    for i2_2_init, i3_2_init, i1_3_init in T.grid(2, 28, 2):
                        for i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(16, i0_0_i1_0_i2_0_fused // 2 * 2 + i1_3_init)
                                oh = T.axis.spatial(56, i0_0_i1_0_i2_0_fused % 2 * 28 + i2_1 * 2 + i2_2_init)
                                ow = T.axis.spatial(56, i3_1 * 28 + i3_2_init)
                                oci = T.axis.spatial(4, i4_0 * 2 + i2_3_i3_3_i4_3_fused_init)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3 in T.grid(3, 3, 1, 1, 2, 28, 1, 1, 1, 1, 2):
                        for i2_3_i3_3_i4_3_fused in T.vectorized(2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(16, i0_0_i1_0_i2_0_fused // 2 * 2 + i1_3)
                                oh = T.axis.spatial(56, i0_0_i1_0_i2_0_fused % 2 * 28 + i2_1 * 2 + i2_2)
                                ow = T.axis.spatial(56, i3_1 * 28 + i3_2)
                                oci = T.axis.spatial(4, i4_0 * 2 + i2_3_i3_3_i4_3_fused)
                                kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 2, 28, 56):
                    for ax4_fused in T.vectorized(2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(16, i0_0_i1_0_i2_0_fused // 2 * 2 + ax1)
                            i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_fused % 2 * 28 + ax2)
                            i3 = T.axis.spatial(56, ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + ax4_fused)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 14, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 28, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73 = sch.get_loops(block=b63)
l74 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l74)
l75 = sch.fuse(l73)
sch.vectorize(loop=l75)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
l98 = sch.fuse(l95, l96, l97)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b65)
l107 = sch.fuse(l106)
sch.vectorize(loop=l107)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b108 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b108)
b129 = sch.decompose_reduction(block=b108, loop=l117)
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 16, 113, 113, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(1808, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(113):
                for i4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(16, i0_i1_i2_fused // 113)
                        i2 = T.axis.spatial(113, i0_i1_i2_fused % 113)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2, i3_1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3_1, i4])
                        PaddedInput[i0, i1, i2, i3_1, i4] = T.if_then_else(0 <= i2 and i2 < 112 and 0 <= i3_1 and i3_1 < 112, placeholder[i0, i1, i2, i3_1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i5_0 in T.serial(1):
                for i2_2_init, i1_3_init, i3_3_init in T.grid(56, 4, 28):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 4 + i1_3_init)
                            oh = T.axis.spatial(56, i2_2_init)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 28 + i3_3_init)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 1, 1, 56, 1, 1, 3, 1, 1, 4, 1, 28):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 4 + i1_3)
                            oh = T.axis.spatial(56, i2_2)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 28 + i3_3)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 56, 28):
                for ax4_fused in T.vectorized(2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 4 + ax1)
                        i2 = T.axis.spatial(56, ax2)
                        i3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 28 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 56, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 28])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70 = sch.get_loops(block=b63)
l71 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l71)
l72 = sch.fuse(l70)
sch.vectorize(loop=l72)
sch.annotate(block_or_loop=l71, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l71, ann_key="pragma_unroll_explicit", ann_val=1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l73, l74, l75, l76, l77, l78, l79, l80, l81, l82)
sch.parallel(loop=l97)
l98 = sch.fuse(l96)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b65)
l105 = sch.fuse(l104)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b106)
b122 = sch.decompose_reduction(block=b106, loop=l109)
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #20: GFLOPs: 0.2691. Time: 15.6622 ms. Best GFLOPs: 0.8918
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #21: GFLOPs: 0.5528. Time: 7.6242 ms. Best GFLOPs: 0.8918
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 16, 113, 113, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 15, 17, 1):
                with T.block("PaddedInput"):
                    i0, i1 = T.axis.remap("SS", [ax0, ax1])
                    i2 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 224 // 28 * 14 + ax2)
                    i3 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 4 * 16 + ax3)
                    i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 + ax4)
                    T.reads(placeholder[i0, i1, i2, i3, i4])
                    T.writes(PaddedInput[i0, i1, i2, i3, i4])
                    PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 112 and 0 <= i3 and i3 < 112, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 7, 1, 1):
                for i1_2_init, i3_2_init in T.grid(2, 8):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(16, i1_1 * 2 + i1_2_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 7 + i2_1)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 4 * 8 + i3_2_init)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 2, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(16, i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 7 + i2_1)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 4 * 8 + i3_2)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 7, 8, 1):
                with T.block("compute"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(16, ax1)
                    i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 7 + ax2)
                    i3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 4 * 8 + ax3)
                    i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                    T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                    T.writes(compute[i0, i1, i2, i3, i4])
                    compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 8, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[8, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 8, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75 = sch.get_loops(block=b63)
l76 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l76)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l77, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l77, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b65)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b103)
b124 = sch.decompose_reduction(block=b103, loop=l110)
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #23: GFLOPs: 0.1716. Time: 24.5687 ms. Best GFLOPs: 0.8918
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 16, 113, 113, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 3, 113, 1):
                with T.block("PaddedInput"):
                    i0 = T.axis.spatial(1, ax0)
                    i1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 14 * 4 + ax1)
                    i2 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 896 // 224 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14 * 2 + ax2)
                    i3 = T.axis.spatial(113, ax3)
                    i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 224 // 56 + ax4)
                    T.reads(placeholder[i0, i1, i2, i3, i4])
                    T.writes(PaddedInput[i0, i1, i2, i3, i4])
                    PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 112 and 0 <= i3 and i3 < 112, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
            for i3_1, i4_1 in T.grid(2, 1):
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 2, 1, 1, 1):
                    for i1_3_init, i3_3_init in T.grid(2, 28):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 14 * 4 + i1_2 * 2 + i1_3_init)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 224 * 14 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14)
                            ow = T.axis.spatial(56, i3_1 * 28 + i3_3_init)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 224 // 56)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 2, 1, 28, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 14 * 4 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 224 * 14 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14)
                            ow = T.axis.spatial(56, i3_1 * 28 + i3_3)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 224 // 56)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 1, 28, 1):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 14 * 4 + ax1)
                        i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 224 * 14 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14)
                        i3 = T.axis.spatial(56, i3_1 * 28 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 224 // 56)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 14, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 28])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78 = sch.get_loops(block=b63)
l79 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73)
sch.parallel(loop=l79)
sch.annotate(block_or_loop=l79, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l79, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b65)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b105)
b123 = sch.decompose_reduction(block=b105, loop=l116)
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #25: GFLOPs: 0.3733. Time: 11.2920 ms. Best GFLOPs: 0.8918
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #26: GFLOPs: 0.3590. Time: 11.7396 ms. Best GFLOPs: 0.8918
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #27: GFLOPs: 0.1757. Time: 23.9943 ms. Best GFLOPs: 0.8918
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #28: GFLOPs: 0.4518. Time: 9.3289 ms. Best GFLOPs: 0.8918
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 16, 113, 113, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 5, 113, 1):
                with T.block("PaddedInput"):
                    i0 = T.axis.spatial(1, ax0)
                    i1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 224 // 28 * 2 + ax1)
                    i2 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 * 4 + ax2)
                    i3 = T.axis.spatial(113, ax3)
                    i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 896 // 224 + ax4)
                    T.reads(placeholder[i0, i1, i2, i3, i4])
                    T.writes(PaddedInput[i0, i1, i2, i3, i4])
                    PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 112 and 0 <= i3 and i3 < 112, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
            for i3_1, i4_1 in T.grid(1, 1):
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 2, 2, 4, 1):
                    for i3_3_init in T.serial(14):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 224 // 28 * 2 + i1_2)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 * 2 + i2_2)
                            ow = T.axis.spatial(56, i3_2 * 14 + i3_3_init)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 224)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 1, 14, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 224 // 28 * 2 + i1_2)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 * 2 + i2_2)
                            ow = T.axis.spatial(56, i3_2 * 14 + i3_3)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 224)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 2, 56, 1):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 224 // 28 * 2 + ax1)
                        i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 * 2 + ax2)
                        i3 = T.axis.spatial(56, ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 224)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 8, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 28, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 14])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78 = sch.get_loops(block=b63)
l79 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73)
sch.parallel(loop=l79)
sch.annotate(block_or_loop=l79, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l79, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b65)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b105)
b123 = sch.decompose_reduction(block=b105, loop=l116)
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #30: GFLOPs: 0.5173. Time: 8.1476 ms. Best GFLOPs: 0.8918
[02:04:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], compute: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 1, 4):
                for i2_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 2, 14, 14):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 8 * 8 + i1_1 * 2 + i1_3_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 4 * 28 + i2_2_init * 14 + i2_3_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 * 14 + i3_3_init)
                        oci = T.axis.spatial(4, i4_1)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 2, 1, 1, 1, 1, 1, 2, 14, 14, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 8 * 8 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 4 * 28 + i2_2 * 14 + i2_3)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 * 14 + i3_3)
                        oci, kh, kw = T.axis.remap("SRR", [i4_1, i5_0, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(0 <= oh * 2 + kh and oh * 2 + kh < 112 and 0 <= ow * 2 + kw and ow * 2 + kw < 112, placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2 in T.grid(1, 8, 28):
                for ax3_ax4_fused in T.vectorized(56):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 8 * 8 + ax1)
                        i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 4 * 28 + ax2)
                        i3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 * 14 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 2, 14])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69)
sch.parallel(loop=l89)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b64)
l96 = sch.fuse(l94, l95)
sch.vectorize(loop=l96)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
b97 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b97)
b118 = sch.decompose_reduction(block=b97, loop=l104)
[02:05:14] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 132
Total latency (us): 12220.8

[02:05:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #0: GFLOPs: 3.2414. Time: 16.2228 ms. Best GFLOPs: 3.2414
[02:05:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 4, 28, 2):
                for i1_2_init, i2_2_init in T.grid(2, 14):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 8 + i1_1 * 2 + i1_2_init)
                        oh = T.axis.spatial(56, i2_1 * 14 + i2_2_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 28 + i3_1)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 2, 14, 1, 1, 32, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 8 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(56, i2_1 * 14 + i2_2)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 28 + i3_1)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        ic = T.axis.reduce(64, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 56, 28):
                for ax4_fused in T.vectorized(2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 8 + ax1)
                        i2 = T.axis.spatial(56, ax2)
                        i3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 28 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 28, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[02:05:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #2: GFLOPs: 2.6888. Time: 19.5569 ms. Best GFLOPs: 3.2414
[02:05:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #3: GFLOPs: 5.4408. Time: 9.6649 ms. Best GFLOPs: 5.4408
[02:05:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 4, 2, 2):
                for i1_2_init, i2_2_init, i3_3_init in T.grid(8, 14, 7):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 8 * 8 + i1_2_init)
                        oh = T.axis.spatial(56, i2_1 * 14 + i2_2_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 14 + i3_1 * 7 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 8, 14, 1, 1, 4, 1, 1, 1, 1, 1, 7, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 8 * 8 + i1_2)
                        oh = T.axis.spatial(56, i2_1 * 14 + i2_2)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 14 + i3_1 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        ic = T.axis.reduce(64, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 56, 14):
                for ax4_fused in T.vectorized(2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 8 * 8 + ax1)
                        i2 = T.axis.spatial(56, ax2)
                        i3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 14 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 2, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[02:05:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #5: GFLOPs: 3.0683. Time: 17.1379 ms. Best GFLOPs: 5.4408
[02:05:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #6: GFLOPs: 6.9369. Time: 7.5804 ms. Best GFLOPs: 6.9369
[02:05:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #7: GFLOPs: 2.5286. Time: 20.7961 ms. Best GFLOPs: 6.9369
[02:05:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_1 in T.serial(4):
                for i1_2_init, i2_2_init, i2_3_init, i3_3_init in T.grid(4, 28, 2, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 28 * 4 + i1_2_init)
                        oh = T.axis.spatial(56, i2_2_init * 2 + i2_3_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 28 // 2 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 4, 28, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 28 * 4 + i1_2)
                        oh = T.axis.spatial(56, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 28 // 2 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(64, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(56):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_i1_i2_fused // 56)
                        i2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3_1, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_1, i4])
                        compute[i0, i1, i2, i3_1, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3_1, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 28, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 2, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b101)
b120 = sch.decompose_reduction(block=b101, loop=l104)
[02:05:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1, i4_1 in T.grid(2, 1):
                for i2_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 4, 4, 2, 14):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 56 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 4 // 2 * 4 + i1_3_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 8 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 2 * 4 + i2_2_init * 2 + i2_3_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 8 // 4 * 28 + i3_1 * 14 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 1, 2, 1, 4, 32, 1, 1, 1, 4, 2, 14, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 56 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 4 // 2 * 4 + i1_3)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 8 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 2 * 4 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 8 // 4 * 28 + i3_1 * 14 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(64, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(56):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_i1_i2_fused // 56)
                        i2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        i3_4, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3_4, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_4, i4])
                        compute[i0, i1, i2, i3_4, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3_4, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 2, 2, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b101)
b121 = sch.decompose_reduction(block=b101, loop=l105)
[02:05:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 4, 2, 2, 1, 1, 1, 1, 1, 4, 7, 7, 1):
                for i2_3_init, i3_3_init in T.grid(4, 2):
                    for i4_3_fused_init in T.vectorized(4):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 16 + i1_1 * 4 + i1_2)
                            oh = T.axis.spatial(56, i2_1 * 28 + i2_2 * 4 + i2_3_init)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 28 + i3_1 * 14 + i3_2 * 2 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(64, 1, 1, 1, 1, 4, 2):
                    for i4_3_fused in T.vectorized(4):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 16 + i1_1 * 4 + i1_2)
                            oh = T.axis.spatial(56, i2_1 * 28 + i2_2 * 4 + i2_3)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 28 + i3_1 * 14 + i3_2 * 2 + i3_3)
                            oc_block, ic = T.axis.remap("SR", [i4_3_fused, i5_1])
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 16, 56, 28):
                for ax4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 16 + ax1)
                        i2 = T.axis.spatial(56, ax2)
                        i3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 28 + ax3)
                        i4 = T.axis.spatial(4, ax4_fused)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 4])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 7, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b103)
b126 = sch.decompose_reduction(block=b103, loop=l118)
[02:05:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #11: GFLOPs: 4.9750. Time: 10.5697 ms. Best GFLOPs: 6.9369
[02:05:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(64, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 4, 8, 2):
                for i2_2_init, i3_3_init in T.grid(14, 7):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2)
                        oh = T.axis.spatial(56, i2_1 * 14 + i2_2_init)
                        ow = T.axis.spatial(56, i3_1 * 7 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 1, 14, 1, 1, 4, 1, 1, 1, 1, 1, 7, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2)
                        oh = T.axis.spatial(56, i2_1 * 14 + i2_2)
                        ow = T.axis.spatial(56, i3_1 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        ic = T.axis.reduce(64, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 56, 56):
                for ax4_fused in T.vectorized(2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2)
                        i2, i3 = T.axis.remap("SS", [ax2, ax3])
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[32, 1, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 8, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[02:05:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #13: GFLOPs: 5.2222. Time: 10.0695 ms. Best GFLOPs: 6.9369
[02:05:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #14: GFLOPs: 2.4654. Time: 21.3291 ms. Best GFLOPs: 6.9369
[02:05:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #15: GFLOPs: 1.9242. Time: 27.3283 ms. Best GFLOPs: 6.9369
[02:05:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(448, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0 in T.serial(1):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 7, 2, 1):
                    for i1_2_init, i1_3_init in T.grid(2, 4):
                        for i2_3_i3_3_i4_3_fused_init in T.vectorized(4):
                            with T.block("conv2d_NCHWc_init"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 224 * 16 + i1_1 * 8 + i1_2_init * 4 + i1_3_init)
                                oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 224 // 28 * 7 + i2_1)
                                ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 * 2 + i3_1)
                                oc_block = T.axis.spatial(4, i2_3_i3_3_i4_3_fused_init)
                                T.reads()
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3 in T.grid(32, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 4):
                        for i2_3_i3_3_i4_3_fused in T.vectorized(4):
                            with T.block("conv2d_NCHWc_update"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 224 * 16 + i1_1 * 8 + i1_2 * 4 + i1_3)
                                oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 224 // 28 * 7 + i2_1)
                                ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 * 2 + i3_1)
                                oc_block = T.axis.spatial(4, i2_3_i3_3_i4_3_fused)
                                ic = T.axis.reduce(64, i5_0 * 2 + i5_1)
                                kh = T.axis.reduce(1, 0)
                                kw = T.axis.reduce(1, 0)
                                T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2 in T.grid(1, 16, 7):
                    for ax3_ax4_fused in T.vectorized(8):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 224 * 16 + ax1)
                            i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 224 // 28 * 7 + ax2)
                            i3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 * 2 + ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[28, 2, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71)
sch.parallel(loop=l94)
l95 = sch.fuse(l91, l92, l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b67)
l103 = sch.fuse(l101, l102)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b104)
b126 = sch.decompose_reduction(block=b104, loop=l112)
[02:05:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(448, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1, i4_1 in T.grid(1, 1):
                for i2_2_init, i3_2_init, i4_2_init, i3_3_init in T.grid(8, 7, 2, 8):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 112 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 7)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7 * 8 + i2_2_init)
                        ow = T.axis.spatial(56, i3_2_init * 8 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 112 // 56 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 1, 8, 7, 2, 1, 1, 1, 1, 1, 1, 8, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 112 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 7)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7 * 8 + i2_2)
                        ow = T.axis.spatial(56, i3_2 * 8 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 112 // 56 * 2 + i4_2)
                        ic = T.axis.reduce(64, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 8, 56):
                    for ax4_fused in T.vectorized(2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 112 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 7)
                            i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7 * 8 + ax2)
                            i3 = T.axis.spatial(56, ax3)
                            i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 112 // 56 * 2 + ax4_fused)
                            T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 8, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 8, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 8])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b67)
l103 = sch.fuse(l102)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b104)
b124 = sch.decompose_reduction(block=b104, loop=l108)
[02:05:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_2_init, i3_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(4, 14, 4, 2, 2):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 2 * 4 + i1_3_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 32 * 8 + i2_2_init * 2 + i2_3_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 16 * 28 + i3_2_init * 2 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(64, 1, 1, 1, 1, 4, 14, 1, 1, 1, 1, 1, 4, 2, 2):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 2 * 4 + i1_3)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 32 * 8 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 16 * 28 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused)
                        ic = T.axis.reduce(64, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 8, 28):
                for ax4_fused in T.vectorized(2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 2 * 4 + ax1)
                        i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 32 * 8 + ax2)
                        i3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 16 * 28 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 4, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 14, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b103)
b121 = sch.decompose_reduction(block=b103, loop=l105)
[02:05:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #19: GFLOPs: 3.9089. Time: 13.4526 ms. Best GFLOPs: 6.9369
[02:05:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 7, 2):
                for i1_2_init, i2_2_init, i3_3_init in T.grid(8, 14, 4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 16 * 16 + i1_1 * 8 + i1_2_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 16 // 4 * 14 + i2_2_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 28 + i3_1 * 4 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 1, 1, 8, 14, 1, 1, 16, 1, 1, 1, 1, 1, 4, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 16 * 16 + i1_1 * 8 + i1_2)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 16 // 4 * 14 + i2_2)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 28 + i3_1 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        ic = T.axis.reduce(64, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 16, 14, 28):
                for ax4_fused in T.vectorized(2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 16 * 16 + ax1)
                        i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 16 // 4 * 14 + ax2)
                        i3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 28 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 7, 1, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[02:05:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 2, 14, 2, 4, 2, 2):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 56 * 8 + i1_2_init * 4 + i1_3_init)
                    oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 56 // 28 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 28 // 4 * 4 + i2_2_init * 2 + i2_3_init)
                    ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 28 + i3_2_init * 2 + i3_3_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 1, 1, 2, 2, 14, 2, 16, 1, 1, 1, 4, 2, 2, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 56 * 8 + i1_2 * 4 + i1_3)
                    oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 56 // 28 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 28 // 4 * 4 + i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 28 + i3_2 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2)
                    ic = T.axis.reduce(64, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(56):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_i1_i2_fused // 56)
                        i2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3_1, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_1, i4])
                        compute[i0, i1, i2, i3_1, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3_1, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 2, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 14, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[02:05:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #22: GFLOPs: 2.1915. Time: 23.9952 ms. Best GFLOPs: 6.9369
[02:05:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #23: GFLOPs: 3.2531. Time: 16.1643 ms. Best GFLOPs: 6.9369
[02:05:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #24: GFLOPs: 3.4214. Time: 15.3693 ms. Best GFLOPs: 6.9369
[02:05:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #25: GFLOPs: 4.7814. Time: 10.9976 ms. Best GFLOPs: 6.9369
[02:05:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #26: GFLOPs: 5.1658. Time: 10.1793 ms. Best GFLOPs: 6.9369
[02:05:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #27: GFLOPs: 3.1839. Time: 16.5159 ms. Best GFLOPs: 6.9369
[02:05:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #28: GFLOPs: 1.2522. Time: 41.9951 ms. Best GFLOPs: 6.9369
[02:05:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #29: GFLOPs: 1.8784. Time: 27.9942 ms. Best GFLOPs: 6.9369
[02:05:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #30: GFLOPs: 6.2089. Time: 8.4692 ms. Best GFLOPs: 6.9369
[02:05:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"] Trial #31: GFLOPs: 5.9945. Time: 8.7721 ms. Best GFLOPs: 6.9369
[02:05:41] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 164
Total latency (us): 19801.2

[02:05:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #0: GFLOPs: 1.2115. Time: 6.9582 ms. Best GFLOPs: 1.2115
[02:05:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #1: GFLOPs: 0.7025. Time: 11.9988 ms. Best GFLOPs: 1.2115
[02:05:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #2: GFLOPs: 0.6524. Time: 12.9203 ms. Best GFLOPs: 1.2115
[02:05:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 32, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(112, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 6, 58):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_fused % 112 // 14 * 4 + ax1)
                        i2 = T.axis.spatial(58, i0_0_i1_0_i2_0_fused % 14 * 4 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 4, 2, 1):
                for i3_2_init, i4_2_init, i1_3_init, i3_3_init in T.grid(4, 2, 4, 7):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(32, i0_0_i1_0_i2_0_fused // 14 * 4 + i1_3_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_fused % 14 * 4 + i2_1)
                        ow = T.axis.spatial(56, i3_1 * 28 + i3_2_init * 7 + i3_3_init)
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 1, 4, 2, 1, 3, 1, 4, 1, 7, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(32, i0_0_i1_0_i2_0_fused // 14 * 4 + i1_3)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_fused % 14 * 4 + i2_1)
                        ow = T.axis.spatial(56, i3_1 * 28 + i3_2 * 7 + i3_3)
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(56):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_i1_i2_fused // 56)
                        i2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        i3_4, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3_4, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_4, i4])
                        compute[i0, i1, i2, i3_4, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3_4, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 4, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 4, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72 = sch.get_loops(block=b62)
l73 = sch.fuse(l65, l66, l67)
sch.parallel(loop=l73)
l74 = sch.fuse(l72)
sch.vectorize(loop=l74)
sch.annotate(block_or_loop=l73, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l73, ann_key="pragma_unroll_explicit", ann_val=1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b63)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b64)
l102 = sch.fuse(l97, l98, l99)
sch.parallel(loop=l102)
l103 = sch.fuse(l101)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b104)
b127 = sch.decompose_reduction(block=b104, loop=l113)
[02:05:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #4: GFLOPs: 0.9892. Time: 8.5218 ms. Best GFLOPs: 1.2115
[02:05:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #5: GFLOPs: 0.2456. Time: 34.3211 ms. Best GFLOPs: 1.2115
[02:05:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #6: GFLOPs: 0.9687. Time: 8.7022 ms. Best GFLOPs: 1.2115
[02:05:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 32, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(196, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0, i0_1, i1_1 in T.grid(1, 1, 8):
                for ax0, ax1, ax2 in T.grid(1, 4, 6):
                    for ax3_ax4_fused in T.vectorized(24):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(32, i1_1 * 4 + ax1)
                            i2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 196 // 14 * 4 + ax2)
                            i3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1 in T.grid(4, 1, 2):
                    for i3_2_init, i4_2_init, i1_3_init in T.grid(4, 2, 4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i1_1 * 4 + i1_3_init)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 4 + i2_1)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i3_2_init)
                            oci = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 1, 4, 2, 1, 3, 1, 4, 1, 1, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i1_1 * 4 + i1_3)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 4 + i2_1)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i3_2)
                            oci = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(56):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_i1_i2_fused // 56)
                        i2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        i3_4, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3_4, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_4, i4])
                        compute[i0, i1, i2, i3_4, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3_4, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 8, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 4, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[14, 1, 4, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b62)
l77 = sch.fuse(l65, l66, l67, l68)
sch.parallel(loop=l77)
l78 = sch.fuse(l75, l76)
sch.vectorize(loop=l78)
sch.annotate(block_or_loop=l77, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l77, ann_key="pragma_unroll_explicit", ann_val=1)
l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b63)
sch.annotate(block_or_loop=l79, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l79, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b64)
l105 = sch.fuse(l100, l101, l102)
sch.parallel(loop=l105)
l106 = sch.fuse(l104)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l105, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l105, ann_key="pragma_unroll_explicit", ann_val=1)
b107 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b107)
b129 = sch.decompose_reduction(block=b107, loop=l115)
[02:05:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_1 in T.serial(1):
                for i1_2_init, i2_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(16, 7, 2, 2, 2):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 112 * 16 + i1_2_init)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 8 // 2 * 14 + i2_2_init * 2 + i2_3_init)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 112 // 8 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_3_init)
                            oci = T.axis.spatial(4, i4_2_init * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 3, 1, 16, 7, 1, 2, 1, 1, 1, 1, 2, 2):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 112 * 16 + i1_2)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 8 // 2 * 14 + i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 112 // 8 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_3)
                            oci = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh + kh and oh + kh < 57 and 1 <= ow + kw and ow + kw < 57, placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(56):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_i1_i2_fused // 56)
                        i2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3_1, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_1, i4])
                        compute[i0, i1, i2, i3_1, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3_1, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 16, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[14, 2, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63 = sch.get_child_blocks(b61)
l64, l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b62)
l88 = sch.fuse(l64, l65, l66, l67, l68, l69, l70, l71, l72)
sch.parallel(loop=l88)
l89 = sch.fuse(l87)
sch.vectorize(loop=l89)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94 = sch.get_loops(block=b63)
l95 = sch.fuse(l90, l91, l92)
sch.parallel(loop=l95)
l96 = sch.fuse(l94)
sch.vectorize(loop=l96)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b97 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b97)
b114 = sch.decompose_reduction(block=b97, loop=l100)
[02:05:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 1):
                for i1_2_init, i2_2_init, i3_2_init, i2_3_init, i3_3_init in T.grid(8, 2, 7, 7, 8):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 8 * 16 + i1_1 * 8 + i1_2_init)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 14 + i2_2_init * 7 + i2_3_init)
                            ow = T.axis.spatial(56, i3_2_init * 8 + i3_3_init)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 3, 1, 8, 2, 7, 1, 1, 1, 1, 1, 7, 8):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 8 * 16 + i1_1 * 8 + i1_2)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 14 + i2_2 * 7 + i2_3)
                            ow = T.axis.spatial(56, i3_2 * 8 + i3_3)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_3_fused)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh + kh and oh + kh < 57 and 1 <= ow + kw and ow + kw < 57, placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 16, 14, 56):
                for ax4_fused in T.vectorized(2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 8 * 16 + ax1)
                        i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 14 + ax2)
                        i3 = T.axis.spatial(56, ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 8])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69)
sch.parallel(loop=l89)
l90 = sch.fuse(l88)
sch.vectorize(loop=l90)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l96)
sch.vectorize(loop=l97)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
b98 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b98)
b119 = sch.decompose_reduction(block=b98, loop=l105)
[02:05:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 32, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0 in T.serial(2):
                for i0_1, i1_1, i2_1, i3_1 in T.grid(1, 8, 1, 4):
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 16, 3):
                        for ax4_fused in T.vectorized(2):
                            with T.block("PaddedInput"):
                                i0 = T.axis.spatial(1, ax0)
                                i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused % 224 // 56 * 8 + i1_1 + ax1)
                                i2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 56 // 14 * 14 + ax2)
                                i3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i3_1 + ax3)
                                i4 = T.axis.spatial(4, i4_0 * 2 + ax4_fused)
                                T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                T.writes(PaddedInput[i0, i1, i2, i3, i4])
                                PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i4_1, i5_0 in T.grid(2, 1):
                        for i2_2_init, i2_3_init in T.grid(2, 7):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 56 * 8 + i1_1)
                                oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 56 // 14 * 14 + i2_2_init * 7 + i2_3_init)
                                ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i3_1)
                                oci = T.axis.spatial(4, i4_0 * 2 + i4_1)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 2, 1, 1, 3, 1, 1, 1, 7, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 56 * 8 + i1_1)
                                oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 56 // 14 * 14 + i2_2 * 7 + i2_3)
                                ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i3_1)
                                oci = T.axis.spatial(4, i4_0 * 2 + i4_1)
                                kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 8, 14, 4):
                    for ax4_fused in T.vectorized(2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 56 * 8 + ax1)
                            i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 56 // 14 * 14 + ax2)
                            i3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + ax4_fused)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 8, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[14, 4, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79 = sch.get_loops(block=b63)
l80 = sch.fuse(l66, l67, l68, l69)
sch.parallel(loop=l80)
l81 = sch.fuse(l79)
sch.vectorize(loop=l81)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l103, l104, l105, l106, l107, l108, l109 = sch.get_loops(block=b65)
l110 = sch.fuse(l109)
sch.vectorize(loop=l110)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b111)
b133 = sch.decompose_reduction(block=b111, loop=l120)
[02:05:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #11: GFLOPs: 0.4517. Time: 18.6618 ms. Best GFLOPs: 1.2115
[02:05:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #12: GFLOPs: 0.1839. Time: 45.8260 ms. Best GFLOPs: 1.2115
[02:05:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #13: GFLOPs: 0.1920. Time: 43.9081 ms. Best GFLOPs: 1.2115
[02:05:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 32, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 1, 1, 2, 1):
                for i3_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(2, 2, 56, 7):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco, oh = T.axis.remap("SS", [i0_0_i1_0_fused, i2_3_init])
                            ow = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + i3_2_init * 7 + i3_3_init)
                            oci = T.axis.spatial(4, i4_2_init * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0 in T.serial(3):
                    for ax0, ax1, ax2 in T.grid(1, 1, 56):
                        for ax3_ax4_fused in T.vectorized(64):
                            with T.block("PaddedInput"):
                                i0 = T.axis.spatial(1, ax0)
                                i1 = T.axis.spatial(32, i0_0_i1_0_fused % 32 + ax1)
                                i2 = T.axis.spatial(58, i5_0 + ax2)
                                i3 = T.axis.spatial(58, i3_0 * 28 + i3_1 * 14 + ax3_ax4_fused // 4)
                                i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                                T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                T.writes(PaddedInput[i0, i1, i2, i3, i4])
                                PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 1, 2, 2, 1, 3, 1, 1, 56, 7):
                        for i4_3_fused in T.vectorized(2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco, oh = T.axis.remap("SS", [i0_0_i1_0_fused, i2_3])
                                ow = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + i3_2 * 7 + i3_3)
                                oci = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                                kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i2, i3 in T.grid(56, 56):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2_4, i3_4, i4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4_fused])
                        T.reads(DepthwiseConv2d[i0, i1, i2_4, i3_4, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2_4, i3_4, i4])
                        compute[i0, i1, i2_4, i3_4, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2_4, i3_4, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[32, 1, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 56])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=16)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b62)
l81 = sch.fuse(l65, l66)
sch.parallel(loop=l81)
l82 = sch.fuse(l79, l80)
sch.vectorize(loop=l82)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b63)
l106 = sch.fuse(l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l83, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l83, ann_key="pragma_unroll_explicit", ann_val=1)
l107, l108, l109, l110, l111 = sch.get_loops(block=b64)
l112 = sch.fuse(l107, l108)
sch.parallel(loop=l112)
l113 = sch.fuse(l111)
sch.vectorize(loop=l113)
sch.annotate(block_or_loop=l112, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l112, ann_key="pragma_unroll_explicit", ann_val=1)
b114 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137 = sch.get_loops(block=b114)
b138 = sch.decompose_reduction(block=b114, loop=l124)
[02:05:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 32, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(112, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 6, 30):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 112 // 56 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 // 14 * 8 + ax1)
                        i2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14 * 4 + ax2)
                        i3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 28 * 28 + ax3)
                        i4 = T.axis.spatial(4, ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i3_1, i4_1 in T.grid(14, 1):
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 2, 4, 2, 4):
                    for i1_3_init in T.serial(4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 56 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 // 14 * 8 + i1_2 * 4 + i1_3_init)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14 * 4 + i2_2)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 28 * 28 + i3_1 * 2 + i3_2)
                            oci = T.axis.spatial(4, i4_2)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 4, 1, 1, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 56 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 // 14 * 8 + i1_2 * 4 + i1_3)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14 * 4 + i2_2)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 28 * 28 + i3_1 * 2 + i3_2)
                            oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_1, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2 in T.grid(1, 8, 4):
                    for ax3_ax4_fused in T.vectorized(8):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 56 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 // 14 * 8 + ax1)
                            i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14 * 4 + ax2)
                            i3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 28 * 28 + i3_1 * 2 + ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 4, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 14, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78 = sch.get_loops(block=b63)
l79 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73)
sch.parallel(loop=l79)
l80 = sch.fuse(l78)
sch.vectorize(loop=l80)
sch.annotate(block_or_loop=l79, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l79, ann_key="pragma_unroll_explicit", ann_val=1)
l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b65)
l106 = sch.fuse(l104, l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b107 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b107)
b125 = sch.decompose_reduction(block=b107, loop=l118)
[02:05:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 32, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(512, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 2, 16):
                for ax3_ax4_fused in T.vectorized(36):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 512 // 256 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 8 * 2 + ax1)
                        i2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 256 // 64 * 14 + ax2)
                        i3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 64 // 8 * 7 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_1, i3_1, i4_1 in T.grid(1, 1, 1):
                for i1_2_init, i2_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(2, 2, 2, 7, 7):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 256 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 8 * 2 + i1_2_init)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 256 // 64 * 14 + i2_2_init * 7 + i2_3_init)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 64 // 8 * 7 + i3_3_init)
                            oci = T.axis.spatial(4, i4_2_init * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 3, 1, 2, 2, 1, 2, 1, 1, 1, 1, 7, 7):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 256 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 8 * 2 + i1_2)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 256 // 64 * 14 + i2_2 * 7 + i2_3)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 64 // 8 * 7 + i3_3)
                            oci = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2 in T.grid(1, 2, 14):
                    for ax3_ax4_fused in T.vectorized(28):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 256 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 8 * 2 + ax1)
                            i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 256 // 64 * 14 + ax2)
                            i3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 64 // 8 * 7 + ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 8, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[8, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77 = sch.get_loops(block=b63)
l78 = sch.fuse(l66, l67, l68, l69, l70, l71, l72)
sch.parallel(loop=l78)
l79 = sch.fuse(l76, l77)
sch.vectorize(loop=l79)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
l98 = sch.fuse(l97)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b65)
l108 = sch.fuse(l106, l107)
sch.vectorize(loop=l108)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b109 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b109)
b128 = sch.decompose_reduction(block=b109, loop=l114)
[02:05:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #17: GFLOPs: 0.3826. Time: 22.0347 ms. Best GFLOPs: 1.2115
[02:05:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #18: GFLOPs: 0.4034. Time: 20.8980 ms. Best GFLOPs: 1.2115
[02:05:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #19: GFLOPs: 1.1439. Time: 7.3688 ms. Best GFLOPs: 1.2115
[02:05:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #20: GFLOPs: 0.6775. Time: 12.4417 ms. Best GFLOPs: 1.2115
[02:05:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 32, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused_fused in T.parallel(32):
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 16, 30):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused_fused % 32 // 8 * 8 + ax1)
                        i2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused_fused % 8 // 2 * 14 + ax2)
                        i3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused_fused % 2 * 28 + ax3)
                        i4 = T.axis.spatial(4, ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 4, 2, 1, 1):
                for i1_2_init, i3_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(2, 7, 4, 7, 4):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused_fused // 8 * 8 + i1_1 * 2 + i1_2_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused_fused % 8 // 2 * 14 + i2_1 * 7 + i2_3_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused_fused % 2 * 28 + i3_2_init * 4 + i3_3_init)
                        oci = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 2, 1, 7, 4, 1, 3, 1, 1, 7, 4, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused_fused // 8 * 8 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused_fused % 8 // 2 * 14 + i2_1 * 7 + i2_3)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused_fused % 2 * 28 + i3_2 * 4 + i3_3)
                        oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_0, i6_1])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(1792):
            for i3 in T.serial(56):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_i1_i2_fused // 56)
                        i2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        i3_4, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3_4, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_4, i4])
                        compute[i0, i1, i2, i3_4, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3_4, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 2, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 7, 4])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72, l73 = sch.get_loops(block=b62)
l74 = sch.fuse(l65, l66, l67, l68)
sch.parallel(loop=l74)
l75 = sch.fuse(l73)
sch.vectorize(loop=l75)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b63)
l97 = sch.fuse(l76)
sch.parallel(loop=l97)
l98, l99, l100, l101, l102 = sch.get_loops(block=b64)
l103 = sch.fuse(l98, l99, l100)
sch.parallel(loop=l103)
l104 = sch.fuse(l102)
sch.vectorize(loop=l104)
b105 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b105)
b127 = sch.decompose_reduction(block=b105, loop=l113)
[02:05:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #22: GFLOPs: 1.0258. Time: 8.2173 ms. Best GFLOPs: 1.2115
[02:05:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 32, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_1 in T.serial(1):
                for i1_2_init, i2_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(4, 7, 4, 2, 8):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 56 * 8 + i1_2_init * 2 + i1_3_init)
                        oh = T.axis.spatial(56, i2_2_init * 8 + i2_3_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 56 // 8 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 8)
                        oci = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0 in T.serial(3):
                    for ax0, ax1, ax2 in T.grid(1, 8, 56):
                        for ax3_ax4_fused in T.vectorized(12):
                            with T.block("PaddedInput"):
                                i0 = T.axis.spatial(1, ax0)
                                i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 224 // 56 * 8 + ax1)
                                i2 = T.axis.spatial(58, i5_0 + ax2)
                                i3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 56 // 8 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 8 + ax3_ax4_fused // 4)
                                i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                                T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                T.writes(PaddedInput[i0, i1, i2, i3, i4])
                                PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 4, 7, 1, 4, 1, 3, 1, 2, 8, 1, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 56 * 8 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(56, i2_2 * 8 + i2_3)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 56 // 8 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 8)
                            oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_0, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2 in T.grid(1, 8, 56):
                    for ax3_ax4_fused in T.vectorized(4):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 56 * 8 + ax1)
                            i2 = T.axis.spatial(56, ax2)
                            i3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 56 // 8 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 8)
                            i4 = T.axis.spatial(4, ax3_ax4_fused)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 8])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 8, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b63)
l82 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l82)
l83 = sch.fuse(l80, l81)
sch.vectorize(loop=l83)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l84, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l84, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b65)
l107 = sch.fuse(l105, l106)
sch.vectorize(loop=l107)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b108 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b108)
b125 = sch.decompose_reduction(block=b108, loop=l111)
[02:05:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #24: GFLOPs: 0.5214. Time: 16.1677 ms. Best GFLOPs: 1.2115
[02:05:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #25: GFLOPs: 0.4383. Time: 19.2329 ms. Best GFLOPs: 1.2115
[02:05:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #26: GFLOPs: 0.4484. Time: 18.7993 ms. Best GFLOPs: 1.2115
[02:05:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #27: GFLOPs: 1.0526. Time: 8.0083 ms. Best GFLOPs: 1.2115
[02:05:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 32, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 16, 30, 58):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_fused % 4 // 2 * 16 + ax1)
                        i2 = T.axis.spatial(58, i0_0_i1_0_i2_0_fused % 2 * 28 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 7, 14, 2):
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 4, 1, 1, 1):
                    for i1_3_init, i2_3_init, i3_3_init in T.grid(4, 4, 4):
                        for i4_3_fused_init in T.vectorized(2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(32, i0_0_i1_0_i2_0_fused // 2 * 16 + i1_2 * 4 + i1_3_init)
                                oh = T.axis.spatial(56, i0_0_i1_0_i2_0_fused % 2 * 28 + i2_1 * 4 + i2_3_init)
                                ow = T.axis.spatial(56, i3_1 * 4 + i3_3_init)
                                oci = T.axis.spatial(4, i4_1 * 2 + i4_3_fused_init)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 3, 1, 4, 4, 4):
                        for i4_3_fused in T.vectorized(2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(32, i0_0_i1_0_i2_0_fused // 2 * 16 + i1_2 * 4 + i1_3)
                                oh = T.axis.spatial(56, i0_0_i1_0_i2_0_fused % 2 * 28 + i2_1 * 4 + i2_3)
                                ow = T.axis.spatial(56, i3_1 * 4 + i3_3)
                                oci = T.axis.spatial(4, i4_1 * 2 + i4_3_fused)
                                kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 16, 4, 4):
                    for ax4_fused in T.vectorized(2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_fused // 2 * 16 + ax1)
                            i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_fused % 2 * 28 + i2_1 * 4 + ax2)
                            i3 = T.axis.spatial(56, i3_1 * 4 + ax3)
                            i4 = T.axis.spatial(4, i4_1 * 2 + ax4_fused)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 4, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 7, 1, 4])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 14, 1, 4])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73 = sch.get_loops(block=b63)
l74 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l74)
l75 = sch.fuse(l73)
sch.vectorize(loop=l75)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
l98 = sch.fuse(l97)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b65)
l112 = sch.fuse(l111)
sch.vectorize(loop=l112)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b113 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135 = sch.get_loops(block=b113)
b136 = sch.decompose_reduction(block=b113, loop=l129)
[02:05:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1):
                for i1_2_init, i2_2_init, i3_2_init, i4_2_init, i1_3_init, i3_3_init in T.grid(4, 7, 2, 2, 8, 7):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i1_2_init * 8 + i1_3_init)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 7 + i2_2_init)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 * 14 + i3_2_init * 7 + i3_3_init)
                            oci = T.axis.spatial(4, i4_2_init * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 3, 1, 4, 7, 2, 2, 1, 1, 1, 8, 1, 7):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i1_2 * 8 + i1_3)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 7 + i2_2)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 * 14 + i3_2 * 7 + i3_3)
                            oci = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh + kh and oh + kh < 57 and 1 <= ow + kw and ow + kw < 57, placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2 in T.grid(1, 32, 7):
                for ax3_ax4_fused in T.vectorized(56):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, ax1)
                        i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 7 + ax2)
                        i3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 * 14 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 4, 8])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[8, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69)
sch.parallel(loop=l89)
l90 = sch.fuse(l88)
sch.vectorize(loop=l90)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l95, l96)
sch.vectorize(loop=l97)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
b98 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b98)
b119 = sch.decompose_reduction(block=b98, loop=l105)
[02:05:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 32, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(1856, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i3 in T.serial(58):
                for i4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_i1_i2_fused // 58)
                        i2 = T.axis.spatial(58, i0_i1_i2_fused % 58)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3_1 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3_1, i4])
                        PaddedInput[i0, i1, i2, i3_1, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0, i1, i2 - 1, i3_1 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(64, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1_1, i4_1 in T.grid(1, 2, 7, 2, 1):
                for i1_2_init, i2_2_init, i2_3_init, i3_3_init in T.grid(2, 2, 4, 7):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 8 * 4 + i1_1 * 2 + i1_2_init)
                            oh = T.axis.spatial(56, i2_1 * 8 + i2_2_init * 4 + i2_3_init)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 14 + i3_1_1 * 7 + i3_3_init)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 3, 1, 2, 2, 1, 1, 1, 1, 1, 1, 4, 7):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 8 * 4 + i1_1 * 2 + i1_2)
                            oh = T.axis.spatial(56, i2_1 * 8 + i2_2 * 4 + i2_3)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 14 + i3_1_1 * 7 + i3_3)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_3_fused)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 56, 14):
                for ax4_fused in T.vectorized(2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 8 * 4 + ax1)
                        i2 = T.axis.spatial(56, ax2)
                        i3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 14 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 2, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 4])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 2, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70 = sch.get_loops(block=b63)
l71 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l71)
l72 = sch.fuse(l70)
sch.vectorize(loop=l72)
sch.annotate(block_or_loop=l71, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l71, ann_key="pragma_unroll_explicit", ann_val=1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l73, l74, l75, l76, l77)
sch.parallel(loop=l97)
l98 = sch.fuse(l96)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b65)
l105 = sch.fuse(l104)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b106)
b127 = sch.decompose_reduction(block=b106, loop=l113)
[02:05:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 32, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 16, 30, 30):
                for ax4_fused in T.vectorized(2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 16 // 8 * 16 + ax1)
                        i2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 4 * 28 + ax2)
                        i3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 28 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 16, 1, 1, 1, 1):
                for i2_2_init, i3_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(4, 7, 2, 7, 4):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 8 * 16 + i1_1)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 4 * 28 + i2_2_init * 7 + i2_3_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 28 + i3_2_init * 4 + i3_3_init)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 4, 7, 2, 3, 1, 1, 1, 7, 4, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 8 * 16 + i1_1)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 4 * 28 + i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 28 + i3_2 * 4 + i3_3)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(56):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_i1_i2_fused // 56)
                        i2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        i3_4, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3_4, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_4, i4])
                        compute[i0, i1, i2, i3_4, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3_4, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 16, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 4, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 7, 4])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74 = sch.get_loops(block=b62)
l75 = sch.fuse(l65, l66, l67, l68, l69)
sch.parallel(loop=l75)
l76 = sch.fuse(l74)
sch.vectorize(loop=l76)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b63)
sch.annotate(block_or_loop=l77, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l77, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b64)
l102 = sch.fuse(l97, l98, l99)
sch.parallel(loop=l102)
l103 = sch.fuse(l101)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b104)
b125 = sch.decompose_reduction(block=b104, loop=l112)
[02:06:21] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 196
Total latency (us): 26759.5

[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #0: GFLOPs: 1.0105. Time: 102.8843 ms. Best GFLOPs: 1.0105
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 16, 1, 7, 2):
                for i2_2_init, i1_3_init, i3_3_init in T.grid(14, 2, 4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 2 + i1_3_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 14 + i2_2_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 28 + i3_1 * 4 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 1, 14, 1, 1, 16, 1, 1, 1, 2, 1, 4, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 14 + i2_2)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 28 + i3_1 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        ic = T.axis.reduce(128, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 32, 14, 28):
                for ax4_fused in T.vectorized(2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, ax1)
                        i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 14 + ax2)
                        i3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 28 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 16, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 7, 1, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #2: GFLOPs: 3.8747. Time: 26.8314 ms. Best GFLOPs: 3.8747
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #3: GFLOPs: 1.2998. Time: 79.9872 ms. Best GFLOPs: 3.8747
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #4: GFLOPs: 2.8900. Time: 35.9734 ms. Best GFLOPs: 3.8747
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #5: GFLOPs: 3.3185. Time: 31.3288 ms. Best GFLOPs: 3.8747
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(448, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 4):
                for i2_2_init, i3_2_init, i1_3_init in T.grid(7, 2, 16):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 224 * 16 + i1_3_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 224 // 28 * 7 + i2_2_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 * 2 + i3_2_init)
                        oc_block = T.axis.spatial(4, i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 1, 7, 2, 1, 4, 1, 1, 1, 16, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 224 * 16 + i1_3)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 224 // 28 * 7 + i2_2)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(128, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(56):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_i1_i2_fused // 56)
                        i2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        i3_4, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3_4, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_4, i4])
                        compute[i0, i1, i2, i3_4, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3_4, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[28, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b101)
b125 = sch.decompose_reduction(block=b101, loop=l109)
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #7: GFLOPs: 2.9782. Time: 34.9081 ms. Best GFLOPs: 3.8747
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #8: GFLOPs: 2.5997. Time: 39.9911 ms. Best GFLOPs: 3.8747
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #9: GFLOPs: 8.2714. Time: 12.5692 ms. Best GFLOPs: 8.2714
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(3584, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1, i4_1 in T.grid(14, 1):
                for i4_2_init, i3_3_init in T.grid(2, 2):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 1792 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 896 // 56)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 1792 // 896 * 28 + i3_1 * 2 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_2_init * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(4, 1, 1, 1, 1, 1, 1, 2, 32, 1, 1, 1, 1, 1, 2):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 1792 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 896 // 56)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 1792 // 896 * 28 + i3_1 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                            ic = T.axis.reduce(128, i5_0 * 32 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0_ax1_ax2_ax3_ax4_fused in T.vectorized(8):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 1792 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 896 // 56)
                        i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56)
                        i3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 1792 // 896 * 28 + i3_1 * 2 + ax0_ax1_ax2_ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax0_ax1_ax2_ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 16, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 56, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 14, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b67)
l104 = sch.fuse(l99, l100, l101, l102, l103)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b105)
b125 = sch.decompose_reduction(block=b105, loop=l109)
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #11: GFLOPs: 2.4370. Time: 42.6605 ms. Best GFLOPs: 8.2714
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #12: GFLOPs: 2.8153. Time: 36.9289 ms. Best GFLOPs: 8.2714
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #13: GFLOPs: 5.5257. Time: 18.8148 ms. Best GFLOPs: 8.2714
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #14: GFLOPs: 1.9997. Time: 51.9906 ms. Best GFLOPs: 8.2714
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #15: GFLOPs: 1.4997. Time: 69.3239 ms. Best GFLOPs: 8.2714
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #16: GFLOPs: 2.8305. Time: 36.7300 ms. Best GFLOPs: 8.2714
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #17: GFLOPs: 5.3524. Time: 19.4240 ms. Best GFLOPs: 8.2714
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #18: GFLOPs: 3.0206. Time: 34.4191 ms. Best GFLOPs: 8.2714
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 7, 7, 4):
                for i3_2_init, i1_3_init, i2_3_init in T.grid(2, 8, 8):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 8 + i1_3_init)
                        oh = T.axis.spatial(56, i2_1 * 8 + i2_3_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 14 + i3_1 * 2 + i3_2_init)
                        oc_block = T.axis.spatial(4, i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 1, 1, 2, 1, 8, 1, 1, 1, 8, 8, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 8 + i1_3)
                        oh = T.axis.spatial(56, i2_1 * 8 + i2_3)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 14 + i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(128, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 32, 56):
                for ax3_ax4_fused in T.vectorized(56):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2 = T.axis.remap("SS", [ax1, ax2])
                        i3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 14 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 8])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 7, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l99, l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #20: GFLOPs: 2.6893. Time: 38.6586 ms. Best GFLOPs: 8.2714
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #21: GFLOPs: 0.9510. Time: 109.3184 ms. Best GFLOPs: 8.2714
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #22: GFLOPs: 4.8459. Time: 21.4542 ms. Best GFLOPs: 8.2714
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #23: GFLOPs: 5.0542. Time: 20.5701 ms. Best GFLOPs: 8.2714
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #24: GFLOPs: 2.0006. Time: 51.9663 ms. Best GFLOPs: 8.2714
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(1568, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_1 in T.serial(1):
                for i1_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 4, 4, 2, 4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 56 // 14 * 8 + i1_2_init * 4 + i1_3_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 56 * 2 + i2_3_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 * 4 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 2, 1, 1, 4, 4, 1, 1, 1, 4, 2, 4, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 56 // 14 * 8 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 56 * 2 + i2_3)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(128, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(56):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_i1_i2_fused // 56)
                        i2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3_1, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_1, i4])
                        compute[i0, i1, i2, i3_1, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3_1, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[28, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 1, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b101)
b120 = sch.decompose_reduction(block=b101, loop=l104)
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #26: GFLOPs: 3.9237. Time: 26.4966 ms. Best GFLOPs: 8.2714
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #27: GFLOPs: 2.0523. Time: 50.6571 ms. Best GFLOPs: 8.2714
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #28: GFLOPs: 2.6810. Time: 38.7782 ms. Best GFLOPs: 8.2714
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #29: GFLOPs: 2.6614. Time: 39.0634 ms. Best GFLOPs: 8.2714
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #30: GFLOPs: 1.1639. Time: 89.3235 ms. Best GFLOPs: 8.2714
[02:06:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"] Trial #31: GFLOPs: 2.2922. Time: 45.3553 ms. Best GFLOPs: 8.2714
[02:06:53] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 228
Total latency (us): 39328.7

[02:06:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #0: GFLOPs: 0.4876. Time: 4.3218 ms. Best GFLOPs: 0.4876
[02:06:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #1: GFLOPs: 0.1316. Time: 16.0077 ms. Best GFLOPs: 0.4876
[02:06:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #2: GFLOPs: 0.4939. Time: 4.2670 ms. Best GFLOPs: 0.4939
[02:06:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #3: GFLOPs: 3.5188. Time: 0.5989 ms. Best GFLOPs: 3.5188
[02:06:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #4: GFLOPs: 0.0586. Time: 35.9692 ms. Best GFLOPs: 3.5188
[02:06:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #5: GFLOPs: 0.2025. Time: 10.4068 ms. Best GFLOPs: 3.5188
[02:06:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #6: GFLOPs: 0.3474. Time: 6.0668 ms. Best GFLOPs: 3.5188
[02:06:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i5_0 in T.serial(1):
                for i1_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 16, 7, 2):
                    for i4_3_fused_init in T.vectorized(4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i1_2_init * 16 + i1_3_init)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 7 + i2_3_init)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 2 + i3_3_init)
                            oci = T.axis.spatial(4, i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 1, 2, 1, 1, 1, 3, 1, 1, 16, 7, 2):
                    for i4_3_fused in T.vectorized(4):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i1_2 * 16 + i1_3)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 7 + i2_3)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 2 + i3_3)
                            oci, kh, kw = T.axis.remap("SRR", [i4_3_fused, i5_1, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(0 <= oh * 2 + kh and oh * 2 + kh < 56 and 0 <= ow * 2 + kw and ow * 2 + kw < 56, placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2 in T.grid(1, 32, 7):
                for ax3_ax4_fused in T.vectorized(8):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, ax1)
                        i2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 7 + ax2)
                        i3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 2 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 16])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[14, 1, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l89)
l90 = sch.fuse(l88)
sch.vectorize(loop=l90)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l95, l96)
sch.vectorize(loop=l97)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
b98 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b98)
b114 = sch.decompose_reduction(block=b98, loop=l101)
[02:06:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 32, 57, 57, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(14, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 14, 1):
                for i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(4, 8, 2, 2):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(32, i1_1 * 8 + i1_3_init)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + i2_3_init)
                        ow = T.axis.spatial(28, i3_1 * 2 + i3_3_init)
                        oci = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0 in T.serial(3):
                    for ax0, ax1, ax2 in T.grid(1, 8, 3):
                        for ax3_ax4_fused in T.vectorized(20):
                            with T.block("PaddedInput"):
                                i0 = T.axis.spatial(1, ax0)
                                i1 = T.axis.spatial(32, i1_1 * 8 + ax1)
                                i2 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 * 4 + i5_0 + ax2)
                                i3 = T.axis.spatial(57, i3_1 * 4 + ax3_ax4_fused // 4)
                                i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                                T.reads(placeholder[i0, i1, i2, i3, i4])
                                T.writes(PaddedInput[i0, i1, i2, i3, i4])
                                PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 56 and 0 <= i3 and i3 < 56, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 4, 1, 3, 1, 8, 2, 2, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i1_1 * 8 + i1_3)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + i2_3)
                            ow = T.axis.spatial(28, i3_1 * 2 + i3_3)
                            oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_0, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 32, 2, 28):
                for ax4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, ax1)
                        i2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 14, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b63)
l82 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l82)
l83 = sch.fuse(l80, l81)
sch.vectorize(loop=l83)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l84, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l84, ann_key="pragma_unroll_explicit", ann_val=1)
l104, l105, l106, l107, l108, l109 = sch.get_loops(block=b65)
l110 = sch.fuse(l109)
sch.vectorize(loop=l110)
sch.annotate(block_or_loop=l104, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l104, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b111)
b132 = sch.decompose_reduction(block=b111, loop=l118)
[02:06:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #9: GFLOPs: 0.1429. Time: 14.7455 ms. Best GFLOPs: 3.5188
[02:06:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #10: GFLOPs: 0.0974. Time: 21.6375 ms. Best GFLOPs: 3.5188
[02:06:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #11: GFLOPs: 0.1517. Time: 13.8934 ms. Best GFLOPs: 3.5188
[02:06:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #12: GFLOPs: 0.1756. Time: 11.9995 ms. Best GFLOPs: 3.5188
[02:06:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #13: GFLOPs: 0.1216. Time: 17.3292 ms. Best GFLOPs: 3.5188
[02:06:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #14: GFLOPs: 0.2536. Time: 8.3085 ms. Best GFLOPs: 3.5188
[02:06:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i5_0 in T.serial(1):
                for i3_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 2, 8, 7, 2):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 56 * 8 + i1_3_init)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 7 + i2_3_init)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 56 // 8 * 4 + i3_2_init * 2 + i3_3_init)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 2, 2, 3, 1, 1, 8, 7, 2, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 56 * 8 + i1_3)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 7 + i2_3)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 56 // 8 * 4 + i3_2 * 2 + i3_3)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(0 <= oh * 2 + kh and oh * 2 + kh < 56 and 0 <= ow * 2 + kw and ow * 2 + kw < 56, placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 7, 4):
                for ax4_fused in T.vectorized(2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 56 * 8 + ax1)
                        i2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 7 + ax2)
                        i3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 56 // 8 * 4 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 2, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l89)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b64)
l96 = sch.fuse(l95)
sch.vectorize(loop=l96)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
b97 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b97)
b113 = sch.decompose_reduction(block=b97, loop=l100)
[02:06:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #16: GFLOPs: 0.1329. Time: 15.8529 ms. Best GFLOPs: 3.5188
[02:06:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 32, 57, 57, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(128, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 29, 15):
                for ax4_fused in T.vectorized(2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 128 // 32 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 4 + ax1)
                        i2 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 16 * 28 + ax2)
                        i3 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 14 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + ax4_fused)
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 56 and 0 <= i3 and i3 < 56, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
            for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 2, 1, 1, 2):
                for i1_3_init, i2_3_init, i3_3_init in T.grid(2, 14, 7):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 32 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 4 + i1_2 * 2 + i1_3_init)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 16 * 14 + i2_3_init)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 7 + i3_3_init)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 2, 14, 7, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 32 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 16 * 14 + i2_3)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 7 + i3_3)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 14, 7):
                for ax4_fused in T.vectorized(2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 32 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 4 + ax1)
                        i2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 16 * 14 + ax2)
                        i3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 7 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 2, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 14])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b63)
l81 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l81)
l82 = sch.fuse(l80)
sch.vectorize(loop=l82)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l83, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l83, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b65)
l104 = sch.fuse(l103)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b105)
b121 = sch.decompose_reduction(block=b105, loop=l114)
[02:06:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #18: GFLOPs: 0.4028. Time: 5.2325 ms. Best GFLOPs: 3.5188
[02:06:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #19: GFLOPs: 0.1360. Time: 15.4972 ms. Best GFLOPs: 3.5188
[02:06:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #20: GFLOPs: 0.0586. Time: 35.9922 ms. Best GFLOPs: 3.5188
[02:06:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 32, 57, 57, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 57, 57):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_0_i1_0_fused * 8 + ax1)
                        i2, i3, i4 = T.axis.remap("SSS", [ax2, ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 56 and 0 <= i3 and i3 < 56, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
            for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 4, 1, 4, 4, 2, 1, 1, 1, 1, 1, 1, 7, 1):
                for i1_3_init, i2_3_init, i3_3_init in T.grid(2, 7, 2):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(32, i0_0_i1_0_fused * 8 + i1_1 * 2 + i1_3_init)
                        oh = T.axis.spatial(28, i2_1 * 7 + i2_3_init)
                        ow = T.axis.spatial(28, i3_1 * 14 + i3_2 * 2 + i3_3_init)
                        oci = T.axis.spatial(4, i4_0)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 2, 7, 2, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(32, i0_0_i1_0_fused * 8 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_1 * 7 + i2_3)
                        ow = T.axis.spatial(28, i3_1 * 14 + i3_2 * 2 + i3_3)
                        oci, kh, kw = T.axis.remap("SRR", [i4_0, i5_1, i6_1])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(28):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_i1_i2_fused // 28)
                        i2 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                        i3_4, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3_4, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_4, i4])
                        compute[i0, i1, i2, i3_4, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3_4, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 4, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 7, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71 = sch.get_loops(block=b62)
l72 = sch.fuse(l65, l66)
sch.parallel(loop=l72)
l73 = sch.fuse(l71)
sch.vectorize(loop=l73)
sch.annotate(block_or_loop=l72, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l72, ann_key="pragma_unroll_explicit", ann_val=1)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b63)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b64)
l102 = sch.fuse(l97, l98, l99)
sch.parallel(loop=l102)
l103 = sch.fuse(l101)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b104)
b128 = sch.decompose_reduction(block=b104, loop=l121)
[02:06:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #22: GFLOPs: 0.1078. Time: 19.5530 ms. Best GFLOPs: 3.5188
[02:06:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #23: GFLOPs: 0.2297. Time: 9.1739 ms. Best GFLOPs: 3.5188
[02:06:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #24: GFLOPs: 0.2767. Time: 7.6164 ms. Best GFLOPs: 3.5188
[02:06:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #25: GFLOPs: 0.2634. Time: 7.9994 ms. Best GFLOPs: 3.5188
[02:06:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 32, 57, 57, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0_0 in T.serial(1, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1):
                for i0_1, i1_1, i2_1 in T.grid(1, 2, 14):
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 5, 57):
                        for ax4_fused in T.vectorized(4):
                            with T.block("PaddedInput"):
                                i0 = T.axis.spatial(1, ax0)
                                i1 = T.axis.spatial(32, i1_1 * 16 + ax1)
                                i2 = T.axis.spatial(57, i2_1 * 4 + ax2)
                                i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                                T.reads(placeholder[i0, i1, i2, i3, i4])
                                T.writes(PaddedInput[i0, i1, i2, i3, i4])
                                PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 56 and 0 <= i3 and i3 < 56, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                    for i3_1, i4_1 in T.grid(2, 1):
                        for i1_2_init, i2_2_init, i4_2_init, i1_3_init, i3_3_init in T.grid(4, 2, 4, 4, 14):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(32, i1_1 * 16 + i1_2_init * 4 + i1_3_init)
                                oh = T.axis.spatial(28, i2_1 * 2 + i2_2_init)
                                ow = T.axis.spatial(28, i3_1 * 14 + i3_3_init)
                                oci = T.axis.spatial(4, i4_2_init)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 4, 2, 1, 4, 1, 1, 1, 4, 1, 14, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(32, i1_1 * 16 + i1_2 * 4 + i1_3)
                                oh = T.axis.spatial(28, i2_1 * 2 + i2_2)
                                ow = T.axis.spatial(28, i3_1 * 14 + i3_3)
                                oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_0, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 32, 28, 28):
                    for ax4_fused in T.vectorized(4):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1, i2, i3, i4 = T.axis.remap("SSSS", [ax1, ax2, ax3, ax4_fused])
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 4, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78 = sch.get_loops(block=b63)
l79 = sch.fuse(l78)
sch.vectorize(loop=l79)
sch.annotate(block_or_loop=l66, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l66, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b65)
l114 = sch.fuse(l113)
sch.vectorize(loop=l114)
sch.annotate(block_or_loop=l104, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l104, ann_key="pragma_unroll_explicit", ann_val=1)
b115 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b115)
b140 = sch.decompose_reduction(block=b115, loop=l126)
[02:06:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 32, 57, 57, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(1824, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(57):
                for i4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_i1_i2_fused // 57)
                        i2 = T.axis.spatial(57, i0_i1_i2_fused % 57)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2, i3_1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3_1, i4])
                        PaddedInput[i0, i1, i2, i3_1, i4] = T.if_then_else(0 <= i2 and i2 < 56 and 0 <= i3_1 and i3_1 < 56, placeholder[i0, i1, i2, i3_1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(28, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i4_2_init, i1_3_init, i3_3_init in T.grid(2, 28, 2, 8, 4):
                with T.block("DepthwiseConv2d_init"):
                    b = T.axis.spatial(1, 0)
                    oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 7 * 16 + i1_2_init * 8 + i1_3_init)
                    oh = T.axis.spatial(28, i2_2_init)
                    ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7 * 4 + i3_3_init)
                    oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 14 * 2 + i4_2_init)
                    T.reads()
                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
            for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 2, 28, 1, 2, 1, 3, 1, 8, 1, 4, 1):
                with T.block("DepthwiseConv2d_update"):
                    b = T.axis.spatial(1, 0)
                    oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 7 * 16 + i1_2 * 8 + i1_3)
                    oh = T.axis.spatial(28, i2_2)
                    ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7 * 4 + i3_3)
                    oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 14 * 2 + i4_2)
                    kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                    T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 16, 28, 4):
                for ax4_fused in T.vectorized(2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 7 * 16 + ax1)
                        i2 = T.axis.spatial(28, ax2)
                        i3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7 * 4 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 14 * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 2, 8])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 28, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 1, 4])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70 = sch.get_loops(block=b63)
l71 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l71)
l72 = sch.fuse(l70)
sch.vectorize(loop=l72)
sch.annotate(block_or_loop=l71, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l71, ann_key="pragma_unroll_explicit", ann_val=1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l73, l74, l75, l76, l77, l78, l79, l80, l81, l82)
sch.parallel(loop=l97)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b65)
l104 = sch.fuse(l103)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b105)
b121 = sch.decompose_reduction(block=b105, loop=l107)
[02:06:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], compute: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(128, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1, i4_1 in T.grid(1, 2):
                for i5_0 in T.serial(1):
                    for i1_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(2, 2, 7, 14):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 32 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 16 // 4 * 2 + i1_2_init)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 4 * 7 + i2_3_init)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 32 // 16 * 14 + i3_3_init)
                            oci = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 2, 1, 1, 2, 3, 1, 1, 1, 7, 14, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 32 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 16 // 4 * 2 + i1_2)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 4 * 7 + i2_3)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 32 // 16 * 14 + i3_3)
                            oci = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(0 <= oh * 2 + kh and oh * 2 + kh < 56 and 0 <= ow * 2 + kw and ow * 2 + kw < 56, placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 14):
                    for ax4_fused in T.vectorized(2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 32 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 16 // 4 * 2 + ax1)
                            i2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 4 * 7 + ax2)
                            i3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 32 // 16 * 14 + ax3)
                            i4 = T.axis.spatial(4, i4_1 * 2 + ax4_fused)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=32)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69, l70, l71, l72)
sch.parallel(loop=l89)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
l98 = sch.fuse(l97)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
b99 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b99)
b117 = sch.decompose_reduction(block=b99, loop=l104)
[02:06:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #29: GFLOPs: 0.1757. Time: 11.9975 ms. Best GFLOPs: 3.5188
[02:06:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #30: GFLOPs: 0.2915. Time: 7.2305 ms. Best GFLOPs: 3.5188
[02:06:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"] Trial #31: GFLOPs: 0.1757. Time: 11.9965 ms. Best GFLOPs: 3.5188
[02:07:30] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 260
Total latency (us): 39927.6

[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #0: GFLOPs: 2.5313. Time: 20.5356 ms. Best GFLOPs: 2.5313
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 4, 1, 7, 1):
                for i4_2_init, i1_3_init, i3_3_init in T.grid(2, 4, 2):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 56 * 16 + i1_1 * 4 + i1_3_init)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 56 // 2)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i3_1 * 2 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_2_init * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [64, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(4, 1, 1, 1, 1, 1, 1, 2, 32, 1, 1, 1, 4, 1, 2):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 56 * 16 + i1_1 * 4 + i1_3)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 56 // 2)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i3_1 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                            ic = T.axis.reduce(128, i5_0 * 32 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [64, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(28):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(64, i0_i1_i2_fused // 28)
                        i2 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                        i3_4, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3_4, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_4, i4])
                        compute[i0, i1, i2, i3_4, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3_4, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 4, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[28, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 7, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b102)
b126 = sch.decompose_reduction(block=b102, loop=l110)
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #2: GFLOPs: 2.2282. Time: 23.3294 ms. Best GFLOPs: 2.5313
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #3: GFLOPs: 1.7103. Time: 30.3932 ms. Best GFLOPs: 2.5313
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #4: GFLOPs: 1.9057. Time: 27.2773 ms. Best GFLOPs: 2.5313
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 4):
                for i2_2_init, i1_3_init, i3_3_init in T.grid(14, 4, 14):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 8 + i1_1 * 4 + i1_3_init)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 14 + i2_2_init)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 14 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [64, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 1, 14, 1, 1, 64, 1, 1, 1, 4, 1, 14, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 8 + i1_1 * 4 + i1_3)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 14 + i2_2)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 14 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(128, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [64, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 8, 14):
                for ax3_ax4_fused in T.vectorized(56):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 8 + ax1)
                        i2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 14 + ax2)
                        i3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 14 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 2, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l99, l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(112, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 4, 1):
                for i2_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(2, 2, 8, 7):
                    for i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 14 * 8 + i1_3_init)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 // 7 * 14 + i2_2_init * 7 + i2_3_init)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7 * 4 + i3_1)
                            oc_block = T.axis.spatial(4, i4_2_init * 2 + i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [64, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(2, 1, 1, 1, 1, 2, 1, 2, 64, 1, 1, 1, 8, 7):
                    for i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 14 * 8 + i1_3)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 // 7 * 14 + i2_2 * 7 + i2_3)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7 * 4 + i3_1)
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i3_3_i4_3_fused)
                            ic = T.axis.reduce(128, i5_0 * 64 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [64, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 8, 14):
                for ax3_ax4_fused in T.vectorized(16):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 14 * 8 + ax1)
                        i2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 // 7 * 14 + ax2)
                        i3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7 * 4 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 4, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l92, l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l100, l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b103)
b125 = sch.decompose_reduction(block=b103, loop=l110)
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #7: GFLOPs: 3.6192. Time: 14.3631 ms. Best GFLOPs: 3.6192
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1, i4_1 in T.grid(7, 1):
                for i1_2_init, i2_2_init, i3_2_init, i4_2_init in T.grid(8, 2, 4, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 112 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 // 7 * 8 + i1_2_init)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 112 // 56 * 14 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7 * 2 + i2_2_init)
                        ow = T.axis.spatial(28, i3_1 * 4 + i3_2_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 28 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [64, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 8, 2, 4, 2, 2, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 112 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 // 7 * 8 + i1_2)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 112 // 56 * 14 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7 * 2 + i2_2)
                        ow = T.axis.spatial(28, i3_1 * 4 + i3_2)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 28 * 2 + i4_2)
                        ic = T.axis.reduce(128, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [64, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(28):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(64, i0_i1_i2_fused // 28)
                        i2 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                        i3_4, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3_4, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_4, i4])
                        compute[i0, i1, i2, i3_4, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3_4, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 4, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b101)
b121 = sch.decompose_reduction(block=b101, loop=l105)
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #9: GFLOPs: 1.7102. Time: 30.3950 ms. Best GFLOPs: 3.6192
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #10: GFLOPs: 2.8101. Time: 18.4983 ms. Best GFLOPs: 3.6192
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #11: GFLOPs: 6.0807. Time: 8.5488 ms. Best GFLOPs: 6.0807
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #12: GFLOPs: 1.8569. Time: 27.9937 ms. Best GFLOPs: 6.0807
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #13: GFLOPs: 2.8884. Time: 17.9967 ms. Best GFLOPs: 6.0807
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #14: GFLOPs: 3.4850. Time: 14.9161 ms. Best GFLOPs: 6.0807
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #15: GFLOPs: 3.0000. Time: 17.3273 ms. Best GFLOPs: 6.0807
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #16: GFLOPs: 3.2605. Time: 15.9432 ms. Best GFLOPs: 6.0807
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #17: GFLOPs: 4.5874. Time: 11.3316 ms. Best GFLOPs: 6.0807
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #18: GFLOPs: 4.1926. Time: 12.3987 ms. Best GFLOPs: 6.0807
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #19: GFLOPs: 5.2628. Time: 9.8773 ms. Best GFLOPs: 6.0807
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #20: GFLOPs: 3.1616. Time: 16.4417 ms. Best GFLOPs: 6.0807
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #21: GFLOPs: 7.1714. Time: 7.2485 ms. Best GFLOPs: 7.1714
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #22: GFLOPs: 2.5273. Time: 20.5685 ms. Best GFLOPs: 7.1714
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #23: GFLOPs: 1.6226. Time: 32.0365 ms. Best GFLOPs: 7.1714
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #24: GFLOPs: 3.7135. Time: 13.9982 ms. Best GFLOPs: 7.1714
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #25: GFLOPs: 4.9288. Time: 10.5467 ms. Best GFLOPs: 7.1714
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #26: GFLOPs: 2.0006. Time: 25.9838 ms. Best GFLOPs: 7.1714
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #27: GFLOPs: 2.0717. Time: 25.0910 ms. Best GFLOPs: 7.1714
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #28: GFLOPs: 4.6005. Time: 11.2993 ms. Best GFLOPs: 7.1714
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #29: GFLOPs: 1.6427. Time: 31.6445 ms. Best GFLOPs: 7.1714
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #30: GFLOPs: 2.1394. Time: 24.2980 ms. Best GFLOPs: 7.1714
[02:07:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"] Trial #31: GFLOPs: 2.9248. Time: 17.7727 ms. Best GFLOPs: 7.1714
[02:08:11] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 292
Total latency (us): 47176.1

[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #0: GFLOPs: 0.2732. Time: 15.4273 ms. Best GFLOPs: 0.2732
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #1: GFLOPs: 0.7151. Time: 5.8936 ms. Best GFLOPs: 0.7151
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #2: GFLOPs: 0.2927. Time: 14.3982 ms. Best GFLOPs: 0.7151
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #3: GFLOPs: 0.4517. Time: 9.3317 ms. Best GFLOPs: 0.7151
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #4: GFLOPs: 0.3242. Time: 12.9997 ms. Best GFLOPs: 0.7151
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #5: GFLOPs: 0.2731. Time: 15.4314 ms. Best GFLOPs: 0.7151
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 64, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 32, 9, 30):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(64, i0_0_i1_0_i2_0_fused % 8 // 4 * 32 + ax1)
                        i2 = T.axis.spatial(30, i0_0_i1_0_i2_0_fused % 4 * 7 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i3_0, i4_0 in T.grid(14, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 7, 1, 1):
                    for i1_2_init, i3_2_init, i4_2_init, i1_3_init in T.grid(8, 2, 4, 4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(64, i0_0_i1_0_i2_0_fused // 4 * 32 + i1_2_init * 4 + i1_3_init)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_fused % 4 * 7 + i2_1)
                            ow = T.axis.spatial(28, i3_0 * 2 + i3_2_init)
                            oci = T.axis.spatial(4, i4_2_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 8, 1, 2, 4, 1, 1, 1, 4, 1, 1, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(64, i0_0_i1_0_i2_0_fused // 4 * 32 + i1_2 * 4 + i1_3)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_fused % 4 * 7 + i2_1)
                            ow = T.axis.spatial(28, i3_0 * 2 + i3_2)
                            oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_0, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2 in T.grid(1, 32, 7):
                    for ax3_ax4_fused in T.vectorized(8):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(64, i0_0_i1_0_i2_0_fused // 4 * 32 + ax1)
                            i2 = T.axis.spatial(28, i0_0_i1_0_i2_0_fused % 4 * 7 + ax2)
                            i3 = T.axis.spatial(28, i3_0 * 2 + ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 8, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[14, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73 = sch.get_loops(block=b63)
l74 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l74)
l75 = sch.fuse(l73)
sch.vectorize(loop=l75)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b65)
l106 = sch.fuse(l104, l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b107 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b107)
b130 = sch.decompose_reduction(block=b107, loop=l116)
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #7: GFLOPs: 0.5271. Time: 7.9969 ms. Best GFLOPs: 0.7151
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #8: GFLOPs: 0.4714. Time: 8.9403 ms. Best GFLOPs: 0.7151
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #9: GFLOPs: 0.7347. Time: 5.7364 ms. Best GFLOPs: 0.7347
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 64, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(7, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 64, 6, 30):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(30, i0_0_i1_0_i2_0_fused % 7 * 4 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(2, 1, 1, 1, 4, 7, 4):
                for i1_2_init, i1_3_init, i3_3_init in T.grid(8, 8, 2):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(64, i1_2_init * 8 + i1_3_init)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_fused * 4 + i2_1)
                        ow = T.axis.spatial(28, i3_0 * 14 + i3_1 * 2 + i3_3_init)
                        oci = T.axis.spatial(4, i4_1)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 8, 1, 1, 1, 1, 3, 1, 8, 1, 2, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(64, i1_2 * 8 + i1_3)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_fused * 4 + i2_1)
                        ow = T.axis.spatial(28, i3_0 * 14 + i3_1 * 2 + i3_3)
                        oci, kh, kw = T.axis.remap("SRR", [i4_1, i5_0, i6_1])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 1, 2, 1):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(64, ax1)
                        i2 = T.axis.spatial(28, i0_0_i1_0_i2_0_fused * 4 + i2_1)
                        i3 = T.axis.spatial(28, i3_0 * 14 + i3_1 * 2 + ax3)
                        i4 = T.axis.spatial(4, i4_1)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 8, 8])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 4, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 7, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73 = sch.get_loops(block=b63)
l74 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l74)
l75 = sch.fuse(l73)
sch.vectorize(loop=l75)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110 = sch.get_loops(block=b65)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b111)
b134 = sch.decompose_reduction(block=b111, loop=l120)
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #11: GFLOPs: 0.1129. Time: 37.3171 ms. Best GFLOPs: 0.7347
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #12: GFLOPs: 0.5896. Time: 7.1483 ms. Best GFLOPs: 0.7347
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 64, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(14, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 32, 6, 30):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(64, i0_0_i1_0_i2_0_fused % 14 // 7 * 32 + ax1)
                        i2 = T.axis.spatial(30, i0_0_i1_0_i2_0_fused % 7 * 4 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 1, 1, 14, 1, 1, 1, 1, 1, 2, 1, 4):
                for i1_3_init, i2_3_init, i3_3_init in T.grid(32, 2, 2):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(64, i0_0_i1_0_i2_0_fused // 7 * 32 + i1_3_init)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_fused % 7 * 4 + i2_2 * 2 + i2_3_init)
                        ow = T.axis.spatial(28, i3_1 * 2 + i3_3_init)
                        oci = T.axis.spatial(4, i4_2)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 32, 2, 2, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(64, i0_0_i1_0_i2_0_fused // 7 * 32 + i1_3)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_fused % 7 * 4 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(28, i3_1 * 2 + i3_3)
                        oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_1, i6_1])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(28):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(64, i0_i1_i2_fused // 28)
                        i2 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                        i3_4, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3_4, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_4, i4])
                        compute[i0, i1, i2, i3_4, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3_4, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 1, 32])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 14, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72 = sch.get_loops(block=b62)
l73 = sch.fuse(l65, l66, l67)
sch.parallel(loop=l73)
l74 = sch.fuse(l72)
sch.vectorize(loop=l74)
sch.annotate(block_or_loop=l73, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l73, ann_key="pragma_unroll_explicit", ann_val=1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b63)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b64)
l102 = sch.fuse(l97, l98, l99)
sch.parallel(loop=l102)
l103 = sch.fuse(l101)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b104)
b127 = sch.decompose_reduction(block=b104, loop=l120)
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #14: GFLOPs: 0.5102. Time: 8.2617 ms. Best GFLOPs: 0.7347
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #15: GFLOPs: 0.1215. Time: 34.6871 ms. Best GFLOPs: 0.7347
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #16: GFLOPs: 0.0811. Time: 51.9868 ms. Best GFLOPs: 0.7347
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #17: GFLOPs: 0.5373. Time: 7.8442 ms. Best GFLOPs: 0.7347
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #18: GFLOPs: 0.5701. Time: 7.3931 ms. Best GFLOPs: 0.7347
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #19: GFLOPs: 0.2688. Time: 15.6786 ms. Best GFLOPs: 0.7347
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #20: GFLOPs: 0.4720. Time: 8.9298 ms. Best GFLOPs: 0.7347
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #21: GFLOPs: 0.1174. Time: 35.9118 ms. Best GFLOPs: 0.7347
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #22: GFLOPs: 0.5451. Time: 7.7321 ms. Best GFLOPs: 0.7347
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #23: GFLOPs: 0.4546. Time: 9.2716 ms. Best GFLOPs: 0.7347
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #24: GFLOPs: 0.5859. Time: 7.1936 ms. Best GFLOPs: 0.7347
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #25: GFLOPs: 0.3831. Time: 11.0014 ms. Best GFLOPs: 0.7347
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #26: GFLOPs: 0.1073. Time: 39.2646 ms. Best GFLOPs: 0.7347
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #27: GFLOPs: 0.3682. Time: 11.4474 ms. Best GFLOPs: 0.7347
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #28: GFLOPs: 0.6635. Time: 6.3526 ms. Best GFLOPs: 0.7347
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #29: GFLOPs: 0.1633. Time: 25.8168 ms. Best GFLOPs: 0.7347
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(112, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 7, 2, 1):
                for i1_2_init, i1_3_init in T.grid(4, 2):
                    for i2_3_i3_3_i4_3_fused_init in T.vectorized(4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 56 * 32 + i1_1 * 8 + i1_2_init * 2 + i1_3_init)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 56 // 14 * 7 + i2_1)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 * 2 + i3_1)
                            oci = T.axis.spatial(4, i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3 in T.grid(3, 3, 1, 4, 1, 1, 1, 1, 1, 1, 2):
                    for i2_3_i3_3_i4_3_fused in T.vectorized(4):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 56 * 32 + i1_1 * 8 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 56 // 14 * 7 + i2_1)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 * 2 + i3_1)
                            oci, kh, kw = T.axis.remap("SRR", [i2_3_i3_3_i4_3_fused, i5_0, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh + kh and oh + kh < 29 and 1 <= ow + kw and ow + kw < 29, placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2 in T.grid(1, 32, 7):
                for ax3_ax4_fused in T.vectorized(8):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 56 * 32 + ax1)
                        i2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 56 // 14 * 7 + ax2)
                        i3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 * 2 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[14, 2, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69)
sch.parallel(loop=l89)
l90 = sch.fuse(l86, l87, l88)
sch.vectorize(loop=l90)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l95, l96)
sch.vectorize(loop=l97)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
b98 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b98)
b117 = sch.decompose_reduction(block=b98, loop=l105)
[02:08:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"] Trial #31: GFLOPs: 0.1980. Time: 21.2901 ms. Best GFLOPs: 0.7347
[02:08:59] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 324
Total latency (us): 52912.6

[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #0: GFLOPs: 2.7695. Time: 37.3218 ms. Best GFLOPs: 2.7695
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #1: GFLOPs: 2.3091. Time: 44.7623 ms. Best GFLOPs: 2.7695
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #2: GFLOPs: 6.8235. Time: 15.1480 ms. Best GFLOPs: 6.8235
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #3: GFLOPs: 0.5922. Time: 174.5410 ms. Best GFLOPs: 6.8235
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #4: GFLOPs: 2.1518. Time: 48.0356 ms. Best GFLOPs: 6.8235
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #5: GFLOPs: 3.1327. Time: 32.9951 ms. Best GFLOPs: 6.8235
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(112, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 16, 2, 7, 1):
                for i2_2_init, i1_3_init in T.grid(2, 2):
                    for i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 56 * 32 + i1_1 * 2 + i1_3_init)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 56 // 8 * 4 + i2_1 * 2 + i2_2_init)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 7 + i3_1)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3 in T.grid(8, 1, 1, 1, 1, 2, 1, 1, 32, 1, 1, 1, 2):
                    for i2_3_i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 56 * 32 + i1_1 * 2 + i1_3)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 56 // 8 * 4 + i2_1 * 2 + i2_2)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 7 + i3_1)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i2_3_i3_3_i4_3_fused)
                            ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 32, 4, 7):
                for ax4_fused in T.vectorized(2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 56 * 32 + ax1)
                        i2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 56 // 8 * 4 + ax2)
                        i3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 7 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 16, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 2, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l91, l92, l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b103)
b124 = sch.decompose_reduction(block=b103, loop=l110)
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #7: GFLOPs: 4.5934. Time: 22.5022 ms. Best GFLOPs: 6.8235
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #8: GFLOPs: 3.6934. Time: 27.9861 ms. Best GFLOPs: 6.8235
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #9: GFLOPs: 2.5844. Time: 39.9953 ms. Best GFLOPs: 6.8235
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #10: GFLOPs: 6.9310. Time: 14.9131 ms. Best GFLOPs: 6.9310
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #11: GFLOPs: 2.7694. Time: 37.3230 ms. Best GFLOPs: 6.9310
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #12: GFLOPs: 3.9326. Time: 26.2838 ms. Best GFLOPs: 6.9310
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #13: GFLOPs: 4.9703. Time: 20.7962 ms. Best GFLOPs: 6.9310
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #14: GFLOPs: 1.8483. Time: 55.9239 ms. Best GFLOPs: 6.9310
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #15: GFLOPs: 4.6520. Time: 22.2192 ms. Best GFLOPs: 6.9310
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(64, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i3_2_init, i4_2_init, i2_3_init in T.grid(8, 14, 2, 14):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 8 + i1_2_init)
                    oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 14 + i2_3_init)
                    ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 14 + i3_2_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(256, 1, 1, 1, 8, 1, 14, 2, 1, 1, 1, 1, 1, 14, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 8 + i1_2)
                    oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 14 + i2_3)
                    ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 14 + i3_2)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2)
                    ic = T.axis.reduce(256, i5_0)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(28):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(64, i0_i1_i2_fused // 28)
                        i2 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3_1, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_1, i4])
                        compute[i0, i1, i2, i3_1, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3_1, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 1, 14])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 14, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #17: GFLOPs: 5.7278. Time: 18.0458 ms. Best GFLOPs: 6.9310
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #18: GFLOPs: 1.1928. Time: 86.6585 ms. Best GFLOPs: 6.9310
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 16, 7, 7, 2):
                for i1_3_init, i2_3_init in T.grid(4, 4):
                    for i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i1_1 * 4 + i1_3_init)
                            oh = T.axis.spatial(28, i2_1 * 4 + i2_3_init)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 7 + i3_1)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(16, 1, 1, 1, 1, 1, 1, 1, 16, 1, 1, 1, 4, 4):
                    for i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i1_1 * 4 + i1_3)
                            oh = T.axis.spatial(28, i2_1 * 4 + i2_3)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 7 + i3_1)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i3_3_i4_3_fused)
                            ic = T.axis.reduce(256, i5_0 * 16 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 64, 28):
                for ax3_ax4_fused in T.vectorized(28):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2 = T.axis.remap("SS", [ax1, ax2])
                        i3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 7 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 16, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 4])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l92, l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l100, l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b103)
b125 = sch.decompose_reduction(block=b103, loop=l110)
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #20: GFLOPs: 5.4670. Time: 18.9066 ms. Best GFLOPs: 6.9310
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #21: GFLOPs: 1.6892. Time: 61.1915 ms. Best GFLOPs: 6.9310
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(196, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_1 in T.serial(1):
                for i1_2_init, i2_2_init, i1_3_init, i3_3_init in T.grid(32, 4, 2, 2):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i1_2_init * 2 + i1_3_init)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 28 * 4 + i2_2_init)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 28 // 4 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_3_init)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(256, 1, 1, 1, 32, 4, 1, 1, 1, 1, 1, 1, 2, 1, 2):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 28 * 4 + i2_2)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 28 // 4 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + i4_3_fused)
                            ic = T.axis.reduce(256, i5_0)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(28):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(64, i0_i1_i2_fused // 28)
                        i2 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3_1, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_1, i4])
                        compute[i0, i1, i2, i3_1, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3_1, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 32, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 4, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b102)
b121 = sch.decompose_reduction(block=b102, loop=l105)
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #23: GFLOPs: 1.6905. Time: 61.1447 ms. Best GFLOPs: 6.9310
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #24: GFLOPs: 8.0210. Time: 12.8865 ms. Best GFLOPs: 8.0210
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #25: GFLOPs: 2.8065. Time: 36.8299 ms. Best GFLOPs: 8.0210
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #26: GFLOPs: 2.2807. Time: 45.3197 ms. Best GFLOPs: 8.0210
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #27: GFLOPs: 0.6922. Time: 149.3222 ms. Best GFLOPs: 8.0210
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #28: GFLOPs: 2.7693. Time: 37.3238 ms. Best GFLOPs: 8.0210
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #29: GFLOPs: 3.8818. Time: 26.6278 ms. Best GFLOPs: 8.0210
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #30: GFLOPs: 4.3055. Time: 24.0069 ms. Best GFLOPs: 8.0210
[02:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"] Trial #31: GFLOPs: 1.6735. Time: 61.7638 ms. Best GFLOPs: 8.0210
[02:09:29] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 356
Total latency (us): 65799.1

[02:09:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #0: GFLOPs: 0.1894. Time: 5.5628 ms. Best GFLOPs: 0.1894
[02:09:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #1: GFLOPs: 0.0152. Time: 69.3405 ms. Best GFLOPs: 0.1894
[02:09:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #2: GFLOPs: 0.0620. Time: 16.9989 ms. Best GFLOPs: 0.1894
[02:09:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #3: GFLOPs: 0.1694. Time: 6.2214 ms. Best GFLOPs: 0.1894
[02:09:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #4: GFLOPs: 0.1497. Time: 7.0393 ms. Best GFLOPs: 0.1894
[02:09:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #5: GFLOPs: 0.0865. Time: 12.1784 ms. Best GFLOPs: 0.1894
[02:09:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #6: GFLOPs: 0.0816. Time: 12.9202 ms. Best GFLOPs: 0.1894
[02:09:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #7: GFLOPs: 0.1682. Time: 6.2662 ms. Best GFLOPs: 0.1894
[02:09:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #8: GFLOPs: 0.1668. Time: 6.3162 ms. Best GFLOPs: 0.1894
[02:09:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #9: GFLOPs: 0.0219. Time: 48.0213 ms. Best GFLOPs: 0.1894
[02:09:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #10: GFLOPs: 0.1916. Time: 5.4992 ms. Best GFLOPs: 0.1916
[02:09:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 64, 29, 29, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(1856, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(29):
                for i4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(64, i0_i1_i2_fused // 29)
                        i2 = T.axis.spatial(29, i0_i1_i2_fused % 29)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2, i3_1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3_1, i4])
                        PaddedInput[i0, i1, i2, i3_1, i4] = T.if_then_else(0 <= i2 and i2 < 28 and 0 <= i3_1 and i3_1 < 28, placeholder[i0, i1, i2, i3_1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(448, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1_1, i4_1 in T.grid(1, 1):
                for i2_2_init, i3_2_init, i1_3_init in T.grid(2, 7, 2):
                    for i2_3_i3_3_i4_3_fused_init in T.vectorized(4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 224 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 112 // 7 * 2 + i1_3_init)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7 * 2 + i2_2_init)
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 224 // 112 * 7 + i3_2_init)
                            oci = T.axis.spatial(4, i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3 in T.grid(3, 1, 1, 1, 2, 7, 1, 1, 3, 1, 2):
                    for i2_3_i3_3_i4_3_fused in T.vectorized(4):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 224 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 112 // 7 * 2 + i1_3)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7 * 2 + i2_2)
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 224 // 112 * 7 + i3_2)
                            oci, kh, kw = T.axis.remap("SRR", [i2_3_i3_3_i4_3_fused, i5_0, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(56):
                with T.block("compute"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(64, i0_i1_i2_fused // 14)
                    i2 = T.axis.spatial(14, i0_i1_i2_fused % 14)
                    i3 = T.axis.spatial(14, i3_i4_fused // 4)
                    i4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                    T.writes(compute[i0, i1, i2, i3, i4])
                    compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 16, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69 = sch.get_loops(block=b62)
l70 = sch.fuse(l65, l66, l67)
sch.parallel(loop=l70)
l71 = sch.fuse(l69)
sch.vectorize(loop=l71)
sch.annotate(block_or_loop=l70, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l70, ann_key="pragma_unroll_explicit", ann_val=1)
l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b63)
l96 = sch.fuse(l72, l73, l74, l75, l76, l77, l78, l79)
sch.parallel(loop=l96)
l97 = sch.fuse(l93, l94, l95)
sch.vectorize(loop=l97)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b64)
l103 = sch.fuse(l98, l99, l100)
sch.parallel(loop=l103)
l104 = sch.fuse(l101, l102)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b105)
b121 = sch.decompose_reduction(block=b105, loop=l109)
[02:09:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #12: GFLOPs: 0.0282. Time: 37.3234 ms. Best GFLOPs: 0.1916
[02:09:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #13: GFLOPs: 0.1317. Time: 7.9985 ms. Best GFLOPs: 0.1916
[02:09:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #14: GFLOPs: 0.0611. Time: 17.2314 ms. Best GFLOPs: 0.1916
[02:09:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #15: GFLOPs: 0.1177. Time: 8.9524 ms. Best GFLOPs: 0.1916
[02:09:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #16: GFLOPs: 0.1581. Time: 6.6662 ms. Best GFLOPs: 0.1916
[02:09:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #17: GFLOPs: 0.0847. Time: 12.4433 ms. Best GFLOPs: 0.1916
[02:09:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #18: GFLOPs: 0.0659. Time: 15.9992 ms. Best GFLOPs: 0.1916
[02:09:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #19: GFLOPs: 0.0906. Time: 11.6343 ms. Best GFLOPs: 0.1916
[02:09:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #20: GFLOPs: 0.0511. Time: 20.6143 ms. Best GFLOPs: 0.1916
[02:09:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 64, 29, 29, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 29, 15):
                for ax4_fused in T.vectorized(2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 32 // 4 * 8 + ax1)
                        i2 = T.axis.spatial(29, ax2)
                        i3 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + ax4_fused)
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 28 and 0 <= i3 and i3 < 28, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
            for i4_1 in T.serial(1):
                for i5_0 in T.serial(1):
                    for i1_2_init, i2_3_init, i3_3_init in T.grid(8, 14, 7):
                        for i4_3_fused_init in T.vectorized(2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 4 * 8 + i1_2_init)
                                oh = T.axis.spatial(14, i2_3_init)
                                ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i3_3_init)
                                oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + i4_3_fused_init)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 1, 8, 1, 1, 1, 3, 1, 1, 1, 14, 7):
                        for i4_3_fused in T.vectorized(2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 4 * 8 + i1_2)
                                oh = T.axis.spatial(14, i2_3)
                                ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i3_3)
                                oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + i4_3_fused)
                                kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 8, 14, 7):
                    for ax4_fused in T.vectorized(2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 4 * 8 + ax1)
                            i2 = T.axis.spatial(14, ax2)
                            i3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + ax3)
                            i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + ax4_fused)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79 = sch.get_loops(block=b63)
l80 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l80)
l81 = sch.fuse(l79)
sch.vectorize(loop=l81)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
l98 = sch.fuse(l97)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b65)
l106 = sch.fuse(l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b107 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b107)
b124 = sch.decompose_reduction(block=b107, loop=l111)
[02:09:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 64, 29, 29, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i5_0, i6_0, i0_2, i1_2 in T.grid(1, 1, 1, 4):
                for ax0, ax1, ax2 in T.grid(1, 4, 5):
                    for ax3_ax4_fused in T.vectorized(60):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 16 + i1_2 * 4 + ax1)
                            i2 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 56 // 8 * 4 + ax2)
                            i3 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 14 + ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(placeholder[i0, i1, i2, i3, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 28 and 0 <= i3 and i3 < 28, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                for i2_2, i3_2, i4_2 in T.grid(2, 1, 4):
                    for i1_3_init, i3_3_init in T.grid(4, 7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 16 + i1_2 * 4 + i1_3_init)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 2 + i2_2)
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 7 + i3_3_init)
                            oci = T.axis.spatial(4, i4_2)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 4, 1, 7, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 16 + i1_2 * 4 + i1_3)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 2 + i2_2)
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 7 + i3_3)
                            oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_1, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2 in T.grid(1, 16, 2):
                for ax3_ax4_fused in T.vectorized(28):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 16 + ax1)
                        i2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 2 + ax2)
                        i3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 7 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 4, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=13)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b63)
l85 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l85)
l86 = sch.fuse(l83, l84)
sch.vectorize(loop=l86)
sch.annotate(block_or_loop=l85, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l85, ann_key="pragma_unroll_explicit", ann_val=1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b65)
l108 = sch.fuse(l106, l107)
sch.vectorize(loop=l108)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
b109 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b109)
b125 = sch.decompose_reduction(block=b109, loop=l118)
[02:09:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #23: GFLOPs: 0.1852. Time: 5.6899 ms. Best GFLOPs: 0.1916
[02:09:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 2, 7, 1, 2, 1):
                for i1_2_init, i2_2_init, i4_2_init, i1_3_init, i3_3_init in T.grid(8, 2, 2, 2, 7):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 32 + i1_1 * 16 + i1_2_init * 2 + i1_3_init)
                        oh = T.axis.spatial(14, i2_1 * 2 + i2_2_init)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 7 + i3_3_init)
                        oci = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 8, 2, 1, 2, 3, 1, 1, 2, 1, 7, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 32 + i1_1 * 16 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 7 + i3_3)
                        oci = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(0 <= oh * 2 + kh and oh * 2 + kh < 28 and 0 <= ow * 2 + kw and ow * 2 + kw < 28, placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2 in T.grid(1, 32, 14):
                for ax3_ax4_fused in T.vectorized(28):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 32 + ax1)
                        i2 = T.axis.spatial(14, ax2)
                        i3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 7 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 8, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69)
sch.parallel(loop=l89)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b64)
l96 = sch.fuse(l94, l95)
sch.vectorize(loop=l96)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
b97 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b97)
b118 = sch.decompose_reduction(block=b97, loop=l105)
[02:09:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #25: GFLOPs: 0.1777. Time: 5.9303 ms. Best GFLOPs: 0.1916
[02:09:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #26: GFLOPs: 0.0829. Time: 12.7073 ms. Best GFLOPs: 0.1916
[02:09:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #27: GFLOPs: 0.1449. Time: 7.2720 ms. Best GFLOPs: 0.1916
[02:09:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #28: GFLOPs: 0.0226. Time: 46.6632 ms. Best GFLOPs: 0.1916
[02:09:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], compute: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 64, 29, 29, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(1856, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(29):
                for i4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(64, i0_i1_i2_fused // 29)
                        i2 = T.axis.spatial(29, i0_i1_i2_fused % 29)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2, i3_1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3_1, i4])
                        PaddedInput[i0, i1, i2, i3_1, i4] = T.if_then_else(0 <= i2 and i2 < 28 and 0 <= i3_1 and i3_1 < 28, placeholder[i0, i1, i2, i3_1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(7, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(16, 7, 2, 2, 4, 2):
                for i3_3_i4_3_fused_init in T.vectorized(2):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(64, i1_2_init * 4 + i1_3_init)
                        oh = T.axis.spatial(14, i2_2_init * 2 + i2_3_init)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 2 + i3_2_init)
                        oci = T.axis.spatial(4, i4_2_init * 2 + i3_3_i4_3_fused_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
            for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3 in T.grid(3, 3, 1, 16, 7, 2, 2, 1, 1, 1, 4, 2):
                for i3_3_i4_3_fused in T.vectorized(2):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(64, i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 2 + i3_2)
                        oci = T.axis.spatial(4, i4_2 * 2 + i3_3_i4_3_fused)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2 in T.grid(1, 64, 14):
                for ax3_ax4_fused in T.vectorized(8):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2 = T.axis.remap("SS", [ax1, ax2])
                        i3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 2 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 16, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70 = sch.get_loops(block=b63)
l71 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l71)
l72 = sch.fuse(l70)
sch.vectorize(loop=l72)
sch.annotate(block_or_loop=l71, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l71, ann_key="pragma_unroll_explicit", ann_val=1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l73, l74, l75, l76, l77, l78, l79, l80, l81, l82)
sch.parallel(loop=l97)
l98 = sch.fuse(l95, l96)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b65)
l105 = sch.fuse(l103, l104)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b106)
b121 = sch.decompose_reduction(block=b106, loop=l108)
[02:09:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #30: GFLOPs: 0.2457. Time: 4.2894 ms. Best GFLOPs: 0.2457
[02:09:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"] Trial #31: GFLOPs: 0.0788. Time: 13.3781 ms. Best GFLOPs: 0.2457
[02:10:16] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 388
Total latency (us): 70088.5

[02:10:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #0: GFLOPs: 3.1587. Time: 16.3616 ms. Best GFLOPs: 3.1587
[02:10:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #1: GFLOPs: 1.3211. Time: 39.1210 ms. Best GFLOPs: 3.1587
[02:10:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1, i4_1 in T.grid(7, 2):
                for i4_2_init, i1_3_init, i3_3_init in T.grid(2, 2, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 14 * 2 + i1_3_init)
                        oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14)
                        ow = T.axis.spatial(14, i3_1 * 2 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 1, 1, 1, 1, 1, 2, 64, 1, 1, 1, 2, 1, 2, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 14 * 2 + i1_3)
                        oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14)
                        ow = T.axis.spatial(14, i3_1 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(256, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_fused in T.parallel(128, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(14):
                for i3_i4_fused in T.vectorized(56):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2_1 = T.axis.remap("SS", [i0_i1_fused, i2])
                        i3 = T.axis.spatial(14, i3_i4_fused // 4)
                        i4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(conv2d_NCHWc[i0, i1, i2_1, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2_1, i3, i4])
                        compute[i0, i1, i2_1, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2_1, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 64, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=64)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95)
sch.parallel(loop=l99)
l100 = sch.fuse(l97, l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b101)
b121 = sch.decompose_reduction(block=b101, loop=l105)
[02:10:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #3: GFLOPs: 1.3483. Time: 38.3311 ms. Best GFLOPs: 3.1587
[02:10:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #4: GFLOPs: 2.6999. Time: 19.1417 ms. Best GFLOPs: 3.1587
[02:10:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 2, 1, 1):
                for i1_2_init, i3_2_init, i2_3_init, i3_3_init in T.grid(8, 7, 7, 2):
                    for i4_3_fused_init in T.vectorized(4):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 16 + i1_1 * 8 + i1_2_init)
                            oh = T.axis.spatial(14, i2_1 * 7 + i2_3_init)
                            ow = T.axis.spatial(14, i3_2_init * 2 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(64, 1, 1, 1, 8, 1, 7, 1, 4, 1, 1, 1, 1, 7, 2):
                    for i4_3_fused in T.vectorized(4):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 16 + i1_1 * 8 + i1_2)
                            oh = T.axis.spatial(14, i2_1 * 7 + i2_3)
                            ow = T.axis.spatial(14, i3_2 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_3_fused)
                            ic = T.axis.reduce(256, i5_0 * 4 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 16, 14):
                for ax3_ax4_fused in T.vectorized(56):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 16 + ax1)
                        i2 = T.axis.spatial(14, ax2)
                        i3 = T.axis.spatial(14, ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 2, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l100, l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b103)
b126 = sch.decompose_reduction(block=b103, loop=l110)
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(784, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1, i4_1 in T.grid(1, 1):
                for i1_2_init, i4_2_init, i1_3_init in T.grid(16, 4, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 196 * 32 + i1_2_init * 2 + i1_3_init)
                        oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 196 // 14)
                        oc_block = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 16, 1, 1, 4, 4, 1, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 196 * 32 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 196 // 14)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(256, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(56):
                with T.block("compute"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(128, i0_i1_i2_fused // 14)
                    i2 = T.axis.spatial(14, i0_i1_i2_fused % 14)
                    i3 = T.axis.spatial(14, i3_i4_fused // 4)
                    i4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                    T.writes(compute[i0, i1, i2, i3, i4])
                    compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 16, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l97, l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b101)
b121 = sch.decompose_reduction(block=b101, loop=l105)
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #7: GFLOPs: 3.9379. Time: 13.1241 ms. Best GFLOPs: 3.9379
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #8: GFLOPs: 3.6910. Time: 14.0021 ms. Best GFLOPs: 3.9379
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #9: GFLOPs: 0.8566. Time: 60.3318 ms. Best GFLOPs: 3.9379
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #10: GFLOPs: 5.6482. Time: 9.1501 ms. Best GFLOPs: 5.6482
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #11: GFLOPs: 1.1267. Time: 45.8686 ms. Best GFLOPs: 5.6482
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #12: GFLOPs: 0.9068. Time: 56.9919 ms. Best GFLOPs: 5.6482
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #13: GFLOPs: 3.0308. Time: 17.0521 ms. Best GFLOPs: 5.6482
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #14: GFLOPs: 3.0152. Time: 17.1400 ms. Best GFLOPs: 5.6482
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #15: GFLOPs: 1.5083. Time: 34.2645 ms. Best GFLOPs: 5.6482
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #16: GFLOPs: 2.1359. Time: 24.1963 ms. Best GFLOPs: 5.6482
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #17: GFLOPs: 1.4704. Time: 35.1475 ms. Best GFLOPs: 5.6482
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #18: GFLOPs: 2.9159. Time: 17.7240 ms. Best GFLOPs: 5.6482
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #19: GFLOPs: 0.8076. Time: 63.9916 ms. Best GFLOPs: 5.6482
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #20: GFLOPs: 3.0729. Time: 16.8185 ms. Best GFLOPs: 5.6482
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #21: GFLOPs: 1.8973. Time: 27.2394 ms. Best GFLOPs: 5.6482
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i4_2_init in T.grid(32, 7, 2, 2):
                for i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 28 // 7 * 32 + i1_2_init)
                        oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 28 * 7 + i2_2_init)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7 * 2 + i3_2_init)
                        oc_block = T.axis.spatial(4, i4_2_init * 2 + i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1 in T.grid(4, 1, 1, 1, 32, 7, 2, 2, 64):
                for i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 28 // 7 * 32 + i1_2)
                        oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 28 * 7 + i2_2)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused)
                        ic = T.axis.reduce(256, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(56):
                with T.block("compute"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(128, i0_i1_i2_fused // 14)
                    i2 = T.axis.spatial(14, i0_i1_i2_fused % 14)
                    i3 = T.axis.spatial(14, i3_i4_fused // 4)
                    i4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                    T.writes(compute[i0, i1, i2, i3, i4])
                    compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 32, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
l94 = sch.fuse(l86, l87, l88, l89, l90, l91, l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l98, l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b102)
b114 = sch.decompose_reduction(block=b102, loop=l104)
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #23: GFLOPs: 0.4052. Time: 127.5521 ms. Best GFLOPs: 5.6482
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #24: GFLOPs: 3.2212. Time: 16.0442 ms. Best GFLOPs: 5.6482
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #25: GFLOPs: 2.6599. Time: 19.4294 ms. Best GFLOPs: 5.6482
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #26: GFLOPs: 1.6408. Time: 31.4978 ms. Best GFLOPs: 5.6482
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #27: GFLOPs: 6.2049. Time: 8.3291 ms. Best GFLOPs: 6.2049
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #28: GFLOPs: 3.0474. Time: 16.9589 ms. Best GFLOPs: 6.2049
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #29: GFLOPs: 4.5605. Time: 11.3324 ms. Best GFLOPs: 6.2049
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #30: GFLOPs: 2.0554. Time: 25.1441 ms. Best GFLOPs: 6.2049
[02:10:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"] Trial #31: GFLOPs: 1.1327. Time: 45.6281 ms. Best GFLOPs: 6.2049
[02:10:54] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 420
Total latency (us): 78417.6

[02:10:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #0: GFLOPs: 0.1916. Time: 10.9989 ms. Best GFLOPs: 0.1916
[02:10:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #1: GFLOPs: 0.2431. Time: 8.6692 ms. Best GFLOPs: 0.2431
[02:10:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #2: GFLOPs: 0.2905. Time: 7.2543 ms. Best GFLOPs: 0.2905
[02:10:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #3: GFLOPs: 0.5709. Time: 3.6914 ms. Best GFLOPs: 0.5709
[02:10:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #4: GFLOPs: 0.1870. Time: 11.2702 ms. Best GFLOPs: 0.5709
[02:10:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #5: GFLOPs: 0.1756. Time: 12.0019 ms. Best GFLOPs: 0.5709
[02:10:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 128, 16, 16, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i5_0 in T.serial(1):
                for i3_2_init, i1_3_init, i2_3_init in T.grid(14, 16, 14):
                    for i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2 * 16 + i1_3_init)
                            oh, ow = T.axis.remap("SS", [i2_3_init, i3_2_init])
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i6_0 in T.serial(3):
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 16, 14):
                        for ax4_fused in T.vectorized(2):
                            with T.block("PaddedInput"):
                                i0 = T.axis.spatial(1, ax0)
                                i1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 2 * 16 + ax1)
                                i2 = T.axis.spatial(16, ax2)
                                i3 = T.axis.spatial(16, i6_0 + ax3)
                                i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + ax4_fused)
                                T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                T.writes(PaddedInput[i0, i1, i2, i3, i4])
                                PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3 in T.grid(1, 1, 1, 14, 1, 3, 1, 1, 16, 14):
                        for i3_3_i4_3_fused in T.vectorized(2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2 * 16 + i1_3)
                                oh, ow = T.axis.remap("SS", [i2_3, i3_2])
                                oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i3_3_i4_3_fused)
                                kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 16, 14, 14):
                for ax4_fused in T.vectorized(2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2 * 16 + ax1)
                        i2, i3 = T.axis.remap("SS", [ax2, ax3])
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 1, 16])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 14, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b63)
l83 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l83)
l84 = sch.fuse(l82)
sch.vectorize(loop=l84)
sch.annotate(block_or_loop=l83, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l83, ann_key="pragma_unroll_explicit", ann_val=1)
l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b64)
l100 = sch.fuse(l98, l99)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l85, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l85, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b65)
l107 = sch.fuse(l106)
sch.vectorize(loop=l107)
sch.annotate(block_or_loop=l101, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l101, ann_key="pragma_unroll_explicit", ann_val=1)
b108 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b108)
b123 = sch.decompose_reduction(block=b108, loop=l111)
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #7: GFLOPs: 0.1054. Time: 19.9991 ms. Best GFLOPs: 0.5709
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #8: GFLOPs: 0.4388. Time: 4.8030 ms. Best GFLOPs: 0.5709
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #9: GFLOPs: 0.3941. Time: 5.3477 ms. Best GFLOPs: 0.5709
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #10: GFLOPs: 0.5703. Time: 3.6950 ms. Best GFLOPs: 0.5709
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #11: GFLOPs: 0.5004. Time: 4.2117 ms. Best GFLOPs: 0.5709
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #12: GFLOPs: 0.4613. Time: 4.5687 ms. Best GFLOPs: 0.5709
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #13: GFLOPs: 0.1525. Time: 13.8155 ms. Best GFLOPs: 0.5709
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #14: GFLOPs: 0.6091. Time: 3.4598 ms. Best GFLOPs: 0.6091
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #15: GFLOPs: 0.3161. Time: 6.6664 ms. Best GFLOPs: 0.6091
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1, i4_1 in T.grid(1, 1):
                for i5_0 in T.serial(1):
                    for i1_2_init, i2_2_init, i3_2_init, i4_2_init in T.grid(16, 7, 2, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 56 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 4 // 2 * 16 + i1_2_init)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 2 * 7 + i2_2_init)
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 8 * 2 + i3_2_init)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 8 // 4 * 2 + i4_2_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 16, 7, 2, 2, 3, 1, 1, 1, 1, 1, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 56 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 4 // 2 * 16 + i1_2)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 2 * 7 + i2_2)
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 8 * 2 + i3_2)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 8 // 4 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh + kh and oh + kh < 15 and 1 <= ow + kw and ow + kw < 15, placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 16, 7, 2):
                    for ax4_fused in T.vectorized(2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 56 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 4 // 2 * 16 + ax1)
                            i2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 2 * 7 + ax2)
                            i3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 8 * 2 + ax3)
                            i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 8 // 4 * 2 + ax4_fused)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 2, 16, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69, l70, l71, l72)
sch.parallel(loop=l89)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
l98 = sch.fuse(l97)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
b99 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b99)
b117 = sch.decompose_reduction(block=b99, loop=l104)
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 128, 16, 16, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(196, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 64, 4, 4):
                for ax4_fused in T.vectorized(2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 64 + ax1)
                        i2 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 196 // 28 * 2 + ax2)
                        i3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 28 // 14 * 2 + ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i4_1 in T.serial(1):
                for i1_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 32, 2, 2):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 64 + i1_2_init * 32 + i1_3_init)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 28 * 2 + i2_3_init)
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i3_3_init)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 28 // 14 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 1, 1, 2, 1, 1, 1, 1, 3, 1, 32, 2, 2):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 64 + i1_2 * 32 + i1_3)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 28 * 2 + i2_3)
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i3_3)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 28 // 14 * 2 + i4_3_fused)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(56):
                with T.block("compute"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(128, i0_i1_i2_fused // 14)
                    i2 = T.axis.spatial(14, i0_i1_i2_fused % 14)
                    i3 = T.axis.spatial(14, i3_i4_fused // 4)
                    i4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                    T.writes(compute[i0, i1, i2, i3, i4])
                    compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 2, 32])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78 = sch.get_loops(block=b62)
l79 = sch.fuse(l65, l66, l67, l68, l69, l70, l71, l72, l73)
sch.parallel(loop=l79)
l80 = sch.fuse(l78)
sch.vectorize(loop=l80)
sch.annotate(block_or_loop=l79, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l79, ann_key="pragma_unroll_explicit", ann_val=1)
l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b63)
l97 = sch.fuse(l96)
sch.vectorize(loop=l97)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b64)
l103 = sch.fuse(l98, l99, l100)
sch.parallel(loop=l103)
l104 = sch.fuse(l101, l102)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b105)
b122 = sch.decompose_reduction(block=b105, loop=l108)
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #18: GFLOPs: 0.4609. Time: 4.5726 ms. Best GFLOPs: 0.6091
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #19: GFLOPs: 0.2904. Time: 7.2558 ms. Best GFLOPs: 0.6091
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #20: GFLOPs: 0.1817. Time: 11.5975 ms. Best GFLOPs: 0.6091
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #21: GFLOPs: 0.3592. Time: 5.8661 ms. Best GFLOPs: 0.6091
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #22: GFLOPs: 0.4895. Time: 4.3055 ms. Best GFLOPs: 0.6091
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #23: GFLOPs: 0.0452. Time: 46.6665 ms. Best GFLOPs: 0.6091
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #24: GFLOPs: 0.4657. Time: 4.5251 ms. Best GFLOPs: 0.6091
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(112, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i3_2_init, i1_3_init in T.grid(4, 14, 8):
                for i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 56 // 14 * 32 + i1_2_init * 8 + i1_3_init)
                        oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14)
                        ow = T.axis.spatial(14, i3_2_init)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 56 * 2 + i2_3_i3_3_i4_3_fused_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
            for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3 in T.grid(3, 3, 1, 4, 1, 14, 1, 1, 1, 1, 8):
                for i2_3_i3_3_i4_3_fused in T.vectorized(2):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 56 // 14 * 32 + i1_2 * 8 + i1_3)
                        oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14)
                        ow = T.axis.spatial(14, i3_2)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 56 * 2 + i2_3_i3_3_i4_3_fused)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh + kh and oh + kh < 15 and 1 <= ow + kw and ow + kw < 15, placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(56):
                with T.block("compute"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(128, i0_i1_i2_fused // 14)
                    i2 = T.axis.spatial(14, i0_i1_i2_fused % 14)
                    i3 = T.axis.spatial(14, i3_i4_fused // 4)
                    i4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                    T.writes(compute[i0, i1, i2, i3, i4])
                    compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 4, 8])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 14, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63 = sch.get_child_blocks(b61)
l64, l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b62)
l88 = sch.fuse(l64, l65, l66, l67, l68, l69, l70, l71, l72, l73)
sch.parallel(loop=l88)
l89 = sch.fuse(l85, l86, l87)
sch.vectorize(loop=l89)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94 = sch.get_loops(block=b63)
l95 = sch.fuse(l90, l91, l92)
sch.parallel(loop=l95)
l96 = sch.fuse(l93, l94)
sch.vectorize(loop=l96)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b97 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110 = sch.get_loops(block=b97)
b111 = sch.decompose_reduction(block=b97, loop=l99)
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #26: GFLOPs: 0.5030. Time: 4.1893 ms. Best GFLOPs: 0.6091
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #27: GFLOPs: 0.3155. Time: 6.6795 ms. Best GFLOPs: 0.6091
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 128, 16, 16, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(448, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_1, i3_1, i4_1 in T.grid(1, 7, 1):
                for i5_0 in T.serial(1):
                    for i1_2_init, i2_2_init, i4_2_init, i1_3_init in T.grid(4, 2, 2, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 112 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 * 8 + i1_2_init * 2 + i1_3_init)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 112 // 16 * 2 + i2_2_init)
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 16 // 8 * 7 + i3_1)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 8 // 4 * 2 + i4_2_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0 in T.serial(3):
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 4, 1):
                            for ax4_fused in T.vectorized(2):
                                with T.block("PaddedInput"):
                                    i0 = T.axis.spatial(1, ax0)
                                    i1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 448 // 112 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 * 8 + ax1)
                                    i2 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 112 // 16 * 2 + ax2)
                                    i3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 16 // 8 * 7 + i3_1 + i6_0 + ax3)
                                    i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 8 // 4 * 2 + ax4_fused)
                                    T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                    T.writes(PaddedInput[i0, i1, i2, i3, i4])
                                    PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                        for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 2, 1, 2, 3, 1, 1, 2, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 112 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 * 8 + i1_2 * 2 + i1_3)
                                oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 112 // 16 * 2 + i2_2)
                                ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 16 // 8 * 7 + i3_1)
                                oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 8 // 4 * 2 + i4_2)
                                kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2 in T.grid(1, 8, 2):
                    for ax3_ax4_fused in T.vectorized(2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 112 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 * 8 + ax1)
                            i2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 112 // 16 * 2 + ax2)
                            i3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 16 // 8 * 7 + i3_1)
                            i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 8 // 4 * 2 + ax3_ax4_fused)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 4, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b63)
l83 = sch.fuse(l66, l67, l68, l69, l70, l71, l72)
sch.parallel(loop=l83)
l84 = sch.fuse(l82)
sch.vectorize(loop=l84)
sch.annotate(block_or_loop=l83, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l83, ann_key="pragma_unroll_explicit", ann_val=1)
l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l85, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l85, ann_key="pragma_unroll_explicit", ann_val=1)
l103, l104, l105, l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b65)
l112 = sch.fuse(l110, l111)
sch.vectorize(loop=l112)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
b113 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b113)
b132 = sch.decompose_reduction(block=b113, loop=l119)
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #29: GFLOPs: 0.5006. Time: 4.2101 ms. Best GFLOPs: 0.6091
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #30: GFLOPs: 0.5721. Time: 3.6836 ms. Best GFLOPs: 0.6091
[02:11:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 8, 7, 7, 1, 1):
                for i1_2_init, i2_2_init, i3_2_init in T.grid(8, 2, 2):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 64 + i1_1 * 8 + i1_2_init)
                        oh = T.axis.spatial(14, i2_1 * 2 + i2_2_init)
                        ow = T.axis.spatial(14, i3_1 * 2 + i3_2_init)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 8, 2, 2, 1, 3, 1, 1, 1, 1, 1, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 64 + i1_1 * 8 + i1_2)
                        oh = T.axis.spatial(14, i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(14, i3_1 * 2 + i3_2)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh + kh and oh + kh < 15 and 1 <= ow + kw and ow + kw < 15, placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 14, 14, 1):
                with T.block("compute"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 64 + ax1)
                    i2, i3 = T.axis.remap("SS", [ax2, ax3])
                    i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                    T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                    T.writes(compute[i0, i1, i2, i3, i4])
                    compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 8, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69)
sch.parallel(loop=l89)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
b96 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l97, l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b96)
b117 = sch.decompose_reduction(block=b96, loop=l104)
[02:11:40] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 452
Total latency (us): 95716.5

[02:11:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #0: GFLOPs: 10.6407. Time: 9.6856 ms. Best GFLOPs: 10.6407
[02:11:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #1: GFLOPs: 10.6999. Time: 9.6320 ms. Best GFLOPs: 10.6999
[02:11:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #2: GFLOPs: 2.0764. Time: 49.6358 ms. Best GFLOPs: 10.6999
[02:11:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 4, 7, 7):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 4 + i1_3_init)
                    oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 7 + i2_3_init)
                    ow = T.axis.spatial(14, i3_2_init * 7 + i3_3_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 1, 1, 2, 1, 32, 1, 1, 1, 4, 7, 7, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 4 + i1_3)
                    oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 7 + i2_3)
                    ow = T.axis.spatial(14, i3_2 * 7 + i3_3)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(56):
                with T.block("compute"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(128, i0_i1_i2_fused // 14)
                    i2 = T.axis.spatial(14, i0_i1_i2_fused % 14)
                    i3 = T.axis.spatial(14, i3_i4_fused // 4)
                    i4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                    T.writes(compute[i0, i1, i2, i3, i4])
                    compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 2, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l97, l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[02:11:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #4: GFLOPs: 2.8558. Time: 36.0886 ms. Best GFLOPs: 10.6999
[02:11:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #5: GFLOPs: 2.8635. Time: 35.9918 ms. Best GFLOPs: 10.6999
[02:11:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #6: GFLOPs: 10.2897. Time: 10.0160 ms. Best GFLOPs: 10.6999
[02:11:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #7: GFLOPs: 2.2686. Time: 45.4294 ms. Best GFLOPs: 10.6999
[02:11:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #8: GFLOPs: 5.6276. Time: 18.3136 ms. Best GFLOPs: 10.6999
[02:11:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_1, i3_1, i4_1 in T.grid(1, 2, 1):
                for i2_2_init, i4_2_init, i2_3_init in T.grid(7, 2, 2):
                    for i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 112 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 16)
                            oh = T.axis.spatial(14, i2_2_init * 2 + i2_3_init)
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 112 // 16 * 2 + i3_1)
                            oc_block = T.axis.spatial(4, i4_2_init * 2 + i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(32, 1, 1, 1, 1, 7, 1, 2, 16, 1, 1, 1, 1, 2):
                    for i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 112 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 16)
                            oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 112 // 16 * 2 + i3_1)
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i3_3_i4_3_fused)
                            ic = T.axis.reduce(512, i5_0 * 16 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2 in T.grid(1, 1, 14):
                    for ax3_ax4_fused in T.vectorized(4):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 112 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 16)
                            i2 = T.axis.spatial(14, ax2)
                            i3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 112 // 16 * 2 + i3_1)
                            i4 = T.axis.spatial(4, ax3_ax4_fused)
                            T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 16, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l94)
l95 = sch.fuse(l92, l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b67)
l105 = sch.fuse(l103, l104)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b106)
b126 = sch.decompose_reduction(block=b106, loop=l111)
[02:11:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #10: GFLOPs: 2.5539. Time: 40.3543 ms. Best GFLOPs: 10.6999
[02:11:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #11: GFLOPs: 3.0902. Time: 33.3508 ms. Best GFLOPs: 10.6999
[02:11:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #12: GFLOPs: 2.7584. Time: 37.3631 ms. Best GFLOPs: 10.6999
[02:11:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #13: GFLOPs: 2.0612. Time: 49.9997 ms. Best GFLOPs: 10.6999
[02:11:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #14: GFLOPs: 2.9025. Time: 35.5079 ms. Best GFLOPs: 10.6999
[02:11:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #15: GFLOPs: 0.6964. Time: 147.9949 ms. Best GFLOPs: 10.6999
[02:11:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #16: GFLOPs: 1.9852. Time: 51.9160 ms. Best GFLOPs: 10.6999
[02:11:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #17: GFLOPs: 1.0454. Time: 98.5876 ms. Best GFLOPs: 10.6999
[02:11:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #18: GFLOPs: 0.3983. Time: 258.7374 ms. Best GFLOPs: 10.6999
[02:11:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #19: GFLOPs: 3.2208. Time: 31.9987 ms. Best GFLOPs: 10.6999
[02:11:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #20: GFLOPs: 1.4626. Time: 70.4631 ms. Best GFLOPs: 10.6999
[02:11:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_2_init, i3_2_init, i1_3_init, i3_3_init in T.grid(14, 2, 16, 7):
                for i4_3_fused_init in T.vectorized(4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 16 + i1_3_init)
                        oh = T.axis.spatial(14, i2_2_init)
                        ow = T.axis.spatial(14, i3_2_init * 7 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(128, 1, 1, 1, 1, 14, 2, 1, 4, 1, 1, 1, 16, 1, 7):
                for i4_3_fused in T.vectorized(4):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 16 + i1_3)
                        oh = T.axis.spatial(14, i2_2)
                        ow = T.axis.spatial(14, i3_2 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3_fused)
                        ic = T.axis.reduce(512, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(56):
                with T.block("compute"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(128, i0_i1_i2_fused // 14)
                    i2 = T.axis.spatial(14, i0_i1_i2_fused % 14)
                    i3 = T.axis.spatial(14, i3_i4_fused // 4)
                    i4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                    T.writes(compute[i0, i1, i2, i3, i4])
                    compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 1, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l98, l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l104)
[02:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #22: GFLOPs: 5.1564. Time: 19.9872 ms. Best GFLOPs: 10.6999
[02:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #23: GFLOPs: 0.4742. Time: 217.3246 ms. Best GFLOPs: 10.6999
[02:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #24: GFLOPs: 1.8742. Time: 54.9899 ms. Best GFLOPs: 10.6999
[02:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #25: GFLOPs: 2.7456. Time: 37.5374 ms. Best GFLOPs: 10.6999
[02:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #26: GFLOPs: 4.3070. Time: 23.9287 ms. Best GFLOPs: 10.6999
[02:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #27: GFLOPs: 1.5187. Time: 67.8608 ms. Best GFLOPs: 10.6999
[02:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #28: GFLOPs: 2.8937. Time: 35.6153 ms. Best GFLOPs: 10.6999
[02:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #29: GFLOPs: 2.1557. Time: 47.8084 ms. Best GFLOPs: 10.6999
[02:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #30: GFLOPs: 3.4453. Time: 29.9134 ms. Best GFLOPs: 10.6999
[02:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"] Trial #31: GFLOPs: 3.9132. Time: 26.3369 ms. Best GFLOPs: 10.6999
[02:12:15] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 484
Total latency (us): 143876

[02:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 64, 7, 1, 2):
                for i3_3_init in T.serial(7):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 64 + i1_1)
                            oh, ow = T.axis.remap("SS", [i2_1, i3_3_init])
                            oci = T.axis.spatial(4, i4_1 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 64 + i1_1)
                            oh, ow = T.axis.remap("SS", [i2_1, i3_3])
                            oci = T.axis.spatial(4, i4_1 * 2 + i4_3_fused)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(0 <= oh * 2 + kh and oh * 2 + kh < 14 and 0 <= ow * 2 + kw and ow * 2 + kw < 14, placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2 in T.grid(1, 64, 7):
                for ax3_ax4_fused in T.vectorized(28):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 64 + ax1)
                        i2 = T.axis.spatial(7, ax2)
                        i3 = T.axis.spatial(7, ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 64, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69)
sch.parallel(loop=l89)
l90 = sch.fuse(l88)
sch.vectorize(loop=l90)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l95, l96)
sch.vectorize(loop=l97)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
b98 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b98)
b119 = sch.decompose_reduction(block=b98, loop=l105)
[02:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #1: GFLOPs: 0.0542. Time: 9.7143 ms. Best GFLOPs: 0.0542
[02:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 2):
                for i2_2_init, i1_3_init in T.grid(7, 16):
                    for i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 7 * 16 + i1_3_init)
                            oh = T.axis.spatial(7, i2_2_init)
                            ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7)
                            oci = T.axis.spatial(4, i4_1 * 2 + i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3 in T.grid(3, 1, 1, 1, 7, 1, 1, 1, 3, 1, 16):
                    for i2_3_i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 7 * 16 + i1_3)
                            oh = T.axis.spatial(7, i2_2)
                            ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7)
                            oci = T.axis.spatial(4, i4_1 * 2 + i2_3_i3_3_i4_3_fused)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(0 <= oh * 2 + kh and oh * 2 + kh < 14 and 0 <= ow * 2 + kw and ow * 2 + kw < 14, placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2 in T.grid(1, 16, 7):
                for ax3_ax4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 7 * 16 + ax1)
                        i2 = T.axis.spatial(7, ax2)
                        i3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7)
                        i4 = T.axis.spatial(4, ax3_ax4_fused)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 1, 16])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69)
sch.parallel(loop=l89)
l90 = sch.fuse(l86, l87, l88)
sch.vectorize(loop=l90)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l95, l96)
sch.vectorize(loop=l97)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
b98 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b98)
b117 = sch.decompose_reduction(block=b98, loop=l105)
[02:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #3: GFLOPs: 0.1108. Time: 4.7548 ms. Best GFLOPs: 0.1108
[02:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #4: GFLOPs: 0.1392. Time: 3.7860 ms. Best GFLOPs: 0.1392
[02:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #5: GFLOPs: 0.0464. Time: 11.3501 ms. Best GFLOPs: 0.1392
[02:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #6: GFLOPs: 0.0821. Time: 6.4204 ms. Best GFLOPs: 0.1392
[02:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(7, 1, 1, 1, 1, 1, 7, 1, 1):
                for i1_2_init, i4_2_init, i1_3_init in T.grid(16, 2, 4):
                    for i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(128, i0_0_i1_0_fused * 64 + i1_2_init * 4 + i1_3_init)
                            oh, ow = T.axis.remap("SS", [i2_0, i3_1])
                            oci = T.axis.spatial(4, i4_2_init * 2 + i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3 in T.grid(3, 1, 16, 1, 1, 2, 3, 1, 1, 4):
                    for i2_3_i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(128, i0_0_i1_0_fused * 64 + i1_2 * 4 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_0, i3_1])
                            oci = T.axis.spatial(4, i4_2 * 2 + i2_3_i3_3_i4_3_fused)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(0 <= oh * 2 + kh and oh * 2 + kh < 14 and 0 <= ow * 2 + kw and ow * 2 + kw < 14, placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_fused in T.parallel(128, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for i3_i4_fused in T.vectorized(28):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2_3 = T.axis.remap("SS", [i0_i1_fused, i2])
                        i3 = T.axis.spatial(7, i3_i4_fused // 4)
                        i4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(DepthwiseConv2d[i0, i1, i2_3, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2_3, i3, i4])
                        compute[i0, i1, i2_3, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2_3, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 16, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63 = sch.get_child_blocks(b61)
l64, l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b62)
l88 = sch.fuse(l64, l65)
sch.parallel(loop=l88)
l89 = sch.fuse(l85, l86, l87)
sch.vectorize(loop=l89)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94 = sch.get_loops(block=b63)
l95 = sch.fuse(l90, l91)
sch.parallel(loop=l95)
l96 = sch.fuse(l93, l94)
sch.vectorize(loop=l96)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b97 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b97)
b119 = sch.decompose_reduction(block=b97, loop=l108)
[02:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 128, 15, 15, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(14, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 64, 3):
                for ax3_ax4_fused in T.vectorized(60):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(128, i0_0_i1_0_i2_0_fused % 14 // 7 * 64 + ax1)
                        i2 = T.axis.spatial(15, i0_0_i1_0_i2_0_fused % 7 * 2 + ax2)
                        i3 = T.axis.spatial(15, ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 14 and 0 <= i3 and i3 < 14, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
            for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1):
                for i1_3_init in T.serial(64):
                    for i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(128, i0_0_i1_0_i2_0_fused // 7 * 64 + i1_3_init)
                            oh = T.axis.spatial(7, i0_0_i1_0_i2_0_fused % 7)
                            ow = T.axis.spatial(7, i3_2)
                            oci = T.axis.spatial(4, i4_0 * 2 + i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_1, i6_1, i0_3, i1_3 in T.grid(3, 3, 1, 64):
                    for i2_3_i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(128, i0_0_i1_0_i2_0_fused // 7 * 64 + i1_3)
                            oh = T.axis.spatial(7, i0_0_i1_0_i2_0_fused % 7)
                            ow = T.axis.spatial(7, i3_2)
                            oci = T.axis.spatial(4, i4_0 * 2 + i2_3_i3_3_i4_3_fused)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(28):
                with T.block("compute"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(128, i0_i1_i2_fused // 7)
                    i2 = T.axis.spatial(7, i0_i1_i2_fused % 7)
                    i3 = T.axis.spatial(7, i3_i4_fused // 4)
                    i4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                    T.writes(compute[i0, i1, i2, i3, i4])
                    compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 1, 64])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72 = sch.get_loops(block=b62)
l73 = sch.fuse(l65, l66, l67)
sch.parallel(loop=l73)
l74 = sch.fuse(l71, l72)
sch.vectorize(loop=l74)
sch.annotate(block_or_loop=l73, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l73, ann_key="pragma_unroll_explicit", ann_val=1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b63)
l97 = sch.fuse(l94, l95, l96)
sch.vectorize(loop=l97)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b64)
l103 = sch.fuse(l98, l99, l100)
sch.parallel(loop=l103)
l104 = sch.fuse(l101, l102)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b105)
b126 = sch.decompose_reduction(block=b105, loop=l121)
[02:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #9: GFLOPs: 0.3636. Time: 1.4488 ms. Best GFLOPs: 0.3636
[02:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #10: GFLOPs: 0.0854. Time: 6.1694 ms. Best GFLOPs: 0.3636
[02:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 128, 15, 15, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 8, 15):
                for ax3_ax4_fused in T.vectorized(60):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(128, i0_0_i1_0_fused * 8 + ax1)
                        i2 = T.axis.spatial(15, ax2)
                        i3 = T.axis.spatial(15, ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 14 and 0 <= i3 and i3 < 14, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
            for i2_0, i3_0, i4_0 in T.grid(1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 2):
                    for i1_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(4, 2, 2, 7, 7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(128, i0_0_i1_0_fused * 8 + i1_2_init * 2 + i1_3_init)
                            oh, ow = T.axis.remap("SS", [i2_3_init, i3_3_init])
                            oci = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 4, 1, 1, 2, 1, 3, 1, 2, 7, 7, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(128, i0_0_i1_0_fused * 8 + i1_2 * 2 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_3, i3_3])
                            oci = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2 in T.grid(1, 8, 7):
                    for ax3_ax4_fused in T.vectorized(28):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(128, i0_0_i1_0_fused * 8 + ax1)
                            i2 = T.axis.spatial(7, ax2)
                            i3 = T.axis.spatial(7, ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[16, 1, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72 = sch.get_loops(block=b63)
l73 = sch.fuse(l66, l67)
sch.parallel(loop=l73)
l74 = sch.fuse(l71, l72)
sch.vectorize(loop=l74)
sch.annotate(block_or_loop=l73, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l73, ann_key="pragma_unroll_explicit", ann_val=1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b65)
l107 = sch.fuse(l105, l106)
sch.vectorize(loop=l107)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b108 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b108)
b132 = sch.decompose_reduction(block=b108, loop=l118)
[02:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #12: GFLOPs: 0.2681. Time: 1.9651 ms. Best GFLOPs: 0.3636
[02:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 128, 15, 15, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(28, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 32, 15):
                for ax3_ax4_fused in T.vectorized(12):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 * 32 + ax1)
                        i2 = T.axis.spatial(15, ax2)
                        i3 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 28 // 4 * 2 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 14 and 0 <= i3 and i3 < 14, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
            for i2_1, i3_1, i4_1 in T.grid(1, 1, 1):
                for i5_0 in T.serial(1):
                    for i4_2_init, i1_3_init, i2_3_init in T.grid(2, 32, 7):
                        for i3_3_i4_3_fused_init in T.vectorized(2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 * 32 + i1_3_init)
                                oh = T.axis.spatial(7, i2_3_init)
                                ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 4)
                                oci = T.axis.spatial(4, i4_2_init * 2 + i3_3_i4_3_fused_init)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3 in T.grid(3, 1, 1, 1, 1, 2, 3, 1, 1, 32, 7):
                        for i3_3_i4_3_fused in T.vectorized(2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 * 32 + i1_3)
                                oh = T.axis.spatial(7, i2_3)
                                ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 4)
                                oci = T.axis.spatial(4, i4_2 * 2 + i3_3_i4_3_fused)
                                kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2 in T.grid(1, 32, 7):
                    for ax3_ax4_fused in T.vectorized(4):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 * 32 + ax1)
                            i2 = T.axis.spatial(7, ax2)
                            i3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 1, 32])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77 = sch.get_loops(block=b63)
l78 = sch.fuse(l66, l67, l68, l69, l70, l71, l72)
sch.parallel(loop=l78)
l79 = sch.fuse(l76, l77)
sch.vectorize(loop=l79)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
l98 = sch.fuse(l96, l97)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b65)
l108 = sch.fuse(l106, l107)
sch.vectorize(loop=l108)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b109 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b109)
b127 = sch.decompose_reduction(block=b109, loop=l115)
[02:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #14: GFLOPs: 0.1142. Time: 4.6148 ms. Best GFLOPs: 0.3636
[02:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #15: GFLOPs: 0.1054. Time: 4.9973 ms. Best GFLOPs: 0.3636
[02:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #16: GFLOPs: 0.0683. Time: 7.7123 ms. Best GFLOPs: 0.3636
[02:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #17: GFLOPs: 0.1777. Time: 2.9655 ms. Best GFLOPs: 0.3636
[02:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #18: GFLOPs: 0.0999. Time: 5.2712 ms. Best GFLOPs: 0.3636
[02:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 128, 15, 15, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(7, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 128, 15):
                for ax3_ax4_fused in T.vectorized(12):
                    with T.block("PaddedInput"):
                        i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        i3 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 14 and 0 <= i3 and i3 < 14, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
            for i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 1, 8, 1, 1, 2):
                for i1_3_init, i2_3_init in T.grid(16, 7):
                    for i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(128, i1_2 * 16 + i1_3_init)
                            oh, ow = T.axis.remap("SS", [i2_3_init, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused])
                            oci = T.axis.spatial(4, i4_2 * 2 + i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_1, i6_1, i0_3, i1_3, i2_3 in T.grid(3, 3, 1, 16, 7):
                    for i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(128, i1_2 * 16 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_3, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused])
                            oci = T.axis.spatial(4, i4_2 * 2 + i3_3_i4_3_fused)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(28):
                with T.block("compute"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(128, i0_i1_i2_fused // 7)
                    i2 = T.axis.spatial(7, i0_i1_i2_fused % 7)
                    i3 = T.axis.spatial(7, i3_i4_fused // 4)
                    i4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                    T.writes(compute[i0, i1, i2, i3, i4])
                    compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 8, 16])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78 = sch.get_loops(block=b62)
l79 = sch.fuse(l65, l66, l67, l68, l69, l70, l71, l72, l73)
sch.parallel(loop=l79)
l80 = sch.fuse(l77, l78)
sch.vectorize(loop=l80)
sch.annotate(block_or_loop=l79, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l79, ann_key="pragma_unroll_explicit", ann_val=1)
l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b63)
l97 = sch.fuse(l95, l96)
sch.vectorize(loop=l97)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b64)
l103 = sch.fuse(l98, l99, l100)
sch.parallel(loop=l103)
l104 = sch.fuse(l101, l102)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b105)
b121 = sch.decompose_reduction(block=b105, loop=l115)
[02:12:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 128, 15, 15, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(1920, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(60):
                with T.block("PaddedInput"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(128, i0_i1_i2_fused // 15)
                    i2 = T.axis.spatial(15, i0_i1_i2_fused % 15)
                    i3 = T.axis.spatial(15, i3_i4_fused // 4)
                    i4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(placeholder[i0, i1, i2, i3, i4])
                    T.writes(PaddedInput[i0, i1, i2, i3, i4])
                    PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 14 and 0 <= i3 and i3 < 14, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(16, 2, 4, 7, 7):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_fused * 64 + i1_2_init * 4 + i1_3_init)
                        oh, ow = T.axis.remap("SS", [i2_3_init, i3_3_init])
                        oci = T.axis.spatial(4, i4_2_init * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
            for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 1, 16, 1, 1, 2, 3, 1, 1, 4, 7, 7):
                for i4_3_fused in T.vectorized(2):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_fused * 64 + i1_2 * 4 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_3])
                        oci = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(28):
                with T.block("compute"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(128, i0_i1_i2_fused // 7)
                    i2 = T.axis.spatial(7, i0_i1_i2_fused % 7)
                    i3 = T.axis.spatial(7, i3_i4_fused // 4)
                    i4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                    T.writes(compute[i0, i1, i2, i3, i4])
                    compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 16, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69 = sch.get_loops(block=b62)
l70 = sch.fuse(l65, l66, l67)
sch.parallel(loop=l70)
l71 = sch.fuse(l68, l69)
sch.vectorize(loop=l71)
sch.annotate(block_or_loop=l70, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l70, ann_key="pragma_unroll_explicit", ann_val=1)
l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b63)
l96 = sch.fuse(l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82)
sch.parallel(loop=l96)
l97 = sch.fuse(l95)
sch.vectorize(loop=l97)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b64)
l103 = sch.fuse(l98, l99, l100)
sch.parallel(loop=l103)
l104 = sch.fuse(l101, l102)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b105)
b120 = sch.decompose_reduction(block=b105, loop=l107)
[02:12:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #21: GFLOPs: 0.0124. Time: 42.6538 ms. Best GFLOPs: 0.3636
[02:12:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #22: GFLOPs: 0.0591. Time: 8.9155 ms. Best GFLOPs: 0.3636
[02:12:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #23: GFLOPs: 0.0700. Time: 7.5302 ms. Best GFLOPs: 0.3636
[02:12:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #24: GFLOPs: 0.0888. Time: 5.9309 ms. Best GFLOPs: 0.3636
[02:12:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #25: GFLOPs: 0.1227. Time: 4.2925 ms. Best GFLOPs: 0.3636
[02:12:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #26: GFLOPs: 0.1201. Time: 4.3881 ms. Best GFLOPs: 0.3636
[02:12:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(28, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i5_0 in T.serial(1):
                for i1_2_init, i3_2_init, i1_3_init in T.grid(2, 7, 64):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(128, i1_2_init * 64 + i1_3_init)
                        oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4)
                        ow = T.axis.spatial(7, i3_2_init)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 2, 1, 7, 1, 3, 1, 1, 64, 1, 1, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(128, i1_2 * 64 + i1_3)
                        oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4)
                        ow = T.axis.spatial(7, i3_2)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(0 <= oh * 2 + kh and oh * 2 + kh < 14 and 0 <= ow * 2 + kw and ow * 2 + kw < 14, placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 1, 7, 1):
                with T.block("compute"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(128, ax1)
                    i2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4)
                    i3 = T.axis.spatial(7, ax3)
                    i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                    T.writes(compute[i0, i1, i2, i3, i4])
                    compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 64])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l89)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
b96 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l97, l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b96)
b112 = sch.decompose_reduction(block=b96, loop=l99)
[02:12:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #28: GFLOPs: 0.1346. Time: 3.9145 ms. Best GFLOPs: 0.3636
[02:12:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 128, 15, 15, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(1920, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(60):
                with T.block("PaddedInput"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(128, i0_i1_i2_fused // 15)
                    i2 = T.axis.spatial(15, i0_i1_i2_fused % 15)
                    i3 = T.axis.spatial(15, i3_i4_fused // 4)
                    i4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(placeholder[i0, i1, i2, i3, i4])
                    T.writes(PaddedInput[i0, i1, i2, i3, i4])
                    PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 14 and 0 <= i3 and i3 < 14, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_3_init, i3_3_init in T.grid(32, 7):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 28 * 64 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 7 * 32 + i1_3_init)
                        oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7)
                        ow = T.axis.spatial(7, i3_3_init)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 28 // 14 * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
            for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 32, 1, 7):
                for i4_3_fused in T.vectorized(2):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 28 * 64 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 7 * 32 + i1_3)
                        oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7)
                        ow = T.axis.spatial(7, i3_3)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 28 // 14 * 2 + i4_3_fused)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 7):
                for ax4_fused in T.vectorized(2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 28 * 64 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 7 * 32 + ax1)
                        i2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7)
                        i3 = T.axis.spatial(7, ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 28 // 14 * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 1, 32])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70 = sch.get_loops(block=b63)
l71 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l71)
l72 = sch.fuse(l69, l70)
sch.vectorize(loop=l72)
sch.annotate(block_or_loop=l71, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l71, ann_key="pragma_unroll_explicit", ann_val=1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l73, l74, l75, l76, l77, l78, l79, l80, l81, l82)
sch.parallel(loop=l97)
l98 = sch.fuse(l96)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b65)
l105 = sch.fuse(l104)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b106)
b122 = sch.decompose_reduction(block=b106, loop=l108)
[02:12:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i1_3_init, i3_3_init in T.grid(16, 2, 7):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 32 + i1_2_init * 2 + i1_3_init)
                        oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8)
                        ow = T.axis.spatial(7, i3_3_init)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
            for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 1, 1, 16, 1, 1, 1, 1, 3, 1, 2, 1, 7):
                for i4_3_fused in T.vectorized(2):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 32 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8)
                        ow = T.axis.spatial(7, i3_3)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 2 + i4_3_fused)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(0 <= oh * 2 + kh and oh * 2 + kh < 14 and 0 <= ow * 2 + kw and ow * 2 + kw < 14, placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 7):
                for ax4_fused in T.vectorized(2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 32 + ax1)
                        i2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8)
                        i3 = T.axis.spatial(7, ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 16, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l89)
l90 = sch.fuse(l88)
sch.vectorize(loop=l90)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l96)
sch.vectorize(loop=l97)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
b98 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b98)
b114 = sch.decompose_reduction(block=b98, loop=l100)
[02:12:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], compute: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i1_3_init, i3_3_init in T.grid(64, 7, 2, 7):
                with T.block("DepthwiseConv2d_init"):
                    b = T.axis.spatial(1, 0)
                    oco = T.axis.spatial(128, i1_2_init * 2 + i1_3_init)
                    oh, ow, oci = T.axis.remap("SSS", [i2_2_init, i3_3_init, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused])
                    T.reads()
                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
            for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 64, 7, 1, 1, 1, 1, 1, 2, 1, 7, 1):
                with T.block("DepthwiseConv2d_update"):
                    b = T.axis.spatial(1, 0)
                    oco = T.axis.spatial(128, i1_2 * 2 + i1_3)
                    oh, ow, oci, kh, kw = T.axis.remap("SSSRR", [i2_2, i3_3, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused, i5_0, i6_0])
                    T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(0 <= oh * 2 + kh and oh * 2 + kh < 14 and 0 <= ow * 2 + kw and ow * 2 + kw < 14, placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 7, 7, 1):
                with T.block("compute"):
                    i0 = T.axis.spatial(1, 0)
                    i1, i2, i3, i4 = T.axis.remap("SSSS", [ax1, ax2, ax3, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused])
                    T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                    T.writes(compute[i0, i1, i2, i3, i4])
                    compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 64, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l89)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
b96 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l97, l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b96)
b112 = sch.decompose_reduction(block=b96, loop=l98)
[02:13:10] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 516
Total latency (us): 145325

[02:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #0: GFLOPs: 0.8220. Time: 62.6931 ms. Best GFLOPs: 0.8220
[02:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #1: GFLOPs: 4.9506. Time: 10.4090 ms. Best GFLOPs: 4.9506
[02:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #2: GFLOPs: 1.4908. Time: 34.5666 ms. Best GFLOPs: 4.9506
[02:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #3: GFLOPs: 3.8896. Time: 13.2483 ms. Best GFLOPs: 4.9506
[02:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #4: GFLOPs: 1.4871. Time: 34.6528 ms. Best GFLOPs: 4.9506
[02:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i4_2_init in T.grid(64, 7, 2):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 14 * 64 + i1_2_init)
                    oh = T.axis.spatial(7, i2_2_init)
                    ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 2)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 64, 7, 1, 2, 8, 1, 1, 1, 1, 1, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 14 * 64 + i1_2)
                    oh = T.axis.spatial(7, i2_2)
                    ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 2)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2)
                    ic = T.axis.reduce(512, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 64, 7):
                for ax3_ax4_fused in T.vectorized(2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 14 * 64 + ax1)
                        i2 = T.axis.spatial(7, ax2)
                        i3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 2)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + ax3_ax4_fused)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 64, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l99, l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l104)
[02:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #6: GFLOPs: 2.6058. Time: 19.7750 ms. Best GFLOPs: 4.9506
[02:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #7: GFLOPs: 1.3420. Time: 38.3986 ms. Best GFLOPs: 4.9506
[02:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #8: GFLOPs: 0.9857. Time: 52.2757 ms. Best GFLOPs: 4.9506
[02:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #9: GFLOPs: 4.1416. Time: 12.4423 ms. Best GFLOPs: 4.9506
[02:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 1, 4):
                for i1_2_init, i2_2_init, i1_3_init, i3_3_init in T.grid(8, 7, 2, 7):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 64 + i1_1 * 16 + i1_2_init * 2 + i1_3_init)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_2_init, i3_3_init, i4_1])
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 8, 7, 1, 1, 16, 1, 1, 1, 2, 1, 7, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 64 + i1_1 * 16 + i1_2 * 2 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_2, i3_3, i4_1])
                        ic = T.axis.reduce(512, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 64, 7):
                for ax3_ax4_fused in T.vectorized(28):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 64 + ax1)
                        i2 = T.axis.spatial(7, ax2)
                        i3 = T.axis.spatial(7, ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 4, 8, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l99, l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[02:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1, i4_1 in T.grid(1, 2):
                for i4_2_init, i1_3_init, i3_3_init in T.grid(2, 8, 7):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 112 * 128 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 112 // 7 * 8 + i1_3_init)
                        oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7)
                        ow = T.axis.spatial(7, i3_3_init)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 1, 1, 1, 2, 8, 1, 1, 1, 8, 1, 7, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 112 * 128 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 112 // 7 * 8 + i1_3)
                        oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7)
                        ow = T.axis.spatial(7, i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(512, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for i3_i4_fused in T.vectorized(28):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2_1 = T.axis.remap("SS", [i0_i1_fused, i2])
                        i3 = T.axis.spatial(7, i3_i4_fused // 4)
                        i4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(conv2d_NCHWc[i0, i1, i2_1, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2_1, i3, i4])
                        compute[i0, i1, i2_1, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2_1, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 16, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95)
sch.parallel(loop=l99)
l100 = sch.fuse(l97, l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b101)
b121 = sch.decompose_reduction(block=b101, loop=l105)
[02:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(448, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_1, i3_1, i4_1 in T.grid(1, 1, 2):
                for i1_2_init, i3_3_init in T.grid(8, 7):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 56 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 * 8 + i1_2_init)
                        oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 56 // 8)
                        ow = T.axis.spatial(7, i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 8 // 4 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 8, 1, 1, 1, 8, 1, 1, 1, 1, 1, 7, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 56 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 * 8 + i1_2)
                        oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 56 // 8)
                        ow = T.axis.spatial(7, i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 8 // 4 * 2 + i4_1)
                        ic = T.axis.reduce(512, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for i3_i4_fused in T.vectorized(28):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2_4 = T.axis.remap("SS", [i0_i1_fused, i2])
                        i3 = T.axis.spatial(7, i3_i4_fused // 4)
                        i4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(conv2d_NCHWc[i0, i1, i2_4, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2_4, i3, i4])
                        compute[i0, i1, i2_4, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2_4, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 4, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95)
sch.parallel(loop=l99)
l100 = sch.fuse(l97, l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b101)
b122 = sch.decompose_reduction(block=b101, loop=l106)
[02:13:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #13: GFLOPs: 3.1349. Time: 16.4378 ms. Best GFLOPs: 4.9506
[02:13:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #14: GFLOPs: 3.4110. Time: 15.1070 ms. Best GFLOPs: 4.9506
[02:13:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #15: GFLOPs: 1.7256. Time: 29.8630 ms. Best GFLOPs: 4.9506
[02:13:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #16: GFLOPs: 2.4037. Time: 21.4377 ms. Best GFLOPs: 4.9506
[02:13:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #17: GFLOPs: 1.1044. Time: 46.6614 ms. Best GFLOPs: 4.9506
[02:13:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #18: GFLOPs: 3.1238. Time: 16.4964 ms. Best GFLOPs: 4.9506
[02:13:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #19: GFLOPs: 1.7773. Time: 28.9945 ms. Best GFLOPs: 4.9506
[02:13:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #20: GFLOPs: 2.1067. Time: 24.4608 ms. Best GFLOPs: 4.9506
[02:13:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #21: GFLOPs: 2.4158. Time: 21.3310 ms. Best GFLOPs: 4.9506
[02:13:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #22: GFLOPs: 1.4318. Time: 35.9908 ms. Best GFLOPs: 4.9506
[02:13:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        for i0_0 in T.serial(1, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1):
                    for i1_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(64, 4, 7, 7):
                        for i4_3_fused_init in T.vectorized(4):
                            with T.block("conv2d_NCHWc_init"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(256, i1_2_init * 4 + i1_3_init)
                                oh, ow, oc_block = T.axis.remap("SSS", [i2_3_init, i3_3_init, i4_3_fused_init])
                                T.reads()
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(256, 1, 1, 1, 64, 1, 1, 1, 2, 1, 1, 1, 4, 7, 7):
                        for i4_3_fused in T.vectorized(4):
                            with T.block("conv2d_NCHWc_update"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(256, i1_2 * 4 + i1_3)
                                oh, ow, oc_block = T.axis.remap("SSS", [i2_3, i3_3, i4_3_fused])
                                ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                                kh = T.axis.reduce(1, 0)
                                kw = T.axis.reduce(1, 0)
                                T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2 in T.grid(1, 256, 7):
                    for ax3_ax4_fused in T.vectorized(28):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1, i2 = T.axis.remap("SS", [ax1, ax2])
                            i3 = T.axis.spatial(7, ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 64, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l93)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l68, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l68, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b67)
l105 = sch.fuse(l103, l104)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b106)
b133 = sch.decompose_reduction(block=b106, loop=l117)
[02:13:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #24: GFLOPs: 0.4112. Time: 125.3217 ms. Best GFLOPs: 4.9506
[02:13:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 7, 1, 1):
                for i1_2_init, i3_2_init in T.grid(64, 7):
                    for i4_2_i5_1_i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init in T.vectorized(4):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 64 + i1_2_init)
                            oh, ow = T.axis.remap("SS", [i2_1, i3_2_init])
                            oc_block = T.axis.spatial(4, i4_2_i5_1_i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init // 2 * 2 + i4_2_i5_1_i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init % 2)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2 in T.grid(512, 1, 1, 1, 64, 1, 7):
                    for i4_2_i5_1_i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused in T.vectorized(4):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 64 + i1_2)
                            oh, ow = T.axis.remap("SS", [i2_1, i3_2])
                            oc_block = T.axis.spatial(4, i4_2_i5_1_i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused // 2 * 2 + i4_2_i5_1_i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused % 2)
                            ic = T.axis.reduce(512, i5_0)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 64, 7):
                for ax3_ax4_fused in T.vectorized(28):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 64 + ax1)
                        i2 = T.axis.spatial(7, ax2)
                        i3 = T.axis.spatial(7, ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 64, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[512, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l85, l86, l87, l88, l89, l90, l91, l92, l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l100, l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b103)
b118 = sch.decompose_reduction(block=b103, loop=l110)
[02:13:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #26: GFLOPs: 1.5159. Time: 33.9933 ms. Best GFLOPs: 4.9506
[02:13:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #27: GFLOPs: 2.5207. Time: 20.4429 ms. Best GFLOPs: 4.9506
[02:13:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i4_2_init, i1_3_init, i3_3_init in T.grid(64, 7, 2, 2, 7):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 128 + i1_2_init * 2 + i1_3_init)
                        oh, ow = T.axis.remap("SS", [i2_2_init, i3_3_init])
                        oc_block = T.axis.spatial(4, i4_2_init * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(16, 1, 1, 1, 64, 7, 1, 2, 32, 1, 1, 1, 2, 1, 7):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 128 + i1_2 * 2 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_3])
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                        ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [256, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for i3_i4_fused in T.vectorized(28):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2_1 = T.axis.remap("SS", [i0_i1_fused, i2])
                        i3 = T.axis.spatial(7, i3_i4_fused // 4)
                        i4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(conv2d_NCHWc[i0, i1, i2_1, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2_1, i3, i4])
                        compute[i0, i1, i2_1, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2_1, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 64, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96)
sch.parallel(loop=l100)
l101 = sch.fuse(l98, l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l104)
[02:13:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #29: GFLOPs: 0.4343. Time: 118.6568 ms. Best GFLOPs: 4.9506
[02:13:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #30: GFLOPs: 0.8439. Time: 61.0616 ms. Best GFLOPs: 4.9506
[02:13:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"] Trial #31: GFLOPs: 2.5022. Time: 20.5940 ms. Best GFLOPs: 4.9506
[02:13:56] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |         4.9506 |   10408.9894 |            10408.9894 |     32 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 548
Total latency (us): 155734

[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #0: GFLOPs: 0.1586. Time: 6.6417 ms. Best GFLOPs: 0.1586
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #1: GFLOPs: 0.1707. Time: 6.1726 ms. Best GFLOPs: 0.1707
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #2: GFLOPs: 0.1593. Time: 6.6149 ms. Best GFLOPs: 0.1707
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #3: GFLOPs: 0.1202. Time: 8.7687 ms. Best GFLOPs: 0.1707
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #4: GFLOPs: 0.0988. Time: 10.6655 ms. Best GFLOPs: 0.1707
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #5: GFLOPs: 0.1401. Time: 7.5199 ms. Best GFLOPs: 0.1707
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #6: GFLOPs: 0.0758. Time: 13.8937 ms. Best GFLOPs: 0.1707
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #7: GFLOPs: 0.0293. Time: 35.9982 ms. Best GFLOPs: 0.1707
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #8: GFLOPs: 0.1850. Time: 5.6942 ms. Best GFLOPs: 0.1850
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 256, 9, 9, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(196, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 1, 1, 1, 1, 1):
                for i1_2_init in T.serial(128):
                    for i6_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 98 * 128 + i1_2_init)
                            oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 98 // 14)
                            ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 // 2)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i6_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i6_0 in T.serial(3):
                    for ax0, ax1, ax2, ax3 in T.grid(1, 128, 3, 1):
                        for ax4_fused in T.vectorized(2):
                            with T.block("PaddedInput"):
                                i0 = T.axis.spatial(1, ax0)
                                i1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 196 // 98 * 128 + ax1)
                                i2 = T.axis.spatial(9, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 98 // 14 + ax2)
                                i3 = T.axis.spatial(9, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 // 2 + i6_0 + ax3)
                                i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                                T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                T.writes(PaddedInput[i0, i1, i2, i3, i4])
                                PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1 in T.grid(1, 128, 1, 1, 1, 3):
                        for i6_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused in T.vectorized(2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 98 * 128 + i1_2)
                                oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 98 // 14)
                                ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 // 2)
                                oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i6_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused)
                                kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1 in T.grid(1, 128):
                for ax2_ax3_ax4_fused in T.vectorized(2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 98 * 128 + ax1)
                        i2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 98 // 14)
                        i3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 // 2)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax2_ax3_ax4_fused)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 128, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b63)
l83 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l83)
l84 = sch.fuse(l82)
sch.vectorize(loop=l84)
sch.annotate(block_or_loop=l83, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l83, ann_key="pragma_unroll_explicit", ann_val=1)
l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b64)
l105 = sch.fuse(l99, l100, l101, l102, l103, l104)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l85, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l85, ann_key="pragma_unroll_explicit", ann_val=1)
l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b65)
l112 = sch.fuse(l109, l110, l111)
sch.vectorize(loop=l112)
sch.annotate(block_or_loop=l106, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l106, ann_key="pragma_unroll_explicit", ann_val=1)
b113 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b113)
b129 = sch.decompose_reduction(block=b113, loop=l121)
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #10: GFLOPs: 0.1839. Time: 5.7289 ms. Best GFLOPs: 0.1850
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #11: GFLOPs: 0.1627. Time: 6.4771 ms. Best GFLOPs: 0.1850
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #12: GFLOPs: 0.2889. Time: 3.6471 ms. Best GFLOPs: 0.2889
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #13: GFLOPs: 0.0626. Time: 16.8344 ms. Best GFLOPs: 0.2889
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #14: GFLOPs: 4.7165. Time: 0.2234 ms. Best GFLOPs: 4.7165
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #15: GFLOPs: 0.1143. Time: 9.2157 ms. Best GFLOPs: 4.7165
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #16: GFLOPs: 0.1317. Time: 7.9999 ms. Best GFLOPs: 4.7165
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #17: GFLOPs: 0.0561. Time: 18.7988 ms. Best GFLOPs: 4.7165
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #18: GFLOPs: 0.0738. Time: 14.2714 ms. Best GFLOPs: 4.7165
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #19: GFLOPs: 0.0671. Time: 15.7128 ms. Best GFLOPs: 4.7165
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 256, 9, 9, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(64, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 4, 9):
                for ax3_ax4_fused in T.vectorized(36):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 32 * 128 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 32 * 4 + ax1)
                        i2 = T.axis.spatial(9, ax2)
                        i3 = T.axis.spatial(9, ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_1, i3_1, i4_1, i5_0 in T.grid(1, 1, 2, 1):
                for i1_2_init, i3_2_init, i4_2_init, i2_3_init in T.grid(4, 7, 2, 7):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 32 * 128 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 32 * 4 + i1_2_init)
                        oh, ow = T.axis.remap("SS", [i2_3_init, i3_2_init])
                        oci = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 4, 1, 7, 2, 3, 1, 1, 1, 7, 1, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 32 * 128 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 32 * 4 + i1_2)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_2])
                        oci = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for i3_i4_fused in T.vectorized(28):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2_4 = T.axis.remap("SS", [i0_i1_fused, i2])
                        i3 = T.axis.spatial(7, i3_i4_fused // 4)
                        i4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(DepthwiseConv2d[i0, i1, i2_4, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2_4, i3, i4])
                        compute[i0, i1, i2_4, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2_4, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 32, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b62)
l77 = sch.fuse(l65, l66, l67, l68, l69, l70, l71)
sch.parallel(loop=l77)
l78 = sch.fuse(l75, l76)
sch.vectorize(loop=l78)
sch.annotate(block_or_loop=l77, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l77, ann_key="pragma_unroll_explicit", ann_val=1)
l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b63)
sch.annotate(block_or_loop=l79, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l79, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b64)
l102 = sch.fuse(l97, l98)
sch.parallel(loop=l102)
l103 = sch.fuse(l100, l101)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b104)
b123 = sch.decompose_reduction(block=b104, loop=l110)
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #21: GFLOPs: 0.0828. Time: 12.7256 ms. Best GFLOPs: 4.7165
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #22: GFLOPs: 0.0734. Time: 14.3516 ms. Best GFLOPs: 4.7165
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #23: GFLOPs: 0.1565. Time: 6.7330 ms. Best GFLOPs: 4.7165
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 256, 9, 9, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(9):
                for i3_i4_fused in T.vectorized(36):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2_1 = T.axis.remap("SS", [i0_i1_fused, i2])
                        i3 = T.axis.spatial(9, i3_i4_fused // 4)
                        i4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(placeholder[i0, i1, i2_1 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2_1, i3, i4])
                        PaddedInput[i0, i1, i2_1, i3, i4] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2_1 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 1):
                for i1_2_init, i3_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(2, 7, 2, 8, 7):
                    for i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 32 + i1_1 * 16 + i1_2_init * 8 + i1_3_init)
                            oh, ow = T.axis.remap("SS", [i2_3_init, i3_2_init])
                            oci = T.axis.spatial(4, i4_2_init * 2 + i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3 in T.grid(3, 1, 1, 2, 1, 7, 2, 1, 3, 1, 8, 7):
                    for i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 32 + i1_1 * 16 + i1_2 * 8 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_3, i3_2])
                            oci = T.axis.spatial(4, i4_2 * 2 + i3_3_i4_3_fused)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2 in T.grid(1, 32, 7):
                for ax3_ax4_fused in T.vectorized(28):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 32 + ax1)
                        i2 = T.axis.spatial(7, ax2)
                        i3 = T.axis.spatial(7, ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 2, 2, 8])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70 = sch.get_loops(block=b63)
l71 = sch.fuse(l66, l67)
sch.parallel(loop=l71)
l72 = sch.fuse(l69, l70)
sch.vectorize(loop=l72)
sch.annotate(block_or_loop=l71, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l71, ann_key="pragma_unroll_explicit", ann_val=1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l73, l74, l75, l76, l77)
sch.parallel(loop=l97)
l98 = sch.fuse(l95, l96)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b65)
l105 = sch.fuse(l103, l104)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b106)
b126 = sch.decompose_reduction(block=b106, loop=l113)
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 256, 9, 9, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(9):
                for i3_i4_fused in T.vectorized(36):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2_1 = T.axis.remap("SS", [i0_i1_fused, i2])
                        i3 = T.axis.spatial(9, i3_i4_fused // 4)
                        i4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(placeholder[i0, i1, i2_1 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2_1, i3, i4])
                        PaddedInput[i0, i1, i2_1, i3, i4] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2_1 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1_1, i3_1, i4_1 in T.grid(1, 16, 7, 1, 1):
                for i1_2_init, i3_2_init, i1_3_init in T.grid(4, 7, 4):
                    for i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(256, i1_1 * 16 + i1_2_init * 4 + i1_3_init)
                            oh, ow = T.axis.remap("SS", [i2_1_1, i3_2_init])
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3 in T.grid(3, 1, 1, 4, 1, 7, 1, 1, 3, 1, 4):
                    for i2_3_i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(256, i1_1 * 16 + i1_2 * 4 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_1_1, i3_2])
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + i2_3_i3_3_i4_3_fused)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 256, 7, 7):
                for ax4_fused in T.vectorized(2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2, i3 = T.axis.remap("SSS", [ax1, ax2, ax3])
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 16, 4, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70 = sch.get_loops(block=b63)
l71 = sch.fuse(l66, l67)
sch.parallel(loop=l71)
l72 = sch.fuse(l69, l70)
sch.vectorize(loop=l72)
sch.annotate(block_or_loop=l71, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l71, ann_key="pragma_unroll_explicit", ann_val=1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l73, l74, l75, l76, l77)
sch.parallel(loop=l97)
l98 = sch.fuse(l94, l95, l96)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b65)
l105 = sch.fuse(l104)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b106)
b125 = sch.decompose_reduction(block=b106, loop=l113)
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #26: GFLOPs: 0.1668. Time: 6.3164 ms. Best GFLOPs: 4.7165
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #27: GFLOPs: 11.3630. Time: 0.0927 ms. Best GFLOPs: 11.3630
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #28: GFLOPs: 0.1615. Time: 6.5241 ms. Best GFLOPs: 11.3630
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #29: GFLOPs: 0.2015. Time: 5.2290 ms. Best GFLOPs: 11.3630
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #30: GFLOPs: 0.0781. Time: 13.4989 ms. Best GFLOPs: 11.3630
[02:14:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"] Trial #31: GFLOPs: 0.2440. Time: 4.3178 ms. Best GFLOPs: 11.3630
[02:14:42] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |         4.9506 |   10408.9894 |            10408.9894 |     32 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |        11.3630 |      92.7300 |               92.7300 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 580
Total latency (us): 155827

[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #0: GFLOPs: 0.9306. Time: 110.5846 ms. Best GFLOPs: 0.9306
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #1: GFLOPs: 1.1545. Time: 89.1368 ms. Best GFLOPs: 1.1545
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #2: GFLOPs: 1.8056. Time: 56.9956 ms. Best GFLOPs: 1.8056
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #3: GFLOPs: 5.9804. Time: 17.2080 ms. Best GFLOPs: 5.9804
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #4: GFLOPs: 4.4367. Time: 23.1956 ms. Best GFLOPs: 5.9804
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #5: GFLOPs: 1.4035. Time: 73.3234 ms. Best GFLOPs: 5.9804
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #6: GFLOPs: 1.9299. Time: 53.3258 ms. Best GFLOPs: 5.9804
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #7: GFLOPs: 3.5246. Time: 29.1978 ms. Best GFLOPs: 5.9804
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #8: GFLOPs: 2.4912. Time: 41.3103 ms. Best GFLOPs: 5.9804
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #9: GFLOPs: 6.1380. Time: 16.7661 ms. Best GFLOPs: 6.1380
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i1_3_init, i3_3_init in T.grid(32, 7, 4, 7):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 128 + i1_2_init * 4 + i1_3_init)
                    oh, ow = T.axis.remap("SS", [i2_2_init, i3_3_init])
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1024, 1, 1, 1, 32, 7, 1, 1, 1, 1, 1, 1, 4, 1, 7, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 128 + i1_2 * 4 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_2, i3_3])
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2)
                    ic = T.axis.reduce(1024, i5_0)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for i3_i4_fused in T.vectorized(28):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2_1 = T.axis.remap("SS", [i0_i1_fused, i2])
                        i3 = T.axis.spatial(7, i3_i4_fused // 4)
                        i4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(conv2d_NCHWc[i0, i1, i2_1, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2_1, i3, i4])
                        compute[i0, i1, i2_1, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2_1, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 32, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1024, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95)
sch.parallel(loop=l99)
l100 = sch.fuse(l97, l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #11: GFLOPs: 9.5739. Time: 10.7491 ms. Best GFLOPs: 9.5739
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #12: GFLOPs: 8.5772. Time: 11.9983 ms. Best GFLOPs: 9.5739
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #13: GFLOPs: 1.0156. Time: 101.3320 ms. Best GFLOPs: 9.5739
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #14: GFLOPs: 9.8713. Time: 10.4253 ms. Best GFLOPs: 9.8713
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(64, 2, 7, 7):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 128 + i1_2_init * 2 + i1_3_init)
                    oh, ow = T.axis.remap("SS", [i2_3_init, i3_3_init])
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(256, 1, 1, 1, 64, 1, 1, 1, 4, 1, 1, 1, 2, 7, 7, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 128 + i1_2 * 2 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_3, i3_3])
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    ic = T.axis.reduce(1024, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 7, 7, 1):
                with T.block("compute"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 128 + ax1)
                    i2, i3 = T.axis.remap("SS", [ax2, ax3])
                    i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                    T.writes(compute[i0, i1, i2, i3, i4])
                    compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 64, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #16: GFLOPs: 0.3853. Time: 267.0663 ms. Best GFLOPs: 9.8713
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #17: GFLOPs: 7.0195. Time: 14.6608 ms. Best GFLOPs: 9.8713
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #18: GFLOPs: 9.3715. Time: 10.9813 ms. Best GFLOPs: 9.8713
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_1, i3_1, i4_1 in T.grid(7, 1, 2):
                for i1_2_init in T.serial(8):
                    for i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 112 * 128 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 16 * 8 + i1_2_init)
                            oh = T.axis.spatial(7, i2_1)
                            ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 112 // 16)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1 in T.grid(16, 1, 1, 1, 8, 1, 1, 1, 64):
                    for i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 112 * 128 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 16 * 8 + i1_2)
                            oh = T.axis.spatial(7, i2_1)
                            ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 112 // 16)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused)
                            ic = T.axis.reduce(1024, i5_0 * 64 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1 in T.grid(1, 8):
                    for ax2_ax3_ax4_fused in T.vectorized(2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 112 * 128 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 16 * 8 + ax1)
                            i2 = T.axis.spatial(7, i2_1)
                            i3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 112 // 16)
                            i4 = T.axis.spatial(4, i4_1 * 2 + ax2_ax3_ax4_fused)
                            T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 16, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l94)
l95 = sch.fuse(l87, l88, l89, l90, l91, l92, l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b67)
l105 = sch.fuse(l102, l103, l104)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b106)
b121 = sch.decompose_reduction(block=b106, loop=l111)
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #20: GFLOPs: 2.7994. Time: 36.7621 ms. Best GFLOPs: 9.8713
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #21: GFLOPs: 7.4697. Time: 13.7770 ms. Best GFLOPs: 9.8713
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #22: GFLOPs: 9.5357. Time: 10.7922 ms. Best GFLOPs: 9.8713
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #23: GFLOPs: 4.2889. Time: 23.9950 ms. Best GFLOPs: 9.8713
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #24: GFLOPs: 9.5293. Time: 10.7994 ms. Best GFLOPs: 9.8713
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #25: GFLOPs: 3.2672. Time: 31.4982 ms. Best GFLOPs: 9.8713
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], compute: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(28, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(64, 2, 2, 7):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 14 * 128 + i1_2_init * 2 + i1_3_init)
                    oh = T.axis.spatial(7, i2_3_init)
                    ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 2)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 64, 1, 1, 2, 16, 1, 1, 1, 2, 7, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 14 * 128 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(7, i2_3)
                    ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 2)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2)
                    ic = T.axis.reduce(1024, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 7, 7, 4], "float32"], ["TENSOR", [256, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for i3_i4_fused in T.vectorized(28):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2_1 = T.axis.remap("SS", [i0_i1_fused, i2])
                        i3 = T.axis.spatial(7, i3_i4_fused // 4)
                        i4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(conv2d_NCHWc[i0, i1, i2_1, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2_1, i3, i4])
                        compute[i0, i1, i2_1, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2_1, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 64, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95)
sch.parallel(loop=l99)
l100 = sch.fuse(l97, l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #27: GFLOPs: 14.6045. Time: 7.0465 ms. Best GFLOPs: 14.6045
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #28: GFLOPs: 5.5496. Time: 18.5437 ms. Best GFLOPs: 14.6045
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #29: GFLOPs: 6.5829. Time: 15.6331 ms. Best GFLOPs: 14.6045
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #30: GFLOPs: 1.2655. Time: 81.3226 ms. Best GFLOPs: 14.6045
[02:14:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"] Trial #31: GFLOPs: 8.5776. Time: 11.9976 ms. Best GFLOPs: 14.6045
[02:15:30] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |         4.9506 |   10408.9894 |            10408.9894 |     32 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |        11.3630 |      92.7300 |               92.7300 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |        14.6045 |    7046.5233 |             7046.5233 |     32 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 612
Total latency (us): 162874

[02:15:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #20: "fused_nn_avg_pool2d"] Trial #0: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 256, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 256, 1, 1, 4], dtype="float32")
        tensor_rf = T.alloc_buffer([1, 256, 1, 1, 4, 7], dtype="float32")
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(7, 1, 1, 1, 1, 4):
                with T.block("tensor_rf_init"):
                    vi5_i6_fused_0 = T.axis.spatial(7, ax0)
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(256, i0_i1_fused)
                    ax2_1 = T.axis.spatial(1, 0)
                    ax3_1 = T.axis.spatial(1, 0)
                    ax4_1 = T.axis.spatial(4, ax5)
                    T.reads()
                    T.writes(tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0])
                    tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0] = T.float32(0)
                for ax6 in T.serial(7):
                    with T.block("tensor_rf_update"):
                        vi5_i6_fused_0 = T.axis.spatial(7, ax0)
                        ax0_2 = T.axis.spatial(1, 0)
                        ax1_2 = T.axis.spatial(256, i0_i1_fused)
                        ax2_2 = T.axis.spatial(1, 0)
                        ax3_2 = T.axis.spatial(1, 0)
                        ax4_2, vi5_i6_fused_1 = T.axis.remap("SR", [ax5, ax6])
                        T.reads(tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_0], placeholder[ax0_2, ax1_2, ax2_2 * 2 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) // 7, ax3_2 * 2 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) % 7, ax4_2])
                        T.writes(tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_0])
                        tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_0] = tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_0] + placeholder[ax0_2, ax1_2, ax2_2 * 2 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) // 7, ax3_2 * 2 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) % 7, ax4_2]
            for i2, i3, i4 in T.grid(1, 1, 4):
                with T.block("tensor_init"):
                    ax0_3 = T.axis.spatial(1, 0)
                    ax1_3 = T.axis.spatial(256, i0_i1_fused)
                    ax2_3 = T.axis.spatial(1, 0)
                    ax3_3 = T.axis.spatial(1, 0)
                    ax4_3 = T.axis.spatial(4, i4)
                    T.reads()
                    T.writes(tensor_1[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3])
                    tensor_1[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3] = T.float32(0)
                for ax0_4, ax1_4, ax2_4, ax3_4, ax4_4, ax5 in T.grid(7, 1, 1, 1, 1, 1):
                    with T.block("tensor_update"):
                        vi5_i6_fused_0 = T.axis.reduce(7, ax0_4)
                        ax0_5 = T.axis.spatial(1, 0)
                        ax1_5 = T.axis.spatial(256, i0_i1_fused)
                        ax2_5 = T.axis.spatial(1, 0)
                        ax3_5 = T.axis.spatial(1, 0)
                        ax4_5 = T.axis.spatial(4, i4)
                        T.reads(tensor_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4_5], tensor_rf[ax0_5, ax1_5, ax2_5, ax3_5, ax4_5, vi5_i6_fused_0])
                        T.writes(tensor_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4_5])
                        tensor_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4_5] = tensor_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4_5] + tensor_rf[ax0_5, ax1_5, ax2_5, ax3_5, ax4_5, vi5_i6_fused_0]
                with T.block("tensor_1"):
                    ax0_6 = T.axis.spatial(1, 0)
                    ax1_6 = T.axis.spatial(256, i0_i1_fused)
                    ax2_6 = T.axis.spatial(1, 0)
                    ax3_6 = T.axis.spatial(1, 0)
                    ax4_6 = T.axis.spatial(4, i4)
                    T.reads(tensor_1[ax0_6, ax1_6, ax2_6, ax3_6, ax4_6])
                    T.writes(tensor[ax0_6, ax1_6, ax2_6, ax3_6, ax4_6])
                    tensor[ax0_6, ax1_6, ax2_6, ax3_6, ax4_6] = tensor_1[ax0_6, ax1_6, ax2_6, ax3_6, ax4_6] / T.cast(T.max((T.min(ax2_6, 0) * 2 + T.min(ax2_6 * 2 + 6, 6) + 1 - ax2_6 * 2) * (T.min(ax3_6, 0) * 2 + T.min(ax3_6 * 2 + 6, 6) + 1 - ax3_6 * 2), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[7, 7])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l12, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
b16, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l17 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b16, decision=1)
sch.compute_at(block=b16, loop=l18, preserve_unit_loops=True)
sch.enter_postproc()
b19 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b19, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b19, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b19, ann_key="meta_schedule.unroll_explicit")
b20, b21, b22 = sch.get_child_blocks(b19)
l23, l24, l25, l26, l27, l28, l29, l30, l31 = sch.get_loops(block=b20)
l32 = sch.fuse(l23, l24)
sch.parallel(loop=l32)
sch.annotate(block_or_loop=l32, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l32, ann_key="pragma_unroll_explicit", ann_val=1)
l33, l34, l35, l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b21)
sch.annotate(block_or_loop=l33, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l33, ann_key="pragma_unroll_explicit", ann_val=1)
l43, l44, l45, l46 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l43, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l43, ann_key="pragma_unroll_explicit", ann_val=1)
b47 = sch.get_block(name="tensor_rf", func_name="main")
l48, l49, l50, l51, l52, l53, l54, l55 = sch.get_loops(block=b47)
b56 = sch.decompose_reduction(block=b47, loop=l55)
b57 = sch.get_block(name="tensor", func_name="main")
l58, l59, l60, l61, l62, l63, l64, l65, l66, l67 = sch.get_loops(block=b57)
b68 = sch.decompose_reduction(block=b57, loop=l62)
[02:15:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #1: GFLOPs: 0.0149. Time: 4.8010 ms. Best GFLOPs: 0.0149
[02:15:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #2: GFLOPs: 0.0043. Time: 16.8575 ms. Best GFLOPs: 0.0149
[02:15:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #3: GFLOPs: 0.0175. Time: 4.1053 ms. Best GFLOPs: 0.0175
[02:15:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #4: GFLOPs: 0.0133. Time: 5.3842 ms. Best GFLOPs: 0.0175
[02:15:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #5: GFLOPs: 0.0082. Time: 8.7480 ms. Best GFLOPs: 0.0175
[02:15:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #6: GFLOPs: 0.0069. Time: 10.4571 ms. Best GFLOPs: 0.0175
[02:15:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #7: GFLOPs: 0.0017. Time: 42.6620 ms. Best GFLOPs: 0.0175
[02:15:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #8: GFLOPs: 0.0079. Time: 9.0768 ms. Best GFLOPs: 0.0175
[02:15:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #20: "fused_nn_avg_pool2d"] Trial #9: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 256, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 256, 1, 1, 4], dtype="float32")
        tensor_rf = T.alloc_buffer([1, 256, 1, 1, 4, 49], dtype="float32")
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2, i3, i4 in T.grid(1, 1, 4):
                with T.block("tensor_init"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(256, i0_i1_fused)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    ax4 = T.axis.spatial(4, i4)
                    T.reads()
                    T.writes(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    tensor_1[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
                for ax0 in T.serial(49):
                    for ax0_1, ax1, ax2, ax3, ax4, ax5 in T.grid(1, 1, 1, 1, 1, 1):
                        with T.block("tensor_rf"):
                            vi5_i6_fused_0 = T.axis.spatial(49, ax0)
                            ax0_2 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(256, i0_i1_fused)
                            ax2_1 = T.axis.spatial(1, 0)
                            ax3_1 = T.axis.spatial(1, 0)
                            ax4_1 = T.axis.spatial(4, i4)
                            T.reads(placeholder[ax0_2, ax1_1, ax2_1 * 2 + vi5_i6_fused_0 // 7, ax3_1 * 2 + vi5_i6_fused_0 % 7, ax4_1])
                            T.writes(tensor_rf[ax0_2, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0])
                            tensor_rf[ax0_2, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0] = placeholder[ax0_2, ax1_1, ax2_1 * 2 + vi5_i6_fused_0 // 7, ax3_1 * 2 + vi5_i6_fused_0 % 7, ax4_1]
                    for ax1, ax2, ax3, ax4, ax5 in T.grid(1, 1, 1, 1, 1):
                        with T.block("tensor_update"):
                            vi5_i6_fused_0 = T.axis.reduce(49, ax0)
                            ax0_3 = T.axis.spatial(1, 0)
                            ax1_2 = T.axis.spatial(256, i0_i1_fused)
                            ax2_2 = T.axis.spatial(1, 0)
                            ax3_2 = T.axis.spatial(1, 0)
                            ax4_2 = T.axis.spatial(4, i4)
                            T.reads(tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2], tensor_rf[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_0])
                            T.writes(tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2])
                            tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2] = tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2] + tensor_rf[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_0]
                with T.block("tensor_1"):
                    ax0_4 = T.axis.spatial(1, 0)
                    ax1_3 = T.axis.spatial(256, i0_i1_fused)
                    ax2_3 = T.axis.spatial(1, 0)
                    ax3_3 = T.axis.spatial(1, 0)
                    ax4_3 = T.axis.spatial(4, i4)
                    T.reads(tensor_1[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3])
                    T.writes(tensor[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3])
                    tensor[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3] = tensor_1[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3] / T.cast(T.max((T.min(ax2_3, 0) * 2 + T.min(ax2_3 * 2 + 6, 6) + 1 - ax2_3 * 2) * (T.min(ax3_3, 0) * 2 + T.min(ax3_3 * 2 + 6, 6) + 1 - ax3_3 * 2), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[49, 1])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l12, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
b16, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l17 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b16, decision=5)
sch.compute_at(block=b16, loop=l18, preserve_unit_loops=True)
sch.enter_postproc()
b19 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b19, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b19, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b19, ann_key="meta_schedule.unroll_explicit")
b20, b21, b22 = sch.get_child_blocks(b19)
l23, l24, l25, l26, l27, l28, l29, l30, l31, l32, l33, l34 = sch.get_loops(block=b20)
l35 = sch.fuse(l23, l24)
sch.parallel(loop=l35)
sch.annotate(block_or_loop=l35, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l35, ann_key="pragma_unroll_explicit", ann_val=1)
l36, l37, l38, l39, l40, l41, l42, l43, l44, l45 = sch.get_loops(block=b21)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
l46, l47, l48, l49 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l46, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l46, ann_key="pragma_unroll_explicit", ann_val=1)
b50 = sch.get_block(name="tensor", func_name="main")
l51, l52, l53, l54, l55, l56, l57, l58, l59, l60 = sch.get_loops(block=b50)
b61 = sch.decompose_reduction(block=b50, loop=l55)
[02:15:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #10: GFLOPs: 0.0343. Time: 2.0910 ms. Best GFLOPs: 0.0343
[02:15:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #11: GFLOPs: 0.0158. Time: 4.5402 ms. Best GFLOPs: 0.0343
[02:15:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #12: GFLOPs: 0.0186. Time: 3.8464 ms. Best GFLOPs: 0.0343
[02:15:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #13: GFLOPs: 0.0229. Time: 3.1275 ms. Best GFLOPs: 0.0343
[02:15:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #14: GFLOPs: 0.0017. Time: 42.3982 ms. Best GFLOPs: 0.0343
[02:15:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #15: GFLOPs: 0.0109. Time: 6.5885 ms. Best GFLOPs: 0.0343
[02:15:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #16: GFLOPs: 0.0086. Time: 8.3159 ms. Best GFLOPs: 0.0343
[02:15:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #17: GFLOPs: 0.0179. Time: 3.9990 ms. Best GFLOPs: 0.0343
[02:15:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #18: GFLOPs: 0.0190. Time: 3.7792 ms. Best GFLOPs: 0.0343
[02:15:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #19: GFLOPs: 0.0118. Time: 6.0688 ms. Best GFLOPs: 0.0343
[02:15:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #20: GFLOPs: 0.0184. Time: 3.8971 ms. Best GFLOPs: 0.0343
[02:15:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #21: GFLOPs: 0.0055. Time: 13.0549 ms. Best GFLOPs: 0.0343
[02:15:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #22: GFLOPs: 0.0083. Time: 8.6168 ms. Best GFLOPs: 0.0343
[02:15:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #23: GFLOPs: 0.0203. Time: 3.5224 ms. Best GFLOPs: 0.0343
[02:15:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #24: GFLOPs: 0.0074. Time: 9.6241 ms. Best GFLOPs: 0.0343
[02:15:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #25: GFLOPs: 0.0262. Time: 2.7378 ms. Best GFLOPs: 0.0343
[02:15:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #20: "fused_nn_avg_pool2d"] Trial #26: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 256, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 256, 1, 1, 4], dtype="float32")
        tensor_rf = T.alloc_buffer([1, 256, 1, 1, 4, 7], dtype="float32")
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i2, i3, i4, i5_i6_fused_0 in T.grid(1, 1, 4, 7):
                with T.block("tensor_rf_init"):
                    vi5_i6_fused_0 = T.axis.spatial(7, i5_i6_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(256, i0_i1_fused)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    ax4 = T.axis.spatial(4, i4)
                    T.reads()
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.float32(0)
                for i5_i6_fused_1 in T.serial(7):
                    with T.block("tensor_rf_update"):
                        vi5_i6_fused_0 = T.axis.spatial(7, i5_i6_fused_0)
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(256, i0_i1_fused)
                        ax2 = T.axis.spatial(1, 0)
                        ax3 = T.axis.spatial(1, 0)
                        ax4, vi5_i6_fused_1 = T.axis.remap("SR", [i4, i5_i6_fused_1])
                        T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0], placeholder[ax0, ax1, ax2 * 2 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) // 7, ax3 * 2 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) % 7, ax4])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] + placeholder[ax0, ax1, ax2 * 2 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) // 7, ax3 * 2 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) % 7, ax4]
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for ax5_init in T.serial(4):
                with T.block("tensor_init"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(256, i0_i1_fused)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    ax4 = T.axis.spatial(4, ax5_init)
                    T.reads()
                    T.writes(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    tensor_1[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
            for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(7, 1, 1, 1, 1, 4):
                with T.block("tensor_update"):
                    vi5_i6_fused_0 = T.axis.reduce(7, ax0)
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(256, i0_i1_fused)
                    ax2_1 = T.axis.spatial(1, 0)
                    ax3_1 = T.axis.spatial(1, 0)
                    ax4_1 = T.axis.spatial(4, ax5)
                    T.reads(tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0])
                    T.writes(tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0]
            for i2_i3_i4_fused in T.vectorized(4):
                with T.block("tensor_1"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(256, i0_i1_fused)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    ax4 = T.axis.spatial(4, i2_i3_i4_fused)
                    T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] / T.cast(T.max((T.min(ax2, 0) * 2 + T.min(ax2 * 2 + 6, 6) + 1 - ax2 * 2) * (T.min(ax3, 0) * 2 + T.min(ax3 * 2 + 6, 6) + 1 - ax3 * 2), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[7, 7])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l12, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
b16, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l17 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b16, decision=-1)
sch.compute_at(block=b16, loop=l18, preserve_unit_loops=True)
sch.enter_postproc()
b19 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b19, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b19, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b19, ann_key="meta_schedule.unroll_explicit")
b20, b21, b22 = sch.get_child_blocks(b19)
l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b20)
l30 = sch.fuse(l23, l24)
sch.parallel(loop=l30)
sch.annotate(block_or_loop=l30, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l30, ann_key="pragma_unroll_explicit", ann_val=1)
l31, l32, l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b21)
l39 = sch.fuse(l31, l32)
sch.parallel(loop=l39)
sch.annotate(block_or_loop=l39, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l39, ann_key="pragma_unroll_explicit", ann_val=1)
l40, l41, l42, l43 = sch.get_loops(block=b22)
l44 = sch.fuse(l41, l42, l43)
sch.vectorize(loop=l44)
sch.annotate(block_or_loop=l40, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l40, ann_key="pragma_unroll_explicit", ann_val=1)
b45 = sch.get_block(name="tensor_rf", func_name="main")
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
b52 = sch.decompose_reduction(block=b45, loop=l51)
b53 = sch.get_block(name="tensor", func_name="main")
l54, l55, l56, l57, l58, l59, l60 = sch.get_loops(block=b53)
b61 = sch.decompose_reduction(block=b53, loop=l55)
[02:15:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #27: GFLOPs: 0.0129. Time: 5.5515 ms. Best GFLOPs: 0.0343
[02:15:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #28: GFLOPs: 0.0080. Time: 8.9233 ms. Best GFLOPs: 0.0343
[02:15:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #29: GFLOPs: 0.0035. Time: 20.5572 ms. Best GFLOPs: 0.0343
[02:15:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #20: "fused_nn_avg_pool2d"] Trial #30: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 256, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 256, 1, 1, 4], dtype="float32")
        tensor_rf = T.alloc_buffer([1, 256, 1, 1, 4, 7], dtype="float32")
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(7, 1, 1, 1, 1, 4):
                with T.block("tensor_rf_init"):
                    vi5_i6_fused_1 = T.axis.spatial(7, ax0)
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(256, i0_i1_fused)
                    ax2_1 = T.axis.spatial(1, 0)
                    ax3_1 = T.axis.spatial(1, 0)
                    ax4_1 = T.axis.spatial(4, ax5)
                    T.reads()
                    T.writes(tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_1])
                    tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_1] = T.float32(0)
                for ax6 in T.serial(7):
                    with T.block("tensor_rf_update"):
                        vi5_i6_fused_1 = T.axis.spatial(7, ax0)
                        ax0_2 = T.axis.spatial(1, 0)
                        ax1_2 = T.axis.spatial(256, i0_i1_fused)
                        ax2_2 = T.axis.spatial(1, 0)
                        ax3_2 = T.axis.spatial(1, 0)
                        ax4_2, vi5_i6_fused_0 = T.axis.remap("SR", [ax5, ax6])
                        T.reads(tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_1], placeholder[ax0_2, ax1_2, ax2_2 * 2 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) // 7, ax3_2 * 2 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) % 7, ax4_2])
                        T.writes(tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_1])
                        tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_1] = tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_1] + placeholder[ax0_2, ax1_2, ax2_2 * 2 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) // 7, ax3_2 * 2 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) % 7, ax4_2]
            for ax5_init in T.serial(4):
                with T.block("tensor_init"):
                    ax0_3 = T.axis.spatial(1, 0)
                    ax1_3 = T.axis.spatial(256, i0_i1_fused)
                    ax2_3 = T.axis.spatial(1, 0)
                    ax3_3 = T.axis.spatial(1, 0)
                    ax4_3 = T.axis.spatial(4, ax5_init)
                    T.reads()
                    T.writes(tensor_1[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3])
                    tensor_1[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3] = T.float32(0)
            for ax0_4, ax1_4, ax2_4, ax3_4, ax4_4, ax5 in T.grid(7, 1, 1, 1, 1, 4):
                with T.block("tensor_update"):
                    vi5_i6_fused_1 = T.axis.reduce(7, ax0_4)
                    ax0_5 = T.axis.spatial(1, 0)
                    ax1_5 = T.axis.spatial(256, i0_i1_fused)
                    ax2_5 = T.axis.spatial(1, 0)
                    ax3_5 = T.axis.spatial(1, 0)
                    ax4_5 = T.axis.spatial(4, ax5)
                    T.reads(tensor_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4_5], tensor_rf[ax0_5, ax1_5, ax2_5, ax3_5, ax4_5, vi5_i6_fused_1])
                    T.writes(tensor_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4_5])
                    tensor_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4_5] = tensor_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4_5] + tensor_rf[ax0_5, ax1_5, ax2_5, ax3_5, ax4_5, vi5_i6_fused_1]
            for i2_i3_i4_fused in T.vectorized(4):
                with T.block("tensor_1"):
                    ax0_6 = T.axis.spatial(1, 0)
                    ax1_6 = T.axis.spatial(256, i0_i1_fused)
                    ax2_6 = T.axis.spatial(1, 0)
                    ax3_6 = T.axis.spatial(1, 0)
                    ax4_6 = T.axis.spatial(4, i2_i3_i4_fused)
                    T.reads(tensor_1[ax0_6, ax1_6, ax2_6, ax3_6, ax4_6])
                    T.writes(tensor[ax0_6, ax1_6, ax2_6, ax3_6, ax4_6])
                    tensor[ax0_6, ax1_6, ax2_6, ax3_6, ax4_6] = tensor_1[ax0_6, ax1_6, ax2_6, ax3_6, ax4_6] / T.cast(T.max((T.min(ax2_6, 0) * 2 + T.min(ax2_6 * 2 + 6, 6) + 1 - ax2_6 * 2) * (T.min(ax3_6, 0) * 2 + T.min(ax3_6 * 2 + 6, 6) + 1 - ax3_6 * 2), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[7, 7])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
b16, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l17 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b16, decision=1)
sch.compute_at(block=b16, loop=l18, preserve_unit_loops=True)
sch.enter_postproc()
b19 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b19, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b19, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b19, ann_key="meta_schedule.unroll_explicit")
b20, b21, b22 = sch.get_child_blocks(b19)
l23, l24, l25, l26, l27, l28, l29, l30, l31 = sch.get_loops(block=b20)
l32 = sch.fuse(l23, l24)
sch.parallel(loop=l32)
sch.annotate(block_or_loop=l32, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l32, ann_key="pragma_unroll_explicit", ann_val=1)
l33, l34, l35, l36, l37, l38, l39 = sch.get_loops(block=b21)
sch.annotate(block_or_loop=l33, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l33, ann_key="pragma_unroll_explicit", ann_val=1)
l40, l41, l42, l43 = sch.get_loops(block=b22)
l44 = sch.fuse(l41, l42, l43)
sch.vectorize(loop=l44)
sch.annotate(block_or_loop=l40, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l40, ann_key="pragma_unroll_explicit", ann_val=1)
b45 = sch.get_block(name="tensor_rf", func_name="main")
l46, l47, l48, l49, l50, l51, l52, l53 = sch.get_loops(block=b45)
b54 = sch.decompose_reduction(block=b45, loop=l53)
b55 = sch.get_block(name="tensor", func_name="main")
l56, l57, l58, l59, l60, l61, l62 = sch.get_loops(block=b55)
b63 = sch.decompose_reduction(block=b55, loop=l57)
[02:15:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_avg_pool2d"] Trial #31: GFLOPs: 0.0019. Time: 37.3204 ms. Best GFLOPs: 0.0343
[02:16:21] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #20: "fused_nn_avg_pool2d"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |         4.9506 |   10408.9894 |            10408.9894 |     32 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |        11.3630 |      92.7300 |               92.7300 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |        14.6045 |    7046.5233 |             7046.5233 |     32 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |         0.0343 |    2090.9746 |             2090.9746 |     32 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 644
Total latency (us): 164965

[02:16:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_layout_transform_1"] Trial #0: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_layout_trans: T.Buffer[(1, 1024, 1, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        for i0_i1_fused in T.parallel(1024):
            for i2, i3, i4 in T.grid(1, 1, 1):
                with T.block("T_layout_trans"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(1024, i0_i1_fused)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    ax4 = T.axis.spatial(1, 0)
                    T.reads(placeholder[ax0, (ax1 + ax4) // 4, ax2, ax3, (ax1 + ax4) % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 + ax4 < 1024 and ax2 < 1 and ax3 < 1, placeholder[ax0, (ax1 + ax4) // 4, ax2, ax3, (ax1 + ax4) % 4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
sch.enter_postproc()
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit")
b3, = sch.get_child_blocks(b2)
l4, l5, l6, l7, l8 = sch.get_loops(block=b3)
l9 = sch.fuse(l4, l5)
sch.parallel(loop=l9)
[02:16:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_layout_transform_1"] Trial #1: GFLOPs: 0.0000. Time: 1.9994 ms. Best GFLOPs: 0.0000
[02:16:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_layout_transform_1"] Trial #2: GFLOPs: 0.0000. Time: 1.8659 ms. Best GFLOPs: 0.0000
[02:16:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_layout_transform_1"] Trial #3: GFLOPs: 0.0000. Time: 1.3405 ms. Best GFLOPs: 0.0000
[02:17:10] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #21: "fused_layout_transform_1"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |         4.9506 |   10408.9894 |            10408.9894 |     32 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |        11.3630 |      92.7300 |               92.7300 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |        14.6045 |    7046.5233 |             7046.5233 |     32 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |         0.0343 |    2090.9746 |             2090.9746 |     32 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |         0.0000 |    1340.4767 |             1340.4767 |      4 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 648
Total latency (us): 166305

[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #0: GFLOPs: 3.0022. Time: 0.6832 ms. Best GFLOPs: 3.0022
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #1: GFLOPs: 0.2664. Time: 7.6982 ms. Best GFLOPs: 3.0022
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #2: GFLOPs: 0.2169. Time: 9.4543 ms. Best GFLOPs: 3.0022
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #3: GFLOPs: 0.2398. Time: 8.5546 ms. Best GFLOPs: 3.0022
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #4: GFLOPs: 0.1711. Time: 11.9854 ms. Best GFLOPs: 3.0022
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #5: GFLOPs: 0.1539. Time: 13.3307 ms. Best GFLOPs: 3.0022
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #6: GFLOPs: 0.1624. Time: 12.6302 ms. Best GFLOPs: 3.0022
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #7: GFLOPs: 0.3114. Time: 6.5864 ms. Best GFLOPs: 3.0022
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #8: GFLOPs: 0.2835. Time: 7.2335 ms. Best GFLOPs: 3.0022
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #9: GFLOPs: 0.2050. Time: 10.0028 ms. Best GFLOPs: 3.0022
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #10: GFLOPs: 0.2754. Time: 7.4476 ms. Best GFLOPs: 3.0022
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #11: GFLOPs: 0.3686. Time: 5.5651 ms. Best GFLOPs: 3.0022
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #12: GFLOPs: 0.2414. Time: 8.4948 ms. Best GFLOPs: 3.0022
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #13: GFLOPs: 1.0388. Time: 1.9744 ms. Best GFLOPs: 3.0022
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #14: GFLOPs: 0.4132. Time: 4.9643 ms. Best GFLOPs: 3.0022
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #15: GFLOPs: 3.7370. Time: 0.5488 ms. Best GFLOPs: 3.7370
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #16: GFLOPs: 0.1026. Time: 19.9982 ms. Best GFLOPs: 3.7370
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #17: GFLOPs: 0.3419. Time: 5.9989 ms. Best GFLOPs: 3.7370
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #18: GFLOPs: 0.3713. Time: 5.5234 ms. Best GFLOPs: 3.7370
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #19: GFLOPs: 0.0195. Time: 105.1051 ms. Best GFLOPs: 3.7370
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #20: GFLOPs: 0.0855. Time: 23.9966 ms. Best GFLOPs: 3.7370
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 1, 1, 1), "float32"], placeholder_1: T.Buffer[(1001, 1024, 1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1001, 1, 1, 1), "float32"], T_add: T.Buffer[(1, 1001, 1, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 1001, 1, 1, 1], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(13, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 7, 1, 1, 1):
                for i1_3_init in T.serial(11):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(1001, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 77 + i1_1 * 11 + i1_3_init)
                        oh = T.axis.spatial(1, 0)
                        ow = T.axis.spatial(1, 0)
                        oc_block = T.axis.spatial(1, 0)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1024, 1, 1, 1], "float32"], ["TENSOR", [1001, 1024, 1, 1, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW1c", "NCHW1c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 1, 1, 1, 1, 32, 1, 1, 1, 11, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(1001, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 77 + i1_1 * 11 + i1_3)
                        oh = T.axis.spatial(1, 0)
                        ow = T.axis.spatial(1, 0)
                        oc_block = T.axis.spatial(1, 0)
                        ic = T.axis.reduce(1024, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic, oh + kh, ow + kw, 0], placeholder_1[oc_chunk, ic, kh, kw, 0, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1024, 1, 1, 1], "float32"], ["TENSOR", [1001, 1024, 1, 1, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW1c", "NCHW1c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic, oh + kh, ow + kw, 0] * placeholder_1[oc_chunk, ic, kh, kw, 0, oc_block]
            for ax0, ax1 in T.grid(1, 77):
                for ax2_ax3_ax4_fused in T.vectorized(1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(1001, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 77 + ax1)
                        ax2 = T.axis.spatial(1, 0)
                        ax3 = T.axis.spatial(1, 0)
                        ax4 = T.axis.spatial(1, 0)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2, ax3, ax4], placeholder_2[ax0_1, ax1_1, ax2, ax3, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2, ax3, ax4])
                        T_add[ax0_1, ax1_1, ax2, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2, ax3, ax4] + placeholder_2[ax0_1, ax1_1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[13, 7, 1, 11])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l97, l98, l99)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b101)
b124 = sch.decompose_reduction(block=b101, loop=l108)
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #22: GFLOPs: 0.3712. Time: 5.5256 ms. Best GFLOPs: 3.7370
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #23: GFLOPs: 0.1595. Time: 12.8594 ms. Best GFLOPs: 3.7370
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #24: GFLOPs: 0.3137. Time: 6.5379 ms. Best GFLOPs: 3.7370
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #25: GFLOPs: 7.3856. Time: 0.2777 ms. Best GFLOPs: 7.3856
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #26: GFLOPs: 0.1249. Time: 16.4201 ms. Best GFLOPs: 7.3856
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #27: GFLOPs: 0.3207. Time: 6.3965 ms. Best GFLOPs: 7.3856
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #28: GFLOPs: 0.1585. Time: 12.9389 ms. Best GFLOPs: 7.3856
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #29: GFLOPs: 0.0560. Time: 36.6183 ms. Best GFLOPs: 7.3856
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #30: GFLOPs: 0.0842. Time: 24.3613 ms. Best GFLOPs: 7.3856
[02:17:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #31: GFLOPs: 0.0900. Time: 22.7933 ms. Best GFLOPs: 7.3856
[02:17:55] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #22: "fused_nn_contrib_conv2d_NCHWc_add"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |         4.9506 |   10408.9894 |            10408.9894 |     32 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |        11.3630 |      92.7300 |               92.7300 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |        14.6045 |    7046.5233 |             7046.5233 |     32 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |         0.0343 |    2090.9746 |             2090.9746 |     32 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |         0.0000 |    1340.4767 |             1340.4767 |      4 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |         7.3856 |     277.7104 |              277.7104 |     32 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 680
Total latency (us): 166583

[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #0: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001, 1, 1, 1), "float32"], T_reshape: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 1001, 1, 1], dtype="float32")
        for i0_i1_fused in T.parallel(1001, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                with T.block("T_layout_trans"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(1001, i0_i1_fused)
                    ax2_1 = T.axis.spatial(1, 0)
                    ax3_1 = T.axis.spatial(1, 0)
                    T.reads(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, 0])
                    T.writes(T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1])
                    T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 1001 and ax2_1 < 1 and ax3_1 < 1, placeholder[ax0_1, ax1_1, ax2_1, ax3_1, 0], T.float32(0), dtype="float32")
            with T.block("T_reshape_1"):
                ax0 = T.axis.spatial(1, 0)
                ax1 = T.axis.spatial(1001, i0_i1_fused)
                T.reads(T_layout_trans[0, ax1 % 1001, 0, 0])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = T_layout_trans[0, ax1 % 1001 % 1001, 0, 0]
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_squeeze", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l6, preserve_unit_loops=True)
sch.enter_postproc()
b7 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit")
b8, b9 = sch.get_child_blocks(b7)
l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b8)
l16 = sch.fuse(l10, l11)
sch.parallel(loop=l16)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
l17, = sch.get_loops(block=b9)
sch.annotate(block_or_loop=l17, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l17, ann_key="pragma_unroll_explicit", ann_val=1)
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #1: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001, 1, 1, 1), "float32"], T_reshape: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 1001, 1, 1], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 1, 1, 1001], dtype="float32")
        for i0_i1_fused in T.parallel(1001, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i2_i3_fused in T.vectorized(1):
                with T.block("T_layout_trans"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(1001, i0_i1_fused)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    T.reads(placeholder[ax0, ax1, ax2, ax3, 0])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 1001 and ax2 < 1 and ax3 < 1, placeholder[ax0, ax1, ax2, ax3, 0], T.float32(0), dtype="float32")
        for i0_i1_fused in T.parallel(1001, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                with T.block("T_reshape"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(1, 0)
                    ax2_1 = T.axis.spatial(1, 0)
                    ax3_1 = T.axis.spatial(1001, i0_i1_fused)
                    T.reads(T_layout_trans[0, ax3_1 % 1001, 0, 0])
                    T.writes(T_reshape_1[ax0_1, ax1_1, ax2_1, ax3_1])
                    T_reshape_1[ax0_1, ax1_1, ax2_1, ax3_1] = T_layout_trans[0, ax3_1 % 1001, 0, 0]
            with T.block("T_reshape_1"):
                ax0 = T.axis.spatial(1, 0)
                ax1 = T.axis.spatial(1001, i0_i1_fused)
                T.reads(T_reshape_1[0, 0, 0, ax1 % 1001])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = T_reshape_1[0, 0, 0, ax1 % 1001]
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_squeeze", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l6, preserve_unit_loops=True)
sch.enter_postproc()
b7 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit")
b8, b9, b10 = sch.get_child_blocks(b7)
l11, l12, l13, l14 = sch.get_loops(block=b8)
l15 = sch.fuse(l11, l12)
sch.parallel(loop=l15)
l16 = sch.fuse(l13, l14)
sch.vectorize(loop=l16)
sch.annotate(block_or_loop=l15, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l15, ann_key="pragma_unroll_explicit", ann_val=1)
l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b9)
l23 = sch.fuse(l17, l18)
sch.parallel(loop=l23)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
l24, = sch.get_loops(block=b10)
sch.annotate(block_or_loop=l24, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l24, ann_key="pragma_unroll_explicit", ann_val=1)
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #2: GFLOPs: 0.0000. Time: 3.8836 ms. Best GFLOPs: 0.0000
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #3: GFLOPs: 0.0000. Time: 3.2734 ms. Best GFLOPs: 0.0000
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #4: GFLOPs: 0.0000. Time: 4.4197 ms. Best GFLOPs: 0.0000
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #5: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001, 1, 1, 1), "float32"], T_reshape: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 1001, 1, 1], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 1, 1, 1001], dtype="float32")
        for i0_i1_fused in T.parallel(1001, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                with T.block("T_layout_trans"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(1001, i0_i1_fused)
                    ax2_1 = T.axis.spatial(1, 0)
                    ax3_1 = T.axis.spatial(1, 0)
                    T.reads(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, 0])
                    T.writes(T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1])
                    T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 1001 and ax2_1 < 1 and ax3_1 < 1, placeholder[ax0_1, ax1_1, ax2_1, ax3_1, 0], T.float32(0), dtype="float32")
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                with T.block("T_reshape"):
                    ax0_2 = T.axis.spatial(1, 0)
                    ax1_2 = T.axis.spatial(1, 0)
                    ax2_2 = T.axis.spatial(1, 0)
                    ax3_2 = T.axis.spatial(1001, i0_i1_fused)
                    T.reads(T_layout_trans[0, ax3_2 % 1001, 0, 0])
                    T.writes(T_reshape_1[ax0_2, ax1_2, ax2_2, ax3_2])
                    T_reshape_1[ax0_2, ax1_2, ax2_2, ax3_2] = T_layout_trans[0, ax3_2 % 1001, 0, 0]
            with T.block("T_reshape_1"):
                ax0_3 = T.axis.spatial(1, 0)
                ax1_3 = T.axis.spatial(1001, i0_i1_fused)
                T.reads(T_reshape_1[0, 0, 0, ax1_3 % 1001])
                T.writes(T_reshape[ax0_3, ax1_3])
                T_reshape[ax0_3, ax1_3] = T_reshape_1[0, 0, 0, ax1_3 % 1001]
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_squeeze", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l6, preserve_unit_loops=True)
sch.enter_postproc()
b7 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit")
b8, b9, b10 = sch.get_child_blocks(b7)
l11, l12, l13, l14, l15, l16 = sch.get_loops(block=b8)
l17 = sch.fuse(l11, l12)
sch.parallel(loop=l17)
sch.annotate(block_or_loop=l17, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l17, ann_key="pragma_unroll_explicit", ann_val=1)
l18, l19, l20, l21, l22 = sch.get_loops(block=b9)
sch.annotate(block_or_loop=l18, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l18, ann_key="pragma_unroll_explicit", ann_val=1)
l23, = sch.get_loops(block=b10)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #6: GFLOPs: 0.0000. Time: 3.8976 ms. Best GFLOPs: 0.0000
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #7: GFLOPs: 0.0000. Time: 4.6375 ms. Best GFLOPs: 0.0000
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #8: GFLOPs: 0.0000. Time: 6.4091 ms. Best GFLOPs: 0.0000
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #9: GFLOPs: 0.0000. Time: 10.1070 ms. Best GFLOPs: 0.0000
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #10: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001, 1, 1, 1), "float32"], T_reshape: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 1001, 1, 1], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 1, 1, 1001], dtype="float32")
        for i0_i1_i2_i3_fused_fused in T.parallel(1001):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                with T.block("T_layout_trans"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(1001, i0_i1_i2_i3_fused_fused)
                    ax2_1 = T.axis.spatial(1, 0)
                    ax3_1 = T.axis.spatial(1, 0)
                    T.reads(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, 0])
                    T.writes(T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1])
                    T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 1001 and ax2_1 < 1 and ax3_1 < 1, placeholder[ax0_1, ax1_1, ax2_1, ax3_1, 0], T.float32(0), dtype="float32")
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(1, 0)
                ax1 = T.axis.spatial(1, 0)
                ax2 = T.axis.spatial(1, 0)
                ax3 = T.axis.spatial(1001, i0_i1_i2_i3_fused_fused)
                T.reads(T_layout_trans[0, ax3 % 1001, 0, 0])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3])
                T_reshape_1[ax0, ax1, ax2, ax3] = T_layout_trans[0, ax3 % 1001, 0, 0]
        for i0_i1_fused in T.parallel(1001):
            with T.block("T_reshape_1"):
                ax0 = T.axis.spatial(1, 0)
                ax1 = T.axis.spatial(1001, i0_i1_fused)
                T.reads(T_reshape_1[0, 0, 0, ax1 % 1001])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = T_reshape_1[0, 0, 0, ax1 % 1001]
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_squeeze", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l6, preserve_unit_loops=True)
sch.enter_postproc()
b7 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit")
b8, b9, b10 = sch.get_child_blocks(b7)
l11, l12, l13, l14, l15, l16, l17, l18 = sch.get_loops(block=b8)
l19 = sch.fuse(l11, l12, l13, l14)
sch.parallel(loop=l19)
l20, = sch.get_loops(block=b9)
l21 = sch.fuse(l20)
sch.parallel(loop=l21)
l22, l23 = sch.get_loops(block=b10)
l24 = sch.fuse(l22, l23)
sch.parallel(loop=l24)
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #11: GFLOPs: 0.0000. Time: 10.1967 ms. Best GFLOPs: 0.0000
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #12: GFLOPs: 0.0000. Time: 2.6292 ms. Best GFLOPs: 0.0000
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #13: GFLOPs: 0.0000. Time: 3.0995 ms. Best GFLOPs: 0.0000
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #14: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001, 1, 1, 1), "float32"], T_reshape: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 1001, 1, 1], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 1, 1, 1001], dtype="float32")
        for i0_i1_fused in T.parallel(1001):
            for i2_i3_fused in T.vectorized(1):
                with T.block("T_layout_trans"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(1001, i0_i1_fused)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    T.reads(placeholder[ax0, ax1, ax2, ax3, 0])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 1001 and ax2 < 1 and ax3 < 1, placeholder[ax0, ax1, ax2, ax3, 0], T.float32(0), dtype="float32")
        for i0_i1_i2_i3_fused in T.parallel(1001):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(1, 0)
                ax1 = T.axis.spatial(1, 0)
                ax2 = T.axis.spatial(1, 0)
                ax3 = T.axis.spatial(1001, i0_i1_i2_i3_fused)
                T.reads(T_layout_trans[0, ax3 % 1001, 0, 0])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3])
                T_reshape_1[ax0, ax1, ax2, ax3] = T_layout_trans[0, ax3 % 1001, 0, 0]
        for i0_i1_fused in T.parallel(1001):
            with T.block("T_reshape_1"):
                ax0 = T.axis.spatial(1, 0)
                ax1 = T.axis.spatial(1001, i0_i1_fused)
                T.reads(T_reshape_1[0, 0, 0, ax1 % 1001])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = T_reshape_1[0, 0, 0, ax1 % 1001]
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_squeeze", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l6, preserve_unit_loops=True)
sch.enter_postproc()
b7 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit")
b8, b9, b10 = sch.get_child_blocks(b7)
l11, l12, l13, l14 = sch.get_loops(block=b8)
l15 = sch.fuse(l11, l12)
sch.parallel(loop=l15)
l16 = sch.fuse(l13, l14)
sch.vectorize(loop=l16)
l17, l18, l19, l20 = sch.get_loops(block=b9)
l21 = sch.fuse(l17, l18, l19, l20)
sch.parallel(loop=l21)
l22, l23 = sch.get_loops(block=b10)
l24 = sch.fuse(l22, l23)
sch.parallel(loop=l24)
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #15: GFLOPs: 0.0000. Time: 14.5976 ms. Best GFLOPs: 0.0000
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #16: GFLOPs: 0.0000. Time: 7.3675 ms. Best GFLOPs: 0.0000
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #17: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001, 1, 1, 1), "float32"], T_reshape: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 1001, 1, 1], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 1, 1, 1001], dtype="float32")
        for i0_i1_fused_fused_fused in T.parallel(1001):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                with T.block("T_layout_trans"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(1001, i0_i1_fused_fused_fused)
                    ax2_1 = T.axis.spatial(1, 0)
                    ax3_1 = T.axis.spatial(1, 0)
                    T.reads(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, 0])
                    T.writes(T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1])
                    T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 1001 and ax2_1 < 1 and ax3_1 < 1, placeholder[ax0_1, ax1_1, ax2_1, ax3_1, 0], T.float32(0), dtype="float32")
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                with T.block("T_reshape"):
                    ax0_2 = T.axis.spatial(1, 0)
                    ax1_2 = T.axis.spatial(1, 0)
                    ax2_2 = T.axis.spatial(1, 0)
                    ax3_2 = T.axis.spatial(1001, i0_i1_fused_fused_fused)
                    T.reads(T_layout_trans[0, ax3_2 % 1001, 0, 0])
                    T.writes(T_reshape_1[ax0_2, ax1_2, ax2_2, ax3_2])
                    T_reshape_1[ax0_2, ax1_2, ax2_2, ax3_2] = T_layout_trans[0, ax3_2 % 1001, 0, 0]
            with T.block("T_reshape_1"):
                ax0_3 = T.axis.spatial(1, 0)
                ax1_3 = T.axis.spatial(1001, i0_i1_fused_fused_fused)
                T.reads(T_reshape_1[0, 0, 0, ax1_3 % 1001])
                T.writes(T_reshape[ax0_3, ax1_3])
                T_reshape[ax0_3, ax1_3] = T_reshape_1[0, 0, 0, ax1_3 % 1001]
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_squeeze", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l6, preserve_unit_loops=True)
sch.enter_postproc()
b7 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit")
b8, b9, b10 = sch.get_child_blocks(b7)
l11, l12, l13, l14, l15, l16 = sch.get_loops(block=b8)
l17 = sch.fuse(l11, l12)
sch.parallel(loop=l17)
l18, l19, l20, l21, l22 = sch.get_loops(block=b9)
l23 = sch.fuse(l18)
sch.parallel(loop=l23)
l24, = sch.get_loops(block=b10)
l25 = sch.fuse(l24)
sch.parallel(loop=l25)
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #18: GFLOPs: 0.0000. Time: 4.6660 ms. Best GFLOPs: 0.0000
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #19: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001, 1, 1, 1), "float32"], T_reshape: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_reshape_1 = T.alloc_buffer([1, 1, 1, 1001], dtype="float32")
        for i0_i1_i2_i3_fused in T.parallel(1001, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(1, 0)
                ax1 = T.axis.spatial(1, 0)
                ax2 = T.axis.spatial(1, 0)
                ax3 = T.axis.spatial(1001, i0_i1_i2_i3_fused)
                T.reads(placeholder[0, ax3 % 1001, 0, 0, 0])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3])
                T_reshape_1[ax0, ax1, ax2, ax3] = T.if_then_else(0 < 1 and ax3 % 1001 < 1001 and 0 < 1 and 0 < 1, placeholder[0, ax3 % 1001, 0, 0, 0], T.float32(0), dtype="float32")
        for i0_i1_fused in T.parallel(1001, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            with T.block("T_reshape_1"):
                ax0 = T.axis.spatial(1, 0)
                ax1 = T.axis.spatial(1001, i0_i1_fused)
                T.reads(T_reshape_1[0, 0, 0, ax1 % 1001])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = T_reshape_1[0, 0, 0, ax1 % 1001]
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_squeeze", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l6, preserve_unit_loops=True)
sch.enter_postproc()
b7 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit")
b8, b9 = sch.get_child_blocks(b7)
l10, l11, l12, l13 = sch.get_loops(block=b8)
l14 = sch.fuse(l10, l11, l12, l13)
sch.parallel(loop=l14)
sch.annotate(block_or_loop=l14, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l14, ann_key="pragma_unroll_explicit", ann_val=1)
l15, l16 = sch.get_loops(block=b9)
l17 = sch.fuse(l15, l16)
sch.parallel(loop=l17)
sch.annotate(block_or_loop=l17, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l17, ann_key="pragma_unroll_explicit", ann_val=1)
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #20: GFLOPs: 0.0000. Time: 3.9999 ms. Best GFLOPs: 0.0000
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #21: GFLOPs: 0.0000. Time: 5.5792 ms. Best GFLOPs: 0.0000
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #22: GFLOPs: 0.0000. Time: 4.9234 ms. Best GFLOPs: 0.0000
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #23: GFLOPs: 0.0000. Time: 7.1568 ms. Best GFLOPs: 0.0000
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #24: GFLOPs: 0.0000. Time: 4.0963 ms. Best GFLOPs: 0.0000
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #25: GFLOPs: 0.0000. Time: 6.2117 ms. Best GFLOPs: 0.0000
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #26: GFLOPs: 0.0000. Time: 5.3325 ms. Best GFLOPs: 0.0000
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #27: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001, 1, 1, 1), "float32"], T_reshape: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 1001, 1, 1], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 1, 1, 1001], dtype="float32")
        for i0_i1_fused in T.parallel(1001, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                with T.block("T_layout_trans"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(1001, i0_i1_fused)
                    ax2_1 = T.axis.spatial(1, 0)
                    ax3_1 = T.axis.spatial(1, 0)
                    T.reads(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, 0])
                    T.writes(T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1])
                    T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 1001 and ax2_1 < 1 and ax3_1 < 1, placeholder[ax0_1, ax1_1, ax2_1, ax3_1, 0], T.float32(0), dtype="float32")
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                with T.block("T_reshape"):
                    ax0_2 = T.axis.spatial(1, 0)
                    ax1_2 = T.axis.spatial(1, 0)
                    ax2_2 = T.axis.spatial(1, 0)
                    ax3_2 = T.axis.spatial(1001, i0_i1_fused)
                    T.reads(T_layout_trans[0, ax3_2 % 1001, 0, 0])
                    T.writes(T_reshape_1[ax0_2, ax1_2, ax2_2, ax3_2])
                    T_reshape_1[ax0_2, ax1_2, ax2_2, ax3_2] = T_layout_trans[0, ax3_2 % 1001, 0, 0]
            with T.block("T_reshape_1"):
                ax0_3 = T.axis.spatial(1, 0)
                ax1_3 = T.axis.spatial(1001, i0_i1_fused)
                T.reads(T_reshape_1[0, 0, 0, ax1_3 % 1001])
                T.writes(T_reshape[ax0_3, ax1_3])
                T_reshape[ax0_3, ax1_3] = T_reshape_1[0, 0, 0, ax1_3 % 1001]
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_squeeze", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l6, preserve_unit_loops=True)
sch.enter_postproc()
b7 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit")
b8, b9, b10 = sch.get_child_blocks(b7)
l11, l12, l13, l14, l15, l16 = sch.get_loops(block=b8)
l17 = sch.fuse(l11, l12)
sch.parallel(loop=l17)
sch.annotate(block_or_loop=l17, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l17, ann_key="pragma_unroll_explicit", ann_val=1)
l18, l19, l20, l21, l22 = sch.get_loops(block=b9)
sch.annotate(block_or_loop=l18, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l18, ann_key="pragma_unroll_explicit", ann_val=1)
l23, = sch.get_loops(block=b10)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #28: GFLOPs: 0.0000. Time: 6.8958 ms. Best GFLOPs: 0.0000
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #29: GFLOPs: 0.0000. Time: 3.7893 ms. Best GFLOPs: 0.0000
[02:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #30: GFLOPs: 0.0000. Time: 2.0189 ms. Best GFLOPs: 0.0000
[02:18:35] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_layout_transform_reshape_squeeze_reshape"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |         4.9506 |   10408.9894 |            10408.9894 |     32 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |        11.3630 |      92.7300 |               92.7300 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |        14.6045 |    7046.5233 |             7046.5233 |     32 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |         0.0343 |    2090.9746 |             2090.9746 |     32 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |         0.0000 |    1340.4767 |             1340.4767 |      4 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |         7.3856 |     277.7104 |              277.7104 |     32 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |         0.0000 |    2018.8928 |             2018.8928 |     31 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 711
Total latency (us): 168602

[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #0: GFLOPs: 0.0002. Time: 18.5561 ms. Best GFLOPs: 0.0002
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #1: GFLOPs: 0.0003. Time: 14.5393 ms. Best GFLOPs: 0.0003
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #2: GFLOPs: 0.0005. Time: 8.6342 ms. Best GFLOPs: 0.0005
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #3: GFLOPs: 0.0002. Time: 20.7688 ms. Best GFLOPs: 0.0005
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #4: GFLOPs: 0.0004. Time: 10.1612 ms. Best GFLOPs: 0.0005
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #5: GFLOPs: 0.0006. Time: 7.0762 ms. Best GFLOPs: 0.0006
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #6: GFLOPs: 0.0011. Time: 3.7264 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #7: GFLOPs: 0.0000. Time: 447.2604 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #8: GFLOPs: 0.0002. Time: 18.4187 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #9: GFLOPs: 0.0003. Time: 11.5964 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #10: GFLOPs: 0.0004. Time: 10.8317 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #11: GFLOPs: 0.0007. Time: 5.4289 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #12: GFLOPs: 0.0005. Time: 8.0249 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #13: GFLOPs: 0.0003. Time: 12.4230 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #14: GFLOPs: 0.0002. Time: 16.9965 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #15: GFLOPs: 0.0001. Time: 49.5962 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #16: GFLOPs: 0.0002. Time: 16.6153 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #17: GFLOPs: 0.0002. Time: 17.3366 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #18: GFLOPs: 0.0002. Time: 17.9962 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #19: GFLOPs: 0.0003. Time: 15.9982 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #20: GFLOPs: 0.0002. Time: 18.6629 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #21: GFLOPs: 0.0001. Time: 51.9999 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #22: GFLOPs: 0.0004. Time: 9.7134 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #23: GFLOPs: 0.0002. Time: 19.9918 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #24: GFLOPs: 0.0003. Time: 15.6377 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #25: GFLOPs: 0.0001. Time: 45.3272 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #26: GFLOPs: 0.0003. Time: 12.4989 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #27: GFLOPs: 0.0003. Time: 15.0758 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #28: GFLOPs: 0.0003. Time: 11.9997 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #24: "fused_nn_softmax"] Trial #29: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_softmax_norm: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
        T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
        T_softmax_expsum_rf = T.alloc_buffer([1, 7], dtype="float32")
        for i0 in T.serial(1):
            with T.block("T_softmax_maxelem_init"):
                i0_1 = T.axis.spatial(1, 0)
                T.reads()
                T.writes(T_softmax_maxelem[i0_1])
                T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
            for i1_0, i1_1 in T.grid(1001, 1):
                with T.block("T_softmax_maxelem_update"):
                    vi1_0 = T.axis.reduce(1001, i1_0)
                    i0_2 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_maxelem[i0_2], placeholder[i0_2, vi1_0])
                    T.writes(T_softmax_maxelem[i0_2])
                    T_softmax_maxelem[i0_2] = T.max(T_softmax_maxelem[i0_2], placeholder[i0_2, vi1_0])
        for i0_3 in T.serial(1):
            for i1_1_fused_init in T.vectorized(7):
                with T.block("T_softmax_expsum_rf_init"):
                    vi1_1 = T.axis.spatial(7, i1_1_fused_init)
                    i0_4 = T.axis.spatial(1, 0)
                    T.reads()
                    T.writes(T_softmax_expsum_rf[i0_4, vi1_1])
                    T_softmax_expsum_rf[i0_4, vi1_1] = T.float32(0)
            for i1_0 in T.serial(143):
                for i1_1_fused in T.vectorized(7):
                    with T.block("T_softmax_expsum_rf_update"):
                        vi1_1 = T.axis.spatial(7, i1_1_fused)
                        i0_5 = T.axis.spatial(1, 0)
                        vi1_0 = T.axis.reduce(143, i1_0)
                        T.reads(T_softmax_expsum_rf[i0_5, vi1_1], placeholder[i0_5, vi1_0 * 7 + vi1_1], T_softmax_maxelem[i0_5])
                        T.writes(T_softmax_expsum_rf[i0_5, vi1_1])
                        T_softmax_expsum_rf[i0_5, vi1_1] = T_softmax_expsum_rf[i0_5, vi1_1] + T.exp(placeholder[i0_5, vi1_0 * 7 + vi1_1] - T_softmax_maxelem[i0_5], dtype="float32")
        for i0_i1_fused_fused in T.parallel(1001):
            with T.block("T_softmax_expsum_init"):
                i0_6 = T.axis.spatial(1, 0)
                T.reads()
                T.writes(T_softmax_expsum[i0_6])
                T_softmax_expsum[i0_6] = T.float32(0)
            for ax0, ax1 in T.grid(7, 1):
                with T.block("T_softmax_expsum_update"):
                    vi1_1 = T.axis.reduce(7, ax0)
                    i0_7 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_expsum[i0_7], T_softmax_expsum_rf[i0_7, vi1_1])
                    T.writes(T_softmax_expsum[i0_7])
                    T_softmax_expsum[i0_7] = T_softmax_expsum[i0_7] + T_softmax_expsum_rf[i0_7, vi1_1]
            with T.block("T_softmax_norm"):
                i0_8 = T.axis.spatial(1, 0)
                i1 = T.axis.spatial(1001, i0_i1_fused_fused)
                T.reads(placeholder[i0_8, i1], T_softmax_maxelem[i0_8], T_softmax_expsum[i0_8])
                T.writes(T_softmax_norm[i0_8, i1])
                T.block_attr({"axis":1})
                T_softmax_norm[i0_8, i1] = T.exp(placeholder[i0_8, i1] - T_softmax_maxelem[i0_8], dtype="float32") / T_softmax_expsum[i0_8]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b2)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[143, 7])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l9, factor_axis=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer", ann_val=1)
l11, l12 = sch.get_loops(block=b0)
v13, v14 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[1001, 1])
l15, l16 = sch.split(loop=l12, factors=[v13, v14])
b17 = sch.rfactor(loop=l15, factor_axis=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
b19, = sch.get_producers(block=b2)
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer")
l20 = sch.sample_compute_location(block=b2, decision=1)
sch.compute_at(block=b2, loop=l20, preserve_unit_loops=True)
l21 = sch.sample_compute_location(block=b19, decision=-1)
sch.compute_at(block=b19, loop=l21, preserve_unit_loops=True)
l22 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l22, preserve_unit_loops=True)
b23, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l24 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l24, preserve_unit_loops=True)
l25 = sch.sample_compute_location(block=b23, decision=-2)
sch.compute_at(block=b23, loop=l25, preserve_unit_loops=True)
sch.enter_postproc()
b26 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b26, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b26, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b26, ann_key="meta_schedule.unroll_explicit")
b27, b28, b29, b30 = sch.get_child_blocks(b26)
l31, l32, l33 = sch.get_loops(block=b27)
l34, l35, l36 = sch.get_loops(block=b28)
l37 = sch.fuse(l36)
sch.vectorize(loop=l37)
l38, l39, l40, l41 = sch.get_loops(block=b29)
l42 = sch.fuse(l38, l39)
sch.parallel(loop=l42)
l43, = sch.get_loops(block=b30)
l44 = sch.fuse(l43)
sch.parallel(loop=l44)
b45 = sch.get_block(name="T_softmax_maxelem", func_name="main")
l46, l47, l48 = sch.get_loops(block=b45)
b49 = sch.decompose_reduction(block=b45, loop=l47)
b50 = sch.get_block(name="T_softmax_expsum_rf", func_name="main")
l51, l52, l53 = sch.get_loops(block=b50)
b54 = sch.decompose_reduction(block=b50, loop=l52)
b55 = sch.get_block(name="T_softmax_expsum", func_name="main")
l56, l57, l58 = sch.get_loops(block=b55)
b59 = sch.decompose_reduction(block=b55, loop=l57)
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #30: GFLOPs: 0.0001. Time: 35.1141 ms. Best GFLOPs: 0.0011
[02:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_softmax"] Trial #31: GFLOPs: 0.0004. Time: 9.9026 ms. Best GFLOPs: 0.0011
[02:19:22] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #24: "fused_nn_softmax"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |         4.9506 |   10408.9894 |            10408.9894 |     32 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |        11.3630 |      92.7300 |               92.7300 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |        14.6045 |    7046.5233 |             7046.5233 |     32 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |         0.0343 |    2090.9746 |             2090.9746 |     32 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |         0.0000 |    1340.4767 |             1340.4767 |      4 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |         7.3856 |     277.7104 |              277.7104 |     32 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |         0.0000 |    2018.8928 |             2018.8928 |     31 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |         0.0011 |    3726.3539 |             3726.3539 |     32 |            
 25 |                                      fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 743
Total latency (us): 172328

[02:19:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #25: "fused_reshape"] Trial #0: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_reshape: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        for i0_i1_fused in T.parallel(1001, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(1, 0)
                ax1 = T.axis.spatial(1001, i0_i1_fused)
                T.reads(placeholder[0, ax1 % 1001])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = placeholder[0, ax1 % 1001]
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
sch.enter_postproc()
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit")
b3, = sch.get_child_blocks(b2)
l4, l5 = sch.get_loops(block=b3)
l6 = sch.fuse(l4, l5)
sch.parallel(loop=l6)
sch.annotate(block_or_loop=l6, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l6, ann_key="pragma_unroll_explicit", ann_val=1)
[02:19:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #25: "fused_reshape"] Trial #1: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_reshape: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        for i0_i1_fused in T.parallel(1001):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(1, 0)
                ax1 = T.axis.spatial(1001, i0_i1_fused)
                T.reads(placeholder[0, ax1 % 1001])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = placeholder[0, ax1 % 1001]
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
sch.enter_postproc()
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit")
b3, = sch.get_child_blocks(b2)
l4, l5 = sch.get_loops(block=b3)
l6 = sch.fuse(l4, l5)
sch.parallel(loop=l6)
[02:19:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape"] Trial #2: GFLOPs: 0.0000. Time: 7.3138 ms. Best GFLOPs: 0.0000
[02:19:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape"] Trial #3: GFLOPs: 0.0000. Time: 11.7732 ms. Best GFLOPs: 0.0000
[02:20:00] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #25: "fused_reshape"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |            
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |            
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |            
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |            
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |         4.9506 |   10408.9894 |            10408.9894 |     32 |            
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |        11.3630 |      92.7300 |               92.7300 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |        14.6045 |    7046.5233 |             7046.5233 |     32 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |         0.0343 |    2090.9746 |             2090.9746 |     32 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |         0.0000 |    1340.4767 |             1340.4767 |      4 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |         7.3856 |     277.7104 |              277.7104 |     32 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |         0.0000 |    2018.8928 |             2018.8928 |     31 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |         0.0011 |    3726.3539 |             3726.3539 |     32 |            
 25 |                                      fused_reshape |         1 |      1 |         0.0000 |    7313.7595 |             7313.7595 |      4 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 747
Total latency (us): 179642

[02:20:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"
[02:20:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #15 has finished. Remaining task(s): 25
[02:20:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"
[02:20:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #14 has finished. Remaining task(s): 24
[02:20:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #11: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"
[02:20:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #11 has finished. Remaining task(s): 23
[02:20:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"
[02:20:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #7 has finished. Remaining task(s): 22
[02:20:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #17: "fused_nn_contrib_conv2d_NCHWc_add_clip_8"
[02:20:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #17 has finished. Remaining task(s): 21
[02:20:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #13: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"
[02:20:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #13 has finished. Remaining task(s): 20
[02:20:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"
[02:20:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #5 has finished. Remaining task(s): 19
[02:20:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #25: "fused_reshape"
[02:20:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:20:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[02:20:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:20:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[02:20:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:20:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:20:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:21:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:21:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:21:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:21:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:21:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:21:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:21:26] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #25: "fused_reshape"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |            
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |          Y 
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |            
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |          Y 
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |            
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |            
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |          Y 
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |            
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |          Y 
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |          Y 
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |          Y 
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |         4.9506 |   10408.9894 |            10408.9894 |     32 |          Y 
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |        11.3630 |      92.7300 |               92.7300 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |        14.6045 |    7046.5233 |             7046.5233 |     32 |            
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |         0.0343 |    2090.9746 |             2090.9746 |     32 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |         0.0000 |    1340.4767 |             1340.4767 |      4 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |         7.3856 |     277.7104 |              277.7104 |     32 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |         0.0000 |    2018.8928 |             2018.8928 |     31 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |         0.0011 |    3726.3539 |             3726.3539 |     32 |            
 25 |                                      fused_reshape |         1 |      1 |         0.0000 |    7313.7595 |             7313.7595 |      4 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 747
Total latency (us): 179642

[02:21:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"
[02:21:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #9 has finished. Remaining task(s): 18
[02:21:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_9"
[02:21:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #19 has finished. Remaining task(s): 17
[02:21:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"
[02:21:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #6 has finished. Remaining task(s): 16
[02:21:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"
[02:21:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #3 has finished. Remaining task(s): 15
[02:21:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"
[02:21:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #10 has finished. Remaining task(s): 14
[02:21:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"
[02:21:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #4 has finished. Remaining task(s): 13
[02:21:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"
[02:21:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #12 has finished. Remaining task(s): 12
[02:21:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #24: "fused_nn_softmax"
[02:21:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #24 has finished. Remaining task(s): 11
[02:21:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #25: "fused_reshape"
[02:21:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:21:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[02:21:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:21:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[02:21:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:21:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:21:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:22:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:22:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:22:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:22:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:22:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:22:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:22:38] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #25: "fused_reshape"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |          Y 
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |          Y 
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |          Y 
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |          Y 
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |          Y 
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |          Y 
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |          Y 
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |          Y 
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |          Y 
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |          Y 
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |          Y 
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |          Y 
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |         4.9506 |   10408.9894 |            10408.9894 |     32 |          Y 
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |        11.3630 |      92.7300 |               92.7300 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |        14.6045 |    7046.5233 |             7046.5233 |     32 |          Y 
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |         0.0343 |    2090.9746 |             2090.9746 |     32 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |         0.0000 |    1340.4767 |             1340.4767 |      4 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |         7.3856 |     277.7104 |              277.7104 |     32 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |         0.0000 |    2018.8928 |             2018.8928 |     31 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |         0.0011 |    3726.3539 |             3726.3539 |     32 |          Y 
 25 |                                      fused_reshape |         1 |      1 |         0.0000 |    7313.7595 |             7313.7595 |      4 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 747
Total latency (us): 179642

[02:22:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #25: "fused_reshape"
[02:22:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:22:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[02:22:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:22:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[02:22:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:23:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:23:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:23:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:23:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:23:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:23:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:23:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:23:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:23:57] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #25: "fused_reshape"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |          Y 
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |          Y 
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |          Y 
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |          Y 
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |          Y 
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |          Y 
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |          Y 
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |          Y 
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |          Y 
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |          Y 
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |          Y 
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |          Y 
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |         4.9506 |   10408.9894 |            10408.9894 |     32 |          Y 
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |        11.3630 |      92.7300 |               92.7300 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |        14.6045 |    7046.5233 |             7046.5233 |     32 |          Y 
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |         0.0343 |    2090.9746 |             2090.9746 |     32 |            
 21 |                           fused_layout_transform_1 |         1 |      1 |         0.0000 |    1340.4767 |             1340.4767 |      4 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |         7.3856 |     277.7104 |              277.7104 |     32 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |         0.0000 |    2018.8928 |             2018.8928 |     31 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |         0.0011 |    3726.3539 |             3726.3539 |     32 |          Y 
 25 |                                      fused_reshape |         1 |      1 |         0.0000 |    7313.7595 |             7313.7595 |      4 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 747
Total latency (us): 179642

[02:23:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #20: "fused_nn_avg_pool2d"
[02:23:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #20 has finished. Remaining task(s): 10
[02:23:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_layout_transform_reshape_squeeze_reshape"
[02:23:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:23:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 31 candidate(s) from database
[02:24:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706118818)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061e6698)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627057a84d8)]: 0 failure(s)
[02:24:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2017 candidate(s)
[02:24:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706118818)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061e6698)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627057a84d8)]: 0 failure(s)
[02:25:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706118818)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061e6698)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627057a84d8)]: 0 failure(s)
[02:25:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706118818)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061e6698)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627057a84d8)]: 0 failure(s)
[02:26:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562706118818)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627061e6698)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627057a84d8)]: 0 failure(s)
[02:27:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 1 candidates:
[1 : 1]:	0.5075
[02:27:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 1 candidate(s) with evolutionary search
[02:27:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 1 candidates(s) for measurement
[02:27:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 1 sample(s) to builder
[02:27:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 1 sample(s) to runner
[02:27:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_layout_transform_reshape_squeeze_reshape"] Trial #31: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001, 1, 1, 1), "float32"], T_reshape: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 1001, 1, 1], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 1, 1, 1001], dtype="float32")
        for i0_i1_i2_i3_fused in T.parallel(1001, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                with T.block("T_layout_trans"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(1001, i0_i1_i2_i3_fused)
                    ax2_1 = T.axis.spatial(1, 0)
                    ax3_1 = T.axis.spatial(1, 0)
                    T.reads(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, 0])
                    T.writes(T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1])
                    T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 1001 and ax2_1 < 1 and ax3_1 < 1, placeholder[ax0_1, ax1_1, ax2_1, ax3_1, 0], T.float32(0), dtype="float32")
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(1, 0)
                ax1 = T.axis.spatial(1, 0)
                ax2 = T.axis.spatial(1, 0)
                ax3 = T.axis.spatial(1001, i0_i1_i2_i3_fused)
                T.reads(T_layout_trans[0, ax3 % 1001, 0, 0])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3])
                T_reshape_1[ax0, ax1, ax2, ax3] = T_layout_trans[0, ax3 % 1001, 0, 0]
        for i0_i1_fused in T.parallel(1001, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            with T.block("T_reshape_1"):
                ax0 = T.axis.spatial(1, 0)
                ax1 = T.axis.spatial(1001, i0_i1_fused)
                T.reads(T_reshape_1[0, 0, 0, ax1 % 1001])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = T_reshape_1[0, 0, 0, ax1 % 1001]
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_squeeze", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l6, preserve_unit_loops=True)
sch.enter_postproc()
b7 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit")
b8, b9, b10 = sch.get_child_blocks(b7)
l11, l12, l13, l14, l15, l16, l17, l18 = sch.get_loops(block=b8)
l19 = sch.fuse(l11, l12, l13, l14)
sch.parallel(loop=l19)
sch.annotate(block_or_loop=l19, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l19, ann_key="pragma_unroll_explicit", ann_val=1)
l20, = sch.get_loops(block=b9)
sch.annotate(block_or_loop=l20, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l20, ann_key="pragma_unroll_explicit", ann_val=1)
l21, l22 = sch.get_loops(block=b10)
l23 = sch.fuse(l21, l22)
sch.parallel(loop=l23)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
[02:28:31] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_layout_transform_reshape_squeeze_reshape"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |          Y 
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |          Y 
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |          Y 
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |          Y 
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |          Y 
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |          Y 
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |          Y 
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |          Y 
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |          Y 
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |          Y 
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |          Y 
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |          Y 
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |         4.9506 |   10408.9894 |            10408.9894 |     32 |          Y 
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |        11.3630 |      92.7300 |               92.7300 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |        14.6045 |    7046.5233 |             7046.5233 |     32 |          Y 
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |         0.0343 |    2090.9746 |             2090.9746 |     32 |          Y 
 21 |                           fused_layout_transform_1 |         1 |      1 |         0.0000 |    1340.4767 |             1340.4767 |      4 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |         7.3856 |     277.7104 |              277.7104 |     32 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |         0.0000 |    2018.8928 |             2018.8928 |     32 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |         0.0011 |    3726.3539 |             3726.3539 |     32 |          Y 
 25 |                                      fused_reshape |         1 |      1 |         0.0000 |    7313.7595 |             7313.7595 |      4 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 748
Total latency (us): 179642

[02:28:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #25: "fused_reshape"
[02:28:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:28:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[02:28:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:28:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[02:28:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:28:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:29:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:29:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:29:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:29:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:29:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:29:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:29:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:29:32] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #25: "fused_reshape"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |          Y 
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |          Y 
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |          Y 
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |          Y 
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |          Y 
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |          Y 
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |          Y 
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |          Y 
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |          Y 
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |          Y 
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |          Y 
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |          Y 
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |            
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |         4.9506 |   10408.9894 |            10408.9894 |     32 |          Y 
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |        11.3630 |      92.7300 |               92.7300 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |        14.6045 |    7046.5233 |             7046.5233 |     32 |          Y 
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |         0.0343 |    2090.9746 |             2090.9746 |     32 |          Y 
 21 |                           fused_layout_transform_1 |         1 |      1 |         0.0000 |    1340.4767 |             1340.4767 |      4 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |         7.3856 |     277.7104 |              277.7104 |     32 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |         0.0000 |    2018.8928 |             2018.8928 |     32 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |         0.0011 |    3726.3539 |             3726.3539 |     32 |          Y 
 25 |                                      fused_reshape |         1 |      1 |         0.0000 |    7313.7595 |             7313.7595 |      4 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 748
Total latency (us): 179642

[02:29:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #25: "fused_reshape"
[02:29:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:29:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[02:29:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:29:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[02:29:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:30:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:30:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:30:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x562705a05ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x562706143038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x562705770308)]: 0 failure(s)
[02:30:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:30:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:30:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:30:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #25 has finished. Remaining task(s): 9
[02:30:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #16: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"
[02:30:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #16 has finished. Remaining task(s): 8
[02:30:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #21: "fused_layout_transform_1"
[02:30:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:30:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[02:30:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:30:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[02:31:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:31:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:31:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:32:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:32:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:32:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:32:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:32:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:32:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:32:25] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #21: "fused_layout_transform_1"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |            
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |          Y 
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |          Y 
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |          Y 
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |          Y 
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |          Y 
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |          Y 
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |          Y 
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |          Y 
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |          Y 
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |          Y 
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |          Y 
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |          Y 
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |          Y 
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |         4.9506 |   10408.9894 |            10408.9894 |     32 |          Y 
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |        11.3630 |      92.7300 |               92.7300 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |        14.6045 |    7046.5233 |             7046.5233 |     32 |          Y 
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |         0.0343 |    2090.9746 |             2090.9746 |     32 |          Y 
 21 |                           fused_layout_transform_1 |         1 |      1 |         0.0000 |    1340.4767 |             1340.4767 |      4 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |         7.3856 |     277.7104 |              277.7104 |     32 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |         0.0000 |    2018.8928 |             2018.8928 |     32 |            
 24 |                                   fused_nn_softmax |      4004 |      1 |         0.0011 |    3726.3539 |             3726.3539 |     32 |          Y 
 25 |                                      fused_reshape |         1 |      1 |         0.0000 |    7313.7595 |             7313.7595 |      4 |          Y 
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 748
Total latency (us): 179642

[02:32:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_layout_transform_reshape_squeeze_reshape"
[02:32:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #23 has finished. Remaining task(s): 7
[02:32:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"
[02:32:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #1 has finished. Remaining task(s): 6
[02:32:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #21: "fused_layout_transform_1"
[02:32:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:32:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[02:32:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:32:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[02:32:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:33:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:33:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:33:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:34:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:34:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:34:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:34:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:34:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:34:21] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #21: "fused_layout_transform_1"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |          Y 
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |          Y 
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |          Y 
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |          Y 
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |          Y 
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |          Y 
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |            
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |          Y 
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |          Y 
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |          Y 
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |          Y 
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |          Y 
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |          Y 
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |          Y 
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |          Y 
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |         4.9506 |   10408.9894 |            10408.9894 |     32 |          Y 
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |        11.3630 |      92.7300 |               92.7300 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |        14.6045 |    7046.5233 |             7046.5233 |     32 |          Y 
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |         0.0343 |    2090.9746 |             2090.9746 |     32 |          Y 
 21 |                           fused_layout_transform_1 |         1 |      1 |         0.0000 |    1340.4767 |             1340.4767 |      4 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |         7.3856 |     277.7104 |              277.7104 |     32 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |         0.0000 |    2018.8928 |             2018.8928 |     32 |          Y 
 24 |                                   fused_nn_softmax |      4004 |      1 |         0.0011 |    3726.3539 |             3726.3539 |     32 |          Y 
 25 |                                      fused_reshape |         1 |      1 |         0.0000 |    7313.7595 |             7313.7595 |      4 |          Y 
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 748
Total latency (us): 179642

[02:34:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"
[02:34:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #8 has finished. Remaining task(s): 5
[02:34:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #21: "fused_layout_transform_1"
[02:34:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:34:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[02:34:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:34:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[02:34:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:35:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:35:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:36:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:36:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:36:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:36:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:36:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:36:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:36:26] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #21: "fused_layout_transform_1"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |          Y 
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |            
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |          Y 
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |          Y 
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |          Y 
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |          Y 
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |          Y 
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |          Y 
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |          Y 
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |          Y 
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |          Y 
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |          Y 
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |          Y 
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |          Y 
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |          Y 
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |          Y 
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |         4.9506 |   10408.9894 |            10408.9894 |     32 |          Y 
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |        11.3630 |      92.7300 |               92.7300 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |        14.6045 |    7046.5233 |             7046.5233 |     32 |          Y 
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |         0.0343 |    2090.9746 |             2090.9746 |     32 |          Y 
 21 |                           fused_layout_transform_1 |         1 |      1 |         0.0000 |    1340.4767 |             1340.4767 |      4 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |         7.3856 |     277.7104 |              277.7104 |     32 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |         0.0000 |    2018.8928 |             2018.8928 |     32 |          Y 
 24 |                                   fused_nn_softmax |      4004 |      1 |         0.0011 |    3726.3539 |             3726.3539 |     32 |          Y 
 25 |                                      fused_reshape |         1 |      1 |         0.0000 |    7313.7595 |             7313.7595 |      4 |          Y 
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 748
Total latency (us): 179642

[02:36:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"
[02:36:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #2 has finished. Remaining task(s): 4
[02:36:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #21: "fused_layout_transform_1"
[02:36:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:36:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[02:36:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:36:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[02:36:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:37:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:37:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:37:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:38:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:38:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:38:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:38:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:38:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:38:06] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #21: "fused_layout_transform_1"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |          Y 
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |          Y 
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |          Y 
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |          Y 
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |          Y 
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |          Y 
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |          Y 
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |          Y 
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |          Y 
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |          Y 
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |          Y 
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |          Y 
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |          Y 
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |          Y 
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |          Y 
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |          Y 
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |         4.9506 |   10408.9894 |            10408.9894 |     32 |          Y 
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |        11.3630 |      92.7300 |               92.7300 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |        14.6045 |    7046.5233 |             7046.5233 |     32 |          Y 
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |         0.0343 |    2090.9746 |             2090.9746 |     32 |          Y 
 21 |                           fused_layout_transform_1 |         1 |      1 |         0.0000 |    1340.4767 |             1340.4767 |      4 |            
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |         7.3856 |     277.7104 |              277.7104 |     32 |            
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |         0.0000 |    2018.8928 |             2018.8928 |     32 |          Y 
 24 |                                   fused_nn_softmax |      4004 |      1 |         0.0011 |    3726.3539 |             3726.3539 |     32 |          Y 
 25 |                                      fused_reshape |         1 |      1 |         0.0000 |    7313.7595 |             7313.7595 |      4 |          Y 
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 748
Total latency (us): 179642

[02:38:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #22: "fused_nn_contrib_conv2d_NCHWc_add"
[02:38:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #22 has finished. Remaining task(s): 3
[02:38:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #21: "fused_layout_transform_1"
[02:38:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:38:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[02:38:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:38:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[02:38:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:39:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:39:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:39:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56270577b2f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627056f9238)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5627062f4d58)]: 0 failure(s)
[02:40:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:40:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:40:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:40:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #21 has finished. Remaining task(s): 2
[02:40:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"
[02:40:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #18 has finished. Remaining task(s): 1
[02:40:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_layout_transform"
[02:40:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:40:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[02:40:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:40:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[02:40:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:40:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:41:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:41:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:42:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:42:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:42:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:42:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:42:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:42:07] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_layout_transform"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |          Y 
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |          Y 
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |          Y 
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |          Y 
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |          Y 
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |          Y 
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |          Y 
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |          Y 
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |          Y 
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |          Y 
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |          Y 
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |          Y 
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |          Y 
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |          Y 
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |          Y 
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |          Y 
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |         4.9506 |   10408.9894 |            10408.9894 |     32 |          Y 
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |        11.3630 |      92.7300 |               92.7300 |     32 |          Y 
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |        14.6045 |    7046.5233 |             7046.5233 |     32 |          Y 
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |         0.0343 |    2090.9746 |             2090.9746 |     32 |          Y 
 21 |                           fused_layout_transform_1 |         1 |      1 |         0.0000 |    1340.4767 |             1340.4767 |      4 |          Y 
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |         7.3856 |     277.7104 |              277.7104 |     32 |          Y 
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |         0.0000 |    2018.8928 |             2018.8928 |     32 |          Y 
 24 |                                   fused_nn_softmax |      4004 |      1 |         0.0011 |    3726.3539 |             3726.3539 |     32 |          Y 
 25 |                                      fused_reshape |         1 |      1 |         0.0000 |    7313.7595 |             7313.7595 |      4 |          Y 
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 748
Total latency (us): 179642

[02:42:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_layout_transform"
[02:42:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:42:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[02:42:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:42:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[02:42:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:43:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:43:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:43:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:44:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:44:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:44:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:44:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:44:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:44:08] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_layout_transform"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |          Y 
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |          Y 
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |          Y 
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |          Y 
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |          Y 
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |          Y 
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |          Y 
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |          Y 
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |          Y 
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |          Y 
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |          Y 
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |          Y 
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |          Y 
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |          Y 
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |          Y 
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |          Y 
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |         4.9506 |   10408.9894 |            10408.9894 |     32 |          Y 
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |        11.3630 |      92.7300 |               92.7300 |     32 |          Y 
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |        14.6045 |    7046.5233 |             7046.5233 |     32 |          Y 
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |         0.0343 |    2090.9746 |             2090.9746 |     32 |          Y 
 21 |                           fused_layout_transform_1 |         1 |      1 |         0.0000 |    1340.4767 |             1340.4767 |      4 |          Y 
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |         7.3856 |     277.7104 |              277.7104 |     32 |          Y 
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |         0.0000 |    2018.8928 |             2018.8928 |     32 |          Y 
 24 |                                   fused_nn_softmax |      4004 |      1 |         0.0011 |    3726.3539 |             3726.3539 |     32 |          Y 
 25 |                                      fused_reshape |         1 |      1 |         0.0000 |    7313.7595 |             7313.7595 |      4 |          Y 
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 748
Total latency (us): 179642

[02:44:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_layout_transform"
[02:44:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:44:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[02:44:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:44:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[02:44:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:44:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:45:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:45:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:45:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:45:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:45:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:45:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:45:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:45:52] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_layout_transform"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |          Y 
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |          Y 
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |          Y 
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |          Y 
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |          Y 
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |          Y 
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |          Y 
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |          Y 
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |          Y 
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |          Y 
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |          Y 
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |          Y 
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |          Y 
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |          Y 
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |          Y 
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |          Y 
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |         4.9506 |   10408.9894 |            10408.9894 |     32 |          Y 
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |        11.3630 |      92.7300 |               92.7300 |     32 |          Y 
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |        14.6045 |    7046.5233 |             7046.5233 |     32 |          Y 
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |         0.0343 |    2090.9746 |             2090.9746 |     32 |          Y 
 21 |                           fused_layout_transform_1 |         1 |      1 |         0.0000 |    1340.4767 |             1340.4767 |      4 |          Y 
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |         7.3856 |     277.7104 |              277.7104 |     32 |          Y 
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |         0.0000 |    2018.8928 |             2018.8928 |     32 |          Y 
 24 |                                   fused_nn_softmax |      4004 |      1 |         0.0011 |    3726.3539 |             3726.3539 |     32 |          Y 
 25 |                                      fused_reshape |         1 |      1 |         0.0000 |    7313.7595 |             7313.7595 |      4 |          Y 
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 748
Total latency (us): 179642

[02:45:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_layout_transform"
[02:45:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:45:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[02:45:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:45:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[02:46:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:46:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:47:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:47:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:47:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:47:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:47:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:47:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:47:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:47:23] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_layout_transform"
 ID |                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                             fused_layout_transform |         1 |      1 |         0.0002 |       4.7715 |                4.7715 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_clip |  22880256 |      1 |        29.7439 |     769.2430 |              769.2430 |     32 |          Y 
  2 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |   8429568 |      1 |        19.0070 |     443.4975 |              443.4975 |     32 |          Y 
  3 |           fused_nn_contrib_conv2d_NCHWc_add_clip_1 |  53788672 |      1 |         8.5692 |    6276.9789 |             6276.9789 |     32 |          Y 
  4 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |   4214784 |      1 |         0.8918 |    4726.3401 |             4726.3401 |     32 |          Y 
  5 |           fused_nn_contrib_conv2d_NCHWc_add_clip_2 |  52584448 |      1 |         6.9369 |    7580.4180 |             7580.4180 |     32 |          Y 
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 |   8429568 |      1 |         1.2115 |    6958.2224 |             6958.2224 |     32 |          Y 
  7 |           fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 103964672 |      1 |         8.2714 |   12569.2192 |            12569.2192 |     32 |          Y 
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |   2107392 |      1 |         3.5188 |     598.8982 |              598.8982 |     32 |          Y 
  9 |           fused_nn_contrib_conv2d_NCHWc_add_clip_4 |  51982336 |      1 |         7.1714 |    7248.5408 |             7248.5408 |     32 |          Y 
 10 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 |   4214784 |      1 |         0.7347 |    5736.4428 |             5736.4428 |     32 |          Y 
 11 |           fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 103362560 |      1 |         8.0210 |   12886.5286 |            12886.5286 |     32 |          Y 
 12 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |   1053696 |      1 |         0.2457 |    4289.4173 |             4289.4173 |     32 |          Y 
 13 |           fused_nn_contrib_conv2d_NCHWc_add_clip_6 |  51681280 |      1 |         6.2049 |    8329.0952 |             8329.0952 |     32 |          Y 
 14 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |   2107392 |      5 |         0.6091 |    3459.7787 |            17298.8936 |     32 |          Y 
 15 |           fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 103061504 |      5 |        10.6999 |    9631.9981 |            48159.9907 |     32 |          Y 
 16 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |    526848 |      1 |         0.3636 |    1448.8124 |             1448.8124 |     32 |          Y 
 17 |           fused_nn_contrib_conv2d_NCHWc_add_clip_8 |  51530752 |      1 |         4.9506 |   10408.9894 |            10408.9894 |     32 |          Y 
 18 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |   1053696 |      1 |        11.3630 |      92.7300 |               92.7300 |     32 |          Y 
 19 |           fused_nn_contrib_conv2d_NCHWc_add_clip_9 | 102910976 |      1 |        14.6045 |    7046.5233 |             7046.5233 |     32 |          Y 
 20 |                                fused_nn_avg_pool2d |     71680 |      1 |         0.0343 |    2090.9746 |             2090.9746 |     32 |          Y 
 21 |                           fused_layout_transform_1 |         1 |      1 |         0.0000 |    1340.4767 |             1340.4767 |      4 |          Y 
 22 |                  fused_nn_contrib_conv2d_NCHWc_add |   2051049 |      1 |         7.3856 |     277.7104 |              277.7104 |     32 |          Y 
 23 |     fused_layout_transform_reshape_squeeze_reshape |         1 |      1 |         0.0000 |    2018.8928 |             2018.8928 |     32 |          Y 
 24 |                                   fused_nn_softmax |      4004 |      1 |         0.0011 |    3726.3539 |             3726.3539 |     32 |          Y 
 25 |                                      fused_reshape |         1 |      1 |         0.0000 |    7313.7595 |             7313.7595 |      4 |          Y 
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 748
Total latency (us): 179642

[02:47:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_layout_transform"
[02:47:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:47:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[02:47:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:47:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[02:47:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:48:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:48:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:48:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5627060ebc38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5627057b38e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56270618f388)]: 0 failure(s)
[02:49:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:49:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:49:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:49:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #0 has finished. Remaining task(s): 0
Starting to build with relay.
/home/yj/anaconda3/lib/python3.7/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
The result is correct!
