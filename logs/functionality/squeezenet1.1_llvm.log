nohup: ignoring input
[23:16:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"
[23:16:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 16, 15, 15, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 15, 15, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 14 and 1 <= i3_1 and i3_1 < 14, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 13, 13, 4, 64, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 13, 13, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 13, 13, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:16:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:16:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 16, 15, 15, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1 in T.grid(1, 1, 1, 1, 1, 1, 8, 13):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 3, 15, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(15, i2_1 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 8, 3, 1, 1, 4, 1, 13, 2, 8, 1, 3, 1, 2, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_1 * 8 + i1_2 * 2 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_2])
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 13, 13, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 8, 4, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[8, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[23:16:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 16, 15, 15, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1 in T.grid(1, 1, 1, 1, 1, 1, 8):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 15, 15, 4):
                    with T.block("data_pad"):
                        i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1 in T.grid(13, 1, 1):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 3, 1, 1, 4, 1, 13, 2, 8, 1, 3, 1, 2, 1, 1, 2):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i1_1 * 8 + i1_2 * 2 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_1, i3_2])
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                            ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 1, 13, 4):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(64, i1_1 * 8 + ax1)
                            ax2_1 = T.axis.spatial(13, i2_1 + ax2)
                            ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 8, 4, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[8, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[23:16:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 16, 15, 15, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 15, 15, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 14 and 1 <= i3_1 and i3_1 < 14, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 1):
                for i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 13, 1, 1, 8, 3, 1, 1, 4, 1, 13, 2, 8, 1, 3, 1, 2, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_1_1 * 8 + i1_2 * 2 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_1_1, i3_2])
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 13, 13, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 8, 4, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[8, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[23:16:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"
[23:16:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 12, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(48, 12, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 12, 15, 15, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 48, 13, 13, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 48, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 12, 15, 15, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 14 and 1 <= i3_1 and i3_1 < 14, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 48, 13, 13, 4, 48, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 12, 13, 13, 4], "float32"], ["TENSOR", [48, 12, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 13, 13, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 13, 13, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 12, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(48, 12, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 12, 15, 15, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 48, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0 in T.grid(1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 12, 3):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 13, 15, 4):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(12, i5_0 + ax1)
                        i2 = T.axis.spatial(15, i6_0 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 4, 1, 1, 2, 4, 1, 1, 1, 6, 13, 13, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(48, i1_1 * 24 + i1_2 * 6 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_3])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(48, i5_0 * 4 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 12, 13, 13, 4], "float32"], ["TENSOR", [48, 12, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 48, 13, 13, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 4, 6])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[12, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[23:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 12, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(48, 12, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 12, 15, 15, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 48, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1 in T.grid(1, 1, 1, 1, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 12, 15, 15, 4):
                    with T.block("data_pad"):
                        i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1 in T.grid(1, 1, 2):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(12, 3, 3, 1, 4, 1, 1, 2, 4, 1, 1, 1, 6, 13, 13, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(48, i1_1 * 24 + i1_2 * 6 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_3, i3_3])
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            ic = T.axis.reduce(48, i5_0 * 4 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 12, 13, 13, 4], "float32"], ["TENSOR", [48, 12, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 24, 13, 13, 2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(48, i1_1 * 24 + ax1)
                            ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                            ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 4, 6])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[12, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[23:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 12, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(48, 12, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 48, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 2, 12, 3, 3, 1, 4, 1, 1, 2, 4, 1, 1, 1, 6, 13, 13, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(48, i1_1 * 24 + i1_2 * 6 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_3])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(48, i5_0 * 4 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                        T.reads(placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 12, 13, 13, 4], "float32"], ["TENSOR", [48, 12, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh + kh and oh + kh < 14 and 1 <= ow + kw and ow + kw < 14, placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 48, 13, 13, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 4, 6])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[12, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[23:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"
[23:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(32, 8, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 8, 29, 29, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 27, 27, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 27, 27, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 29, 29, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 28 and 1 <= i3_1 and i3_1 < 28, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 27, 27, 4, 32, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 27, 27, 4], "float32"], ["TENSOR", [32, 8, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 27, 27, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 27, 27, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:16:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:16:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(32, 8, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 8, 29, 29, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 27, 27, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 8, 29, 29, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 28 and 1 <= i3_1 and i3_1 < 28, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 3, 1, 1, 1, 2, 1, 27, 1, 2, 3, 3, 1, 4, 1, 1, 2, 16, 1, 1, 1, 2, 9, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1_1 * 8 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(27, i2_0 * 9 + i2_3)
                    ow = T.axis.spatial(27, i3_1_1)
                    oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                    ic = T.axis.reduce(32, i5_0 * 16 + i5_1)
                    kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                    T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 27, 27, 4], "float32"], ["TENSOR", [32, 8, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 27, 27, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 4, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[3, 1, 1, 9])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 27, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[2, 16])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[23:16:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(32, 8, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 8, 29, 29, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 27, 27, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1 in T.grid(1, 2, 3, 1, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 11, 29, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(29, i2_0 * 9 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 28 and 1 <= i3 and i3 < 28, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1 in T.grid(1, 27, 1):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 3, 3, 1, 4, 1, 1, 2, 16, 1, 1, 1, 2, 9, 1, 2):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 8 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(27, i2_0 * 9 + i2_3)
                            ow = T.axis.spatial(27, i3_1)
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                            ic = T.axis.reduce(32, i5_0 * 16 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 27, 27, 4], "float32"], ["TENSOR", [32, 8, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 9, 1, 4):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(32, i1_0 * 16 + i1_1 * 8 + ax1)
                            ax2_1 = T.axis.spatial(27, i2_0 * 9 + ax2)
                            ax3_1 = T.axis.spatial(27, i3_1 + ax3)
                            ax4_1 = T.axis.spatial(4, ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 4, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[3, 1, 1, 9])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 27, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[2, 16])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[23:16:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(32, 8, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 8, 29, 29, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 27, 27, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 3, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0 in T.grid(1, 2, 1, 27, 1, 2, 3, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 9, 1, 4):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(8, i5_0 * 4 + ax1)
                            i2 = T.axis.spatial(29, i2_0 * 9 + i6_0 + ax2)
                            i3 = T.axis.spatial(29, i3_1 + i7_0 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 28 and 1 <= i3 and i3 < 28, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 1, 2, 16, 1, 1, 1, 2, 9, 1, 2):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 8 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(27, i2_0 * 9 + i2_3)
                            ow = T.axis.spatial(27, i3_1)
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                            ic = T.axis.reduce(32, i5_0 * 16 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 27, 27, 4], "float32"], ["TENSOR", [32, 8, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 9, 27, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 16 + ax1)
                        ax2_1 = T.axis.spatial(27, i2_0 * 9 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 4, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[3, 1, 1, 9])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 27, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[2, 16])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=12)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[23:16:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"
[23:16:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 4, 57, 57, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 4, 57, 57, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 56 and 1 <= i3_1 and i3_1 < 56, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 55, 55, 4, 16, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 55, 55, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 55, 55, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:16:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:16:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 4, 57, 57, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1 in T.grid(1, 2, 1, 1, 1, 1, 2, 11, 5):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 7, 13, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(57, i2_1 * 5 + ax2)
                        i3 = T.axis.spatial(57, i3_1 * 11 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 56 and 1 <= i3 and i3 < 56, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 3, 3, 1, 1, 5, 11, 4, 4, 1, 1, 1, 4, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + i1_3)
                        oh = T.axis.spatial(55, i2_1 * 5 + i2_2)
                        ow = T.axis.spatial(55, i3_1 * 11 + i3_2)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(16, i5_0 * 4 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 55, 55, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 1, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 11, 5, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 5, 11, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[4, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[23:16:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 4, 57, 57, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 4, 57, 57, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 56 and 1 <= i3_1 and i3_1 < 56, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1 in T.grid(1, 2, 1, 1, 1, 1, 2, 11, 5, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 3, 3, 1, 1, 5, 11, 4, 4, 1, 1, 1, 4, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1_1 * 4 + i1_3)
                        oh = T.axis.spatial(55, i2_1_1 * 5 + i2_2)
                        ow = T.axis.spatial(55, i3_1_1 * 11 + i3_2)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(16, i5_0 * 4 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 5, 11, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + i1_1_1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(55, i2_1_1 * 5 + ax2)
                        ax3_1 = T.axis.spatial(55, i3_1_1 * 11 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 1, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 11, 5, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 5, 11, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[4, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[23:16:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 4, 57, 57, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 57, 57, 4):
                    with T.block("data_pad"):
                        i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 56 and 1 <= i3 and i3 < 56, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0 in T.grid(1, 1, 1):
                    for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 11, 5, 1, 4, 3, 3, 1, 1, 5, 11, 4, 4, 1, 1, 1, 4, 1, 1, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + i1_3)
                            oh = T.axis.spatial(55, i2_1 * 5 + i2_2)
                            ow = T.axis.spatial(55, i3_1 * 11 + i3_2)
                            oc_block = T.axis.spatial(4, i4_2)
                            ic = T.axis.reduce(16, i5_0 * 4 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 55, 55, 4):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                            ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 1, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 11, 5, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 5, 11, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[4, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[23:16:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #4: "fused_layout_transform"
[23:16:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], T_layout_trans: T.Buffer[(1, 1, 224, 224, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 224, 224, 3):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1 * 3 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 224 and ax3 < 224, placeholder[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[23:16:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:16:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], T_layout_trans: T.Buffer[(1, 1, 224, 224, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 1, 224, 224, 3):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1 * 3 + ax4, ax2, ax3])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 224 and ax3 < 224, placeholder[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:16:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"
[23:16:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 111, 111, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 111, 111, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 111, 111, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 111, 111, 4, 3, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 111, 111, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 111, 111, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:16:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:16:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 111, 111, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 111, 111, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 111, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 4, 3, 1, 4, 1, 3, 3, 1, 1, 37, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + i1_2)
                    oh = T.axis.spatial(111, i2_2 * 37 + i2_3)
                    ow, oc_block, ic, kh, kw = T.axis.remap("SSRRR", [i3_0, i4_2, i5_0, i6_1, i7_1])
                    T.reads(placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 111, 111, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 3, 37])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[111, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:16:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 111, 111, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 111, 111, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 111, 1, 1, 2, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 4, 3, 1, 4, 1, 3, 3, 1, 1, 37, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + i1_2)
                        oh = T.axis.spatial(111, i2_2 * 37 + i2_3)
                        ow, oc_block, ic, kh, kw = T.axis.remap("SSRRR", [i3_0, i4_2, i5_0, i6_1, i7_1])
                        T.reads(placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 111, 1, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(111, ax2)
                        ax3_1 = T.axis.spatial(111, i3_0 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 3, 37])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[111, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:16:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 111, 111, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 111, 111, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 111, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 1, 3, 1, 1, 1, 4, 3, 1, 4, 1, 3, 3, 1, 1, 37, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + i1_2)
                        oh = T.axis.spatial(111, i2_2 * 37 + i2_3)
                        ow, oc_block, ic, kh, kw = T.axis.remap("SSRRR", [i3_0, i4_2, i5_0, i6_1, i7_1])
                        T.reads(placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 111, 1, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(111, ax2)
                        ax3_1 = T.axis.spatial(111, i3_0 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 3, 37])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[111, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:16:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #6: "fused_nn_max_pool2d"
[23:16:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 111, 111, 4), "float32"], tensor: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 16, 55, 55, 4, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
    

[23:16:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:16:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 111, 111, 4), "float32"], tensor: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            tensor_rf = T.alloc_buffer([1, 16, 55, 55, 4, 9], dtype="float32")
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 16, 55, 55, 4, 9, 1):
                with T.block("tensor_rf"):
                    vi5_i6_fused_0 = T.axis.spatial(9, i5_i6_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + vi5_i6_fused_0 // 3, ax3 * 2 + vi5_i6_fused_0 % 3, ax4])
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = placeholder[ax0, ax1, ax2 * 2 + vi5_i6_fused_0 // 3, ax3 * 2 + vi5_i6_fused_0 % 3, ax4]
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 16, 55, 55, 4, 9, 1):
                with T.block("tensor"):
                    vi5_i6_fused_0 = T.axis.reduce(9, i5_i6_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[9, 1])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l12, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
[23:16:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 111, 111, 4), "float32"], tensor: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            tensor_rf = T.alloc_buffer([1, 16, 55, 55, 4, 1], dtype="float32")
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 16, 55, 55, 4, 9, 1):
                with T.block("tensor_rf"):
                    vi5_i6_fused_1 = T.axis.spatial(1, i5_i6_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    rv0 = T.axis.reduce(3, i5_i6_fused_0 // 3)
                    rv1 = T.axis.reduce(3, i5_i6_fused_0 % 3)
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    with T.init():
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.float32(-3.4028234663852886e+38)
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
            for i0, i1, i2, i3, i4, i5_i6_fused_1 in T.grid(1, 16, 55, 55, 4, 1):
                with T.block("tensor"):
                    vi5_i6_fused_1 = T.axis.reduce(1, i5_i6_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[9, 1])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
[23:16:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 111, 111, 4), "float32"], tensor: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 16, 55, 55, 4, 3, 3):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:16:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"
[23:16:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(4, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 4, 55, 55, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 4, 55, 55, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 4, 55, 55, 4, 64, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 55, 55, 4], "float32"], ["TENSOR", [4, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 4, 55, 55, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 4, 55, 55, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:16:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:16:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(4, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 4, 55, 55, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 1, 2, 1, 55, 4, 1, 1, 1, 1, 1, 55, 1, 1, 64, 1, 1, 1, 2, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(4, i1_1 * 2 + i1_3)
                    oh, ow, oc_block, ic = T.axis.remap("SSSR", [i2_2, i3_1, i4_1, i5_1])
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 55, 55, 4], "float32"], ["TENSOR", [4, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 4, 55, 55, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 55, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 55, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:16:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(4, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 4, 55, 55, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 1, 2, 1, 55, 4):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 55, 1, 1, 64, 1, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(4, i1_1 * 2 + i1_3)
                        oh, ow, oc_block, ic = T.axis.remap("SSSR", [i2_2, i3_1, i4_1, i5_1])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 55, 55, 4], "float32"], ["TENSOR", [4, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 55, 1, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(4, i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(55, ax2)
                        ax3_1 = T.axis.spatial(55, i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 55, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 55, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:16:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(4, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 4, 55, 55, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 55, 4, 1, 1, 1, 1, 1, 55, 1, 1, 64, 1, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(4, i1_1 * 2 + i1_3)
                        oh, ow, oc_block, ic = T.axis.remap("SSSR", [i2_2, i3_1, i4_1, i5_1])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 55, 55, 4], "float32"], ["TENSOR", [4, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 55, 55, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 55, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 55, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:16:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"
[23:16:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(4, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 4, 55, 55, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 4, 55, 55, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 4, 55, 55, 4, 128, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 55, 55, 4], "float32"], ["TENSOR", [4, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 4, 55, 55, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 4, 55, 55, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:16:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:16:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(4, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 4, 55, 55, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 11, 1, 1, 1, 2, 1, 1, 4, 128, 1, 1, 1, 1, 5, 11, 1, 1, 1, 1, 1, 2, 1, 5, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(4, i1_1 * 2 + i1_3)
                    oh = T.axis.spatial(55, i2_0 * 5 + i2_2)
                    ow = T.axis.spatial(55, i3_2 * 5 + i3_3)
                    oc_block, ic = T.axis.remap("SR", [i4_1, i5_0])
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 55, 55, 4], "float32"], ["TENSOR", [4, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 4, 55, 55, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[11, 1, 5, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 11, 5])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:16:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(4, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 4, 55, 55, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 11, 1, 1, 1, 2, 1, 1, 4):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 1, 1, 1, 1, 5, 11, 1, 1, 1, 1, 1, 2, 1, 5, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(4, i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(55, i2_0 * 5 + i2_2)
                        ow = T.axis.spatial(55, i3_2 * 5 + i3_3)
                        oc_block, ic = T.axis.remap("SR", [i4_1, i5_0])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 55, 55, 4], "float32"], ["TENSOR", [4, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 5, 55, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(4, i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(55, i2_0 * 5 + ax2)
                        ax3_1 = T.axis.spatial(55, ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[11, 1, 5, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 11, 5])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:16:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(4, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 4, 55, 55, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 11, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 4, 128, 1, 1, 1, 1, 5, 11, 1, 1, 1, 1, 1, 2, 1, 5, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(4, i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(55, i2_0 * 5 + i2_2)
                        ow = T.axis.spatial(55, i3_2 * 5 + i3_3)
                        oc_block, ic = T.axis.remap("SR", [i4_1, i5_0])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 55, 55, 4], "float32"], ["TENSOR", [4, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 5, 55, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(55, i2_0 * 5 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[11, 1, 5, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 11, 5])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:16:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"
[23:16:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 55, 55, 4, 16, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 55, 55, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 55, 55, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:17:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:17:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 11, 5, 1, 1, 1, 1, 1, 2, 8, 1, 1, 1, 4, 5, 11, 1, 2, 1, 1, 1, 1, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i1_0 * 4 + i1_2)
                    oh = T.axis.spatial(55, i2_0 * 5 + i2_2)
                    ow = T.axis.spatial(55, i3_0 * 11 + i3_2)
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                    ic = T.axis.reduce(16, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 55, 55, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[11, 1, 5, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[5, 1, 11, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:17:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 11, 5, 1, 1, 1, 1, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 4, 5, 11, 1, 2, 1, 1, 1, 1, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 4 + i1_2)
                        oh = T.axis.spatial(55, i2_0 * 5 + i2_2)
                        ow = T.axis.spatial(55, i3_0 * 11 + i3_2)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(16, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 5, 11, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(55, i2_0 * 5 + ax2)
                        ax3_1 = T.axis.spatial(55, i3_0 * 11 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[11, 1, 5, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[5, 1, 11, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:17:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 11, 5, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 8, 1, 1, 1, 4, 5, 11, 1, 2, 1, 1, 1, 1, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 4 + i1_2)
                        oh = T.axis.spatial(55, i2_0 * 5 + i2_2)
                        ow = T.axis.spatial(55, i3_0 * 11 + i3_2)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(16, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 5, 11, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(55, i2_0 * 5 + ax2)
                        ax3_1 = T.axis.spatial(55, i3_0 * 11 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[11, 1, 5, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[5, 1, 11, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:17:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #10: "fused_concatenate"
[23:17:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(1, 16, 55, 55, 4), "float32"], T_concat: T.Buffer[(1, 32, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 55, 55, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 16, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(16 <= ax1, placeholder_1[ax0, ax1 - 16, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[23:17:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:17:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(1, 16, 55, 55, 4), "float32"], T_concat: T.Buffer[(1, 32, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 55, 55, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 16, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(16 <= ax1, placeholder_1[ax0, ax1 - 16, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:17:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #11: "fused_nn_max_pool2d_1"
[23:17:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 55, 55, 4), "float32"], tensor: T.Buffer[(1, 32, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 27, 27, 4, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
    

[23:17:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:17:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 55, 55, 4), "float32"], tensor: T.Buffer[(1, 32, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            tensor_rf = T.alloc_buffer([1, 32, 27, 27, 4, 9], dtype="float32")
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 32, 27, 27, 4, 9, 1):
                with T.block("tensor_rf"):
                    vi5_i6_fused_0 = T.axis.spatial(9, i5_i6_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + vi5_i6_fused_0 // 3, ax3 * 2 + vi5_i6_fused_0 % 3, ax4])
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = placeholder[ax0, ax1, ax2 * 2 + vi5_i6_fused_0 // 3, ax3 * 2 + vi5_i6_fused_0 % 3, ax4]
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 32, 27, 27, 4, 9, 1):
                with T.block("tensor"):
                    vi5_i6_fused_0 = T.axis.reduce(9, i5_i6_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[9, 1])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l12, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
[23:17:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 55, 55, 4), "float32"], tensor: T.Buffer[(1, 32, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            tensor_rf = T.alloc_buffer([1, 32, 27, 27, 4, 1], dtype="float32")
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 32, 27, 27, 4, 9, 1):
                with T.block("tensor_rf"):
                    vi5_i6_fused_1 = T.axis.spatial(1, i5_i6_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    rv0 = T.axis.reduce(3, i5_i6_fused_0 // 3)
                    rv1 = T.axis.reduce(3, i5_i6_fused_0 % 3)
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    with T.init():
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.float32(-3.4028234663852886e+38)
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
            for i0, i1, i2, i3, i4, i5_i6_fused_1 in T.grid(1, 32, 27, 27, 4, 1):
                with T.block("tensor"):
                    vi5_i6_fused_1 = T.axis.reduce(1, i5_i6_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[9, 1])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
[23:17:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 55, 55, 4), "float32"], tensor: T.Buffer[(1, 32, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 27, 27, 4, 3, 3):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:17:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"
[23:17:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 27, 27, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 8, 27, 27, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 8, 27, 27, 4, 128, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 27, 27, 4], "float32"], ["TENSOR", [8, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 27, 27, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 27, 27, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 27, 27, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 1, 2, 9, 3, 1, 128, 1, 1, 1, 4, 3, 9, 2, 1, 1, 1, 1, 1, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(8, i1_1 * 4 + i1_2)
                    oh = T.axis.spatial(27, i2_1 * 3 + i2_2)
                    ow = T.axis.spatial(27, i3_1 * 9 + i3_2)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                    ic = T.axis.reduce(128, i5_0)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 27, 27, 4], "float32"], ["TENSOR", [8, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 8, 27, 27, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 9, 3, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 3, 9, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 27, 27, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 2, 1, 2, 9, 3, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 1, 1, 1, 4, 3, 9, 2, 1, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_1 * 4 + i1_2)
                        oh = T.axis.spatial(27, i2_1 * 3 + i2_2)
                        ow = T.axis.spatial(27, i3_1 * 9 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(128, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 27, 27, 4], "float32"], ["TENSOR", [8, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 3, 9, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(8, i1_1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(27, i2_1 * 3 + ax2)
                        ax3_1 = T.axis.spatial(27, i3_1 * 9 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 9, 3, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 3, 9, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 27, 27, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 9, 3, 1, 128, 1, 1, 1, 4, 3, 9, 2, 1, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_1 * 4 + i1_2)
                        oh = T.axis.spatial(27, i2_1 * 3 + i2_2)
                        ow = T.axis.spatial(27, i3_1 * 9 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(128, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 27, 27, 4], "float32"], ["TENSOR", [8, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 27, 27, 2):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 9, 3, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 3, 9, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"
[23:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(8, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 27, 27, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 8, 27, 27, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 8, 27, 27, 4, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 27, 27, 4], "float32"], ["TENSOR", [8, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 27, 27, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 27, 27, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:17:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:17:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(8, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 27, 27, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 3, 3, 1, 1, 2, 3, 3, 1, 8, 1, 1, 1, 1, 3, 1, 1, 32, 1, 1, 1, 2, 1, 3, 4):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(8, i1_0 * 4 + i1_1 * 2 + i1_3)
                    oh = T.axis.spatial(27, i2_0 * 9 + i2_1 * 3 + i2_2)
                    ow = T.axis.spatial(27, i3_0 * 9 + i3_1 * 3 + i3_3)
                    oc_block = T.axis.spatial(4, i4_3)
                    ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 27, 27, 4], "float32"], ["TENSOR", [8, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 8, 27, 27, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 3, 3, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[3, 3, 1, 3])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:17:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(8, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 27, 27, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 3, 3, 1, 1, 2, 3, 3, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 1, 3, 1, 1, 32, 1, 1, 1, 2, 1, 3, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0 * 4 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(27, i2_0 * 9 + i2_1 * 3 + i2_2)
                        ow = T.axis.spatial(27, i3_0 * 9 + i3_1 * 3 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 27, 27, 4], "float32"], ["TENSOR", [8, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 3, 3, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(8, i1_0 * 4 + i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(27, i2_0 * 9 + i2_1 * 3 + ax2)
                        ax3_1 = T.axis.spatial(27, i3_0 * 9 + i3_1 * 3 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 3, 3, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[3, 3, 1, 3])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:17:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(8, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 27, 27, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 3, 3, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 3, 3, 1, 8, 1, 1, 1, 1, 3, 1, 1, 32, 1, 1, 1, 2, 1, 3, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0 * 4 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(27, i2_0 * 9 + i2_1 * 3 + i2_2)
                        ow = T.axis.spatial(27, i3_0 * 9 + i3_1 * 3 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 27, 27, 4], "float32"], ["TENSOR", [8, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 9, 9, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(8, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(27, i2_0 * 9 + ax2)
                        ax3_1 = T.axis.spatial(27, i3_0 * 9 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 3, 3, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[3, 3, 1, 3])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:17:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"
[23:17:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(32, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 27, 27, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 27, 27, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 27, 27, 4, 32, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 27, 27, 4], "float32"], ["TENSOR", [32, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 27, 27, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 27, 27, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:17:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:17:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(32, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 27, 27, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 16, 3, 1, 2, 1, 1, 3, 1, 1, 16, 1, 1, 1, 2, 3, 27, 1, 2, 1, 1, 1, 1, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 2 + i1_2)
                    oh = T.axis.spatial(27, i2_0 * 9 + i2_1 * 3 + i2_2)
                    ow = T.axis.spatial(27, i3_2)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                    ic = T.axis.reduce(32, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 27, 27, 4], "float32"], ["TENSOR", [32, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 27, 27, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 1, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 3, 3, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 27, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:17:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(32, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 27, 27, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 16, 3, 1, 2, 1, 1, 3, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 2, 3, 27, 1, 2, 1, 1, 1, 1, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 2 + i1_2)
                        oh = T.axis.spatial(27, i2_0 * 9 + i2_1 * 3 + i2_2)
                        ow = T.axis.spatial(27, i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(32, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 27, 27, 4], "float32"], ["TENSOR", [32, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 3, 27, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(27, i2_0 * 9 + i2_1 * 3 + ax2)
                        ax3_1 = T.axis.spatial(27, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 1, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 3, 3, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 27, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:17:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(32, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 27, 27, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 16, 3, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 3, 1, 1, 16, 1, 1, 1, 2, 3, 27, 1, 2, 1, 1, 1, 1, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 2 + i1_2)
                        oh = T.axis.spatial(27, i2_0 * 9 + i2_1 * 3 + i2_2)
                        ow = T.axis.spatial(27, i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(32, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 27, 27, 4], "float32"], ["TENSOR", [32, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 9, 27, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(27, i2_0 * 9 + ax2)
                        ax3_1 = T.axis.spatial(27, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 1, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 3, 3, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 27, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:17:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #15: "fused_concatenate_1"
[23:17:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(1, 32, 27, 27, 4), "float32"], T_concat: T.Buffer[(1, 64, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 27, 27, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 32, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(32 <= ax1, placeholder_1[ax0, ax1 - 32, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[23:17:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:17:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(1, 32, 27, 27, 4), "float32"], T_concat: T.Buffer[(1, 64, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 27, 27, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 32, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(32 <= ax1, placeholder_1[ax0, ax1 - 32, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:17:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #16: "fused_nn_max_pool2d_2"
[23:17:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 27, 27, 4), "float32"], tensor: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 13, 13, 4, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
    

[23:17:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:17:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 27, 27, 4), "float32"], tensor: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            tensor_rf = T.alloc_buffer([1, 64, 13, 13, 4, 9], dtype="float32")
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 64, 13, 13, 4, 9, 1):
                with T.block("tensor_rf"):
                    vi5_i6_fused_0 = T.axis.spatial(9, i5_i6_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + vi5_i6_fused_0 // 3, ax3 * 2 + vi5_i6_fused_0 % 3, ax4])
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = placeholder[ax0, ax1, ax2 * 2 + vi5_i6_fused_0 // 3, ax3 * 2 + vi5_i6_fused_0 % 3, ax4]
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 64, 13, 13, 4, 9, 1):
                with T.block("tensor"):
                    vi5_i6_fused_0 = T.axis.reduce(9, i5_i6_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[9, 1])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l12, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
[23:17:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 27, 27, 4), "float32"], tensor: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            tensor_rf = T.alloc_buffer([1, 64, 13, 13, 4, 1], dtype="float32")
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 64, 13, 13, 4, 9, 1):
                with T.block("tensor_rf"):
                    vi5_i6_fused_1 = T.axis.spatial(1, i5_i6_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    rv0 = T.axis.reduce(3, i5_i6_fused_0 // 3)
                    rv1 = T.axis.reduce(3, i5_i6_fused_0 % 3)
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    with T.init():
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.float32(-3.4028234663852886e+38)
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
            for i0, i1, i2, i3, i4, i5_i6_fused_1 in T.grid(1, 64, 13, 13, 4, 1):
                with T.block("tensor"):
                    vi5_i6_fused_1 = T.axis.reduce(1, i5_i6_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[9, 1])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
[23:17:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 27, 27, 4), "float32"], tensor: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 13, 13, 4, 3, 3):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:17:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"
[23:17:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(12, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 12, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 12, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 12, 13, 13, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 12, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 12, 13, 13, 4, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 13, 13, 4], "float32"], ["TENSOR", [12, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 12, 13, 13, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 12, 13, 13, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:17:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:17:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(12, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 12, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 12, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 12, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 1, 6, 1, 1, 2, 64, 1, 1, 1, 2, 1, 1, 1, 4, 1, 1, 1, 1, 13, 13, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(12, i1_1 * 2 + i1_2)
                    oh, ow = T.axis.remap("SS", [i2_3, i3_3])
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                    ic = T.axis.reduce(256, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 13, 13, 4], "float32"], ["TENSOR", [12, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 12, 13, 13, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 6, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:17:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(12, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 12, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 12, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 12, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 1, 6, 1, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 2, 1, 1, 1, 4, 1, 1, 1, 1, 13, 13, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(12, i1_1 * 2 + i1_2)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_3])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(256, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 13, 13, 4], "float32"], ["TENSOR", [12, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 13, 13, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(12, i1_1 * 2 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 6, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:17:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(12, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 12, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 12, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 12, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 6, 1, 1, 2, 64, 1, 1, 1, 2, 1, 1, 1, 4, 1, 1, 1, 1, 13, 13, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(12, i1_1 * 2 + i1_2)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_3])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(256, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 13, 13, 4], "float32"], ["TENSOR", [12, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 12, 13, 13, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 6, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:17:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"
[23:17:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(12, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 12, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 12, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 12, 13, 13, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 12, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 12, 13, 13, 4, 384, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 13, 13, 4], "float32"], ["TENSOR", [12, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 12, 13, 13, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 12, 13, 13, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:17:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:17:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(12, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 12, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 12, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 12, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 13, 4, 1, 1, 13, 1, 1, 128, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 6, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(12, i1_0 * 6 + i1_3)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_0, i4_0])
                    ic = T.axis.reduce(384, i5_0 * 3 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 13, 13, 4], "float32"], ["TENSOR", [12, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 12, 13, 13, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 6])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:17:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(12, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 12, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 12, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 12, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 13, 4, 1, 1, 13, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 6, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(12, i1_0 * 6 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_0, i4_0])
                        ic = T.axis.reduce(384, i5_0 * 3 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 13, 13, 4], "float32"], ["TENSOR", [12, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 6, 1, 1, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(12, i1_0 * 6 + ax1)
                        ax2_1 = T.axis.spatial(13, i2_1 + ax2)
                        ax3_1 = T.axis.spatial(13, i3_0 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 6])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:17:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(12, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 12, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 12, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 12, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 13, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 13, 1, 1, 128, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 6, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(12, i1_0 * 6 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_0, i4_0])
                        ic = T.axis.reduce(384, i5_0 * 3 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 13, 13, 4], "float32"], ["TENSOR", [12, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 6, 13, 1, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(12, i1_0 * 6 + ax1)
                        ax2_1 = T.axis.spatial(13, ax2)
                        ax3_1 = T.axis.spatial(13, i3_0 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 6])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:17:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"
[23:17:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 12, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(48, 12, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 48, 13, 13, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 48, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 48, 13, 13, 4, 48, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 12, 13, 13, 4], "float32"], ["TENSOR", [48, 12, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 13, 13, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 13, 13, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:17:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:17:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 12, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(48, 12, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 48, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 1, 1, 2, 1, 1, 1, 1, 2, 24, 1, 1, 1, 1, 13, 13, 1, 2, 1, 1, 1, 16, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(48, i1_0 * 16 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_2, i3_2])
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                    ic = T.axis.reduce(48, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 12, 13, 13, 4], "float32"], ["TENSOR", [48, 12, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 48, 13, 13, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[3, 1, 1, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[24, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:17:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 12, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(48, 12, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 48, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 3, 1, 1, 2, 1, 1, 1, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(24, 1, 1, 1, 1, 13, 13, 1, 2, 1, 1, 1, 16, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(48, i1_0 * 16 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_2])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(48, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 12, 13, 13, 4], "float32"], ["TENSOR", [48, 12, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 13, 13, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(48, i1_0 * 16 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[3, 1, 1, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[24, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:17:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 12, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(48, 12, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 48, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 3, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 24, 1, 1, 1, 1, 13, 13, 1, 2, 1, 1, 1, 16, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(48, i1_0 * 16 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_2])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(48, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 12, 13, 13, 4], "float32"], ["TENSOR", [48, 12, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 13, 13, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(48, i1_0 * 16 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[3, 1, 1, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[24, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:17:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #20: "fused_concatenate_2"
[23:17:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(1, 48, 13, 13, 4), "float32"], T_concat: T.Buffer[(1, 96, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 13, 13, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 48, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(48 <= ax1, placeholder_1[ax0, ax1 - 48, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[23:17:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:17:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(1, 48, 13, 13, 4), "float32"], T_concat: T.Buffer[(1, 96, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 96, 13, 13, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 48, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(48 <= ax1, placeholder_1[ax0, ax1 - 48, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:17:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"
[23:17:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(16, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 13, 13, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 13, 13, 4, 384, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 13, 13, 4], "float32"], ["TENSOR", [16, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 13, 13, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 13, 13, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:17:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:17:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(16, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 1, 1, 1, 1, 13, 1, 8, 1, 1, 1, 8, 13, 1, 4, 48, 1, 1, 1, 1, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_2)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_2, i3_1, i4_2])
                    ic = T.axis.reduce(384, i5_0 * 48 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 13, 13, 4], "float32"], ["TENSOR", [16, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 13, 13, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 48])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:17:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(16, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 1, 1, 1, 1, 13, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 8, 13, 1, 4, 48, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_2)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_2, i3_1, i4_2])
                        ic = T.axis.reduce(384, i5_0 * 48 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 13, 13, 4], "float32"], ["TENSOR", [16, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 13, 1, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(13, ax2)
                        ax3_1 = T.axis.spatial(13, i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 48])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:17:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(16, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 13, 1, 8, 1, 1, 1, 8, 13, 1, 4, 48, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_2)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_2, i3_1, i4_2])
                        ic = T.axis.reduce(384, i5_0 * 48 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 13, 13, 4], "float32"], ["TENSOR", [16, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 13, 13, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 48])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:17:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"
[23:17:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(16, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 13, 13, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 13, 13, 4, 512, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [16, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 13, 13, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 13, 13, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:17:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:17:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(16, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 2, 1, 8, 1, 1, 1, 256, 1, 1, 1, 1, 1, 13, 2, 2, 1, 1, 1, 1, 13, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1)
                    oh, ow = T.axis.remap("SS", [i2_3, i3_2])
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                    ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [16, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 13, 13, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 8, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:17:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(16, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 2, 1, 8, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(256, 1, 1, 1, 1, 1, 13, 2, 2, 1, 1, 1, 1, 13, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_2])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [16, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 13, 13, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + i1_1 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 8, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:17:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(16, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 1, 1, 1, 256, 1, 1, 1, 1, 1, 13, 2, 2, 1, 1, 1, 1, 13, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_2])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [16, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 13, 13, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 8, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:17:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"
[23:17:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 13, 13, 4, 64, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 13, 13, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 13, 13, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:17:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:17:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 1, 1, 16, 13, 13, 2, 8, 1, 1, 1, 1, 1, 1, 2, 8, 1, 1, 1, 2, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_1 * 2 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_1, i3_1])
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                    ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 13, 13, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 16, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:17:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 1, 1, 16, 13, 13, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 1, 1, 1, 2, 8, 1, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_1 * 2 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_1])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 1, 1, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 32 + i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(13, i2_1 + ax2)
                        ax3_1 = T.axis.spatial(13, i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 16, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:17:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 16, 13, 13, 2, 8, 1, 1, 1, 1, 1, 1, 2, 8, 1, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_1 * 2 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_1])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 13, 13, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 32 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 16, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:17:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #24: "fused_concatenate_3"
[23:17:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(1, 64, 13, 13, 4), "float32"], T_concat: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 64, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(64 <= ax1, placeholder_1[ax0, ax1 - 64, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[23:17:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:17:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(1, 64, 13, 13, 4), "float32"], T_concat: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 64, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(64 <= ax1, placeholder_1[ax0, ax1 - 64, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:17:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"
[23:17:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(250, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 250, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 250, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 250, 13, 13, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 250, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 250, 13, 13, 4, 512, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [250, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 250, 13, 13, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 250, 13, 13, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:17:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:17:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(250, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 250, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 250, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 250, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 1, 1, 13, 1, 2, 256, 1, 1, 1, 25, 1, 13, 1, 2, 1, 1, 1, 10, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(250, i1_2 * 10 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_1, i3_2])
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                    ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [250, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 250, 13, 13, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 25, 10])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:17:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(250, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 250, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 250, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 250, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 1, 1, 13, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(256, 1, 1, 1, 25, 1, 13, 1, 2, 1, 1, 1, 10, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(250, i1_2 * 10 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_2])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [250, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 250, 1, 13, 2):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(13, i2_1 + ax2)
                        ax3_1 = T.axis.spatial(13, ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 25, 10])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:17:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(250, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 250, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 250, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 250, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 13, 1, 2, 256, 1, 1, 1, 25, 1, 13, 1, 2, 1, 1, 1, 10, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(250, i1_2 * 10 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_2])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [250, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 250, 13, 13, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 25, 10])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:17:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #26: "fused_nn_avg_pool2d"
[23:17:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 250, 13, 13, 4), "float32"], tensor: T.Buffer[(1, 250, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 250, 1, 1, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 250, 1, 1, 4, 13, 13):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder[ax0, ax1, ax2 * 13 + rv0, ax3 * 13 + rv1, ax4])
                T.writes(tensor_1[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] + placeholder[ax0, ax1, ax2 * 13 + rv0, ax3 * 13 + rv1, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 250, 1, 1, 4):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] / T.cast(T.max((T.min(ax2, 0) * 13 + T.min(ax2 * 13 + 12, 12) + 1 - ax2 * 13) * (T.min(ax3, 0) * 13 + T.min(ax3 * 13 + 12, 12) + 1 - ax3 * 13), 1), "float32")
    

[23:17:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:17:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 250, 13, 13, 4), "float32"], tensor: T.Buffer[(1, 250, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            tensor_1 = T.alloc_buffer([1, 250, 1, 1, 4], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 250, 1, 1, 4, 13], dtype="float32")
            for i0, i1 in T.grid(1, 250):
                for ax0 in T.serial(13):
                    for ax0_1, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(1, 1, 1, 1, 1, 4, 13):
                        with T.block("tensor_rf"):
                            vi5_i6_fused_0 = T.axis.spatial(13, ax0 + ax0_1)
                            ax0_2 = T.axis.spatial(1, ax1)
                            ax1_1 = T.axis.spatial(250, i1 + ax2)
                            ax2_1, ax3_1, ax4_1, vi5_i6_fused_1 = T.axis.remap("SSSR", [ax3, ax4, ax5, ax6])
                            T.reads(placeholder[ax0_2, ax1_1, ax2_1 * 13 + (vi5_i6_fused_0 * 13 + vi5_i6_fused_1) // 13, ax3_1 * 13 + (vi5_i6_fused_0 * 13 + vi5_i6_fused_1) % 13, ax4_1])
                            T.writes(tensor_rf[ax0_2, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0])
                            with T.init():
                                tensor_rf[ax0_2, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0] = T.float32(0)
                            tensor_rf[ax0_2, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0] = tensor_rf[ax0_2, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0] + placeholder[ax0_2, ax1_1, ax2_1 * 13 + (vi5_i6_fused_0 * 13 + vi5_i6_fused_1) // 13, ax3_1 * 13 + (vi5_i6_fused_0 * 13 + vi5_i6_fused_1) % 13, ax4_1]
                    for ax1, ax2, ax3, ax4, ax5 in T.grid(1, 1, 1, 1, 4):
                        with T.block("tensor"):
                            vi5_i6_fused_0, ax0_3 = T.axis.remap("RS", [ax0, ax1])
                            ax1_2 = T.axis.spatial(250, i1 + ax2)
                            ax2_2, ax3_2, ax4_2 = T.axis.remap("SSS", [ax3, ax4, ax5])
                            T.reads(tensor_rf[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_0])
                            T.writes(tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2])
                            with T.init():
                                tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2] = T.float32(0)
                            tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2] = tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2] + tensor_rf[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_0]
                for i2, i3, i4 in T.grid(1, 1, 4):
                    with T.block("tensor_1"):
                        ax0_4, ax1_3, ax2_3, ax3_3, ax4_3 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(tensor_1[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3])
                        T.writes(tensor[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3])
                        tensor[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3] = tensor_1[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3] / T.cast(T.max((T.min(ax2_3, 0) * 13 + T.min(ax2_3 * 13 + 12, 12) + 1 - ax2_3 * 13) * (T.min(ax3_3, 0) * 13 + T.min(ax3_3 * 13 + 12, 12) + 1 - ax3_3 * 13), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[13, 13])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l12, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
b16, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l17 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b16, decision=2)
sch.compute_at(block=b16, loop=l18, preserve_unit_loops=True)
[23:17:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 250, 13, 13, 4), "float32"], tensor: T.Buffer[(1, 250, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            tensor_1 = T.alloc_buffer([1, 250, 1, 1, 4], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 250, 1, 1, 4, 13], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 250, 1, 1, 4):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(13, 1, 1, 1, 1, 1, 13):
                    with T.block("tensor_rf"):
                        vi5_i6_fused_1, ax0_1 = T.axis.remap("SS", [ax0, ax1])
                        ax1_1 = T.axis.spatial(250, i1 + ax2)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax3, ax4])
                        ax4_1 = T.axis.spatial(4, i4 + ax5)
                        vi5_i6_fused_0 = T.axis.reduce(13, ax6)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 * 13 + (vi5_i6_fused_0 * 13 + vi5_i6_fused_1) // 13, ax3_1 * 13 + (vi5_i6_fused_0 * 13 + vi5_i6_fused_1) % 13, ax4_1])
                        T.writes(tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_1])
                        with T.init():
                            tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_1] = T.float32(0)
                        tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_1] = tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_1] + placeholder[ax0_1, ax1_1, ax2_1 * 13 + (vi5_i6_fused_0 * 13 + vi5_i6_fused_1) // 13, ax3_1 * 13 + (vi5_i6_fused_0 * 13 + vi5_i6_fused_1) % 13, ax4_1]
                for i5_i6_fused_1 in T.serial(13):
                    with T.block("tensor"):
                        vi5_i6_fused_1 = T.axis.reduce(13, i5_i6_fused_1)
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(250, i1)
                        ax2 = T.axis.spatial(1, 0)
                        ax3 = T.axis.spatial(1, 0)
                        ax4 = T.axis.spatial(4, i4)
                        T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                        T.writes(tensor_1[ax0, ax1, ax2, ax3, ax4])
                        with T.init():
                            tensor_1[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
                        tensor_1[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] + tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1]
            for i0, i1, i2, i3, i4 in T.grid(1, 250, 1, 1, 4):
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] / T.cast(T.max((T.min(ax2, 0) * 13 + T.min(ax2 * 13 + 12, 12) + 1 - ax2 * 13) * (T.min(ax3, 0) * 13 + T.min(ax3 * 13 + 12, 12) + 1 - ax3 * 13), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[13, 13])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
b16, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l17 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b16, decision=4)
sch.compute_at(block=b16, loop=l18, preserve_unit_loops=True)
[23:17:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 250, 13, 13, 4), "float32"], tensor: T.Buffer[(1, 250, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            tensor_1 = T.alloc_buffer([1, 250, 1, 1, 4], dtype="float32")
            for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 250, 1, 1, 4, 13, 13):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                    T.reads(placeholder[ax0, ax1, ax2 * 13 + rv0, ax3 * 13 + rv1, ax4])
                    T.writes(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    with T.init():
                        tensor_1[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
                    tensor_1[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] + placeholder[ax0, ax1, ax2 * 13 + rv0, ax3 * 13 + rv1, ax4]
            for i0, i1, i2, i3, i4 in T.grid(1, 250, 1, 1, 4):
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] / T.cast(T.max((T.min(ax2, 0) * 13 + T.min(ax2 * 13 + 12, 12) + 1 - ax2 * 13) * (T.min(ax3, 0) * 13 + T.min(ax3 * 13 + 12, 12) + 1 - ax3 * 13), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[23:17:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #27: "fused_layout_transform_reshape"
[23:17:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 250, 1, 1, 4), "float32"], T_reshape: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 1000, 1, 1], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1000, 1, 1):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 1000 and ax2 < 1 and ax3 < 1, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1 in T.grid(1, 1000):
            with T.block("T_reshape"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_layout_trans[0, ax1 % 1000, 0, 0])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = T_layout_trans[0, ax1 % 1000, 0, 0]
    

[23:17:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:17:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 250, 1, 1, 4), "float32"], T_reshape: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_layout_trans = T.alloc_buffer([1, 1000, 1, 1], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 1000, 1, 1):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 1000 and ax2 < 1 and ax3 < 1, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
            for i0, i1 in T.grid(1, 1000):
                with T.block("T_reshape"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(T_layout_trans[0, ax1 % 1000, 0, 0])
                    T.writes(T_reshape[ax0, ax1])
                    T_reshape[ax0, ax1] = T_layout_trans[0, ax1 % 1000, 0, 0]
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[23:17:38] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |            N/A |          N/A |                   N/A |      0 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |            N/A |          N/A |                   N/A |      0 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |            N/A |          N/A |                   N/A |      0 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |            N/A |          N/A |                   N/A |      0 |            
  4 |                       fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |                            fused_concatenate |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                          fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[23:17:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"
[23:17:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:17:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:18:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68109b48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67eecf08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fadd38)]: 0 failure(s)
[23:18:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[23:20:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68109b48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67eecf08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fadd38)]: 0 failure(s)
[23:22:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68109b48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67eecf08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fadd38)]: 0 failure(s)
[23:23:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68109b48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67eecf08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fadd38)]: 0 failure(s)
[23:25:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68109b48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67eecf08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fadd38)]: 0 failure(s)
[23:25:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9996  0.9995  0.9993  0.9992  0.9989  0.9988  0.9986  0.9986  0.9985  0.9983  0.9983  0.9983  0.9983  0.9982  0.9981
[17 : 32]:	0.9981  0.9980  0.9979  0.9979  0.9978  0.9977  0.9976  0.9975  0.9974  0.9973  0.9972  0.9972  0.9970  0.9969  0.9969  0.9968
[23:25:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[23:25:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[23:25:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[23:26:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[23:26:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"
[23:26:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:26:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:27:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67c96568)]: 122 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6811e808)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67cfc038)]: 0 failure(s)
[23:27:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 1926 candidate(s)
[23:28:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67c96568)]: 9 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6811e808)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67cfc038)]: 0 failure(s)
[23:30:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67c96568)]: 9 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6811e808)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67cfc038)]: 0 failure(s)
[23:32:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67c96568)]: 14 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6811e808)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67cfc038)]: 0 failure(s)
[23:35:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67c96568)]: 19 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6811e808)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67cfc038)]: 0 failure(s)
[23:35:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9996  0.9996  0.9994  0.9993  0.9992  0.9991  0.9990  0.9990  0.9988  0.9987  0.9987  0.9987  0.9985  0.9984  0.9984
[17 : 32]:	0.9983  0.9983  0.9982  0.9979  0.9979  0.9978  0.9978  0.9976  0.9973  0.9973  0.9973  0.9973  0.9972  0.9971  0.9971  0.9969
[23:35:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[23:35:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[23:35:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[23:36:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[23:36:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"
[23:36:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:36:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:37:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68161d38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c678ac168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f75f18)]: 0 failure(s)
[23:37:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[23:39:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68161d38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c678ac168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f75f18)]: 0 failure(s)
[23:41:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68161d38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c678ac168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f75f18)]: 0 failure(s)
[23:43:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68161d38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c678ac168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f75f18)]: 0 failure(s)
[23:45:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68161d38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c678ac168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f75f18)]: 0 failure(s)
[23:45:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9999  0.9997  0.9996  0.9995  0.9994  0.9994  0.9994  0.9993  0.9991  0.9990  0.9990  0.9988  0.9988  0.9988
[17 : 32]:	0.9987  0.9986  0.9984  0.9982  0.9982  0.9982  0.9981  0.9980  0.9980  0.9979  0.9976  0.9976  0.9975  0.9975  0.9975  0.9972
[23:45:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[23:46:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[23:46:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[23:46:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[23:46:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"
[23:46:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:46:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:48:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c678bc508)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f0cee8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c68031f68)]: 0 failure(s)
[23:48:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[23:50:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c678bc508)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f0cee8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c68031f68)]: 0 failure(s)
[23:52:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c678bc508)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f0cee8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c68031f68)]: 0 failure(s)
[23:54:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c678bc508)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f0cee8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c68031f68)]: 0 failure(s)
[23:56:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c678bc508)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f0cee8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c68031f68)]: 0 failure(s)
[23:57:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9999  0.9998  0.9997  0.9996  0.9993  0.9990  0.9988  0.9987  0.9987  0.9984  0.9983  0.9982  0.9981  0.9980
[17 : 32]:	0.9980  0.9977  0.9973  0.9971  0.9968  0.9965  0.9965  0.9964  0.9962  0.9962  0.9961  0.9959  0.9959  0.9958  0.9958  0.9956
[23:57:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[23:57:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[23:57:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[23:57:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[23:58:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_layout_transform"
[23:58:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:58:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:58:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[23:58:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[23:58:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[23:58:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[23:58:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[23:59:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[23:59:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 4 candidates:
[1 : 4]:	0.8939  0.8183  0.6782  0.3210
[23:59:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 4 candidate(s) with evolutionary search
[23:59:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 4 candidates(s) for measurement
[23:59:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 4 sample(s) to builder
[23:59:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 4 sample(s) to runner
[23:59:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"
[23:59:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:59:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:00:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67fa8178)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c680018c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67cb8668)]: 0 failure(s)
[00:00:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:01:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67fa8178)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c680018c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67cb8668)]: 0 failure(s)
[00:02:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67fa8178)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c680018c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67cb8668)]: 0 failure(s)
[00:03:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67fa8178)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c680018c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67cb8668)]: 0 failure(s)
[00:04:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67fa8178)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c680018c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67cb8668)]: 0 failure(s)
[00:04:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9998  0.9997  0.9997  0.9996  0.9996  0.9994  0.9993  0.9992  0.9991  0.9991  0.9990  0.9990  0.9988  0.9988
[17 : 32]:	0.9987  0.9987  0.9984  0.9980  0.9979  0.9979  0.9978  0.9977  0.9977  0.9977  0.9975  0.9975  0.9971  0.9971  0.9970  0.9970
[00:04:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:04:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:04:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:05:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:05:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_max_pool2d"
[00:05:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:05:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:05:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f0c588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c680fa368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6c518)]: 0 failure(s)
[00:05:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:05:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f0c588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c680fa368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6c518)]: 0 failure(s)
[00:06:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f0c588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c680fa368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6c518)]: 0 failure(s)
[00:06:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f0c588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c680fa368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6c518)]: 0 failure(s)
[00:07:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f0c588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c680fa368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6c518)]: 0 failure(s)
[00:08:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9978  0.9752  0.9701  0.9631  0.9331  0.9059  0.8858  0.8625  0.8402  0.8377  0.8337  0.8268  0.8228  0.8170  0.7636  0.7559
[17 : 32]:	0.7508  0.7231  0.7207  0.7175  0.6843  0.6738  0.6667  0.6235  0.6219  0.5797  0.5740  0.5407  0.5376  0.5037  0.4903  0.4826
[00:08:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:08:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 31 candidates(s) for measurement
[00:08:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 31 sample(s) to builder
[00:08:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 31 sample(s) to runner
[00:09:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"
[00:09:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:09:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:09:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67fff588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c678ba1f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c681379c8)]: 0 failure(s)
[00:09:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:10:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67fff588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c678ba1f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c681379c8)]: 0 failure(s)
[00:11:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67fff588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c678ba1f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c681379c8)]: 0 failure(s)
[00:12:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67fff588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c678ba1f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c681379c8)]: 0 failure(s)
[00:13:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67fff588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c678ba1f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c681379c8)]: 0 failure(s)
[00:13:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9997  0.9992  0.9989  0.9988  0.9987  0.9987  0.9986  0.9986  0.9983  0.9982  0.9979  0.9978  0.9977  0.9977
[17 : 32]:	0.9977  0.9976  0.9974  0.9966  0.9965  0.9964  0.9963  0.9962  0.9956  0.9955  0.9955  0.9954  0.9952  0.9951  0.9950  0.9950
[00:13:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:13:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:13:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:14:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:14:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"
[00:14:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:14:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:15:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67c9a288)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f96728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e37fd8)]: 0 failure(s)
[00:15:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:16:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67c9a288)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f96728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e37fd8)]: 0 failure(s)
[00:17:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67c9a288)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f96728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e37fd8)]: 0 failure(s)
[00:18:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67c9a288)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f96728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e37fd8)]: 0 failure(s)
[00:19:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67c9a288)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f96728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e37fd8)]: 0 failure(s)
[00:19:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9999  0.9998  0.9998  0.9997  0.9997  0.9994  0.9992  0.9992  0.9991  0.9990  0.9990  0.9987  0.9987  0.9987
[17 : 32]:	0.9985  0.9985  0.9983  0.9980  0.9980  0.9979  0.9977  0.9974  0.9974  0.9973  0.9972  0.9971  0.9970  0.9969  0.9968  0.9967
[00:19:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:19:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:19:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:20:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:20:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"
[00:20:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:20:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:21:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d2a608)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c678bca68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c68166248)]: 0 failure(s)
[00:21:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:22:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d2a608)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c678bca68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c68166248)]: 0 failure(s)
[00:23:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d2a608)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c678bca68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c68166248)]: 0 failure(s)
[00:24:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d2a608)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c678bca68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c68166248)]: 0 failure(s)
[00:24:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d2a608)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c678bca68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c68166248)]: 0 failure(s)
[00:25:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9998  0.9996  0.9993  0.9993  0.9992  0.9992  0.9992  0.9991  0.9988  0.9988  0.9986  0.9986  0.9985  0.9985
[17 : 32]:	0.9984  0.9984  0.9982  0.9982  0.9980  0.9980  0.9980  0.9979  0.9978  0.9978  0.9978  0.9978  0.9977  0.9976  0.9974  0.9973
[00:25:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:25:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:25:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:25:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:25:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_concatenate"
[00:25:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:25:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:25:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[00:25:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:26:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[00:26:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[00:26:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[00:27:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[00:27:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 8 candidates:
[1 : 8]:	0.9987  0.9268  0.8275  0.8180  0.5620  0.5291  0.3878  0.3649
[00:27:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 8 candidate(s) with evolutionary search
[00:27:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 8 candidates(s) for measurement
[00:27:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 8 sample(s) to builder
[00:27:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 8 sample(s) to runner
[00:27:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #11: "fused_nn_max_pool2d_1"
[00:27:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:27:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:28:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6818cce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67fd4278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67d1de98)]: 0 failure(s)
[00:28:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:28:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6818cce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67fd4278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67d1de98)]: 0 failure(s)
[00:29:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6818cce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67fd4278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67d1de98)]: 0 failure(s)
[00:29:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6818cce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67fd4278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67d1de98)]: 0 failure(s)
[00:30:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6818cce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67fd4278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67d1de98)]: 0 failure(s)
[00:30:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9996  0.9749  0.9599  0.9490  0.9375  0.9305  0.9258  0.8870  0.8345  0.7993  0.7751  0.7653  0.7315  0.7198  0.7147  0.7135
[17 : 32]:	0.7043  0.7025  0.6854  0.6702  0.6536  0.6161  0.6123  0.5843  0.5690  0.5254  0.5239  0.5153  0.4617  0.4280  0.3819  0.3733
[00:30:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:30:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:30:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:31:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:31:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"
[00:31:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:31:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:32:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d2de18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77648)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67ca0b38)]: 0 failure(s)
[00:32:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:33:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d2de18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77648)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67ca0b38)]: 0 failure(s)
[00:34:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d2de18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77648)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67ca0b38)]: 0 failure(s)
[00:35:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d2de18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77648)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67ca0b38)]: 0 failure(s)
[00:36:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d2de18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77648)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67ca0b38)]: 0 failure(s)
[00:36:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9996  0.9996  0.9993  0.9991  0.9990  0.9990  0.9990  0.9989  0.9988  0.9986  0.9986  0.9985  0.9984  0.9984  0.9980
[17 : 32]:	0.9980  0.9979  0.9978  0.9978  0.9976  0.9974  0.9973  0.9972  0.9972  0.9972  0.9972  0.9971  0.9971  0.9970  0.9970  0.9968
[00:36:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:36:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:36:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:37:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:37:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"
[00:37:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:37:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:38:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cbb0a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67fe2938)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c68105d78)]: 0 failure(s)
[00:38:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:39:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cbb0a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67fe2938)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c68105d78)]: 0 failure(s)
[00:40:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cbb0a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67fe2938)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c68105d78)]: 0 failure(s)
[00:41:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cbb0a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67fe2938)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c68105d78)]: 0 failure(s)
[00:42:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cbb0a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67fe2938)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c68105d78)]: 0 failure(s)
[00:42:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9998  0.9997  0.9996  0.9995  0.9994  0.9993  0.9992  0.9992  0.9990  0.9988  0.9987  0.9987  0.9986  0.9986
[17 : 32]:	0.9982  0.9981  0.9979  0.9978  0.9975  0.9973  0.9970  0.9970  0.9968  0.9966  0.9965  0.9963  0.9962  0.9956  0.9955  0.9954
[00:42:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:42:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:42:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:43:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:43:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"
[00:43:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:43:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:44:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d4cc78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67fffb88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f8f578)]: 0 failure(s)
[00:44:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:45:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d4cc78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67fffb88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f8f578)]: 0 failure(s)
[00:46:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d4cc78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67fffb88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f8f578)]: 0 failure(s)
[00:47:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d4cc78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67fffb88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f8f578)]: 0 failure(s)
[00:48:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d4cc78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67fffb88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f8f578)]: 0 failure(s)
[00:48:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9998  0.9998  0.9998  0.9997  0.9996  0.9994  0.9993  0.9992  0.9992  0.9991  0.9991  0.9991  0.9989  0.9989
[17 : 32]:	0.9988  0.9987  0.9983  0.9983  0.9980  0.9979  0.9978  0.9976  0.9975  0.9974  0.9974  0.9973  0.9972  0.9970  0.9969  0.9969
[00:48:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:48:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:48:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:49:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:49:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_concatenate_1"
[00:49:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:49:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:49:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[00:49:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:50:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[00:50:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[00:50:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[00:50:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[00:51:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 8 candidates:
[1 : 8]:	0.8077  0.7516  0.7000  0.6033  0.3775  0.2894  0.1141  0.1112
[00:51:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 8 candidate(s) with evolutionary search
[00:51:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 8 candidates(s) for measurement
[00:51:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 8 sample(s) to builder
[00:51:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 8 sample(s) to runner
[00:51:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #16: "fused_nn_max_pool2d_2"
[00:51:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:51:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:51:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f7b4c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f678c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67445388)]: 0 failure(s)
[00:51:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:51:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f7b4c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f678c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67445388)]: 0 failure(s)
[00:52:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f7b4c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f678c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67445388)]: 0 failure(s)
[00:53:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f7b4c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f678c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67445388)]: 0 failure(s)
[00:53:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f7b4c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f678c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67445388)]: 0 failure(s)
[00:54:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9988  0.9897  0.9157  0.9142  0.8993  0.8823  0.8765  0.8606  0.8518  0.8513  0.8176  0.8134  0.7729  0.7728  0.7027  0.6662
[17 : 32]:	0.6431  0.6386  0.6214  0.6088  0.5949  0.5775  0.5756  0.5453  0.5274  0.5216  0.5151  0.4817  0.4730  0.4568  0.4317  0.4156
[00:54:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:54:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:54:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:54:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:56:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"
[00:56:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:56:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:57:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c681221f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c68130208)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c6816c648)]: 0 failure(s)
[00:57:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:58:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c681221f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c68130208)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c6816c648)]: 0 failure(s)
[00:58:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c681221f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c68130208)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c6816c648)]: 0 failure(s)
[00:59:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c681221f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c68130208)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c6816c648)]: 0 failure(s)
[01:00:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c681221f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c68130208)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c6816c648)]: 0 failure(s)
[01:00:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9999  0.9999  0.9998  0.9997  0.9996  0.9995  0.9995  0.9994  0.9993  0.9993  0.9992  0.9991  0.9991  0.9990
[17 : 32]:	0.9989  0.9988  0.9987  0.9985  0.9985  0.9985  0.9982  0.9980  0.9976  0.9976  0.9976  0.9976  0.9975  0.9975  0.9975  0.9975
[01:01:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:01:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:01:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:01:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:02:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"
[01:02:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:02:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:02:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c680eca08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67c874a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c6810e458)]: 0 failure(s)
[01:02:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:03:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c680eca08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67c874a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c6810e458)]: 0 failure(s)
[01:04:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c680eca08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67c874a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c6810e458)]: 0 failure(s)
[01:05:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c680eca08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67c874a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c6810e458)]: 0 failure(s)
[01:06:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c680eca08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67c874a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c6810e458)]: 0 failure(s)
[01:06:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9998  0.9996  0.9996  0.9996  0.9996  0.9993  0.9992  0.9992  0.9991  0.9991  0.9990  0.9990  0.9990  0.9989  0.9988
[17 : 32]:	0.9988  0.9986  0.9986  0.9985  0.9984  0.9980  0.9978  0.9974  0.9974  0.9974  0.9974  0.9973  0.9973  0.9970  0.9969  0.9967
[01:06:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:06:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:07:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:07:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"
[01:07:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:07:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:08:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67ce8958)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d01a28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67d02978)]: 0 failure(s)
[01:08:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:09:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67ce8958)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d01a28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67d02978)]: 0 failure(s)
[01:10:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67ce8958)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d01a28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67d02978)]: 0 failure(s)
[01:11:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67ce8958)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d01a28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67d02978)]: 0 failure(s)
[01:12:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67ce8958)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d01a28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67d02978)]: 0 failure(s)
[01:12:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9997  0.9996  0.9993  0.9990  0.9990  0.9990  0.9987  0.9985  0.9981  0.9980  0.9977  0.9976  0.9976  0.9973
[17 : 32]:	0.9973  0.9971  0.9967  0.9966  0.9965  0.9964  0.9963  0.9963  0.9962  0.9962  0.9961  0.9960  0.9960  0.9956  0.9956  0.9956
[01:12:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:12:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:12:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:13:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:13:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #20: "fused_concatenate_2"
[01:13:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:13:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:13:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[01:13:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:14:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[01:14:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[01:14:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[01:14:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[01:14:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 8 candidates:
[1 : 8]:	0.9571  0.9466  0.8973  0.7226  0.6177  0.5317  0.4406  0.2171
[01:14:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 8 candidate(s) with evolutionary search
[01:14:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 8 candidates(s) for measurement
[01:14:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 8 sample(s) to builder
[01:14:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 8 sample(s) to runner
[01:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"
[01:14:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:14:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:15:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c680c6b68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67fac928)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67d102e8)]: 0 failure(s)
[01:15:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:16:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c680c6b68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67fac928)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67d102e8)]: 0 failure(s)
[01:17:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c680c6b68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67fac928)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67d102e8)]: 0 failure(s)
[01:18:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c680c6b68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67fac928)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67d102e8)]: 0 failure(s)
[01:18:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c680c6b68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67fac928)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67d102e8)]: 0 failure(s)
[01:19:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9998  0.9995  0.9995  0.9995  0.9995  0.9992  0.9992  0.9991  0.9991  0.9987  0.9986  0.9984  0.9984  0.9983
[17 : 32]:	0.9983  0.9977  0.9976  0.9976  0.9976  0.9975  0.9972  0.9971  0.9967  0.9965  0.9963  0.9962  0.9961  0.9961  0.9961  0.9960
[01:19:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:19:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:19:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:20:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:20:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"
[01:20:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:20:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:20:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68137ea8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6815f678)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67ceab18)]: 0 failure(s)
[01:20:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:21:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68137ea8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6815f678)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67ceab18)]: 0 failure(s)
[01:22:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68137ea8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6815f678)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67ceab18)]: 0 failure(s)
[01:23:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68137ea8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6815f678)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67ceab18)]: 0 failure(s)
[01:24:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68137ea8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6815f678)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67ceab18)]: 0 failure(s)
[01:24:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9999  0.9998  0.9997  0.9996  0.9995  0.9995  0.9995  0.9993  0.9991  0.9991  0.9991  0.9990  0.9986  0.9986
[17 : 32]:	0.9985  0.9984  0.9983  0.9981  0.9981  0.9979  0.9979  0.9979  0.9977  0.9974  0.9974  0.9972  0.9972  0.9969  0.9967  0.9966
[01:24:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:24:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:24:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:25:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:25:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"
[01:25:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:25:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:26:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d53f08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cffc28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67c98798)]: 0 failure(s)
[01:26:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:27:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d53f08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cffc28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67c98798)]: 0 failure(s)
[01:28:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d53f08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cffc28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67c98798)]: 0 failure(s)
[01:28:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d53f08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cffc28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67c98798)]: 0 failure(s)
[01:29:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d53f08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cffc28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67c98798)]: 0 failure(s)
[01:30:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9998  0.9997  0.9996  0.9991  0.9991  0.9990  0.9989  0.9989  0.9987  0.9987  0.9986  0.9986  0.9985  0.9984
[17 : 32]:	0.9983  0.9982  0.9982  0.9981  0.9980  0.9979  0.9979  0.9978  0.9977  0.9976  0.9975  0.9974  0.9973  0.9972  0.9971  0.9967
[01:30:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:30:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:31:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:31:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:31:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #24: "fused_concatenate_3"
[01:31:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:31:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:32:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[01:32:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:32:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[01:32:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[01:32:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[01:32:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[01:33:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 8 candidates:
[1 : 8]:	0.9774  0.8853  0.8568  0.4186  0.3257  0.2072  0.1536  0.0890
[01:33:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 8 candidate(s) with evolutionary search
[01:33:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 8 candidates(s) for measurement
[01:33:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 8 sample(s) to builder
[01:33:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 8 sample(s) to runner
[01:34:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"
[01:34:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:34:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:34:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67ce0c38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c68104cd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67ce5b38)]: 0 failure(s)
[01:34:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:35:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67ce0c38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c68104cd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67ce5b38)]: 0 failure(s)
[01:36:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67ce0c38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c68104cd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67ce5b38)]: 0 failure(s)
[01:37:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67ce0c38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c68104cd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67ce5b38)]: 0 failure(s)
[01:37:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67ce0c38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c68104cd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67ce5b38)]: 0 failure(s)
[01:38:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9996  0.9994  0.9993  0.9991  0.9990  0.9988  0.9987  0.9987  0.9986  0.9984  0.9982  0.9981  0.9979  0.9977  0.9974
[17 : 32]:	0.9973  0.9972  0.9971  0.9970  0.9968  0.9963  0.9963  0.9962  0.9961  0.9960  0.9960  0.9960  0.9959  0.9957  0.9956  0.9954
[01:38:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:38:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:38:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:39:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:39:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #26: "fused_nn_avg_pool2d"
[01:39:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:39:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:39:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67ffb928)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c68176638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c681523f8)]: 0 failure(s)
[01:39:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:40:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67ffb928)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c68176638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c681523f8)]: 0 failure(s)
[01:41:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67ffb928)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c68176638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c681523f8)]: 0 failure(s)
[01:42:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67ffb928)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c68176638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c681523f8)]: 0 failure(s)
[01:43:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67ffb928)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c68176638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c681523f8)]: 0 failure(s)
[01:44:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9888  0.9824  0.9791  0.9700  0.9667  0.9630  0.9599  0.9527  0.9525  0.9420  0.9179  0.9149  0.9099  0.9072  0.9032  0.8958
[17 : 32]:	0.8948  0.8933  0.8910  0.8810  0.8793  0.8755  0.8706  0.8677  0.8573  0.8442  0.8380  0.8372  0.8320  0.8027  0.8020  0.8012
[01:44:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:44:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:44:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:44:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:45:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #27: "fused_layout_transform_reshape"
[01:45:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:45:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:45:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[01:45:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:46:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[01:46:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[01:46:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[01:47:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[01:47:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 12 candidates:
[1 : 12]:	0.9714  0.9612  0.9027  0.8927  0.8889  0.3918  0.3891  0.3046  0.1770  0.1017  0.0377  0.0220
[01:47:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 12 candidate(s) with evolutionary search
[01:47:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 12 candidates(s) for measurement
[01:47:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 12 sample(s) to builder
[01:47:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 12 sample(s) to runner
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #0: GFLOPs: 2.8461. Time: 17.5420 ms. Best GFLOPs: 2.8461
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #1: GFLOPs: 2.2204. Time: 22.4858 ms. Best GFLOPs: 2.8461
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 16, 15, 15, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(52, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 16, 3):
                for ax3_ax4_fused in T.vectorized(60):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 26 // 2 + ax2)
                        i3 = T.axis.spatial(15, ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 13, 2):
                for i1_2_init in T.serial(32):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 26 * 32 + i1_2_init)
                        oh = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 26 // 2)
                        ow = T.axis.spatial(13, i3_1)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 3, 3, 1, 32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 26 * 32 + i1_2)
                        oh = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 26 // 2)
                        ow = T.axis.spatial(13, i3_1)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_0])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(832, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(52):
                with T.block("T_relu"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(64, i0_i1_i2_fused // 13)
                    ax2 = T.axis.spatial(13, i0_i1_i2_fused % 13)
                    ax3 = T.axis.spatial(13, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 32, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[64, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74, l75, l76, l77, l78, l79 = sch.get_loops(block=b67)
l80 = sch.fuse(l70, l71, l72, l73, l74)
sch.parallel(loop=l80)
l81 = sch.fuse(l78, l79)
sch.vectorize(loop=l81)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b68)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l104, l105, l106, l107, l108 = sch.get_loops(block=b69)
l109 = sch.fuse(l104, l105, l106)
sch.parallel(loop=l109)
l110 = sch.fuse(l107, l108)
sch.vectorize(loop=l110)
sch.annotate(block_or_loop=l109, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l109, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b111)
b134 = sch.decompose_reduction(block=b111, loop=l118)
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #3: GFLOPs: 3.1295. Time: 15.9536 ms. Best GFLOPs: 3.1295
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #4: GFLOPs: 4.5266. Time: 11.0296 ms. Best GFLOPs: 4.5266
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #5: GFLOPs: 9.9363. Time: 5.0247 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #6: GFLOPs: 6.2408. Time: 8.0001 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #7: GFLOPs: 2.4309. Time: 20.5387 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #8: GFLOPs: 5.6699. Time: 8.8055 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #9: GFLOPs: 3.7249. Time: 13.4034 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #10: GFLOPs: 3.1786. Time: 15.7071 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #11: GFLOPs: 4.6494. Time: 10.7383 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #12: GFLOPs: 3.2546. Time: 15.3401 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #13: GFLOPs: 3.9526. Time: 12.6313 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 16, 15, 15, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(26, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 16, 3):
                for ax3_ax4_fused in T.vectorized(60):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 26 // 2 + ax2)
                        i3 = T.axis.spatial(15, ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i1_2_init, i3_2_init, i4_2_init, i1_3_init in T.grid(8, 13, 2, 8):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i1_2_init * 8 + i1_3_init)
                    oh = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2)
                    ow = T.axis.spatial(13, i3_2_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 3, 3, 1, 8, 1, 13, 2, 1, 1, 1, 1, 8, 1, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i1_2 * 8 + i1_3)
                    oh = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2)
                    ow = T.axis.spatial(13, i3_2)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2)
                    ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_0])
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 64, 1, 13):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(64, ax1)
                        ax2_1 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2)
                        ax3_1 = T.axis.spatial(13, ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 8, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[64, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b68)
l86 = sch.fuse(l71, l72, l73, l74, l75, l76, l77, l78, l79, l80)
sch.parallel(loop=l86)
l87 = sch.fuse(l84, l85)
sch.vectorize(loop=l87)
sch.annotate(block_or_loop=l86, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l86, ann_key="pragma_unroll_explicit", ann_val=1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b69)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l105, l106, l107, l108, l109, l110 = sch.get_loops(block=b70)
l111 = sch.fuse(l110)
sch.vectorize(loop=l111)
sch.annotate(block_or_loop=l105, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l105, ann_key="pragma_unroll_explicit", ann_val=1)
b112 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b112)
b130 = sch.decompose_reduction(block=b112, loop=l114)
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #15: GFLOPs: 2.8174. Time: 17.7210 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #16: GFLOPs: 2.8874. Time: 17.2914 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #17: GFLOPs: 4.6431. Time: 10.7528 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #18: GFLOPs: 3.1660. Time: 15.7698 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #19: GFLOPs: 1.8669. Time: 26.7427 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #20: GFLOPs: 3.1208. Time: 15.9981 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #21: GFLOPs: 6.8969. Time: 7.2390 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 16, 15, 15, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(104, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i5_0 in T.serial(1):
                for i1_2_init, i3_2_init, i1_3_init in T.grid(8, 13, 2):
                    for i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 16 + i1_2_init * 2 + i1_3_init)
                            oh = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8)
                            ow = T.axis.spatial(13, i3_2_init)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i6_0, i7_0 in T.grid(3, 3):
                    for ax0, ax1, ax2 in T.grid(1, 16, 1):
                        for ax3_ax4_fused in T.vectorized(52):
                            with T.block("data_pad"):
                                i0, i1 = T.axis.remap("SS", [ax0, ax1])
                                i2 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 104 // 8 + i6_0 + ax2)
                                i3 = T.axis.spatial(15, i7_0 + ax3_ax4_fused // 4)
                                i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                                T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                T.writes(data_pad[i0, i1, i2, i3, i4])
                                data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3 in T.grid(1, 8, 1, 13, 1, 64, 1, 1, 1, 2):
                        for i2_3_i3_3_i4_3_fused in T.vectorized(2):
                            with T.block("conv2d_NCHWc_update"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 16 + i1_2 * 2 + i1_3)
                                oh = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8)
                                ow = T.axis.spatial(13, i3_2)
                                oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i2_3_i3_3_i4_3_fused)
                                ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_0, i7_0])
                                T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 13):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 16 + ax1)
                        ax2_1 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8)
                        ax3_1 = T.axis.spatial(13, ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 8, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 64])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=12)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b68)
l89 = sch.fuse(l71, l72, l73, l74, l75, l76, l77, l78, l79, l80)
sch.parallel(loop=l89)
l90 = sch.fuse(l87, l88)
sch.vectorize(loop=l90)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b69)
l108 = sch.fuse(l105, l106, l107)
sch.vectorize(loop=l108)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b70)
l115 = sch.fuse(l114)
sch.vectorize(loop=l115)
sch.annotate(block_or_loop=l109, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l109, ann_key="pragma_unroll_explicit", ann_val=1)
b116 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b116)
b132 = sch.decompose_reduction(block=b116, loop=l119)
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #23: GFLOPs: 2.8133. Time: 17.7468 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #24: GFLOPs: 5.2840. Time: 9.4486 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 16, 15, 15, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 1, 2):
                for i1_2_init, i2_2_init, i1_3_init, i3_3_init in T.grid(4, 13, 2, 13):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 32 + i1_1 * 8 + i1_2_init * 2 + i1_3_init)
                            oh, ow = T.axis.remap("SS", [i2_2_init, i3_3_init])
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0 in T.grid(16, 3, 3):
                    for ax0, ax1, ax2 in T.grid(1, 1, 13):
                        for ax3_ax4_fused in T.vectorized(52):
                            with T.block("data_pad"):
                                i0 = T.axis.spatial(1, ax0)
                                i1 = T.axis.spatial(16, i5_0 + ax1)
                                i2 = T.axis.spatial(15, i6_0 + ax2)
                                i3 = T.axis.spatial(15, i7_0 + ax3_ax4_fused // 4)
                                i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                                T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                T.writes(data_pad[i0, i1, i2, i3, i4])
                                data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 4, 13, 1, 1, 4, 1, 1, 1, 2, 1, 13):
                        for i4_3_fused in T.vectorized(2):
                            with T.block("conv2d_NCHWc_update"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 32 + i1_1 * 8 + i1_2 * 2 + i1_3)
                                oh, ow = T.axis.remap("SS", [i2_2, i3_3])
                                oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3_fused)
                                ic = T.axis.reduce(64, i5_0 * 4 + i5_1)
                                kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                                T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 32, 13):
                for ax3_ax4_fused in T.vectorized(52):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 32 + ax1)
                        ax2_1 = T.axis.spatial(13, ax2)
                        ax3 = T.axis.spatial(13, ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 4, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=12)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b68)
l89 = sch.fuse(l71, l72, l73, l74, l75)
sch.parallel(loop=l89)
l90 = sch.fuse(l87, l88)
sch.vectorize(loop=l90)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b69)
l113 = sch.fuse(l112)
sch.vectorize(loop=l113)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b70)
l120 = sch.fuse(l118, l119)
sch.vectorize(loop=l120)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
b121 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b121)
b144 = sch.decompose_reduction(block=b121, loop=l128)
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #26: GFLOPs: 2.1888. Time: 22.8096 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #27: GFLOPs: 1.6107. Time: 30.9972 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #28: GFLOPs: 3.4678. Time: 14.3972 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #29: GFLOPs: 3.6715. Time: 13.5983 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #30: GFLOPs: 1.2559. Time: 39.7538 ms. Best GFLOPs: 9.9363
[01:48:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 16, 15, 15, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(240, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(60):
                with T.block("data_pad"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(16, i0_i1_i2_fused // 15)
                    i2 = T.axis.spatial(15, i0_i1_i2_fused % 15)
                    i3 = T.axis.spatial(15, i3_i4_fused // 4)
                    i4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                    T.writes(data_pad[i0, i1, i2, i3, i4])
                    data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(104, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 2):
                for i1_2_init, i2_2_init, i4_2_init, i1_3_init in T.grid(4, 13, 2, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 13 * 8 + i1_2_init * 2 + i1_3_init)
                        oh = T.axis.spatial(13, i2_2_init)
                        ow = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 13)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 1, 1, 4, 13, 1, 2, 16, 3, 3, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 13 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(13, i2_2)
                        ow = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 13)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(64, i5_0 * 16 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 8, 13):
                for ax3_ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 13 * 8 + ax1)
                        ax2_1 = T.axis.spatial(13, ax2)
                        ax3 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 13)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 4, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[4, 16])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75 = sch.get_loops(block=b68)
l76 = sch.fuse(l71, l72, l73)
sch.parallel(loop=l76)
l77 = sch.fuse(l74, l75)
sch.vectorize(loop=l77)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b69)
l104 = sch.fuse(l78, l79, l80, l81, l82)
sch.parallel(loop=l104)
sch.annotate(block_or_loop=l104, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l104, ann_key="pragma_unroll_explicit", ann_val=1)
l105, l106, l107, l108, l109, l110 = sch.get_loops(block=b70)
l111 = sch.fuse(l109, l110)
sch.vectorize(loop=l111)
sch.annotate(block_or_loop=l105, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l105, ann_key="pragma_unroll_explicit", ann_val=1)
b112 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b112)
b135 = sch.decompose_reduction(block=b112, loop=l119)
[01:49:02] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |            N/A |          N/A |                   N/A |      0 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |            N/A |          N/A |                   N/A |      0 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |            N/A |          N/A |                   N/A |      0 |            
  4 |                       fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |                            fused_concatenate |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                          fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 32
Total latency (us): 10049.3

[01:49:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #0: GFLOPs: 2.4226. Time: 11.5989 ms. Best GFLOPs: 2.4226
[01:49:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #1: GFLOPs: 1.6062. Time: 17.4946 ms. Best GFLOPs: 2.4226
[01:49:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #2: GFLOPs: 1.9673. Time: 14.2837 ms. Best GFLOPs: 2.4226
[01:49:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #3: GFLOPs: 2.0074. Time: 13.9978 ms. Best GFLOPs: 2.4226
[01:49:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #4: GFLOPs: 1.8218. Time: 15.4240 ms. Best GFLOPs: 2.4226
[01:49:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #5: GFLOPs: 1.3487. Time: 20.8351 ms. Best GFLOPs: 2.4226
[01:49:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 12, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(48, 12, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 12, 15, 15, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 48, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(208, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_0, i4_0 in T.grid(13, 2):
                for ax0, ax1, ax2 in T.grid(1, 12, 3):
                    for ax3_ax4_fused in T.vectorized(12):
                        with T.block("data_pad"):
                            i0, i1 = T.axis.remap("SS", [ax0, ax1])
                            i2 = T.axis.spatial(15, i0_0_i1_0_i2_0_fused % 13 + ax2)
                            i3 = T.axis.spatial(15, i3_0 + ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1):
                    for i1_2_init in T.serial(3):
                        for i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                            with T.block("conv2d_NCHWc_init"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(48, i0_0_i1_0_i2_0_fused // 13 * 3 + i1_2_init)
                                oh = T.axis.spatial(13, i0_0_i1_0_i2_0_fused % 13)
                                ow = T.axis.spatial(13, i3_0)
                                oc_block = T.axis.spatial(4, i4_0 * 2 + i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init)
                                T.reads()
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 12, 13, 13, 4], "float32"], ["TENSOR", [48, 12, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1 in T.grid(4, 1, 3, 1, 3, 1, 1, 1, 12, 3):
                        for i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused in T.vectorized(2):
                            with T.block("conv2d_NCHWc_update"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(48, i0_0_i1_0_i2_0_fused // 13 * 3 + i1_2)
                                oh = T.axis.spatial(13, i0_0_i1_0_i2_0_fused % 13)
                                ow = T.axis.spatial(13, i3_0)
                                oc_block = T.axis.spatial(4, i4_0 * 2 + i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused)
                                ic = T.axis.reduce(48, i5_0 * 12 + i5_1)
                                kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                                T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 12, 13, 13, 4], "float32"], ["TENSOR", [48, 12, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1 in T.grid(1, 3):
                    for ax2_ax3_ax4_fused in T.vectorized(2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(48, i0_0_i1_0_i2_0_fused // 13 * 3 + ax1)
                            ax2 = T.axis.spatial(13, i0_0_i1_0_i2_0_fused % 13)
                            ax3 = T.axis.spatial(13, i3_0)
                            ax4 = T.axis.spatial(4, i4_0 * 2 + ax2_ax3_ax4_fused)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2, ax3, ax4])
                            T_relu[ax0_1, ax1_1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[16, 1, 3, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[4, 12])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b68)
l81 = sch.fuse(l71, l72, l73)
sch.parallel(loop=l81)
l82 = sch.fuse(l79, l80)
sch.vectorize(loop=l82)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b69)
l107 = sch.fuse(l101, l102, l103, l104, l105, l106)
sch.vectorize(loop=l107)
sch.annotate(block_or_loop=l83, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l83, ann_key="pragma_unroll_explicit", ann_val=1)
l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b70)
l116 = sch.fuse(l113, l114, l115)
sch.vectorize(loop=l116)
sch.annotate(block_or_loop=l108, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l108, ann_key="pragma_unroll_explicit", ann_val=1)
b117 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136 = sch.get_loops(block=b117)
b137 = sch.decompose_reduction(block=b117, loop=l126)
[01:49:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #7: GFLOPs: 1.8919. Time: 14.8531 ms. Best GFLOPs: 2.4226
[01:49:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #8: GFLOPs: 3.1224. Time: 8.9994 ms. Best GFLOPs: 3.1224
[01:49:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #9: GFLOPs: 6.7028. Time: 4.1923 ms. Best GFLOPs: 6.7028
[01:49:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #10: GFLOPs: 0.9581. Time: 29.3287 ms. Best GFLOPs: 6.7028
[01:49:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #11: GFLOPs: 0.6800. Time: 41.3218 ms. Best GFLOPs: 6.7028
[01:49:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #12: GFLOPs: 1.2399. Time: 22.6636 ms. Best GFLOPs: 6.7028
[01:49:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #13: GFLOPs: 2.6028. Time: 10.7960 ms. Best GFLOPs: 6.7028
[01:49:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #14: GFLOPs: 23.3347. Time: 1.2042 ms. Best GFLOPs: 23.3347
[01:49:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 12, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(48, 12, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 12, 15, 15, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 48, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(13, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1 in T.grid(1, 6):
                for ax0, ax1, ax2 in T.grid(1, 12, 15):
                    for ax3_ax4_fused in T.vectorized(12):
                        with T.block("data_pad"):
                            i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                            i3 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 13 + ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1 in T.grid(1, 1, 1):
                    for i4_2_init, i1_3_init, i2_3_init in T.grid(4, 8, 13):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(48, i1_1 * 8 + i1_3_init)
                            oh, ow, oc_block = T.axis.remap("SSS", [i2_3_init, i0_0_i1_0_i2_0_i3_0_i4_0_fused, i4_2_init])
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 12, 13, 13, 4], "float32"], ["TENSOR", [48, 12, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 1, 1, 1, 4, 16, 3, 3, 1, 8, 13, 1, 1):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(48, i1_1 * 8 + i1_3)
                            oh, ow, oc_block = T.axis.remap("SSS", [i2_3, i0_0_i1_0_i2_0_i3_0_i4_0_fused, i4_2])
                            ic = T.axis.reduce(48, i5_0 * 16 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 12, 13, 13, 4], "float32"], ["TENSOR", [48, 12, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 48, 13):
                for ax3_ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1, ax3, ax4 = T.axis.remap("SSSS", [ax1, ax2, i0_0_i1_0_i2_0_i3_0_i4_0_fused, ax3_ax4_fused])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 6, 1, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 16])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b68)
l83 = sch.fuse(l71, l72, l73, l74, l75)
sch.parallel(loop=l83)
l84 = sch.fuse(l81, l82)
sch.vectorize(loop=l84)
sch.annotate(block_or_loop=l83, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l83, ann_key="pragma_unroll_explicit", ann_val=1)
l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b69)
sch.annotate(block_or_loop=l85, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l85, ann_key="pragma_unroll_explicit", ann_val=1)
l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b70)
l113 = sch.fuse(l111, l112)
sch.vectorize(loop=l113)
sch.annotate(block_or_loop=l107, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l107, ann_key="pragma_unroll_explicit", ann_val=1)
b114 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136 = sch.get_loops(block=b114)
b137 = sch.decompose_reduction(block=b114, loop=l121)
[01:49:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #16: GFLOPs: 0.6532. Time: 43.0181 ms. Best GFLOPs: 23.3347
[01:49:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #17: GFLOPs: 0.3387. Time: 82.9529 ms. Best GFLOPs: 23.3347
[01:49:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #18: GFLOPs: 0.4566. Time: 61.5426 ms. Best GFLOPs: 23.3347
[01:49:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #19: GFLOPs: 1.5649. Time: 17.9558 ms. Best GFLOPs: 23.3347
[01:49:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #20: GFLOPs: 0.6692. Time: 41.9926 ms. Best GFLOPs: 23.3347
[01:49:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #21: GFLOPs: 0.8109. Time: 34.6534 ms. Best GFLOPs: 23.3347
[01:49:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #22: GFLOPs: 0.9660. Time: 29.0895 ms. Best GFLOPs: 23.3347
[01:49:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #23: GFLOPs: 1.2943. Time: 21.7104 ms. Best GFLOPs: 23.3347
[01:49:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #24: GFLOPs: 1.9381. Time: 14.4988 ms. Best GFLOPs: 23.3347
[01:49:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #25: GFLOPs: 3.3481. Time: 8.3929 ms. Best GFLOPs: 23.3347
[01:49:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #26: GFLOPs: 0.7138. Time: 39.3682 ms. Best GFLOPs: 23.3347
[01:49:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #27: GFLOPs: 0.7527. Time: 37.3302 ms. Best GFLOPs: 23.3347
[01:49:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #28: GFLOPs: 0.4930. Time: 57.0021 ms. Best GFLOPs: 23.3347
[01:49:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 12, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(48, 12, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 12, 15, 15, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 48, 13, 13, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(180, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(60):
                with T.block("data_pad"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(12, i0_i1_i2_fused // 15)
                    i2 = T.axis.spatial(15, i0_i1_i2_fused % 15)
                    i3 = T.axis.spatial(15, i3_i4_fused // 4)
                    i4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                    T.writes(data_pad[i0, i1, i2, i3, i4])
                    data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(26, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i3_3_init in T.grid(48, 13):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(48, i1_2_init)
                        oh = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2)
                        ow = T.axis.spatial(13, i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 12, 13, 13, 4], "float32"], ["TENSOR", [48, 12, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(6, 1, 3, 1, 48, 1, 1, 1, 8, 3, 1, 1, 1, 1, 13):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(48, i1_2)
                        oh = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2)
                        ow = T.axis.spatial(13, i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused)
                        ic = T.axis.reduce(48, i5_0 * 8 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 12, 13, 13, 4], "float32"], ["TENSOR", [48, 12, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(624, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(52):
                with T.block("T_relu"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(48, i0_i1_i2_fused // 13)
                    ax2 = T.axis.spatial(13, i0_i1_i2_fused % 13)
                    ax3 = T.axis.spatial(13, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 48, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[6, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74 = sch.get_loops(block=b67)
l75 = sch.fuse(l70, l71, l72)
sch.parallel(loop=l75)
l76 = sch.fuse(l73, l74)
sch.vectorize(loop=l76)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b68)
l103 = sch.fuse(l77, l78, l79, l80, l81, l82, l83, l84, l85, l86)
sch.parallel(loop=l103)
l104 = sch.fuse(l102)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
l105, l106, l107, l108, l109 = sch.get_loops(block=b69)
l110 = sch.fuse(l105, l106, l107)
sch.parallel(loop=l110)
l111 = sch.fuse(l108, l109)
sch.vectorize(loop=l111)
sch.annotate(block_or_loop=l110, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l110, ann_key="pragma_unroll_explicit", ann_val=1)
b112 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b112)
b130 = sch.decompose_reduction(block=b112, loop=l114)
[01:49:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #30: GFLOPs: 0.9881. Time: 28.4390 ms. Best GFLOPs: 23.3347
[01:49:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #31: GFLOPs: 1.3341. Time: 21.0626 ms. Best GFLOPs: 23.3347
[01:49:28] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |            N/A |          N/A |                   N/A |      0 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |            N/A |          N/A |                   N/A |      0 |            
  4 |                       fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |                            fused_concatenate |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                          fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 64
Total latency (us): 12457.7

[01:49:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #0: GFLOPs: 3.3657. Time: 16.0246 ms. Best GFLOPs: 3.3657
[01:49:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #1: GFLOPs: 1.9417. Time: 27.7772 ms. Best GFLOPs: 3.3657
[01:49:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #2: GFLOPs: 2.8323. Time: 19.0427 ms. Best GFLOPs: 3.3657
[01:49:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #3: GFLOPs: 4.1702. Time: 12.9333 ms. Best GFLOPs: 4.1702
[01:49:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #4: GFLOPs: 0.9632. Time: 55.9971 ms. Best GFLOPs: 4.1702
[01:49:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(32, 8, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 8, 29, 29, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 27, 27, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(144, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_1, i3_1, i4_1 in T.grid(3, 1, 1):
                for i5_0 in T.serial(1):
                    for i1_2_init, i2_2_init, i3_2_init, i1_3_init in T.grid(2, 9, 3, 2):
                        for i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                            with T.block("conv2d_NCHWc_init"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 8 * 4 + i1_2_init * 2 + i1_3_init)
                                oh = T.axis.spatial(27, i2_1 * 9 + i2_2_init)
                                ow = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 16 * 3 + i3_2_init)
                                oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 16 // 8 * 2 + i2_3_i3_3_i4_3_fused_init)
                                T.reads()
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 27, 27, 4], "float32"], ["TENSOR", [32, 8, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i6_0, i7_0 in T.grid(3, 3):
                        for ax0, ax1, ax2 in T.grid(1, 8, 9):
                            for ax3_ax4_fused in T.vectorized(12):
                                with T.block("data_pad"):
                                    i0, i1 = T.axis.remap("SS", [ax0, ax1])
                                    i2 = T.axis.spatial(29, i2_1 * 9 + i6_0 + ax2)
                                    i3 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 144 // 16 * 3 + i7_0 + ax3_ax4_fused // 4)
                                    i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                                    T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                    T.writes(data_pad[i0, i1, i2, i3, i4])
                                    data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 28 and 1 <= i3 and i3 < 28, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                        for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3 in T.grid(1, 2, 9, 3, 1, 32, 1, 1, 1, 2):
                            for i2_3_i3_3_i4_3_fused in T.vectorized(2):
                                with T.block("conv2d_NCHWc_update"):
                                    n = T.axis.spatial(1, 0)
                                    oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 8 * 4 + i1_2 * 2 + i1_3)
                                    oh = T.axis.spatial(27, i2_1 * 9 + i2_2)
                                    ow = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 16 * 3 + i3_2)
                                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 16 // 8 * 2 + i2_3_i3_3_i4_3_fused)
                                    ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_0, i7_0])
                                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 27, 27, 4], "float32"], ["TENSOR", [32, 8, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 9, 3):
                    for ax4_fused in T.vectorized(2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 8 * 4 + ax1)
                            ax2_1 = T.axis.spatial(27, i2_1 * 9 + ax2)
                            ax3_1 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 16 * 3 + ax3)
                            ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 16 // 8 * 2 + ax4_fused)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 8, 2, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 3, 9, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[9, 1, 3, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 32])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=12)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b68)
l89 = sch.fuse(l71, l72, l73, l74, l75, l76, l77)
sch.parallel(loop=l89)
l90 = sch.fuse(l87, l88)
sch.vectorize(loop=l90)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110 = sch.get_loops(block=b69)
l111 = sch.fuse(l108, l109, l110)
sch.vectorize(loop=l111)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b70)
l121 = sch.fuse(l120)
sch.vectorize(loop=l121)
sch.annotate(block_or_loop=l112, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l112, ann_key="pragma_unroll_explicit", ann_val=1)
b122 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b122)
b141 = sch.decompose_reduction(block=b122, loop=l128)
[01:49:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(32, 8, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 8, 29, 29, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 27, 27, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(48, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 8, 29):
                for ax3_ax4_fused in T.vectorized(44):
                    with T.block("data_pad"):
                        i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        i3 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 12 // 4 * 9 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 28 and 1 <= i3 and i3 < 28, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i1_2_init, i2_2_init, i3_2_init, i2_3_init, i3_3_init in T.grid(4, 3, 3, 9, 3):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 12 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 4 + i1_2_init)
                        oh = T.axis.spatial(27, i2_2_init * 9 + i2_3_init)
                        ow = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 12 // 4 * 9 + i3_2_init * 3 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 27, 27, 4], "float32"], ["TENSOR", [32, 8, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(2, 1, 3, 1, 4, 3, 3, 1, 16, 3, 1, 1, 1, 9, 3):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 12 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 4 + i1_2)
                        oh = T.axis.spatial(27, i2_2 * 9 + i2_3)
                        ow = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 12 // 4 * 9 + i3_2 * 3 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused)
                        ic = T.axis.reduce(32, i5_0 * 16 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 27, 27, 4], "float32"], ["TENSOR", [32, 8, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 27, 9):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 12 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 4 + ax1)
                        ax2_1 = T.axis.spatial(27, ax2)
                        ax3_1 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 12 // 4 * 9 + ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 2, 4, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 3, 9])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[3, 1, 3, 3])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[2, 16])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b68)
l86 = sch.fuse(l71, l72, l73, l74, l75, l76, l77, l78, l79, l80)
sch.parallel(loop=l86)
l87 = sch.fuse(l84, l85)
sch.vectorize(loop=l87)
sch.annotate(block_or_loop=l86, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l86, ann_key="pragma_unroll_explicit", ann_val=1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b69)
l105 = sch.fuse(l104)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b70)
l112 = sch.fuse(l111)
sch.vectorize(loop=l112)
sch.annotate(block_or_loop=l106, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l106, ann_key="pragma_unroll_explicit", ann_val=1)
b113 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b113)
b131 = sch.decompose_reduction(block=b113, loop=l115)
[01:49:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #7: GFLOPs: 3.2208. Time: 16.7459 ms. Best GFLOPs: 4.1702
[01:49:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #8: GFLOPs: 1.3485. Time: 39.9961 ms. Best GFLOPs: 4.1702
[01:49:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #9: GFLOPs: 1.1902. Time: 45.3158 ms. Best GFLOPs: 4.1702
[01:49:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(32, 8, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 8, 29, 29, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 27, 27, 4], dtype="float32")
        for i0_0_i1_0_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 29, 29):
                for ax4_fused in T.vectorized(4):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2, i3, i4 = T.axis.remap("SSSS", [ax1, ax2, ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 28 and 1 <= i3 and i3 < 28, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(3, 1, 2, 1, 2, 1, 1, 2):
                for i1_2_init, i2_2_init, i2_3_init, i3_3_init in T.grid(2, 3, 3, 27):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_fused * 4 + i1_1 * 2 + i1_2_init)
                        oh = T.axis.spatial(27, i2_0 * 9 + i2_2_init * 3 + i2_3_init)
                        ow = T.axis.spatial(27, i3_3_init)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 27, 27, 4], "float32"], ["TENSOR", [32, 8, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 2, 3, 1, 1, 16, 3, 3, 1, 1, 3, 27, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_fused * 4 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(27, i2_0 * 9 + i2_2 * 3 + i2_3)
                        ow = T.axis.spatial(27, i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(32, i5_0 * 16 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 27, 27, 4], "float32"], ["TENSOR", [32, 8, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(864, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(27):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(32, i0_i1_i2_fused // 27)
                        ax2 = T.axis.spatial(27, i0_i1_i2_fused % 27)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 2, 2, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[3, 1, 3, 3])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 27])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[2, 16])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l70, l71)
sch.parallel(loop=l77)
l78 = sch.fuse(l76)
sch.vectorize(loop=l78)
sch.annotate(block_or_loop=l77, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l77, ann_key="pragma_unroll_explicit", ann_val=1)
l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b68)
sch.annotate(block_or_loop=l79, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l79, ann_key="pragma_unroll_explicit", ann_val=1)
l104, l105, l106, l107, l108 = sch.get_loops(block=b69)
l109 = sch.fuse(l104, l105, l106)
sch.parallel(loop=l109)
l110 = sch.fuse(l108)
sch.vectorize(loop=l110)
sch.annotate(block_or_loop=l109, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l109, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136 = sch.get_loops(block=b111)
b137 = sch.decompose_reduction(block=b111, loop=l121)
[01:49:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #11: GFLOPs: 0.9919. Time: 54.3738 ms. Best GFLOPs: 4.1702
[01:49:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #12: GFLOPs: 0.4759. Time: 113.3241 ms. Best GFLOPs: 4.1702
[01:49:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #13: GFLOPs: 1.7737. Time: 30.4080 ms. Best GFLOPs: 4.1702
[01:49:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #14: GFLOPs: 3.2891. Time: 16.3979 ms. Best GFLOPs: 4.1702
[01:49:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #15: GFLOPs: 2.0837. Time: 25.8835 ms. Best GFLOPs: 4.1702
[01:49:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #16: GFLOPs: 1.0007. Time: 53.8981 ms. Best GFLOPs: 4.1702
[01:49:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #17: GFLOPs: 1.1728. Time: 45.9878 ms. Best GFLOPs: 4.1702
[01:49:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(32, 8, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 8, 29, 29, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 27, 27, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(36, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 3, 9, 1):
                for i1_2_init, i2_2_init in T.grid(8, 3):
                    for i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init in T.vectorized(4):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 9 * 8 + i1_2_init)
                            oh = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 9 // 3 * 9 + i2_1 * 3 + i2_2_init)
                            ow = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 3 * 9 + i3_1)
                            oc_block = T.axis.spatial(4, i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 27, 27, 4], "float32"], ["TENSOR", [32, 8, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0 in T.serial(16):
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 5, 3):
                        for ax4_fused in T.vectorized(2):
                            with T.block("data_pad"):
                                i0 = T.axis.spatial(1, ax0)
                                i1 = T.axis.spatial(8, i5_0 // 2 + ax1)
                                i2 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 9 // 3 * 9 + i2_1 * 3 + ax2)
                                i3 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 3 * 9 + i3_1 + ax3)
                                i4 = T.axis.spatial(4, i5_0 % 2 * 2 + ax4_fused)
                                T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                T.writes(data_pad[i0, i1, i2, i3, i4])
                                data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 28 and 1 <= i3 and i3 < 28, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1 in T.grid(1, 3, 1, 8, 3, 1, 1, 2, 3):
                        for i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused in T.vectorized(4):
                            with T.block("conv2d_NCHWc_update"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 9 * 8 + i1_2)
                                oh = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 9 // 3 * 9 + i2_1 * 3 + i2_2)
                                ow = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 3 * 9 + i3_1)
                                oc_block = T.axis.spatial(4, i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused)
                                ic = T.axis.reduce(32, i5_0 * 2 + i5_1)
                                kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                                T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 27, 27, 4], "float32"], ["TENSOR", [32, 8, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 8, 9):
                for ax3_ax4_fused in T.vectorized(36):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 9 * 8 + ax1)
                        ax2_1 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 9 // 3 * 9 + ax2)
                        ax3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 3 * 9 + ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 8, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[3, 3, 3, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[3, 9, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 2])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b68)
l87 = sch.fuse(l71, l72, l73, l74, l75)
sch.parallel(loop=l87)
l88 = sch.fuse(l86)
sch.vectorize(loop=l88)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110 = sch.get_loops(block=b69)
l111 = sch.fuse(l105, l106, l107, l108, l109, l110)
sch.vectorize(loop=l111)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b70)
l118 = sch.fuse(l116, l117)
sch.vectorize(loop=l118)
sch.annotate(block_or_loop=l112, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l112, ann_key="pragma_unroll_explicit", ann_val=1)
b119 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136 = sch.get_loops(block=b119)
b137 = sch.decompose_reduction(block=b119, loop=l126)
[01:49:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #19: GFLOPs: 1.5413. Time: 34.9938 ms. Best GFLOPs: 4.1702
[01:49:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #20: GFLOPs: 2.4519. Time: 21.9970 ms. Best GFLOPs: 4.1702
[01:49:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #21: GFLOPs: 0.9550. Time: 56.4783 ms. Best GFLOPs: 4.1702
[01:49:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #22: GFLOPs: 1.9734. Time: 27.3301 ms. Best GFLOPs: 4.1702
[01:49:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #23: GFLOPs: 2.9896. Time: 18.0405 ms. Best GFLOPs: 4.1702
[01:49:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #24: GFLOPs: 3.7360. Time: 14.4363 ms. Best GFLOPs: 4.1702
[01:49:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #25: GFLOPs: 3.5491. Time: 15.1966 ms. Best GFLOPs: 4.1702
[01:49:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #26: GFLOPs: 5.1246. Time: 10.5245 ms. Best GFLOPs: 5.1246
[01:49:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #27: GFLOPs: 1.9038. Time: 28.3299 ms. Best GFLOPs: 5.1246
[01:49:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #28: GFLOPs: 4.0447. Time: 13.3344 ms. Best GFLOPs: 5.1246
[01:49:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #29: GFLOPs: 1.3520. Time: 39.8916 ms. Best GFLOPs: 5.1246
[01:49:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #30: GFLOPs: 2.2955. Time: 23.4956 ms. Best GFLOPs: 5.1246
[01:49:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #31: GFLOPs: 3.0941. Time: 17.4314 ms. Best GFLOPs: 5.1246
[01:49:56] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |            N/A |          N/A |                   N/A |      0 |            
  4 |                       fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |                            fused_concatenate |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                          fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 96
Total latency (us): 33506.8

[01:50:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #0: GFLOPs: 0.9155. Time: 61.3282 ms. Best GFLOPs: 0.9155
[01:50:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #1: GFLOPs: 2.5076. Time: 22.3898 ms. Best GFLOPs: 2.5076
[01:50:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 4, 57, 57, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(228, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(57):
                for i4_fused in T.vectorized(4):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(4, i0_i1_i2_fused // 57)
                        i2 = T.axis.spatial(57, i0_i1_i2_fused % 57)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3_1 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3_1, i4])
                        data_pad[i0, i1, i2, i3_1, i4] = T.if_then_else(1 <= i2 and i2 < 56 and 1 <= i3_1 and i3_1 < 56, placeholder[i0, i1, i2 - 1, i3_1 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(88, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(4, 2, 5, 55):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 11 * 2 + i1_3_init)
                    oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 11 * 5 + i2_3_init)
                    ow, oc_block = T.axis.remap("SS", [i3_3_init, i4_2_init])
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 3, 1, 1, 1, 1, 4, 8, 3, 1, 1, 2, 5, 55, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 11 * 2 + i1_3)
                    oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 11 * 5 + i2_3)
                    ow, oc_block = T.axis.remap("SS", [i3_3, i4_2])
                    ic = T.axis.reduce(16, i5_0 * 8 + i5_1)
                    kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(880, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(55):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 55)
                        ax2 = T.axis.spatial(55, i0_i1_i2_fused % 55)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 1, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[11, 1, 1, 5])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 55])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[2, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74 = sch.get_loops(block=b67)
l75 = sch.fuse(l70, l71, l72)
sch.parallel(loop=l75)
l76 = sch.fuse(l74)
sch.vectorize(loop=l76)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b68)
l103 = sch.fuse(l77, l78, l79, l80, l81, l82, l83, l84, l85, l86)
sch.parallel(loop=l103)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
l104, l105, l106, l107, l108 = sch.get_loops(block=b69)
l109 = sch.fuse(l104, l105, l106)
sch.parallel(loop=l109)
l110 = sch.fuse(l108)
sch.vectorize(loop=l110)
sch.annotate(block_or_loop=l109, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l109, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
b129 = sch.decompose_reduction(block=b111, loop=l113)
[01:50:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #3: GFLOPs: 1.5043. Time: 37.3219 ms. Best GFLOPs: 2.5076
[01:50:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 4, 57, 57, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(228, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(57):
                for i4_fused in T.vectorized(4):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(4, i0_i1_i2_fused // 57)
                        i2 = T.axis.spatial(57, i0_i1_i2_fused % 57)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3_1 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3_1, i4])
                        data_pad[i0, i1, i2, i3_1, i4] = T.if_then_else(1 <= i2 and i2 < 56 and 1 <= i3_1 and i3_1 < 56, placeholder[i0, i1, i2 - 1, i3_1 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_2_init, i3_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(5, 5, 4, 11, 11):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_fused // 2 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_fused % 2)
                    oh = T.axis.spatial(55, i2_2_init * 11 + i2_3_init)
                    ow = T.axis.spatial(55, i3_2_init * 11 + i3_3_init)
                    oc_block = T.axis.spatial(4, i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 5, 5, 4, 16, 1, 3, 1, 1, 11, 11, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_fused // 2 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_fused % 2)
                    oh = T.axis.spatial(55, i2_2 * 11 + i2_3)
                    ow = T.axis.spatial(55, i3_2 * 11 + i3_3)
                    oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_2, i5_1, i6_0, i7_1])
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(880, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(55):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 55)
                        ax2 = T.axis.spatial(55, i0_i1_i2_fused % 55)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 2, 1, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 5, 11])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 5, 11])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 16])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74 = sch.get_loops(block=b67)
l75 = sch.fuse(l70, l71, l72)
sch.parallel(loop=l75)
l76 = sch.fuse(l74)
sch.vectorize(loop=l76)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b68)
l103 = sch.fuse(l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87)
sch.parallel(loop=l103)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
l104, l105, l106, l107, l108 = sch.get_loops(block=b69)
l109 = sch.fuse(l104, l105, l106)
sch.parallel(loop=l109)
l110 = sch.fuse(l108)
sch.vectorize(loop=l110)
sch.annotate(block_or_loop=l109, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l109, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b111)
b128 = sch.decompose_reduction(block=b111, loop=l113)
[01:50:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 4, 57, 57, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(400, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 4, 13):
                for ax3_ax4_fused in T.vectorized(52):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 5 * 11 + ax2)
                        i3 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 100 // 20 * 11 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 56 and 1 <= i3 and i3 < 56, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i3_1, i4_1 in T.grid(1, 1):
                for i1_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(2, 2, 11, 11):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 100 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 10 // 5 * 2 + i1_2_init)
                        oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 5 * 11 + i2_3_init)
                        ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 100 // 20 * 11 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 20 // 10 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 3, 3, 1, 2, 1, 1, 2, 4, 1, 1, 1, 1, 11, 11, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 100 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 10 // 5 * 2 + i1_2)
                        oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 5 * 11 + i2_3)
                        ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 100 // 20 * 11 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 20 // 10 * 2 + i4_2)
                        ic = T.axis.reduce(16, i5_0 * 4 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(880, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(55):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 55)
                        ax2 = T.axis.spatial(55, i0_i1_i2_fused % 55)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 2, 2, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 5, 1, 11])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[5, 1, 1, 11])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[4, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b67)
l83 = sch.fuse(l70, l71, l72, l73, l74, l75, l76, l77)
sch.parallel(loop=l83)
l84 = sch.fuse(l81, l82)
sch.vectorize(loop=l84)
sch.annotate(block_or_loop=l83, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l83, ann_key="pragma_unroll_explicit", ann_val=1)
l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b68)
sch.annotate(block_or_loop=l85, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l85, ann_key="pragma_unroll_explicit", ann_val=1)
l104, l105, l106, l107, l108 = sch.get_loops(block=b69)
l109 = sch.fuse(l104, l105, l106)
sch.parallel(loop=l109)
l110 = sch.fuse(l108)
sch.vectorize(loop=l110)
sch.annotate(block_or_loop=l109, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l109, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b111)
b131 = sch.decompose_reduction(block=b111, loop=l115)
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #6: GFLOPs: 2.9769. Time: 18.8601 ms. Best GFLOPs: 2.9769
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #7: GFLOPs: 2.5524. Time: 21.9966 ms. Best GFLOPs: 2.9769
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(400, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_1, i3_1, i4_1 in T.grid(11, 1, 2):
                for i1_2_init, i3_2_init in T.grid(2, 11):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 8 * 2 + i1_2_init)
                        oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 80 * 11 + i2_1)
                        ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 80 // 16 * 11 + i3_2_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 16 // 8 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 3, 1, 2, 1, 11, 1, 8, 3, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 8 * 2 + i1_2)
                        oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 80 * 11 + i2_1)
                        ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 80 // 16 * 11 + i3_2)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 16 // 8 * 2 + i4_1)
                        ic = T.axis.reduce(16, i5_0 * 8 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh + kh and oh + kh < 56 and 1 <= ow + kw and ow + kw < 56, placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(880, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(55):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 55)
                        ax2 = T.axis.spatial(55, i0_i1_i2_fused % 55)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 8, 2, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[5, 11, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[5, 1, 11, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[2, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68 = sch.get_child_blocks(b66)
l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94 = sch.get_loops(block=b67)
l95 = sch.fuse(l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l95)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100 = sch.get_loops(block=b68)
l101 = sch.fuse(l96, l97, l98)
sch.parallel(loop=l101)
l102 = sch.fuse(l100)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l101, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l101, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b103)
b124 = sch.decompose_reduction(block=b103, loop=l108)
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #9: GFLOPs: 3.7799. Time: 14.8531 ms. Best GFLOPs: 3.7799
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 4, 57, 57, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(25, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 11, 1, 1):
                for i1_2_init, i3_2_init, i1_3_init in T.grid(2, 11, 8):
                    for i2_3_i3_3_i4_3_fused_init in T.vectorized(4):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_2_init * 8 + i1_3_init)
                            oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 5 * 11 + i2_1)
                            ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 5 * 11 + i3_2_init)
                            oc_block = T.axis.spatial(4, i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0 in T.serial(2):
                    for ax0, ax1, ax2 in T.grid(1, 2, 3):
                        for ax3_ax4_fused in T.vectorized(52):
                            with T.block("data_pad"):
                                i0 = T.axis.spatial(1, ax0)
                                i1 = T.axis.spatial(4, i5_0 * 2 + ax1)
                                i2 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 25 // 5 * 11 + i2_1 + ax2)
                                i3 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 5 * 11 + ax3_ax4_fused // 4)
                                i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                                T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                T.writes(data_pad[i0, i1, i2, i3, i4])
                                data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 56 and 1 <= i3 and i3 < 56, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3 in T.grid(3, 3, 1, 2, 1, 11, 1, 8, 1, 1, 1, 8):
                        for i2_3_i3_3_i4_3_fused in T.vectorized(4):
                            with T.block("conv2d_NCHWc_update"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(16, i1_2 * 8 + i1_3)
                                oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 5 * 11 + i2_1)
                                ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 5 * 11 + i3_2)
                                oc_block = T.axis.spatial(4, i2_3_i3_3_i4_3_fused)
                                ic = T.axis.reduce(16, i5_0 * 8 + i5_1)
                                kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                                T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 16, 11):
                for ax3_ax4_fused in T.vectorized(44):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(16, ax1)
                        ax2_1 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 5 * 11 + ax2)
                        ax3 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 5 * 11 + ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[5, 11, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[5, 1, 11, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[2, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b68)
l87 = sch.fuse(l71, l72, l73, l74, l75)
sch.parallel(loop=l87)
l88 = sch.fuse(l85, l86)
sch.vectorize(loop=l88)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110 = sch.get_loops(block=b69)
l111 = sch.fuse(l108, l109, l110)
sch.vectorize(loop=l111)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b70)
l118 = sch.fuse(l116, l117)
sch.vectorize(loop=l118)
sch.annotate(block_or_loop=l112, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l112, ann_key="pragma_unroll_explicit", ann_val=1)
b119 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b119)
b140 = sch.decompose_reduction(block=b119, loop=l126)
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #11: GFLOPs: 1.3163. Time: 42.6533 ms. Best GFLOPs: 3.7799
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #12: GFLOPs: 2.7236. Time: 20.6138 ms. Best GFLOPs: 3.7799
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #13: GFLOPs: 4.4666. Time: 12.5697 ms. Best GFLOPs: 4.4666
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #14: GFLOPs: 2.0638. Time: 27.2043 ms. Best GFLOPs: 4.4666
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #15: GFLOPs: 2.4064. Time: 23.3307 ms. Best GFLOPs: 4.4666
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #16: GFLOPs: 2.4774. Time: 22.6626 ms. Best GFLOPs: 4.4666
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 4, 57, 57, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(176, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 57):
                for ax4_fused in T.vectorized(4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 22 // 2 * 5 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 56 and 1 <= i3 and i3 < 56, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_1, i3_1, i4_1 in T.grid(1, 1, 2):
                for i2_2_init, i3_3_init in T.grid(5, 55):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 22 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 2)
                            oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 22 // 2 * 5 + i2_2_init)
                            ow = T.axis.spatial(55, i3_3_init)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(4, 1, 1, 1, 1, 5, 1, 1, 4, 3, 3, 1, 1, 1, 55):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 22 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 2)
                            oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 22 // 2 * 5 + i2_2)
                            ow = T.axis.spatial(55, i3_3)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3_fused)
                            ic = T.axis.reduce(16, i5_0 * 4 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(880, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(55):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 55)
                        ax2 = T.axis.spatial(55, i0_i1_i2_fused % 55)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 2, 1, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[11, 1, 5, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 55])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[4, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b67)
l82 = sch.fuse(l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l82)
l83 = sch.fuse(l81)
sch.vectorize(loop=l83)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b68)
l104 = sch.fuse(l103)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l84, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l84, ann_key="pragma_unroll_explicit", ann_val=1)
l105, l106, l107, l108, l109 = sch.get_loops(block=b69)
l110 = sch.fuse(l105, l106, l107)
sch.parallel(loop=l110)
l111 = sch.fuse(l109)
sch.vectorize(loop=l111)
sch.annotate(block_or_loop=l110, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l110, ann_key="pragma_unroll_explicit", ann_val=1)
b112 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b112)
b133 = sch.decompose_reduction(block=b112, loop=l117)
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #18: GFLOPs: 1.3959. Time: 40.2193 ms. Best GFLOPs: 4.4666
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #19: GFLOPs: 3.9667. Time: 14.1537 ms. Best GFLOPs: 4.4666
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #20: GFLOPs: 2.4372. Time: 23.0362 ms. Best GFLOPs: 4.4666
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #21: GFLOPs: 2.0055. Time: 27.9955 ms. Best GFLOPs: 4.4666
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #22: GFLOPs: 2.0054. Time: 27.9963 ms. Best GFLOPs: 4.4666
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #23: GFLOPs: 2.4206. Time: 23.1947 ms. Best GFLOPs: 4.4666
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #24: GFLOPs: 2.9619. Time: 18.9553 ms. Best GFLOPs: 4.4666
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 4, 57, 57, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
        for i0_0_i1_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 57, 57):
                for ax4_fused in T.vectorized(4):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2, i3, i4 = T.axis.remap("SSSS", [ax1, ax2, ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 56 and 1 <= i3 and i3 < 56, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_0, i3_0, i4_0 in T.grid(11, 1, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 5, 5, 1):
                    for i1_2_init, i3_2_init in T.grid(2, 11):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_fused * 4 + i1_1 * 2 + i1_2_init)
                            oh = T.axis.spatial(55, i2_0 * 5 + i2_1)
                            ow = T.axis.spatial(55, i3_1 * 11 + i3_2_init)
                            oc_block = T.axis.spatial(4, i4_0)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 3, 3, 1, 2, 1, 11, 1, 8, 1, 1, 1, 1, 1, 1, 1):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_fused * 4 + i1_1 * 2 + i1_2)
                            oh = T.axis.spatial(55, i2_0 * 5 + i2_1)
                            ow = T.axis.spatial(55, i3_1 * 11 + i3_2)
                            oc_block = T.axis.spatial(4, i4_0)
                            ic = T.axis.reduce(16, i5_0 * 8 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 5, 55, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(16, i0_0_i1_0_fused * 4 + ax1)
                        ax2_1 = T.axis.spatial(55, i2_0 * 5 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, i4_0])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 2, 2, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[11, 5, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 5, 11, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[2, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77 = sch.get_loops(block=b68)
l78 = sch.fuse(l71, l72)
sch.parallel(loop=l78)
l79 = sch.fuse(l77)
sch.vectorize(loop=l79)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b69)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b70)
sch.annotate(block_or_loop=l105, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l105, ann_key="pragma_unroll_explicit", ann_val=1)
b114 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b114)
b140 = sch.decompose_reduction(block=b114, loop=l124)
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #26: GFLOPs: 2.3755. Time: 23.6343 ms. Best GFLOPs: 4.4666
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #27: GFLOPs: 3.0082. Time: 18.6636 ms. Best GFLOPs: 4.4666
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #28: GFLOPs: 1.7346. Time: 32.3667 ms. Best GFLOPs: 4.4666
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #29: GFLOPs: 4.5669. Time: 12.2938 ms. Best GFLOPs: 4.5669
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #30: GFLOPs: 1.2034. Time: 46.6563 ms. Best GFLOPs: 4.5669
[01:50:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #31: GFLOPs: 1.7867. Time: 31.4236 ms. Best GFLOPs: 4.5669
[01:50:26] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |                            fused_concatenate |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                          fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 128
Total latency (us): 58094.4

[01:50:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_layout_transform"] Trial #0: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], T_layout_trans: T.Buffer[(1, 1, 224, 224, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        for i0_i1_i2_fused in T.parallel(224):
            for i3, i4 in T.grid(224, 3):
                with T.block("T_layout_trans"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(1, 0)
                    ax2, ax3, ax4 = T.axis.remap("SSS", [i0_i1_i2_fused, i3, i4])
                    T.reads(placeholder[ax0, ax1 * 3 + ax4, ax2, ax3])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 224 and ax3 < 224, placeholder[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
sch.enter_postproc()
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit")
b3, = sch.get_child_blocks(b2)
l4, l5, l6, l7, l8 = sch.get_loops(block=b3)
l9 = sch.fuse(l4, l5, l6)
sch.parallel(loop=l9)
[01:50:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform"] Trial #1: GFLOPs: 0.0000. Time: 35.9785 ms. Best GFLOPs: 0.0000
[01:50:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform"] Trial #2: GFLOPs: 0.0000. Time: 4.3315 ms. Best GFLOPs: 0.0000
[01:50:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform"] Trial #3: GFLOPs: 0.0000. Time: 5.4994 ms. Best GFLOPs: 0.0000
[01:50:46] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_layout_transform"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |                            fused_concatenate |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                          fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 132
Total latency (us): 62425.9

[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #0: GFLOPs: 3.7952. Time: 11.6353 ms. Best GFLOPs: 3.7952
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #1: GFLOPs: 1.7315. Time: 25.5026 ms. Best GFLOPs: 3.7952
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 111, 111, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 111, 111, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(1776, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1, i4_1, i5_0 in T.grid(37, 2, 1):
                for i3_3_init in T.serial(3):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 592 // 37)
                            oh = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 592 * 37 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 37)
                            ow = T.axis.spatial(111, i3_1 * 3 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 592 // 37)
                            oh = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 592 * 37 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 37)
                            ow = T.axis.spatial(111, i3_1 * 3 + i3_3)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3_fused)
                            ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_0, i7_0])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0_i1_i2_fused in T.parallel(1776, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(111):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 111)
                        ax2 = T.axis.spatial(111, i0_i1_i2_fused % 111)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 16, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 37, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 37, 1, 3])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b102)
b122 = sch.decompose_reduction(block=b102, loop=l107)
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #3: GFLOPs: 1.5775. Time: 27.9927 ms. Best GFLOPs: 3.7952
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 111, 111, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 111, 111, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(222, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0 in T.serial(4):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 37, 1, 1):
                    for i1_2_init, i1_3_init, i2_3_init in T.grid(2, 4, 3):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 111 * 8 + i1_2_init * 4 + i1_3_init)
                            oh = T.axis.spatial(111, i2_1 * 3 + i2_3_init)
                            ow = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused % 111)
                            oc_block = T.axis.spatial(4, i4_0)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 3, 1, 2, 1, 1, 1, 1, 3, 1, 1, 4, 3, 1, 1):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 111 * 8 + i1_2 * 4 + i1_3)
                            oh = T.axis.spatial(111, i2_1 * 3 + i2_3)
                            ow = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused % 111)
                            oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_0, i5_0, i6_1, i7_0])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 111, 1, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 111 * 8 + ax1)
                        ax2_1 = T.axis.spatial(111, ax2)
                        ax3_1 = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused % 111)
                        ax4_1 = T.axis.spatial(4, i4_0)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 37, 1, 3])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[111, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b102)
b126 = sch.decompose_reduction(block=b102, loop=l110)
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #5: GFLOPs: 3.3131. Time: 13.3285 ms. Best GFLOPs: 3.7952
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #6: GFLOPs: 3.9254. Time: 11.2495 ms. Best GFLOPs: 3.9254
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #7: GFLOPs: 2.7080. Time: 16.3064 ms. Best GFLOPs: 3.9254
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #8: GFLOPs: 1.7938. Time: 24.6172 ms. Best GFLOPs: 3.9254
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 111, 111, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 111, 111, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(37, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 1):
                for i2_2_init, i3_2_init, i1_3_init, i2_3_init in T.grid(37, 3, 8, 3):
                    for i3_3_i4_3_fused_init in T.vectorized(4):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_1 * 8 + i1_3_init)
                            oh = T.axis.spatial(111, i2_2_init * 3 + i2_3_init)
                            ow = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 3 + i3_2_init)
                            oc_block = T.axis.spatial(4, i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(3, 3, 1, 1, 1, 37, 3, 1, 1, 1, 3, 1, 8, 3):
                    for i3_3_i4_3_fused in T.vectorized(4):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_1 * 8 + i1_3)
                            oh = T.axis.spatial(111, i2_2 * 3 + i2_3)
                            ow = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 3 + i3_2)
                            oc_block, ic, kh, kw = T.axis.remap("SRRR", [i3_3_i4_3_fused, i5_0, i6_0, i7_1])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 16, 111):
                for ax3_ax4_fused in T.vectorized(12):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1 = T.axis.remap("SS", [ax1, ax2])
                        ax3 = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 3 + ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 37, 3])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[37, 1, 3, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l92, l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l100, l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b103)
b125 = sch.decompose_reduction(block=b103, loop=l110)
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #10: GFLOPs: 2.0450. Time: 21.5936 ms. Best GFLOPs: 3.9254
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 111, 111, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 111, 111, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(4107, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 16, 1, 1, 2):
                for i3_2_init, i4_2_init in T.grid(3, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_1)
                        oh = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused // 37)
                        ow = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused % 37 * 3 + i3_2_init)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 1, 1, 3, 2, 1, 1, 3, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_1)
                        oh = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused // 37)
                        ow = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused % 37 * 3 + i3_2)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_1])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0_i1_i2_fused in T.parallel(1776, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(111):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 111)
                        ax2 = T.axis.spatial(111, i0_i1_i2_fused % 111)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 16, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[111, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[37, 1, 3, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b101)
b125 = sch.decompose_reduction(block=b101, loop=l109)
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 111, 111, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 111, 111, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(37, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(3, 2, 16, 3, 37):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_3_init)
                        oh = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 3 + i2_3_init)
                        ow = T.axis.spatial(111, i3_2_init * 37 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_2_init * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 3, 1, 1, 1, 1, 3, 2, 1, 1, 3, 1, 16, 3, 37):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_3)
                        oh = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 3 + i2_3)
                        ow = T.axis.spatial(111, i3_2 * 37 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                        ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_1])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0_i1_i2_fused in T.parallel(1776, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(111):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 111)
                        ax2 = T.axis.spatial(111, i0_i1_i2_fused % 111)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 37, 1, 3])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 3, 37])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l104)
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #13: GFLOPs: 7.4139. Time: 5.9562 ms. Best GFLOPs: 7.4139
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #14: GFLOPs: 2.9082. Time: 15.1842 ms. Best GFLOPs: 7.4139
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #15: GFLOPs: 2.8309. Time: 15.5989 ms. Best GFLOPs: 7.4139
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #16: GFLOPs: 1.8516. Time: 23.8486 ms. Best GFLOPs: 7.4139
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #17: GFLOPs: 3.0747. Time: 14.3621 ms. Best GFLOPs: 7.4139
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #18: GFLOPs: 3.3171. Time: 13.3126 ms. Best GFLOPs: 7.4139
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #19: GFLOPs: 4.2470. Time: 10.3975 ms. Best GFLOPs: 7.4139
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #20: GFLOPs: 0.6047. Time: 73.0265 ms. Best GFLOPs: 7.4139
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #21: GFLOPs: 3.4269. Time: 12.8857 ms. Best GFLOPs: 7.4139
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 111, 111, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 111, 111, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(2738, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1, i4_1 in T.grid(1, 2):
                for i1_2_init, i2_2_init, i1_3_init, i3_3_init in T.grid(8, 3, 2, 3):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_2_init * 2 + i1_3_init)
                        oh = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 37 * 3 + i2_2_init)
                        ow = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 74 * 3 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 74 // 37 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 3, 1, 8, 3, 1, 1, 1, 3, 1, 1, 2, 1, 3, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 37 * 3 + i2_2)
                        ow = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 74 * 3 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 74 // 37 * 2 + i4_1)
                        ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_1, i7_0])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0_i1_i2_fused in T.parallel(1776, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(111):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 111)
                        ax2 = T.axis.spatial(111, i0_i1_i2_fused % 111)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 8, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 37, 3, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[37, 1, 1, 3])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b101)
b121 = sch.decompose_reduction(block=b101, loop=l105)
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #23: GFLOPs: 3.0086. Time: 14.6774 ms. Best GFLOPs: 7.4139
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #24: GFLOPs: 1.1043. Time: 39.9876 ms. Best GFLOPs: 7.4139
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 111, 111, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 111, 111, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(222, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 2, 1, 1, 1, 1, 1, 1, 1, 8, 3, 1, 1):
                for i2_3_init in T.serial(37):
                    for i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_1 * 8 + i1_2)
                            oh = T.axis.spatial(111, i2_2 * 37 + i2_3_init)
                            ow = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(3, 3, 3, 1, 1, 37):
                    for i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_1 * 8 + i1_2)
                            oh = T.axis.spatial(111, i2_2 * 37 + i2_3)
                            ow = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i3_3_i4_3_fused)
                            ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_1, i7_1])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 16, 111):
                for ax3_ax4_fused in T.vectorized(2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1 = T.axis.remap("SS", [ax1, ax2])
                        ax3 = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax3_ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 3, 37])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[111, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l92, l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l100, l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b103)
b125 = sch.decompose_reduction(block=b103, loop=l118)
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #26: GFLOPs: 4.1344. Time: 10.6807 ms. Best GFLOPs: 7.4139
[01:50:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 111, 111, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 111, 111, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(222, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_1, i3_1, i4_1, i5_0, i6_0 in T.grid(1, 1, 2, 1, 1):
                for i2_2_init, i1_3_init, i2_3_init in T.grid(37, 8, 3):
                    for i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 2 * 8 + i1_3_init)
                            oh = T.axis.spatial(111, i2_2_init * 3 + i2_3_init)
                            ow = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 2)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(3, 1, 1, 37, 1, 1, 3, 3, 1, 1, 8, 3):
                    for i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 2 * 8 + i1_3)
                            oh = T.axis.spatial(111, i2_2 * 3 + i2_3)
                            ow = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 2)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i3_3_i4_3_fused)
                            ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_1, i7_0])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0_i1_i2_fused in T.parallel(1776, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(111):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 111)
                        ax2 = T.axis.spatial(111, i0_i1_i2_fused % 111)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 37, 3])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[111, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73)
sch.parallel(loop=l93)
l94 = sch.fuse(l91, l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b102)
b122 = sch.decompose_reduction(block=b102, loop=l109)
[01:50:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #28: GFLOPs: 1.4157. Time: 31.1926 ms. Best GFLOPs: 7.4139
[01:50:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #29: GFLOPs: 1.2989. Time: 33.9958 ms. Best GFLOPs: 7.4139
[01:50:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #30: GFLOPs: 2.5209. Time: 17.5167 ms. Best GFLOPs: 7.4139
[01:50:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #31: GFLOPs: 2.6788. Time: 16.4843 ms. Best GFLOPs: 7.4139
[01:51:12] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |                            fused_concatenate |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                          fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 164
Total latency (us): 68382.1

[01:51:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_max_pool2d"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 111, 111, 4), "float32"], tensor: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        tensor_rf = T.alloc_buffer([1, 16, 55, 55, 4, 1], dtype="float32")
        for i0_i1_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2, i3, i4, i5_i6_fused_0 in T.grid(55, 55, 4, 1):
                with T.block("tensor_rf_init"):
                    vi5_i6_fused_0 = T.axis.spatial(1, 0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                    T.reads()
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.float32(-3.4028234663852886e+38)
                for i5_i6_fused_1 in T.serial(9):
                    with T.block("tensor_rf_update"):
                        vi5_i6_fused_0 = T.axis.spatial(1, 0)
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                        rv0 = T.axis.reduce(3, i5_i6_fused_1 // 3)
                        rv1 = T.axis.reduce(3, i5_i6_fused_1 % 3)
                        T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
        for i0_i1_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2, i3, i4, i5_i6_fused_0 in T.grid(55, 55, 4, 1):
                with T.block("tensor"):
                    vi5_i6_fused_0 = T.axis.reduce(1, 0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 9])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l12, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
sch.enter_postproc()
b16 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.unroll_explicit")
b17, b18 = sch.get_child_blocks(b16)
l19, l20, l21, l22, l23, l24, l25 = sch.get_loops(block=b17)
l26 = sch.fuse(l19, l20)
sch.parallel(loop=l26)
sch.annotate(block_or_loop=l26, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l26, ann_key="pragma_unroll_explicit", ann_val=1)
l27, l28, l29, l30, l31, l32 = sch.get_loops(block=b18)
l33 = sch.fuse(l27, l28)
sch.parallel(loop=l33)
sch.annotate(block_or_loop=l33, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l33, ann_key="pragma_unroll_explicit", ann_val=1)
b34 = sch.get_block(name="tensor_rf", func_name="main")
l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b34)
b41 = sch.decompose_reduction(block=b34, loop=l40)
[01:51:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #1: GFLOPs: 0.4436. Time: 3.9278 ms. Best GFLOPs: 0.4436
[01:51:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #2: GFLOPs: 0.1265. Time: 13.7725 ms. Best GFLOPs: 0.4436
[01:51:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #3: GFLOPs: 0.0747. Time: 23.3302 ms. Best GFLOPs: 0.4436
[01:51:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #4: GFLOPs: 0.0467. Time: 37.3256 ms. Best GFLOPs: 0.4436
[01:51:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #5: GFLOPs: 0.1546. Time: 11.2705 ms. Best GFLOPs: 0.4436
[01:51:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #6: GFLOPs: 0.0843. Time: 20.6596 ms. Best GFLOPs: 0.4436
[01:51:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #7: GFLOPs: 0.0272. Time: 63.9844 ms. Best GFLOPs: 0.4436
[01:51:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #8: GFLOPs: 0.0503. Time: 34.6459 ms. Best GFLOPs: 0.4436
[01:51:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #9: GFLOPs: 0.2331. Time: 7.4761 ms. Best GFLOPs: 0.4436
[01:51:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #10: GFLOPs: 0.1562. Time: 11.1579 ms. Best GFLOPs: 0.4436
[01:51:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #11: GFLOPs: 0.2680. Time: 6.5021 ms. Best GFLOPs: 0.4436
[01:51:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #12: GFLOPs: 0.0444. Time: 39.2726 ms. Best GFLOPs: 0.4436
[01:51:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #13: GFLOPs: 0.2178. Time: 7.9994 ms. Best GFLOPs: 0.4436
[01:51:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #14: GFLOPs: 0.1042. Time: 16.7247 ms. Best GFLOPs: 0.4436
[01:51:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #15: GFLOPs: 0.0614. Time: 28.3855 ms. Best GFLOPs: 0.4436
[01:51:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #16: GFLOPs: 0.1687. Time: 10.3309 ms. Best GFLOPs: 0.4436
[01:51:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #17: GFLOPs: 0.1452. Time: 11.9960 ms. Best GFLOPs: 0.4436
[01:51:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #18: GFLOPs: 0.0871. Time: 19.9988 ms. Best GFLOPs: 0.4436
[01:51:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #19: GFLOPs: 0.0968. Time: 17.9978 ms. Best GFLOPs: 0.4436
[01:51:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #20: GFLOPs: 0.1409. Time: 12.3621 ms. Best GFLOPs: 0.4436
[01:51:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_max_pool2d"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 111, 111, 4), "float32"], tensor: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        tensor_rf = T.alloc_buffer([1, 16, 55, 55, 4, 9], dtype="float32")
        for i0_i1_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i2, i3 in T.grid(55, 55):
                for i4_i5_i6_fused_0_i5_i6_fused_1_fused in T.vectorized(36):
                    with T.block("tensor_rf"):
                        vi5_i6_fused_1 = T.axis.spatial(9, i4_i5_i6_fused_0_i5_i6_fused_1_fused % 9)
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3 = T.axis.remap("SSS", [i0_i1_fused, i2, i3])
                        ax4 = T.axis.spatial(4, i4_i5_i6_fused_0_i5_i6_fused_1_fused // 9)
                        T.reads(placeholder[ax0, ax1, ax2 * 2 + vi5_i6_fused_1 // 3, ax3 * 2 + vi5_i6_fused_1 % 3, ax4])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = placeholder[ax0, ax1, ax2 * 2 + vi5_i6_fused_1 // 3, ax3 * 2 + vi5_i6_fused_1 % 3, ax4]
        for i0_i1_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i2, i3, i4, i5_i6_fused_0 in T.grid(55, 55, 4, 1):
                with T.block("tensor_init"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                    T.reads()
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                for i5_i6_fused_1 in T.serial(9):
                    with T.block("tensor_update"):
                        vi5_i6_fused_1 = T.axis.reduce(9, i5_i6_fused_1)
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                        T.reads(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                        T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                        T.block_attr({"meta_schedule.random_compute_producer":True})
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 9])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
sch.enter_postproc()
b16 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.unroll_explicit")
b17, b18 = sch.get_child_blocks(b16)
l19, l20, l21, l22, l23, l24, l25 = sch.get_loops(block=b17)
l26 = sch.fuse(l19, l20)
sch.parallel(loop=l26)
l27 = sch.fuse(l23, l24, l25)
sch.vectorize(loop=l27)
sch.annotate(block_or_loop=l26, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l26, ann_key="pragma_unroll_explicit", ann_val=1)
l28, l29, l30, l31, l32, l33, l34 = sch.get_loops(block=b18)
l35 = sch.fuse(l28, l29)
sch.parallel(loop=l35)
sch.annotate(block_or_loop=l35, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l35, ann_key="pragma_unroll_explicit", ann_val=1)
b36 = sch.get_block(name="tensor", func_name="main")
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
b43 = sch.decompose_reduction(block=b36, loop=l42)
[01:51:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #22: GFLOPs: 0.1108. Time: 15.7315 ms. Best GFLOPs: 0.4436
[01:51:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #23: GFLOPs: 0.1549. Time: 11.2492 ms. Best GFLOPs: 0.4436
[01:51:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #24: GFLOPs: 0.0327. Time: 53.3234 ms. Best GFLOPs: 0.4436
[01:51:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #25: GFLOPs: 0.1687. Time: 10.3311 ms. Best GFLOPs: 0.4436
[01:51:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_max_pool2d"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 111, 111, 4), "float32"], tensor: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        tensor_rf = T.alloc_buffer([1, 16, 55, 55, 4, 1], dtype="float32")
        for i0_i1_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2, i3, i4 in T.grid(55, 55, 4):
                with T.block("tensor_rf_init"):
                    vi5_i6_fused_1 = T.axis.spatial(1, 0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                    T.reads()
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.float32(-3.4028234663852886e+38)
                for i5_i6_fused_0, i5_i6_fused_1 in T.grid(9, 1):
                    with T.block("tensor_rf_update"):
                        vi5_i6_fused_1 = T.axis.spatial(1, 0)
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                        rv0 = T.axis.reduce(3, i5_i6_fused_0 // 3)
                        rv1 = T.axis.reduce(3, i5_i6_fused_0 % 3)
                        T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
        for i0_i1_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2, i3, i4, i5_i6_fused_1 in T.grid(55, 55, 4, 1):
                with T.block("tensor"):
                    vi5_i6_fused_1 = T.axis.reduce(1, 0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[9, 1])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
sch.enter_postproc()
b16 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.unroll_explicit")
b17, b18 = sch.get_child_blocks(b16)
l19, l20, l21, l22, l23, l24, l25 = sch.get_loops(block=b17)
l26 = sch.fuse(l19, l20)
sch.parallel(loop=l26)
sch.annotate(block_or_loop=l26, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l26, ann_key="pragma_unroll_explicit", ann_val=1)
l27, l28, l29, l30, l31, l32 = sch.get_loops(block=b18)
l33 = sch.fuse(l27, l28)
sch.parallel(loop=l33)
sch.annotate(block_or_loop=l33, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l33, ann_key="pragma_unroll_explicit", ann_val=1)
b34 = sch.get_block(name="tensor_rf", func_name="main")
l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b34)
b41 = sch.decompose_reduction(block=b34, loop=l39)
[01:51:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #27: GFLOPs: 0.0871. Time: 19.9980 ms. Best GFLOPs: 0.4436
[01:51:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #28: GFLOPs: 0.0632. Time: 27.5555 ms. Best GFLOPs: 0.4436
[01:51:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #29: GFLOPs: 0.1188. Time: 14.6664 ms. Best GFLOPs: 0.4436
[01:51:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #30: GFLOPs: 0.1295. Time: 13.4525 ms. Best GFLOPs: 0.4436
[01:51:40] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_nn_max_pool2d"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |                            fused_concatenate |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                          fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 195
Total latency (us): 72309.9

[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #0: GFLOPs: 0.4330. Time: 14.5308 ms. Best GFLOPs: 0.4330
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #1: GFLOPs: 0.1701. Time: 36.9924 ms. Best GFLOPs: 0.4330
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #2: GFLOPs: 0.3055. Time: 20.5978 ms. Best GFLOPs: 0.4330
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #3: GFLOPs: 0.4775. Time: 13.1757 ms. Best GFLOPs: 0.4775
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #4: GFLOPs: 0.4495. Time: 13.9967 ms. Best GFLOPs: 0.4775
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #5: GFLOPs: 0.8990. Time: 6.9988 ms. Best GFLOPs: 0.8990
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #6: GFLOPs: 0.7403. Time: 8.4995 ms. Best GFLOPs: 0.8990
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #7: GFLOPs: 0.3246. Time: 19.3851 ms. Best GFLOPs: 0.8990
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #8: GFLOPs: 0.9177. Time: 6.8560 ms. Best GFLOPs: 0.9177
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #9: GFLOPs: 0.9528. Time: 6.6040 ms. Best GFLOPs: 0.9528
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(4, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 4, 55, 55, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(22, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 2):
                for i2_2_init, i3_2_init, i1_3_init, i3_3_init in T.grid(5, 5, 2, 11):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 11 * 2 + i1_3_init)
                            oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 11 * 5 + i2_2_init)
                            ow = T.axis.spatial(55, i3_2_init * 11 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 55, 55, 4], "float32"], ["TENSOR", [4, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(16, 1, 1, 1, 1, 5, 5, 1, 4, 1, 1, 1, 2, 1, 11):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 11 * 2 + i1_3)
                            oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 11 * 5 + i2_2)
                            ow = T.axis.spatial(55, i3_2 * 11 + i3_3)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3_fused)
                            ic = T.axis.reduce(64, i5_0 * 4 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 55, 55, 4], "float32"], ["TENSOR", [4, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 2, 5, 55):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 11 * 2 + ax1)
                        ax2_1 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 11 * 5 + ax2)
                        ax3_1, ax4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[11, 1, 5, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 5, 11])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b103)
b126 = sch.decompose_reduction(block=b103, loop=l110)
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #11: GFLOPs: 0.5453. Time: 11.5393 ms. Best GFLOPs: 0.9528
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #12: GFLOPs: 0.3671. Time: 17.1417 ms. Best GFLOPs: 0.9528
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #13: GFLOPs: 0.3630. Time: 17.3313 ms. Best GFLOPs: 0.9528
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #14: GFLOPs: 0.3552. Time: 17.7116 ms. Best GFLOPs: 0.9528
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #15: GFLOPs: 0.2809. Time: 22.3998 ms. Best GFLOPs: 0.9528
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #16: GFLOPs: 0.3012. Time: 20.8883 ms. Best GFLOPs: 0.9528
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(4, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 4, 55, 55, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(220, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i4_2_init, i3_3_init in T.grid(2, 5, 2, 11):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 110 * 2 + i1_2_init)
                    oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 110 // 10 * 5 + i2_2_init)
                    ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 10 // 2 * 11 + i3_3_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 55, 55, 4], "float32"], ["TENSOR", [4, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 2, 5, 1, 2, 1, 1, 1, 1, 1, 1, 11, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 110 * 2 + i1_2)
                    oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 110 // 10 * 5 + i2_2)
                    ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 10 // 2 * 11 + i3_3)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2)
                    ic = T.axis.reduce(64, i5_0)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 55, 55, 4], "float32"], ["TENSOR", [4, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(220, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(55):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(4, i0_i1_i2_fused // 55)
                        ax2 = T.axis.spatial(55, i0_i1_i2_fused % 55)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[11, 1, 5, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[5, 1, 1, 11])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #18: GFLOPs: 0.1522. Time: 41.3300 ms. Best GFLOPs: 0.9528
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #19: GFLOPs: 0.9535. Time: 6.5987 ms. Best GFLOPs: 0.9535
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #20: GFLOPs: 0.5425. Time: 11.5988 ms. Best GFLOPs: 0.9535
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #21: GFLOPs: 0.5613. Time: 11.2101 ms. Best GFLOPs: 0.9535
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #22: GFLOPs: 0.4402. Time: 14.2948 ms. Best GFLOPs: 0.9535
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #23: GFLOPs: 0.4607. Time: 13.6575 ms. Best GFLOPs: 0.9535
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #24: GFLOPs: 0.5408. Time: 11.6342 ms. Best GFLOPs: 0.9535
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #25: GFLOPs: 0.9439. Time: 6.6658 ms. Best GFLOPs: 0.9535
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #26: GFLOPs: 0.4841. Time: 12.9977 ms. Best GFLOPs: 0.9535
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #27: GFLOPs: 0.9082. Time: 6.9283 ms. Best GFLOPs: 0.9535
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #28: GFLOPs: 0.7371. Time: 8.5363 ms. Best GFLOPs: 0.9535
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #29: GFLOPs: 0.1776. Time: 35.4208 ms. Best GFLOPs: 0.9535
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #30: GFLOPs: 0.7035. Time: 8.9437 ms. Best GFLOPs: 0.9535
[01:51:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #31: GFLOPs: 0.3204. Time: 19.6367 ms. Best GFLOPs: 0.9535
[01:52:09] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |                            fused_concatenate |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                          fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 227
Total latency (us): 78908.6

[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(4, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 4, 55, 55, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(440, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_3_init, i2_3_init, i3_3_init in T.grid(2, 5, 11):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 220 * 2 + i1_3_init)
                    oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 220 // 20 * 5 + i2_3_init)
                    ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 20 // 4 * 11 + i3_3_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 55, 55, 4], "float32"], ["TENSOR", [4, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 1, 1, 1, 1, 1, 1, 32, 1, 1, 1, 2, 5, 11, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 220 * 2 + i1_3)
                    oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 220 // 20 * 5 + i2_3)
                    ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 20 // 4 * 11 + i3_3)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    ic = T.axis.reduce(128, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 55, 55, 4], "float32"], ["TENSOR", [4, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(220, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(55):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(4, i0_i1_i2_fused // 55)
                        ax2 = T.axis.spatial(55, i0_i1_i2_fused % 55)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 11, 1, 5])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 5, 1, 11])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(4, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 4, 55, 55, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(44, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(5, 2, 11, 5):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 2 + i1_3_init)
                        oh = T.axis.spatial(55, i2_2_init * 11 + i2_3_init)
                        ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 5 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 55, 55, 4], "float32"], ["TENSOR", [4, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(128, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 2, 11, 5):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 2 + i1_3)
                        oh = T.axis.spatial(55, i2_2 * 11 + i2_3)
                        ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 5 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused)
                        ic = T.axis.reduce(128, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 55, 55, 4], "float32"], ["TENSOR", [4, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(220, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(55):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(4, i0_i1_i2_fused // 55)
                        ax2 = T.axis.spatial(55, i0_i1_i2_fused % 55)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 5, 11])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[11, 1, 1, 5])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l104)
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #2: GFLOPs: 1.9148. Time: 6.5214 ms. Best GFLOPs: 1.9148
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #3: GFLOPs: 2.2852. Time: 5.4643 ms. Best GFLOPs: 2.2852
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #4: GFLOPs: 2.1095. Time: 5.9194 ms. Best GFLOPs: 2.2852
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(4, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 4, 55, 55, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(5, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 1):
                for i2_2_init, i3_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(11, 11, 2, 2, 5):
                    for i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(4, i1_1 * 2 + i1_3_init)
                            oh = T.axis.spatial(55, i2_2_init * 5 + i2_3_init)
                            ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 11 + i3_2_init)
                            oc_block = T.axis.spatial(4, i4_2_init * 2 + i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 55, 55, 4], "float32"], ["TENSOR", [4, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(8, 1, 1, 1, 1, 11, 11, 2, 16, 1, 1, 1, 2, 5):
                    for i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(4, i1_1 * 2 + i1_3)
                            oh = T.axis.spatial(55, i2_2 * 5 + i2_3)
                            ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 11 + i3_2)
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i3_3_i4_3_fused)
                            ic = T.axis.reduce(128, i5_0 * 16 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 55, 55, 4], "float32"], ["TENSOR", [4, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 4, 55):
                for ax3_ax4_fused in T.vectorized(44):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1 = T.axis.remap("SS", [ax1, ax2])
                        ax3 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 11 + ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 11, 5])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[5, 1, 11, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l92, l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l100, l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b103)
b125 = sch.decompose_reduction(block=b103, loop=l110)
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #6: GFLOPs: 3.0063. Time: 4.1536 ms. Best GFLOPs: 3.0063
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(4, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 4, 55, 55, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(11, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 11, 2):
                for i2_2_init, i3_2_init in T.grid(5, 5):
                    for i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(4, i1_1)
                            oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 5 + i2_2_init)
                            ow = T.axis.spatial(55, i3_1 * 5 + i3_2_init)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 55, 55, 4], "float32"], ["TENSOR", [4, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1 in T.grid(16, 1, 1, 1, 1, 5, 5, 1, 8):
                    for i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(4, i1_1)
                            oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 5 + i2_2)
                            ow = T.axis.spatial(55, i3_1 * 5 + i3_2)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused)
                            ic = T.axis.reduce(128, i5_0 * 8 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 55, 55, 4], "float32"], ["TENSOR", [4, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 5, 55):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(4, ax1)
                        ax2_1 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 5 + ax2)
                        ax3_1, ax4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[11, 1, 5, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 11, 5, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l87, l88, l89, l90, l91, l92, l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b103)
b120 = sch.decompose_reduction(block=b103, loop=l110)
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #8: GFLOPs: 0.8091. Time: 15.4335 ms. Best GFLOPs: 3.0063
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #9: GFLOPs: 2.2611. Time: 5.5226 ms. Best GFLOPs: 3.0063
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(4, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 4, 55, 55, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(220, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(55, 1, 1, 1, 1, 1, 1):
                for i4_2_init in T.serial(4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(4, i0_0_i1_0_i2_0_fused // 55)
                        oh = T.axis.spatial(55, i0_0_i1_0_i2_0_fused % 55)
                        ow, oc_block = T.axis.remap("SS", [i3_0, i4_2_init])
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 55, 55, 4], "float32"], ["TENSOR", [4, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 1, 1, 1, 1, 1, 4, 32, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(4, i0_0_i1_0_i2_0_fused // 55)
                        oh = T.axis.spatial(55, i0_0_i1_0_i2_0_fused % 55)
                        ow, oc_block = T.axis.remap("SS", [i3_0, i4_2])
                        ic = T.axis.reduce(128, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 55, 55, 4], "float32"], ["TENSOR", [4, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0_ax1_ax2_ax3_ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(4, i0_0_i1_0_i2_0_fused // 55)
                        ax2 = T.axis.spatial(55, i0_0_i1_0_i2_0_fused % 55)
                        ax3, ax4 = T.axis.remap("SS", [i3_0, ax0_ax1_ax2_ax3_ax4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[55, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[55, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b67)
l108 = sch.fuse(l103, l104, l105, l106, l107)
sch.vectorize(loop=l108)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b109 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b109)
b134 = sch.decompose_reduction(block=b109, loop=l118)
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #11: GFLOPs: 2.0576. Time: 6.0689 ms. Best GFLOPs: 3.0063
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #12: GFLOPs: 3.3532. Time: 3.7240 ms. Best GFLOPs: 3.3532
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #13: GFLOPs: 2.1118. Time: 5.9130 ms. Best GFLOPs: 3.3532
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(4, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 4, 55, 55, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(44, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_2_init, i3_2_init, i1_3_init, i3_3_init in T.grid(5, 5, 2, 11):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 22 // 11 * 2 + i1_3_init)
                        oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 11 * 5 + i2_2_init)
                        ow = T.axis.spatial(55, i3_2_init * 11 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 22 * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 55, 55, 4], "float32"], ["TENSOR", [4, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(128, 1, 1, 1, 1, 5, 5, 1, 1, 1, 1, 1, 2, 1, 11):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 22 // 11 * 2 + i1_3)
                        oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 11 * 5 + i2_2)
                        ow = T.axis.spatial(55, i3_2 * 11 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 22 * 2 + i4_3_fused)
                        ic = T.axis.reduce(128, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 55, 55, 4], "float32"], ["TENSOR", [4, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 2, 5, 55):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 22 // 11 * 2 + ax1)
                        ax2_1 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 11 * 5 + ax2)
                        ax3_1 = T.axis.spatial(55, ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 22 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 11, 5, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 5, 11])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b103)
b121 = sch.decompose_reduction(block=b103, loop=l105)
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #15: GFLOPs: 2.4791. Time: 5.0371 ms. Best GFLOPs: 3.3532
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #16: GFLOPs: 2.3004. Time: 5.4282 ms. Best GFLOPs: 3.3532
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #17: GFLOPs: 2.3576. Time: 5.2965 ms. Best GFLOPs: 3.3532
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #18: GFLOPs: 1.4119. Time: 8.8445 ms. Best GFLOPs: 3.3532
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #19: GFLOPs: 2.7727. Time: 4.5036 ms. Best GFLOPs: 3.3532
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #20: GFLOPs: 1.3502. Time: 9.2487 ms. Best GFLOPs: 3.3532
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #21: GFLOPs: 1.7366. Time: 7.1907 ms. Best GFLOPs: 3.3532
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #22: GFLOPs: 0.9318. Time: 13.4010 ms. Best GFLOPs: 3.3532
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #23: GFLOPs: 1.9768. Time: 6.3168 ms. Best GFLOPs: 3.3532
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #24: GFLOPs: 1.8478. Time: 6.7579 ms. Best GFLOPs: 3.3532
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #25: GFLOPs: 2.1184. Time: 5.8947 ms. Best GFLOPs: 3.3532
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #26: GFLOPs: 1.3317. Time: 9.3771 ms. Best GFLOPs: 3.3532
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #27: GFLOPs: 0.8971. Time: 13.9196 ms. Best GFLOPs: 3.3532
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #28: GFLOPs: 0.7307. Time: 17.0887 ms. Best GFLOPs: 3.3532
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #29: GFLOPs: 1.0407. Time: 11.9988 ms. Best GFLOPs: 3.3532
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #30: GFLOPs: 1.2928. Time: 9.6594 ms. Best GFLOPs: 3.3532
[01:52:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #31: GFLOPs: 1.4423. Time: 8.6577 ms. Best GFLOPs: 3.3532
[01:52:47] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |                            fused_concatenate |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                          fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 259
Total latency (us): 82632.6

[01:52:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
        for i0_0 in T.serial(1, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 11, 1, 4):
                    for i1_2_init, i2_2_init, i3_3_init in T.grid(4, 5, 55):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_1 * 4 + i1_2_init)
                            oh = T.axis.spatial(55, i2_1 * 5 + i2_2_init)
                            ow, oc_block = T.axis.remap("SS", [i3_3_init, i4_1])
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 4, 5, 1, 1, 8, 1, 1, 1, 1, 1, 55, 1):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_1 * 4 + i1_2)
                            oh = T.axis.spatial(55, i2_1 * 5 + i2_2)
                            ow, oc_block = T.axis.remap("SS", [i3_3, i4_1])
                            ic = T.axis.reduce(16, i5_0 * 8 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3 in T.grid(1, 16, 55, 55):
                    for ax4_fused in T.vectorized(4):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1, ax2_1, ax3_1, ax4 = T.axis.remap("SSSS", [ax1, ax2, ax3, ax4_fused])
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 11, 5, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 55])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
sch.annotate(block_or_loop=l68, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l68, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b67)
l104 = sch.fuse(l103)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b105)
b132 = sch.decompose_reduction(block=b105, loop=l116)
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #1: GFLOPs: 0.6587. Time: 9.9929 ms. Best GFLOPs: 0.6587
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #2: GFLOPs: 0.5119. Time: 12.8599 ms. Best GFLOPs: 0.6587
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #3: GFLOPs: 0.8000. Time: 8.2277 ms. Best GFLOPs: 0.8000
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #4: GFLOPs: 0.5143. Time: 12.7984 ms. Best GFLOPs: 0.8000
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #5: GFLOPs: 0.6035. Time: 10.9073 ms. Best GFLOPs: 0.8000
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #6: GFLOPs: 0.6243. Time: 10.5437 ms. Best GFLOPs: 0.8000
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #7: GFLOPs: 0.4003. Time: 16.4425 ms. Best GFLOPs: 0.8000
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #8: GFLOPs: 0.8777. Time: 7.4997 ms. Best GFLOPs: 0.8777
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #9: GFLOPs: 0.5698. Time: 11.5531 ms. Best GFLOPs: 0.8777
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #10: GFLOPs: 0.4055. Time: 16.2316 ms. Best GFLOPs: 0.8777
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(200, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_3_init in T.grid(4, 11, 11):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 50 * 4 + i1_2_init)
                        oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 50 // 10 * 11 + i2_2_init)
                        ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 10 // 2 * 11 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(16, 1, 1, 1, 4, 11, 1, 1, 1, 1, 1, 1, 1, 1, 11):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 50 * 4 + i1_2)
                        oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 50 // 10 * 11 + i2_2)
                        ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 10 // 2 * 11 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused)
                        ic = T.axis.reduce(16, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(880, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(55):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 55)
                        ax2 = T.axis.spatial(55, i0_i1_i2_fused % 55)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[5, 1, 11, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 5, 1, 11])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l104)
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(176, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_1 in T.serial(2):
                for i3_2_init, i4_2_init, i2_3_init in T.grid(5, 2, 55):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 44 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 44 // 11)
                        oh = T.axis.spatial(55, i2_3_init)
                        ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 11 * 5 + i3_2_init)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 1, 1, 5, 2, 8, 1, 1, 1, 1, 55, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 44 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 44 // 11)
                        oh = T.axis.spatial(55, i2_3)
                        ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 11 * 5 + i3_2)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(16, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(880, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(55):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 55)
                        ax2 = T.axis.spatial(55, i0_i1_i2_fused % 55)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 4, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 55])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 11, 5, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b101)
b120 = sch.decompose_reduction(block=b101, loop=l104)
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #13: GFLOPs: 0.3872. Time: 16.9983 ms. Best GFLOPs: 0.8777
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #14: GFLOPs: 0.4489. Time: 14.6636 ms. Best GFLOPs: 0.8777
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #15: GFLOPs: 1.0286. Time: 6.3995 ms. Best GFLOPs: 1.0286
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(484, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_1 in T.serial(1):
                for i1_3_init, i2_3_init, i3_3_init in T.grid(8, 5, 5):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 242 * 8 + i1_3_init)
                            oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 242 // 22 * 5 + i2_3_init)
                            ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 11 * 5 + i3_3_init)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 22 // 11 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 8, 5, 5):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 242 * 8 + i1_3)
                            oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 242 // 22 * 5 + i2_3)
                            ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 11 * 5 + i3_3)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 22 // 11 * 2 + i4_3_fused)
                            ic = T.axis.reduce(16, i5_0 * 4 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(880, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(55):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 55)
                        ax2 = T.axis.spatial(55, i0_i1_i2_fused % 55)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[11, 1, 1, 5])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 11, 1, 5])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b102)
b121 = sch.decompose_reduction(block=b102, loop=l105)
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #17: GFLOPs: 0.9045. Time: 7.2776 ms. Best GFLOPs: 1.0286
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #18: GFLOPs: 0.5402. Time: 12.1856 ms. Best GFLOPs: 1.0286
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #19: GFLOPs: 1.2697. Time: 5.1841 ms. Best GFLOPs: 1.2697
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(176, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_1, i3_1, i4_1 in T.grid(1, 1, 2):
                for i3_2_init, i2_3_init in T.grid(55, 5):
                    for i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 22 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 2)
                            oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 22 // 2 * 5 + i2_3_init)
                            ow = T.axis.spatial(55, i3_2_init)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(16, 1, 1, 1, 1, 1, 55, 1, 1, 1, 1, 1, 1, 5):
                    for i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 22 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 2)
                            oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 22 // 2 * 5 + i2_3)
                            ow = T.axis.spatial(55, i3_2)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i3_3_i4_3_fused)
                            ic = T.axis.reduce(16, i5_0)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(880, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(55):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 55)
                        ax2 = T.axis.spatial(55, i0_i1_i2_fused % 55)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 2, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[11, 1, 1, 5])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 55, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73)
sch.parallel(loop=l93)
l94 = sch.fuse(l91, l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b102)
b122 = sch.decompose_reduction(block=b102, loop=l107)
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #21: GFLOPs: 0.3772. Time: 17.4525 ms. Best GFLOPs: 1.2697
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(220, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_3_init, i3_3_init in T.grid(8, 11, 5):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 44 // 22 * 8 + i1_2_init)
                        oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 44 * 11 + i2_3_init)
                        ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 22 // 2 * 5 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(16, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 11, 5):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 44 // 22 * 8 + i1_2)
                        oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 44 * 11 + i2_3)
                        ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 22 // 2 * 5 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused)
                        ic = T.axis.reduce(16, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(880, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(55):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 55)
                        ax2 = T.axis.spatial(55, i0_i1_i2_fused % 55)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[5, 1, 1, 11])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 11, 1, 5])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l104)
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(275, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0 in T.serial(2):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 1, 1):
                    for i1_2_init, i4_2_init, i2_3_init in T.grid(4, 2, 11):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_1 * 4 + i1_2_init)
                            oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused // 55 * 11 + i2_3_init)
                            ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 55)
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 4, 1, 1, 2, 8, 1, 1, 1, 1, 11, 1, 1):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_1 * 4 + i1_2)
                            oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused // 55 * 11 + i2_3)
                            ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 55)
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                            ic = T.axis.reduce(16, i5_0 * 8 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2 in T.grid(1, 16, 11):
                    for ax3_ax4_fused in T.vectorized(2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(16, ax1)
                            ax2_1 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused // 55 * 11 + ax2)
                            ax3 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 55)
                            ax4 = T.axis.spatial(4, i4_0 * 2 + ax3_ax4_fused)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[5, 1, 1, 11])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[55, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l100, l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b103)
b127 = sch.decompose_reduction(block=b103, loop=l111)
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #24: GFLOPs: 1.0031. Time: 6.5622 ms. Best GFLOPs: 1.2697
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #25: GFLOPs: 0.6100. Time: 10.7911 ms. Best GFLOPs: 1.2697
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #26: GFLOPs: 0.6294. Time: 10.4586 ms. Best GFLOPs: 1.2697
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(220, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_1 in T.serial(2):
                for i2_2_init, i3_2_init, i1_3_init in T.grid(5, 11, 8):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 110 * 8 + i1_3_init)
                        oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 110 // 10 * 5 + i2_2_init)
                        ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 5 * 11 + i3_2_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 10 // 5 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 1, 5, 11, 1, 2, 1, 1, 1, 8, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 110 * 8 + i1_3)
                        oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 110 // 10 * 5 + i2_2)
                        ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 5 * 11 + i3_2)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 10 // 5 * 2 + i4_1)
                        ic = T.axis.reduce(16, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(880, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(55):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 55)
                        ax2 = T.axis.spatial(55, i0_i1_i2_fused % 55)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[11, 1, 5, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 5, 11, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b101)
b120 = sch.decompose_reduction(block=b101, loop=l104)
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #28: GFLOPs: 0.3405. Time: 19.3313 ms. Best GFLOPs: 1.2697
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #29: GFLOPs: 0.8534. Time: 7.7133 ms. Best GFLOPs: 1.2697
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 55, 55, 4), "float32"], placeholder_1: T.Buffer[(16, 4, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 55, 55, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 55, 55, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(1100, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_1 in T.serial(1):
                for i1_2_init, i4_2_init, i2_3_init in T.grid(8, 2, 11):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 550 * 8 + i1_2_init)
                        oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 550 // 110 * 11 + i2_3_init)
                        ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 55)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 110 // 55 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 8, 1, 1, 2, 1, 1, 1, 1, 1, 11, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 550 * 8 + i1_2)
                        oh = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 550 // 110 * 11 + i2_3)
                        ow = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 55)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 110 // 55 * 2 + i4_2)
                        ic = T.axis.reduce(16, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 55, 55, 4], "float32"], ["TENSOR", [16, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(880, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(55):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 55)
                        ax2 = T.axis.spatial(55, i0_i1_i2_fused % 55)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[5, 1, 1, 11])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 55, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b101)
b120 = sch.decompose_reduction(block=b101, loop=l104)
[01:52:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #31: GFLOPs: 0.4593. Time: 14.3321 ms. Best GFLOPs: 1.2697
[01:53:26] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |            
 10 |                            fused_concatenate |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                          fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 291
Total latency (us): 93000.9

[01:53:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_concatenate"] Trial #0: GFLOPs: 0.0000. Time: 8.2099 ms. Best GFLOPs: 0.0000
[01:53:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_concatenate"] Trial #1: GFLOPs: 0.0000. Time: 5.9184 ms. Best GFLOPs: 0.0000
[01:53:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_concatenate"] Trial #2: GFLOPs: 0.0000. Time: 6.5875 ms. Best GFLOPs: 0.0000
[01:53:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_concatenate"] Trial #3: GFLOPs: 0.0000. Time: 4.6683 ms. Best GFLOPs: 0.0000
[01:53:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_concatenate"] Trial #4: GFLOPs: 0.0000. Time: 7.2217 ms. Best GFLOPs: 0.0000
[01:53:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_concatenate"] Trial #5: GFLOPs: 0.0000. Time: 8.9989 ms. Best GFLOPs: 0.0000
[01:53:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_concatenate"] Trial #6: GFLOPs: 0.0000. Time: 4.8267 ms. Best GFLOPs: 0.0000
[01:53:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_concatenate"] Trial #7: GFLOPs: 0.0000. Time: 38.6636 ms. Best GFLOPs: 0.0000
[01:53:52] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_concatenate"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |            
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                          fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 299
Total latency (us): 102337

[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #0: GFLOPs: 0.1260. Time: 6.6639 ms. Best GFLOPs: 0.1260
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #1: GFLOPs: 0.2596. Time: 3.2354 ms. Best GFLOPs: 0.2596
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_max_pool2d_1"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 55, 55, 4), "float32"], tensor: T.Buffer[(1, 32, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        tensor_rf = T.alloc_buffer([1, 32, 27, 27, 4, 3], dtype="float32")
        for i0_i1_i2_fused in T.parallel(864, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3, i4, i5_i6_fused_0 in T.grid(27, 4, 3):
                with T.block("tensor_rf_init"):
                    vi5_i6_fused_0 = T.axis.spatial(3, i5_i6_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(32, i0_i1_i2_fused // 27)
                    ax2 = T.axis.spatial(27, i0_i1_i2_fused % 27)
                    ax3, ax4 = T.axis.remap("SS", [i3, i4])
                    T.reads()
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.float32(-3.4028234663852886e+38)
                for i5_i6_fused_1 in T.serial(3):
                    with T.block("tensor_rf_update"):
                        vi5_i6_fused_0 = T.axis.spatial(3, i5_i6_fused_0)
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(32, i0_i1_i2_fused // 27)
                        ax2 = T.axis.spatial(27, i0_i1_i2_fused % 27)
                        ax3, ax4, vi5_i6_fused_1 = T.axis.remap("SSR", [i3, i4, i5_i6_fused_1])
                        T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0], placeholder[ax0, ax1, ax2 * 2 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) // 3, ax3 * 2 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) % 3, ax4])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0], placeholder[ax0, ax1, ax2 * 2 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) // 3, ax3 * 2 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) % 3, ax4])
        for i0_i1_i2_fused in T.parallel(864, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3, i4 in T.grid(27, 4):
                with T.block("tensor_init"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(32, i0_i1_i2_fused // 27)
                    ax2 = T.axis.spatial(27, i0_i1_i2_fused % 27)
                    ax3, ax4 = T.axis.remap("SS", [i3, i4])
                    T.reads()
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                for i5_i6_fused_0 in T.serial(3):
                    with T.block("tensor_update"):
                        vi5_i6_fused_0 = T.axis.reduce(3, i5_i6_fused_0)
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(32, i0_i1_i2_fused // 27)
                        ax2 = T.axis.spatial(27, i0_i1_i2_fused % 27)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4])
                        T.reads(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                        T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                        T.block_attr({"meta_schedule.random_compute_producer":True})
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 3])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l12, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
sch.enter_postproc()
b16 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.unroll_explicit")
b17, b18 = sch.get_child_blocks(b16)
l19, l20, l21, l22, l23, l24, l25 = sch.get_loops(block=b17)
l26 = sch.fuse(l19, l20, l21)
sch.parallel(loop=l26)
sch.annotate(block_or_loop=l26, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l26, ann_key="pragma_unroll_explicit", ann_val=1)
l27, l28, l29, l30, l31, l32 = sch.get_loops(block=b18)
l33 = sch.fuse(l27, l28, l29)
sch.parallel(loop=l33)
sch.annotate(block_or_loop=l33, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l33, ann_key="pragma_unroll_explicit", ann_val=1)
b34 = sch.get_block(name="tensor_rf", func_name="main")
l35, l36, l37, l38, l39 = sch.get_loops(block=b34)
b40 = sch.decompose_reduction(block=b34, loop=l39)
b41 = sch.get_block(name="tensor", func_name="main")
l42, l43, l44, l45 = sch.get_loops(block=b41)
b46 = sch.decompose_reduction(block=b41, loop=l45)
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #3: GFLOPs: 0.1535. Time: 5.4718 ms. Best GFLOPs: 0.2596
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #4: GFLOPs: 0.5488. Time: 1.5302 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #5: GFLOPs: 0.3975. Time: 2.1126 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #6: GFLOPs: 0.1562. Time: 5.3774 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #7: GFLOPs: 0.1935. Time: 4.3400 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #8: GFLOPs: 0.0989. Time: 8.4941 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #9: GFLOPs: 0.1536. Time: 5.4684 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #10: GFLOPs: 0.1353. Time: 6.2079 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #11: GFLOPs: 0.0759. Time: 11.0696 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_max_pool2d_1"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 55, 55, 4), "float32"], tensor: T.Buffer[(1, 32, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        tensor_rf = T.alloc_buffer([1, 32, 27, 27, 4, 3], dtype="float32")
        for i0_i1_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2, i3, i4 in T.grid(27, 27, 4):
                for i5_i6_fused_1_fused_init in T.vectorized(3):
                    with T.block("tensor_rf_init"):
                        vi5_i6_fused_1 = T.axis.spatial(3, i5_i6_fused_1_fused_init)
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                        T.reads()
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.float32(-3.4028234663852886e+38)
                for i5_i6_fused_0 in T.serial(3):
                    for i5_i6_fused_1_fused in T.vectorized(3):
                        with T.block("tensor_rf_update"):
                            vi5_i6_fused_1 = T.axis.spatial(3, i5_i6_fused_1_fused)
                            ax0 = T.axis.spatial(1, 0)
                            ax1, ax2, ax3, ax4, vi5_i6_fused_0 = T.axis.remap("SSSSR", [i0_i1_fused, i2, i3, i4, i5_i6_fused_0])
                            T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1], placeholder[ax0, ax1, ax2 * 2 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) // 3, ax3 * 2 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) % 3, ax4])
                            T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                            tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1], placeholder[ax0, ax1, ax2 * 2 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) // 3, ax3 * 2 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) % 3, ax4])
        for i0_i1_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2, i3, i4 in T.grid(27, 27, 4):
                with T.block("tensor_init"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                    T.reads()
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                for i5_i6_fused_1 in T.serial(3):
                    with T.block("tensor_update"):
                        vi5_i6_fused_1 = T.axis.reduce(3, i5_i6_fused_1)
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                        T.reads(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                        T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                        T.block_attr({"meta_schedule.random_compute_producer":True})
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 3])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
sch.enter_postproc()
b16 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.unroll_explicit")
b17, b18 = sch.get_child_blocks(b16)
l19, l20, l21, l22, l23, l24, l25 = sch.get_loops(block=b17)
l26 = sch.fuse(l19, l20)
sch.parallel(loop=l26)
l27 = sch.fuse(l25)
sch.vectorize(loop=l27)
sch.annotate(block_or_loop=l26, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l26, ann_key="pragma_unroll_explicit", ann_val=1)
l28, l29, l30, l31, l32, l33 = sch.get_loops(block=b18)
l34 = sch.fuse(l28, l29)
sch.parallel(loop=l34)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
b35 = sch.get_block(name="tensor_rf", func_name="main")
l36, l37, l38, l39, l40, l41 = sch.get_loops(block=b35)
b42 = sch.decompose_reduction(block=b35, loop=l40)
b43 = sch.get_block(name="tensor", func_name="main")
l44, l45, l46, l47, l48 = sch.get_loops(block=b43)
b49 = sch.decompose_reduction(block=b43, loop=l48)
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #13: GFLOPs: 0.1008. Time: 8.3340 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #14: GFLOPs: 0.0420. Time: 19.9988 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #15: GFLOPs: 0.0678. Time: 12.3909 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #16: GFLOPs: 0.1313. Time: 6.3985 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #17: GFLOPs: 0.2297. Time: 3.6562 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #18: GFLOPs: 0.0825. Time: 10.1792 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #19: GFLOPs: 0.0663. Time: 12.6656 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #20: GFLOPs: 0.1373. Time: 6.1176 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #21: GFLOPs: 0.1070. Time: 7.8459 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #22: GFLOPs: 0.0630. Time: 13.3205 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #23: GFLOPs: 0.0813. Time: 10.3337 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #24: GFLOPs: 0.0485. Time: 17.3292 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #25: GFLOPs: 0.0360. Time: 23.3280 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #26: GFLOPs: 0.0646. Time: 12.9981 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #27: GFLOPs: 0.0550. Time: 15.2692 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #28: GFLOPs: 0.0722. Time: 11.6363 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #29: GFLOPs: 0.0751. Time: 11.1780 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #30: GFLOPs: 0.0744. Time: 11.2880 ms. Best GFLOPs: 0.5488
[01:53:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #31: GFLOPs: 0.1173. Time: 7.1588 ms. Best GFLOPs: 0.5488
[01:54:13] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #11: "fused_nn_max_pool2d_1"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |            
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                          fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 331
Total latency (us): 103868

[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #0: GFLOPs: 0.7018. Time: 8.5756 ms. Best GFLOPs: 0.7018
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #1: GFLOPs: 1.1040. Time: 5.4518 ms. Best GFLOPs: 1.1040
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #2: GFLOPs: 0.8814. Time: 6.8287 ms. Best GFLOPs: 1.1040
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #3: GFLOPs: 0.9121. Time: 6.5985 ms. Best GFLOPs: 1.1040
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #4: GFLOPs: 0.5912. Time: 10.1797 ms. Best GFLOPs: 1.1040
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #5: GFLOPs: 0.3902. Time: 15.4247 ms. Best GFLOPs: 1.1040
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #6: GFLOPs: 0.7523. Time: 7.9998 ms. Best GFLOPs: 1.1040
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #7: GFLOPs: 0.9402. Time: 6.4014 ms. Best GFLOPs: 1.1040
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #8: GFLOPs: 0.1736. Time: 34.6598 ms. Best GFLOPs: 1.1040
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #9: GFLOPs: 0.8929. Time: 6.7404 ms. Best GFLOPs: 1.1040
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #10: GFLOPs: 0.5340. Time: 11.2718 ms. Best GFLOPs: 1.1040
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 27, 27, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(6, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1):
                for i3_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(3, 2, 8, 9, 9):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_3_init)
                        oh = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 9 + i2_3_init)
                        ow = T.axis.spatial(27, i3_2_init * 9 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 27, 27, 4], "float32"], ["TENSOR", [8, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 8, 9, 9, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_3)
                        oh = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 9 + i2_3)
                        ow = T.axis.spatial(27, i3_2 * 9 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_2)
                        ic = T.axis.reduce(128, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 27, 27, 4], "float32"], ["TENSOR", [8, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 9, 27):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(8, ax1)
                        ax2_1 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 9 + ax2)
                        ax3_1 = T.axis.spatial(27, ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 1, 1, 9])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 3, 9])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 27, 27, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(6, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 3, 1, 2):
                for i1_2_init, i3_2_init, i4_2_init, i2_3_init in T.grid(4, 27, 2, 3):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 3 * 4 + i1_2_init)
                        oh = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 3 * 9 + i2_1 * 3 + i2_3_init)
                        ow = T.axis.spatial(27, i3_2_init)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 27, 27, 4], "float32"], ["TENSOR", [8, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 4, 1, 27, 2, 2, 1, 1, 1, 1, 3, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 3 * 4 + i1_2)
                        oh = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 3 * 9 + i2_1 * 3 + i2_3)
                        ow = T.axis.spatial(27, i3_2)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(128, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 27, 27, 4], "float32"], ["TENSOR", [8, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 9, 27):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 3 * 4 + ax1)
                        ax2_1 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 3 * 9 + ax2)
                        ax3_1, ax4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 3, 1, 3])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 27, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #13: GFLOPs: 1.0804. Time: 5.5710 ms. Best GFLOPs: 1.1040
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #14: GFLOPs: 0.3473. Time: 17.3315 ms. Best GFLOPs: 1.1040
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 27, 27, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(18, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 9, 1):
                for i2_2_init, i1_3_init, i3_3_init in T.grid(3, 2, 3):
                    for i4_3_fused_init in T.vectorized(4):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 9 * 4 + i1_1 * 2 + i1_3_init)
                            oh = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 9 * 3 + i2_2_init)
                            ow = T.axis.spatial(27, i3_1 * 3 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 27, 27, 4], "float32"], ["TENSOR", [8, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(4, 1, 1, 1, 1, 3, 1, 1, 32, 1, 1, 1, 2, 1, 3):
                    for i4_3_fused in T.vectorized(4):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 9 * 4 + i1_1 * 2 + i1_3)
                            oh = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 9 * 3 + i2_2)
                            ow = T.axis.spatial(27, i3_1 * 3 + i3_3)
                            oc_block = T.axis.spatial(4, i4_3_fused)
                            ic = T.axis.reduce(128, i5_0 * 32 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 27, 27, 4], "float32"], ["TENSOR", [8, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 3, 27):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 9 * 4 + ax1)
                        ax2_1 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 9 * 3 + ax2)
                        ax3_1, ax4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[9, 1, 3, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 9, 1, 3])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b103)
b126 = sch.decompose_reduction(block=b103, loop=l110)
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 27, 27, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(3, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i1_3_init, i3_3_init in T.grid(4, 9, 3, 2, 9):
                for i4_3_fused_init in T.vectorized(4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_2_init * 2 + i1_3_init)
                        oh = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 9 + i2_2_init)
                        ow = T.axis.spatial(27, i3_2_init * 9 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 27, 27, 4], "float32"], ["TENSOR", [8, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(32, 1, 1, 1, 4, 9, 3, 1, 4, 1, 1, 1, 2, 1, 9):
                for i4_3_fused in T.vectorized(4):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 9 + i2_2)
                        ow = T.axis.spatial(27, i3_2 * 9 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3_fused)
                        ic = T.axis.reduce(128, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 27, 27, 4], "float32"], ["TENSOR", [8, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(216, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(27):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(8, i0_i1_i2_fused // 27)
                        ax2 = T.axis.spatial(27, i0_i1_i2_fused % 27)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 3, 9, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 3, 9])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l104)
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #17: GFLOPs: 1.2386. Time: 4.8593 ms. Best GFLOPs: 1.2386
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #18: GFLOPs: 0.3688. Time: 16.3195 ms. Best GFLOPs: 1.2386
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #19: GFLOPs: 1.4472. Time: 4.1588 ms. Best GFLOPs: 1.4472
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #20: GFLOPs: 0.2119. Time: 28.4086 ms. Best GFLOPs: 1.4472
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #21: GFLOPs: 0.5742. Time: 10.4826 ms. Best GFLOPs: 1.4472
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #22: GFLOPs: 1.1951. Time: 5.0360 ms. Best GFLOPs: 1.4472
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #23: GFLOPs: 0.3822. Time: 15.7485 ms. Best GFLOPs: 1.4472
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #24: GFLOPs: 0.9050. Time: 6.6504 ms. Best GFLOPs: 1.4472
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #25: GFLOPs: 0.8239. Time: 7.3046 ms. Best GFLOPs: 1.4472
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #26: GFLOPs: 0.8919. Time: 6.7478 ms. Best GFLOPs: 1.4472
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #27: GFLOPs: 0.7897. Time: 7.6209 ms. Best GFLOPs: 1.4472
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #28: GFLOPs: 0.2772. Time: 21.7089 ms. Best GFLOPs: 1.4472
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 27, 27, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(6, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1):
                for i3_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(3, 4, 4, 9, 9):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 3 * 4 + i1_3_init)
                        oh = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 3 * 9 + i2_3_init)
                        ow = T.axis.spatial(27, i3_2_init * 9 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 27, 27, 4], "float32"], ["TENSOR", [8, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 1, 1, 1, 1, 3, 4, 32, 1, 1, 1, 4, 9, 9, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 3 * 4 + i1_3)
                        oh = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 3 * 9 + i2_3)
                        ow = T.axis.spatial(27, i3_2 * 9 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(128, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 27, 27, 4], "float32"], ["TENSOR", [8, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 9, 27):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 3 * 4 + ax1)
                        ax2_1 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 3 * 9 + ax2)
                        ax3_1, ax4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 1, 1, 9])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 3, 9])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[01:54:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 27, 27, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(24, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i2_3_init, i3_3_init in T.grid(2, 3, 3, 9, 3):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 12 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 12 // 6 * 2 + i1_2_init)
                        oh = T.axis.spatial(27, i2_2_init * 9 + i2_3_init)
                        ow = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 6 // 2 * 9 + i3_2_init * 3 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 27, 27, 4], "float32"], ["TENSOR", [8, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(128, 1, 1, 1, 2, 3, 3, 1, 1, 1, 1, 1, 1, 9, 3):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 12 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 12 // 6 * 2 + i1_2)
                        oh = T.axis.spatial(27, i2_2 * 9 + i2_3)
                        ow = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 6 // 2 * 9 + i3_2 * 3 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused)
                        ic = T.axis.reduce(128, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 27, 27, 4], "float32"], ["TENSOR", [8, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(216, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(27):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(8, i0_i1_i2_fused // 27)
                        ax2 = T.axis.spatial(27, i0_i1_i2_fused % 27)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 3, 9])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 3, 3, 3])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l104)
[01:54:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #31: GFLOPs: 0.1111. Time: 54.1945 ms. Best GFLOPs: 1.4472
[01:54:53] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |            
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                          fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 363
Total latency (us): 108026

[01:54:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #0: GFLOPs: 1.9630. Time: 6.1082 ms. Best GFLOPs: 1.9630
[01:54:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #1: GFLOPs: 0.8374. Time: 14.3188 ms. Best GFLOPs: 1.9630
[01:54:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #2: GFLOPs: 1.2850. Time: 9.3313 ms. Best GFLOPs: 1.9630
[01:54:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #3: GFLOPs: 0.2723. Time: 44.0412 ms. Best GFLOPs: 1.9630
[01:54:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #4: GFLOPs: 1.8449. Time: 6.4994 ms. Best GFLOPs: 1.9630
[01:54:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #5: GFLOPs: 0.7114. Time: 16.8547 ms. Best GFLOPs: 1.9630
[01:54:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #6: GFLOPs: 1.2286. Time: 9.7599 ms. Best GFLOPs: 1.9630
[01:54:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(8, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 27, 27, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(24, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i4_2_init, i3_3_init in T.grid(2, 27, 2, 9):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 6 * 2 + i1_2_init)
                    oh = T.axis.spatial(27, i2_2_init)
                    ow = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 3 * 9 + i3_3_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 6 // 3 * 2 + i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 27, 27, 4], "float32"], ["TENSOR", [8, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 2, 27, 1, 2, 16, 1, 1, 1, 1, 1, 9, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 6 * 2 + i1_2)
                    oh = T.axis.spatial(27, i2_2)
                    ow = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 3 * 9 + i3_3)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 6 // 3 * 2 + i4_2)
                    ic = T.axis.reduce(256, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 27, 27, 4], "float32"], ["TENSOR", [8, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(216, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(27):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(8, i0_i1_i2_fused // 27)
                        ax2 = T.axis.spatial(27, i0_i1_i2_fused % 27)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 27, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 3, 1, 9])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #8: GFLOPs: 0.8925. Time: 13.4341 ms. Best GFLOPs: 1.9630
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #9: GFLOPs: 0.9430. Time: 12.7156 ms. Best GFLOPs: 1.9630
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #10: GFLOPs: 0.2429. Time: 49.3578 ms. Best GFLOPs: 1.9630
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #11: GFLOPs: 0.4575. Time: 26.2102 ms. Best GFLOPs: 1.9630
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #12: GFLOPs: 1.9645. Time: 6.1037 ms. Best GFLOPs: 1.9645
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #13: GFLOPs: 0.6119. Time: 19.5950 ms. Best GFLOPs: 1.9645
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #14: GFLOPs: 0.8050. Time: 14.8946 ms. Best GFLOPs: 1.9645
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #15: GFLOPs: 2.5534. Time: 4.6960 ms. Best GFLOPs: 2.5534
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #16: GFLOPs: 0.2569. Time: 46.6731 ms. Best GFLOPs: 2.5534
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #17: GFLOPs: 1.9026. Time: 6.3023 ms. Best GFLOPs: 2.5534
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #18: GFLOPs: 0.8674. Time: 13.8240 ms. Best GFLOPs: 2.5534
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #19: GFLOPs: 0.6422. Time: 18.6711 ms. Best GFLOPs: 2.5534
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #20: GFLOPs: 1.0315. Time: 11.6249 ms. Best GFLOPs: 2.5534
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #21: GFLOPs: 0.9410. Time: 12.7426 ms. Best GFLOPs: 2.5534
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #22: GFLOPs: 0.6423. Time: 18.6670 ms. Best GFLOPs: 2.5534
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #23: GFLOPs: 1.3922. Time: 8.6125 ms. Best GFLOPs: 2.5534
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #24: GFLOPs: 1.4705. Time: 8.1542 ms. Best GFLOPs: 2.5534
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #25: GFLOPs: 0.2811. Time: 42.6575 ms. Best GFLOPs: 2.5534
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #26: GFLOPs: 0.2204. Time: 54.4157 ms. Best GFLOPs: 2.5534
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(8, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 27, 27, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(12, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 2, 9, 27):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 4 + i1_2_init * 2 + i1_3_init)
                        oh = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 9 + i2_3_init)
                        ow = T.axis.spatial(27, i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 27, 27, 4], "float32"], ["TENSOR", [8, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(32, 1, 1, 1, 2, 1, 1, 1, 8, 1, 1, 1, 2, 9, 27):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 9 + i2_3)
                        ow = T.axis.spatial(27, i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 2 + i4_3_fused)
                        ic = T.axis.reduce(256, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 27, 27, 4], "float32"], ["TENSOR", [8, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(216, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(27):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(8, i0_i1_i2_fused // 27)
                        ax2 = T.axis.spatial(27, i0_i1_i2_fused % 27)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 1, 1, 9])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 27])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l104)
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #28: GFLOPs: 0.8691. Time: 13.7966 ms. Best GFLOPs: 2.5534
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #29: GFLOPs: 1.1275. Time: 10.6345 ms. Best GFLOPs: 2.5534
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #30: GFLOPs: 1.0578. Time: 11.3354 ms. Best GFLOPs: 2.5534
[01:54:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"] Trial #31: GFLOPs: 1.7251. Time: 6.9508 ms. Best GFLOPs: 2.5534
[01:55:24] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |            
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                          fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 395
Total latency (us): 112722

[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #0: GFLOPs: 0.4768. Time: 12.9163 ms. Best GFLOPs: 0.4768
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #1: GFLOPs: 0.7273. Time: 8.4675 ms. Best GFLOPs: 0.7273
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #2: GFLOPs: 0.6360. Time: 9.6831 ms. Best GFLOPs: 0.7273
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 27, 27, 4), "float32"], placeholder_1: T.Buffer[(32, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 27, 27, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(144, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(2, 3, 9, 2, 2, 3):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 72 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 24 // 6 * 4 + i1_2_init * 2 + i1_3_init)
                    oh = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 72 // 24 * 9 + i2_2_init * 3 + i2_3_init)
                    ow = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 6 // 2 * 9 + i3_2_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 27, 27, 4], "float32"], ["TENSOR", [32, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 2, 3, 9, 2, 2, 1, 1, 1, 2, 3, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 72 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 24 // 6 * 4 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 72 // 24 * 9 + i2_2 * 3 + i2_3)
                    ow = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 6 // 2 * 9 + i3_2)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2)
                    ic = T.axis.reduce(32, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 27, 27, 4], "float32"], ["TENSOR", [32, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(864, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(27):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(32, i0_i1_i2_fused // 27)
                        ax2 = T.axis.spatial(27, i0_i1_i2_fused % 27)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 1, 3, 3])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 3, 9, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #4: GFLOPs: 0.6085. Time: 10.1217 ms. Best GFLOPs: 0.7273
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #5: GFLOPs: 0.9147. Time: 6.7328 ms. Best GFLOPs: 0.9147
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #6: GFLOPs: 1.0618. Time: 5.7999 ms. Best GFLOPs: 1.0618
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #7: GFLOPs: 0.5269. Time: 11.6884 ms. Best GFLOPs: 1.0618
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #8: GFLOPs: 0.7700. Time: 7.9986 ms. Best GFLOPs: 1.0618
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #9: GFLOPs: 0.3923. Time: 15.6975 ms. Best GFLOPs: 1.0618
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #10: GFLOPs: 0.9307. Time: 6.6172 ms. Best GFLOPs: 1.0618
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #11: GFLOPs: 0.6898. Time: 8.9279 ms. Best GFLOPs: 1.0618
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #12: GFLOPs: 1.2832. Time: 4.7993 ms. Best GFLOPs: 1.2832
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #13: GFLOPs: 0.9436. Time: 6.5266 ms. Best GFLOPs: 1.2832
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #14: GFLOPs: 0.6544. Time: 9.4108 ms. Best GFLOPs: 1.2832
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #15: GFLOPs: 0.2688. Time: 22.9084 ms. Best GFLOPs: 1.2832
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #16: GFLOPs: 0.9241. Time: 6.6641 ms. Best GFLOPs: 1.2832
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #17: GFLOPs: 0.2114. Time: 29.1343 ms. Best GFLOPs: 1.2832
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #18: GFLOPs: 0.2972. Time: 20.7252 ms. Best GFLOPs: 1.2832
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #19: GFLOPs: 0.4247. Time: 14.5007 ms. Best GFLOPs: 1.2832
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #20: GFLOPs: 0.4682. Time: 13.1526 ms. Best GFLOPs: 1.2832
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #21: GFLOPs: 0.4376. Time: 14.0739 ms. Best GFLOPs: 1.2832
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #22: GFLOPs: 1.0168. Time: 6.0570 ms. Best GFLOPs: 1.2832
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #23: GFLOPs: 0.5329. Time: 11.5570 ms. Best GFLOPs: 1.2832
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #24: GFLOPs: 0.0722. Time: 85.2992 ms. Best GFLOPs: 1.2832
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #25: GFLOPs: 0.2238. Time: 27.5175 ms. Best GFLOPs: 1.2832
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #26: GFLOPs: 1.0779. Time: 5.7137 ms. Best GFLOPs: 1.2832
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #27: GFLOPs: 1.1923. Time: 5.1653 ms. Best GFLOPs: 1.2832
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #28: GFLOPs: 0.5319. Time: 11.5792 ms. Best GFLOPs: 1.2832
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #29: GFLOPs: 0.7023. Time: 8.7690 ms. Best GFLOPs: 1.2832
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #30: GFLOPs: 1.2034. Time: 5.1176 ms. Best GFLOPs: 1.2832
[01:55:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"] Trial #31: GFLOPs: 1.1395. Time: 5.4047 ms. Best GFLOPs: 1.2832
[01:56:04] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |            
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |            
 15 |                          fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 427
Total latency (us): 122321

[01:56:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_concatenate_1"] Trial #0: GFLOPs: 0.0000. Time: 6.0603 ms. Best GFLOPs: 0.0000
[01:56:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_concatenate_1"] Trial #1: GFLOPs: 0.0000. Time: 17.6459 ms. Best GFLOPs: 0.0000
[01:56:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_concatenate_1"] Trial #2: GFLOPs: 0.0000. Time: 3.2259 ms. Best GFLOPs: 0.0000
[01:56:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_concatenate_1"] Trial #3: GFLOPs: 0.0000. Time: 7.7700 ms. Best GFLOPs: 0.0000
[01:56:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_concatenate_1"] Trial #4: GFLOPs: 0.0000. Time: 7.1992 ms. Best GFLOPs: 0.0000
[01:56:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_concatenate_1"] Trial #5: GFLOPs: 0.0000. Time: 6.0949 ms. Best GFLOPs: 0.0000
[01:56:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_concatenate_1"] Trial #6: GFLOPs: 0.0000. Time: 5.5166 ms. Best GFLOPs: 0.0000
[01:56:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_concatenate_1"] Trial #7: GFLOPs: 0.0000. Time: 4.1061 ms. Best GFLOPs: 0.0000
[01:56:38] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_concatenate_1"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |            
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |            
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 435
Total latency (us): 128773

[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #0: GFLOPs: 0.0748. Time: 5.2087 ms. Best GFLOPs: 0.0748
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #1: GFLOPs: 0.0869. Time: 4.4804 ms. Best GFLOPs: 0.0869
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #2: GFLOPs: 0.0514. Time: 7.5791 ms. Best GFLOPs: 0.0869
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #3: GFLOPs: 0.1394. Time: 2.7926 ms. Best GFLOPs: 0.1394
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #4: GFLOPs: 0.0526. Time: 7.4001 ms. Best GFLOPs: 0.1394
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #5: GFLOPs: 0.0490. Time: 7.9433 ms. Best GFLOPs: 0.1394
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #6: GFLOPs: 0.0517. Time: 7.5379 ms. Best GFLOPs: 0.1394
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #7: GFLOPs: 0.0396. Time: 9.8368 ms. Best GFLOPs: 0.1394
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #8: GFLOPs: 0.0584. Time: 6.6709 ms. Best GFLOPs: 0.1394
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #9: GFLOPs: 0.0351. Time: 11.1030 ms. Best GFLOPs: 0.1394
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #10: GFLOPs: 0.0401. Time: 9.7130 ms. Best GFLOPs: 0.1394
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #11: GFLOPs: 0.1593. Time: 2.4436 ms. Best GFLOPs: 0.1593
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #12: GFLOPs: 0.0762. Time: 5.1098 ms. Best GFLOPs: 0.1593
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #13: GFLOPs: 0.0274. Time: 14.2006 ms. Best GFLOPs: 0.1593
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #14: GFLOPs: 0.0557. Time: 6.9962 ms. Best GFLOPs: 0.1593
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #15: GFLOPs: 0.0335. Time: 11.6338 ms. Best GFLOPs: 0.1593
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #16: GFLOPs: 0.0440. Time: 8.8483 ms. Best GFLOPs: 0.1593
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #17: GFLOPs: 0.0452. Time: 8.6190 ms. Best GFLOPs: 0.1593
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_max_pool2d_2"] Trial #18: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 27, 27, 4), "float32"], tensor: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        for i0_i1_fused in T.parallel(64, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2, i3, i4 in T.grid(13, 13, 4):
                with T.block("tensor_init"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                    T.reads()
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                for i5, i6 in T.grid(3, 3):
                    with T.block("tensor_update"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSRR", [i0_i1_fused, i2, i3, i4, i5, i6])
                        T.reads(tensor[ax0, ax1, ax2, ax3, ax4], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                        T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
sch.enter_postproc()
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit")
b3, = sch.get_child_blocks(b2)
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b3)
l11 = sch.fuse(l4, l5)
sch.parallel(loop=l11)
sch.annotate(block_or_loop=l11, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l11, ann_key="pragma_unroll_explicit", ann_val=1)
b12 = sch.get_block(name="tensor", func_name="main")
l13, l14, l15, l16, l17, l18 = sch.get_loops(block=b12)
b19 = sch.decompose_reduction(block=b12, loop=l17)
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #19: GFLOPs: 0.1947. Time: 1.9995 ms. Best GFLOPs: 0.1947
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_max_pool2d_2"] Trial #20: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 27, 27, 4), "float32"], tensor: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        tensor_rf = T.alloc_buffer([1, 64, 13, 13, 4, 1], dtype="float32")
        for i0_i1_i2_fused in T.parallel(832, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i3, i4 in T.grid(13, 4):
                with T.block("tensor_rf_init"):
                    vi5_i6_fused_1 = T.axis.spatial(1, 0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(64, i0_i1_i2_fused // 13)
                    ax2 = T.axis.spatial(13, i0_i1_i2_fused % 13)
                    ax3, ax4 = T.axis.remap("SS", [i3, i4])
                    T.reads()
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.float32(-3.4028234663852886e+38)
                for i5_i6_fused_0, i5_i6_fused_1 in T.grid(9, 1):
                    with T.block("tensor_rf_update"):
                        vi5_i6_fused_1 = T.axis.spatial(1, 0)
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(64, i0_i1_i2_fused // 13)
                        ax2 = T.axis.spatial(13, i0_i1_i2_fused % 13)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4])
                        rv0 = T.axis.reduce(3, i5_i6_fused_0 // 3)
                        rv1 = T.axis.reduce(3, i5_i6_fused_0 % 3)
                        T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
        for i0_i1_i2_fused in T.parallel(832, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i3, i4, i5_i6_fused_1 in T.grid(13, 4, 1):
                with T.block("tensor"):
                    vi5_i6_fused_1 = T.axis.reduce(1, 0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(64, i0_i1_i2_fused // 13)
                    ax2 = T.axis.spatial(13, i0_i1_i2_fused % 13)
                    ax3, ax4 = T.axis.remap("SS", [i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[9, 1])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
sch.enter_postproc()
b16 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.unroll_explicit")
b17, b18 = sch.get_child_blocks(b16)
l19, l20, l21, l22, l23, l24, l25 = sch.get_loops(block=b17)
l26 = sch.fuse(l19, l20, l21)
sch.parallel(loop=l26)
sch.annotate(block_or_loop=l26, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l26, ann_key="pragma_unroll_explicit", ann_val=1)
l27, l28, l29, l30, l31, l32 = sch.get_loops(block=b18)
l33 = sch.fuse(l27, l28, l29)
sch.parallel(loop=l33)
sch.annotate(block_or_loop=l33, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l33, ann_key="pragma_unroll_explicit", ann_val=1)
b34 = sch.get_block(name="tensor_rf", func_name="main")
l35, l36, l37, l38, l39 = sch.get_loops(block=b34)
b40 = sch.decompose_reduction(block=b34, loop=l38)
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #21: GFLOPs: 0.0476. Time: 8.1820 ms. Best GFLOPs: 0.1947
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #22: GFLOPs: 0.0344. Time: 11.3316 ms. Best GFLOPs: 0.1947
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #23: GFLOPs: 0.0932. Time: 4.1788 ms. Best GFLOPs: 0.1947
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #24: GFLOPs: 0.0238. Time: 16.3580 ms. Best GFLOPs: 0.1947
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #25: GFLOPs: 0.0525. Time: 7.4130 ms. Best GFLOPs: 0.1947
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #26: GFLOPs: 0.0816. Time: 4.7689 ms. Best GFLOPs: 0.1947
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #27: GFLOPs: 0.0763. Time: 5.1034 ms. Best GFLOPs: 0.1947
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #28: GFLOPs: 0.0385. Time: 10.1030 ms. Best GFLOPs: 0.1947
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #29: GFLOPs: 0.0506. Time: 7.6918 ms. Best GFLOPs: 0.1947
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #30: GFLOPs: 0.0698. Time: 5.5796 ms. Best GFLOPs: 0.1947
[01:56:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_max_pool2d_2"] Trial #31: GFLOPs: 0.0442. Time: 8.7995 ms. Best GFLOPs: 0.1947
[01:57:22] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #16: "fused_nn_max_pool2d_2"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |            
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |            
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 467
Total latency (us): 130772

[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #0: GFLOPs: 0.3017. Time: 13.8200 ms. Best GFLOPs: 0.3017
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #1: GFLOPs: 0.2317. Time: 17.9983 ms. Best GFLOPs: 0.3017
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #2: GFLOPs: 0.4633. Time: 8.9988 ms. Best GFLOPs: 0.4633
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #3: GFLOPs: 0.1250. Time: 33.3689 ms. Best GFLOPs: 0.4633
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #4: GFLOPs: 0.3221. Time: 12.9464 ms. Best GFLOPs: 0.4633
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #5: GFLOPs: 0.3954. Time: 10.5443 ms. Best GFLOPs: 0.4633
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #6: GFLOPs: 0.1113. Time: 37.4554 ms. Best GFLOPs: 0.4633
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #7: GFLOPs: 0.4103. Time: 10.1619 ms. Best GFLOPs: 0.4633
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #8: GFLOPs: 0.2301. Time: 18.1168 ms. Best GFLOPs: 0.4633
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #9: GFLOPs: 0.6945. Time: 6.0039 ms. Best GFLOPs: 0.6945
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #10: GFLOPs: 14.3699. Time: 0.2902 ms. Best GFLOPs: 14.3699
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(12, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 12, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 12, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 12, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(26, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_2_init, i4_2_init, i1_3_init in T.grid(13, 4, 6):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(12, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 13 * 6 + i1_3_init)
                    oh = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 13)
                    ow, oc_block = T.axis.remap("SS", [i3_2_init, i4_2_init])
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 13, 13, 4], "float32"], ["TENSOR", [12, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 1, 1, 13, 4, 16, 1, 1, 1, 6, 1, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(12, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 13 * 6 + i1_3)
                    oh = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 13)
                    ow, oc_block = T.axis.remap("SS", [i3_2, i4_2])
                    ic = T.axis.reduce(256, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 13, 13, 4], "float32"], ["TENSOR", [12, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(156, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(52):
                with T.block("T_relu"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(12, i0_i1_i2_fused // 13)
                    ax2 = T.axis.spatial(13, i0_i1_i2_fused % 13)
                    ax3 = T.axis.spatial(13, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 6])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l97, l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #12: GFLOPs: 0.2606. Time: 15.9969 ms. Best GFLOPs: 14.3699
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #13: GFLOPs: 0.5212. Time: 8.0004 ms. Best GFLOPs: 14.3699
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #14: GFLOPs: 1.0039. Time: 4.1535 ms. Best GFLOPs: 14.3699
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #15: GFLOPs: 0.4107. Time: 10.1534 ms. Best GFLOPs: 14.3699
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #16: GFLOPs: 0.2978. Time: 14.0009 ms. Best GFLOPs: 14.3699
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #17: GFLOPs: 0.3012. Time: 13.8446 ms. Best GFLOPs: 14.3699
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #18: GFLOPs: 0.3475. Time: 11.9971 ms. Best GFLOPs: 14.3699
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #19: GFLOPs: 0.5941. Time: 7.0185 ms. Best GFLOPs: 14.3699
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #20: GFLOPs: 0.6073. Time: 6.8657 ms. Best GFLOPs: 14.3699
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #21: GFLOPs: 0.7089. Time: 5.8815 ms. Best GFLOPs: 14.3699
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #22: GFLOPs: 0.8623. Time: 4.8357 ms. Best GFLOPs: 14.3699
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #23: GFLOPs: 0.5426. Time: 7.6841 ms. Best GFLOPs: 14.3699
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #24: GFLOPs: 0.1277. Time: 32.6630 ms. Best GFLOPs: 14.3699
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #25: GFLOPs: 0.8987. Time: 4.6393 ms. Best GFLOPs: 14.3699
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #26: GFLOPs: 0.5479. Time: 7.6101 ms. Best GFLOPs: 14.3699
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #27: GFLOPs: 0.4832. Time: 8.6298 ms. Best GFLOPs: 14.3699
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #28: GFLOPs: 0.7620. Time: 5.4716 ms. Best GFLOPs: 14.3699
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #29: GFLOPs: 0.4219. Time: 9.8827 ms. Best GFLOPs: 14.3699
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #30: GFLOPs: 0.6373. Time: 6.5426 ms. Best GFLOPs: 14.3699
[01:57:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"] Trial #31: GFLOPs: 0.2647. Time: 15.7494 ms. Best GFLOPs: 14.3699
[01:58:10] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |            
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |            
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 499
Total latency (us): 131062

[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #0: GFLOPs: 0.5582. Time: 11.1898 ms. Best GFLOPs: 0.5582
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #1: GFLOPs: 0.6608. Time: 9.4529 ms. Best GFLOPs: 0.6608
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #2: GFLOPs: 1.8890. Time: 3.3066 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #3: GFLOPs: 1.2515. Time: 4.9912 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #4: GFLOPs: 0.5448. Time: 11.4651 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #5: GFLOPs: 0.7426. Time: 8.4109 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #6: GFLOPs: 0.4642. Time: 13.4554 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #7: GFLOPs: 0.7666. Time: 8.1478 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #8: GFLOPs: 1.0773. Time: 5.7980 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #9: GFLOPs: 1.0853. Time: 5.7552 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #10: GFLOPs: 0.2419. Time: 25.8192 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #11: GFLOPs: 0.9250. Time: 6.7524 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #12: GFLOPs: 0.6875. Time: 9.0855 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #13: GFLOPs: 0.6422. Time: 9.7270 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #14: GFLOPs: 0.7249. Time: 8.6170 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #15: GFLOPs: 0.1616. Time: 38.6595 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #16: GFLOPs: 0.3381. Time: 18.4731 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #17: GFLOPs: 0.1802. Time: 34.6595 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #18: GFLOPs: 0.3225. Time: 19.3705 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #19: GFLOPs: 0.6407. Time: 9.7492 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #20: GFLOPs: 1.5552. Time: 4.0164 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #21: GFLOPs: 0.7789. Time: 8.0195 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #22: GFLOPs: 1.1278. Time: 5.5383 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #23: GFLOPs: 0.8757. Time: 7.1332 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #24: GFLOPs: 1.1293. Time: 5.5313 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #25: GFLOPs: 0.2906. Time: 21.4944 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #26: GFLOPs: 0.4275. Time: 14.6126 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #27: GFLOPs: 0.5206. Time: 11.9992 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #28: GFLOPs: 0.9889. Time: 6.3163 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #29: GFLOPs: 0.6181. Time: 10.1049 ms. Best GFLOPs: 1.8890
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(12, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 12, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 12, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 12, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(156, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_1, i3_1, i4_1 in T.grid(1, 1, 4):
                for i2_3_init in T.serial(13):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(12, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 26 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 2)
                        oh = T.axis.spatial(13, i2_3_init)
                        ow = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 26 // 2)
                        oc_block = T.axis.spatial(4, i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 13, 13, 4], "float32"], ["TENSOR", [12, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 1, 1, 1, 1, 24, 1, 1, 1, 1, 13, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(12, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 26 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 2)
                        oh = T.axis.spatial(13, i2_3)
                        ow = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 26 // 2)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(384, i5_0 * 24 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 13, 13, 4], "float32"], ["TENSOR", [12, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(156, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(52):
                with T.block("T_relu"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(12, i0_i1_i2_fused // 13)
                    ax2 = T.axis.spatial(13, i0_i1_i2_fused % 13)
                    ax3 = T.axis.spatial(13, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[6, 2, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 24])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l97, l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b101)
b122 = sch.decompose_reduction(block=b101, loop=l106)
[01:58:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"] Trial #31: GFLOPs: 1.2664. Time: 4.9324 ms. Best GFLOPs: 1.8890
[01:58:56] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |            
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |            
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 531
Total latency (us): 134369

[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #0: GFLOPs: 0.5034. Time: 6.3162 ms. Best GFLOPs: 0.5034
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #1: GFLOPs: 0.3177. Time: 10.0097 ms. Best GFLOPs: 0.5034
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #2: GFLOPs: 0.4202. Time: 7.5675 ms. Best GFLOPs: 0.5034
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #3: GFLOPs: 0.4361. Time: 7.2922 ms. Best GFLOPs: 0.5034
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #4: GFLOPs: 0.2553. Time: 12.4552 ms. Best GFLOPs: 0.5034
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #5: GFLOPs: 0.5301. Time: 5.9986 ms. Best GFLOPs: 0.5301
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #6: GFLOPs: 0.1529. Time: 20.7953 ms. Best GFLOPs: 0.5301
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #7: GFLOPs: 0.5168. Time: 6.1532 ms. Best GFLOPs: 0.5301
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #8: GFLOPs: 0.5041. Time: 6.3078 ms. Best GFLOPs: 0.5301
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 12, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(48, 12, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 48, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(208, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i3_2_init, i1_3_init in T.grid(6, 13, 2):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 52 * 12 + i1_2_init * 2 + i1_3_init)
                    oh = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 52 // 4)
                    ow = T.axis.spatial(13, i3_2_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 12, 13, 13, 4], "float32"], ["TENSOR", [48, 12, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 1, 1, 6, 1, 13, 1, 12, 1, 1, 1, 2, 1, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 52 * 12 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 52 // 4)
                    ow = T.axis.spatial(13, i3_2)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    ic = T.axis.reduce(48, i5_0 * 12 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 12, 13, 13, 4], "float32"], ["TENSOR", [48, 12, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 12, 1, 13, 1):
                with T.block("T_relu"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 52 * 12 + ax1)
                    ax2_1 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 52 // 4)
                    ax3_1 = T.axis.spatial(13, ax3)
                    ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                    T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 6, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 12])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 12, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(48, 12, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 48, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(12, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1):
                for i1_2_init, i2_2_init, i3_2_init, i1_3_init in T.grid(8, 13, 13, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 16 + i1_2_init * 2 + i1_3_init)
                        oh, ow = T.axis.remap("SS", [i2_2_init, i3_2_init])
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 12, 13, 13, 4], "float32"], ["TENSOR", [48, 12, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(48, 1, 1, 1, 8, 13, 13, 1, 1, 1, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 16 + i1_2 * 2 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_2])
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                        ic = T.axis.reduce(48, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 12, 13, 13, 4], "float32"], ["TENSOR", [48, 12, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 13, 13, 1):
                with T.block("T_relu"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 16 + ax1)
                    ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                    ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                    T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                    T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[3, 1, 8, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[48, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b101)
b124 = sch.decompose_reduction(block=b101, loop=l108)
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #11: GFLOPs: 0.2446. Time: 12.9981 ms. Best GFLOPs: 0.5301
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #12: GFLOPs: 0.3016. Time: 10.5444 ms. Best GFLOPs: 0.5301
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #13: GFLOPs: 0.6361. Time: 4.9988 ms. Best GFLOPs: 0.6361
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #14: GFLOPs: 0.6585. Time: 4.8289 ms. Best GFLOPs: 0.6585
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #15: GFLOPs: 0.3742. Time: 8.4989 ms. Best GFLOPs: 0.6585
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #16: GFLOPs: 0.4398. Time: 7.2296 ms. Best GFLOPs: 0.6585
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #17: GFLOPs: 0.2364. Time: 13.4539 ms. Best GFLOPs: 0.6585
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #18: GFLOPs: 0.6640. Time: 4.7890 ms. Best GFLOPs: 0.6640
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #19: GFLOPs: 0.2840. Time: 11.1964 ms. Best GFLOPs: 0.6640
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #20: GFLOPs: 0.4872. Time: 6.5263 ms. Best GFLOPs: 0.6640
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #21: GFLOPs: 0.5657. Time: 5.6216 ms. Best GFLOPs: 0.6640
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #22: GFLOPs: 0.3221. Time: 9.8712 ms. Best GFLOPs: 0.6640
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #23: GFLOPs: 0.6079. Time: 5.2306 ms. Best GFLOPs: 0.6640
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #24: GFLOPs: 0.7106. Time: 4.4748 ms. Best GFLOPs: 0.7106
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #25: GFLOPs: 0.6890. Time: 4.6154 ms. Best GFLOPs: 0.7106
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #26: GFLOPs: 0.5595. Time: 5.6839 ms. Best GFLOPs: 0.7106
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #27: GFLOPs: 0.2637. Time: 12.0600 ms. Best GFLOPs: 0.7106
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #28: GFLOPs: 0.5587. Time: 5.6919 ms. Best GFLOPs: 0.7106
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #29: GFLOPs: 0.2948. Time: 10.7857 ms. Best GFLOPs: 0.7106
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #30: GFLOPs: 0.3626. Time: 8.7709 ms. Best GFLOPs: 0.7106
[01:59:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"] Trial #31: GFLOPs: 0.5620. Time: 5.6587 ms. Best GFLOPs: 0.7106
[01:59:47] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |            
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |            
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |            
 20 |                          fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 563
Total latency (us): 143319

[01:59:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_concatenate_2"] Trial #0: GFLOPs: 0.0000. Time: 6.5260 ms. Best GFLOPs: 0.0000
[01:59:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_concatenate_2"] Trial #1: GFLOPs: 0.0000. Time: 6.9444 ms. Best GFLOPs: 0.0000
[01:59:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_concatenate_2"] Trial #2: GFLOPs: 0.0000. Time: 1.9664 ms. Best GFLOPs: 0.0000
[01:59:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_concatenate_2"] Trial #3: GFLOPs: 0.0000. Time: 6.3446 ms. Best GFLOPs: 0.0000
[01:59:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_concatenate_2"] Trial #4: GFLOPs: 0.0000. Time: 6.5313 ms. Best GFLOPs: 0.0000
[01:59:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_concatenate_2"] Trial #5: GFLOPs: 0.0000. Time: 9.2501 ms. Best GFLOPs: 0.0000
[01:59:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_concatenate_2"] Trial #6: GFLOPs: 0.0000. Time: 45.3198 ms. Best GFLOPs: 0.0000
[01:59:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_concatenate_2"] Trial #7: GFLOPs: 0.0000. Time: 8.4843 ms. Best GFLOPs: 0.0000
[02:00:14] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #20: "fused_concatenate_2"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |            
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |            
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |            
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 571
Total latency (us): 147252

[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #0: GFLOPs: 1.2225. Time: 6.8127 ms. Best GFLOPs: 1.2225
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #1: GFLOPs: 0.8329. Time: 9.9995 ms. Best GFLOPs: 1.2225
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #2: GFLOPs: 1.5321. Time: 5.4360 ms. Best GFLOPs: 1.5321
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #3: GFLOPs: 1.0767. Time: 7.7347 ms. Best GFLOPs: 1.5321
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(16, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i4_2_init, i1_3_init, i3_3_init in T.grid(2, 13, 2, 2, 13):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 4 + i1_2_init * 2 + i1_3_init)
                        oh, ow = T.axis.remap("SS", [i2_2_init, i3_3_init])
                        oc_block = T.axis.spatial(4, i4_2_init * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 13, 13, 4], "float32"], ["TENSOR", [16, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(24, 1, 1, 1, 2, 13, 1, 2, 16, 1, 1, 1, 2, 1, 13):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 4 + i1_2 * 2 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_3])
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                        ic = T.axis.reduce(384, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 13, 13, 4], "float32"], ["TENSOR", [16, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 4, 13):
                for ax3_ax4_fused in T.vectorized(52):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 4 + ax1)
                        ax2_1 = T.axis.spatial(13, ax2)
                        ax3 = T.axis.spatial(13, ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[24, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l100, l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b103)
b121 = sch.decompose_reduction(block=b103, loop=l105)
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #5: GFLOPs: 15.4357. Time: 0.5396 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #6: GFLOPs: 1.0038. Time: 8.2964 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(16, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(26, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(4, 4, 2, 13):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 13 * 8 + i1_2_init * 2 + i1_3_init)
                    oh = T.axis.spatial(13, i2_3_init)
                    ow = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 13)
                    oc_block = T.axis.spatial(4, i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 13, 13, 4], "float32"], ["TENSOR", [16, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 4, 1, 1, 4, 12, 1, 1, 1, 2, 13, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 13 * 8 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(13, i2_3)
                    ow = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 13)
                    oc_block = T.axis.spatial(4, i4_2)
                    ic = T.axis.reduce(384, i5_0 * 12 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 13, 13, 4], "float32"], ["TENSOR", [16, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(208, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(52):
                with T.block("T_relu"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(16, i0_i1_i2_fused // 13)
                    ax2 = T.axis.spatial(13, i0_i1_i2_fused % 13)
                    ax3 = T.axis.spatial(13, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 12])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l97, l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #8: GFLOPs: 0.5952. Time: 13.9931 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #9: GFLOPs: 0.6463. Time: 12.8868 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #10: GFLOPs: 0.9893. Time: 8.4183 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #11: GFLOPs: 0.6848. Time: 12.1622 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #12: GFLOPs: 0.6602. Time: 12.6139 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #13: GFLOPs: 0.7902. Time: 10.5395 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #14: GFLOPs: 0.3874. Time: 21.4980 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #15: GFLOPs: 1.1638. Time: 7.1563 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #16: GFLOPs: 0.5026. Time: 16.5705 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #17: GFLOPs: 0.2734. Time: 30.4607 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #18: GFLOPs: 0.4030. Time: 20.6650 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #19: GFLOPs: 1.2787. Time: 6.5131 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #20: GFLOPs: 0.4049. Time: 20.5703 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #21: GFLOPs: 0.8061. Time: 10.3314 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #22: GFLOPs: 1.5424. Time: 5.3997 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #23: GFLOPs: 0.5904. Time: 14.1054 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #24: GFLOPs: 0.8444. Time: 9.8631 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #25: GFLOPs: 0.2342. Time: 35.5682 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #26: GFLOPs: 1.4408. Time: 5.7802 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #27: GFLOPs: 0.5586. Time: 14.9084 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #28: GFLOPs: 0.8330. Time: 9.9984 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #29: GFLOPs: 0.1984. Time: 41.9678 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #30: GFLOPs: 5.2265. Time: 1.5935 ms. Best GFLOPs: 15.4357
[02:00:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"] Trial #31: GFLOPs: 0.9081. Time: 9.1712 ms. Best GFLOPs: 15.4357
[02:01:09] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |            
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |            
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |            
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 603
Total latency (us): 147791

[02:01:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #0: GFLOPs: 0.7611. Time: 14.5810 ms. Best GFLOPs: 0.7611
[02:01:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #1: GFLOPs: 1.2683. Time: 8.7495 ms. Best GFLOPs: 1.2683
[02:01:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(16, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(13, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i4_2_init, i1_3_init in T.grid(2, 13, 4, 8):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i1_2_init * 8 + i1_3_init)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_2_init, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused, i4_2_init])
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [16, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 2, 13, 1, 4, 64, 1, 1, 1, 8, 1, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i1_2 * 8 + i1_3)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_2, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused, i4_2])
                    ic = T.axis.reduce(512, i5_0 * 64 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [16, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(208, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(52):
                with T.block("T_relu"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(16, i0_i1_i2_fused // 13)
                    ax2 = T.axis.spatial(13, i0_i1_i2_fused % 13)
                    ax3 = T.axis.spatial(13, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 2, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l97, l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[02:01:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #3: GFLOPs: 1.5308. Time: 7.2493 ms. Best GFLOPs: 1.5308
[02:01:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #4: GFLOPs: 0.8125. Time: 13.6587 ms. Best GFLOPs: 1.5308
[02:01:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #5: GFLOPs: 1.1429. Time: 9.7093 ms. Best GFLOPs: 1.5308
[02:01:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #6: GFLOPs: 2.3149. Time: 4.7937 ms. Best GFLOPs: 2.3149
[02:01:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #7: GFLOPs: 0.8401. Time: 13.2093 ms. Best GFLOPs: 2.3149
[02:01:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #8: GFLOPs: 1.0134. Time: 10.9504 ms. Best GFLOPs: 2.3149
[02:01:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #9: GFLOPs: 1.7254. Time: 6.4318 ms. Best GFLOPs: 2.3149
[02:01:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #10: GFLOPs: 1.4255. Time: 7.7847 ms. Best GFLOPs: 2.3149
[02:01:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(16, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_2_init, i1_3_init, i2_3_init in T.grid(13, 2, 13):
                for i3_3_i4_3_fused_init in T.vectorized(4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 2 + i1_3_init)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_3_init, i3_2_init, i3_3_i4_3_fused_init])
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [16, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(32, 1, 1, 1, 1, 1, 13, 1, 16, 1, 1, 1, 2, 13):
                for i3_3_i4_3_fused in T.vectorized(4):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 2 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_3, i3_2, i3_3_i4_3_fused])
                        ic = T.axis.reduce(512, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [16, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 2, 13):
                for ax3_ax4_fused in T.vectorized(52):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 2 + ax1)
                        ax2_1 = T.axis.spatial(13, ax2)
                        ax3 = T.axis.spatial(13, ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)
sch.parallel(loop=l94)
l95 = sch.fuse(l92, l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l100, l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b103)
b120 = sch.decompose_reduction(block=b103, loop=l105)
[02:01:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #12: GFLOPs: 0.9465. Time: 11.7246 ms. Best GFLOPs: 2.3149
[02:01:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #13: GFLOPs: 0.8801. Time: 12.6084 ms. Best GFLOPs: 2.3149
[02:01:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #14: GFLOPs: 2.2622. Time: 4.9056 ms. Best GFLOPs: 2.3149
[02:01:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #15: GFLOPs: 0.6571. Time: 16.8874 ms. Best GFLOPs: 2.3149
[02:01:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #16: GFLOPs: 0.1711. Time: 64.8606 ms. Best GFLOPs: 2.3149
[02:01:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(16, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_3_init, i2_3_init, i3_3_init in T.grid(4, 13, 13):
                for i4_3_fused_init in T.vectorized(4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 4 + i1_3_init)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_3_init, i3_3_init, i4_3_fused_init])
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [16, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(32, 1, 1, 1, 1, 1, 1, 1, 16, 1, 1, 1, 4, 13, 13):
                for i4_3_fused in T.vectorized(4):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 4 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_3, i3_3, i4_3_fused])
                        ic = T.axis.reduce(512, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [16, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(208, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(52):
                with T.block("T_relu"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(16, i0_i1_i2_fused // 13)
                    ax2 = T.axis.spatial(13, i0_i1_i2_fused % 13)
                    ax3 = T.axis.spatial(13, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l98, l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l104)
[02:01:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #18: GFLOPs: 0.6441. Time: 17.2303 ms. Best GFLOPs: 2.3149
[02:01:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #19: GFLOPs: 3.7770. Time: 2.9381 ms. Best GFLOPs: 3.7770
[02:01:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #20: GFLOPs: 0.6326. Time: 17.5427 ms. Best GFLOPs: 3.7770
[02:01:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #21: GFLOPs: 1.0340. Time: 10.7325 ms. Best GFLOPs: 3.7770
[02:01:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #22: GFLOPs: 1.4013. Time: 7.9190 ms. Best GFLOPs: 3.7770
[02:01:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #23: GFLOPs: 1.1267. Time: 9.8496 ms. Best GFLOPs: 3.7770
[02:01:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #24: GFLOPs: 2.2482. Time: 4.9360 ms. Best GFLOPs: 3.7770
[02:01:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(16, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(104, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i1_3_init in T.grid(2, 13, 4):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 8 + i1_2_init * 4 + i1_3_init)
                    oh = T.axis.spatial(13, i2_2_init)
                    ow = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [16, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 2, 13, 1, 1, 64, 1, 1, 1, 4, 1, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 8 + i1_2 * 4 + i1_3)
                    oh = T.axis.spatial(13, i2_2)
                    ow = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2)
                    ic = T.axis.reduce(512, i5_0 * 64 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [16, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(208, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(52):
                with T.block("T_relu"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(16, i0_i1_i2_fused // 13)
                    ax2 = T.axis.spatial(13, i0_i1_i2_fused % 13)
                    ax3 = T.axis.spatial(13, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l97, l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[02:01:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #26: GFLOPs: 3.7309. Time: 2.9744 ms. Best GFLOPs: 3.7770
[02:01:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #27: GFLOPs: 0.5869. Time: 18.9070 ms. Best GFLOPs: 3.7770
[02:01:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #28: GFLOPs: 0.5314. Time: 20.8842 ms. Best GFLOPs: 3.7770
[02:01:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(16, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 13, 13, 4], dtype="float32")
        for i0_0 in T.serial(1, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 1, 13, 4):
                    for i1_2_init, i2_2_init in T.grid(2, 13):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_1 * 2 + i1_2_init)
                            oh, ow, oc_block = T.axis.remap("SSS", [i2_2_init, i3_1, i4_1])
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [16, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 2, 13, 1, 1, 32, 1, 1, 1, 1, 1, 1, 1):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_1 * 2 + i1_2)
                            oh, ow, oc_block = T.axis.remap("SSS", [i2_2, i3_1, i4_1])
                            ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [16, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2 in T.grid(1, 16, 13):
                    for ax3_ax4_fused in T.vectorized(52):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1, ax2_1 = T.axis.remap("SS", [ax1, ax2])
                            ax3 = T.axis.spatial(13, ax3_ax4_fused // 4)
                            ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
sch.annotate(block_or_loop=l68, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l68, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b67)
l104 = sch.fuse(l102, l103)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b105)
b132 = sch.decompose_reduction(block=b105, loop=l116)
[02:01:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #30: GFLOPs: 2.7699. Time: 4.0064 ms. Best GFLOPs: 3.7770
[02:01:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"] Trial #31: GFLOPs: 4.6353. Time: 2.3941 ms. Best GFLOPs: 4.6353
[02:01:49] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |            
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |            
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |            
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 635
Total latency (us): 150185

[02:01:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 13, 1, 1):
                for i1_2_init, i4_2_init, i3_3_init in T.grid(2, 2, 13):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 16 + i1_1 * 2 + i1_2_init)
                            oh, ow = T.axis.remap("SS", [i2_1, i3_3_init])
                            oc_block = T.axis.spatial(4, i4_2_init * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(8, 1, 1, 1, 2, 1, 1, 2, 8, 1, 1, 1, 1, 1, 13):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 16 + i1_1 * 2 + i1_2)
                            oh, ow = T.axis.remap("SS", [i2_1, i3_3])
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                            ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 16, 13):
                for ax3_ax4_fused in T.vectorized(52):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 16 + ax1)
                        ax2_1 = T.axis.spatial(13, ax2)
                        ax3 = T.axis.spatial(13, ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 8, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l100, l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b103)
b126 = sch.decompose_reduction(block=b103, loop=l110)
[02:01:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #1: GFLOPs: 1.4129. Time: 3.9806 ms. Best GFLOPs: 1.4129
[02:01:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #2: GFLOPs: 0.8445. Time: 6.6602 ms. Best GFLOPs: 1.4129
[02:01:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #3: GFLOPs: 0.6528. Time: 8.6151 ms. Best GFLOPs: 1.4129
[02:01:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #4: GFLOPs: 0.1103. Time: 50.9957 ms. Best GFLOPs: 1.4129
[02:01:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #5: GFLOPs: 0.7212. Time: 7.7989 ms. Best GFLOPs: 1.4129
[02:01:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #6: GFLOPs: 1.0784. Time: 5.2157 ms. Best GFLOPs: 1.4129
[02:01:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #7: GFLOPs: 0.3223. Time: 17.4533 ms. Best GFLOPs: 1.4129
[02:01:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #8: GFLOPs: 0.7571. Time: 7.4283 ms. Best GFLOPs: 1.4129
[02:01:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(104, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 1, 1, 1):
                for i4_2_init, i3_3_init in T.grid(2, 13):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 13 * 8 + i1_1)
                            oh = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 13)
                            ow = T.axis.spatial(13, i3_3_init)
                            oc_block = T.axis.spatial(4, i4_2_init * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(8, 1, 1, 1, 1, 1, 1, 2, 8, 1, 1, 1, 1, 1, 13):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 13 * 8 + i1_1)
                            oh = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 13)
                            ow = T.axis.spatial(13, i3_3)
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                            ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1 in T.grid(1, 8):
                for ax2_ax3_ax4_fused in T.vectorized(52):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 13 * 8 + ax1)
                        ax2 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 13)
                        ax3 = T.axis.spatial(13, ax2_ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax2_ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 8, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l99, l100, l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b103)
b126 = sch.decompose_reduction(block=b103, loop=l110)
[02:01:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #10: GFLOPs: 0.8437. Time: 6.6662 ms. Best GFLOPs: 1.4129
[02:01:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #11: GFLOPs: 1.0312. Time: 5.4542 ms. Best GFLOPs: 1.4129
[02:01:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #12: GFLOPs: 0.4922. Time: 11.4268 ms. Best GFLOPs: 1.4129
[02:01:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #13: GFLOPs: 0.7422. Time: 7.5781 ms. Best GFLOPs: 1.4129
[02:01:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #14: GFLOPs: 0.9766. Time: 5.7591 ms. Best GFLOPs: 1.4129
[02:01:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #15: GFLOPs: 0.9590. Time: 5.8645 ms. Best GFLOPs: 1.4129
[02:01:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(8, 2, 13, 13):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 16 + i1_2_init * 2 + i1_3_init)
                        oh, ow = T.axis.remap("SS", [i2_3_init, i3_3_init])
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(8, 1, 1, 1, 8, 1, 1, 1, 8, 1, 1, 1, 2, 13, 13):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 16 + i1_2 * 2 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_3])
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 2 + i4_3_fused)
                        ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(832, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(52):
                with T.block("T_relu"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(64, i0_i1_i2_fused // 13)
                    ax2 = T.axis.spatial(13, i0_i1_i2_fused % 13)
                    ax3 = T.axis.spatial(13, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 8, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l98, l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l104)
[02:01:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #17: GFLOPs: 0.6304. Time: 8.9223 ms. Best GFLOPs: 1.4129
[02:01:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #18: GFLOPs: 5.6802. Time: 0.9902 ms. Best GFLOPs: 5.6802
[02:01:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #19: GFLOPs: 0.4871. Time: 11.5459 ms. Best GFLOPs: 5.6802
[02:01:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(1664, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_1, i3_1, i4_1 in T.grid(1, 1, 1):
                for i1_2_init, i3_2_init in T.grid(2, 13):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 32 * 2 + i1_2_init)
                        oh = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 128)
                        ow = T.axis.spatial(13, i3_2_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 128 // 32)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 1, 1, 2, 1, 13, 1, 16, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 32 * 2 + i1_2)
                        oh = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 128)
                        ow = T.axis.spatial(13, i3_2)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 128 // 32)
                        ic = T.axis.reduce(64, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(832, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(52):
                with T.block("T_relu"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(64, i0_i1_i2_fused // 13)
                    ax2 = T.axis.spatial(13, i0_i1_i2_fused % 13)
                    ax3 = T.axis.spatial(13, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 32, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l97, l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b101)
b122 = sch.decompose_reduction(block=b101, loop=l106)
[02:01:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(208, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(2, 1, 1, 13, 1, 2):
                for i1_2_init in T.serial(4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 13 * 4 + i1_2_init)
                        oh = T.axis.spatial(13, i2_1)
                        ow = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 4, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 13 * 4 + i1_2)
                        oh = T.axis.spatial(13, i2_1)
                        ow = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(64, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 1, 1, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 13 * 4 + ax1)
                        ax2_1 = T.axis.spatial(13, i2_1)
                        ax3_1 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 1, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b67)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b107 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b107)
b131 = sch.decompose_reduction(block=b107, loop=l115)
[02:01:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #22: GFLOPs: 0.8948. Time: 6.2855 ms. Best GFLOPs: 5.6802
[02:01:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #23: GFLOPs: 0.4458. Time: 12.6154 ms. Best GFLOPs: 5.6802
[02:01:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(338, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 2, 1, 1, 1):
                for i1_2_init, i4_2_init, i1_3_init in T.grid(4, 4, 4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 169 * 32 + i1_1 * 16 + i1_2_init * 4 + i1_3_init)
                        oh = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13)
                        ow = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                        oc_block = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 1, 1, 4, 1, 1, 4, 16, 1, 1, 1, 4, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 169 * 32 + i1_1 * 16 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13)
                        ow = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(64, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 13, 13, 4], "float32"], ["TENSOR", [64, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(832, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(52):
                with T.block("T_relu"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(64, i0_i1_i2_fused // 13)
                    ax2 = T.axis.spatial(13, i0_i1_i2_fused % 13)
                    ax3 = T.axis.spatial(13, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 4, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l97, l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b101)
b125 = sch.decompose_reduction(block=b101, loop=l109)
[02:01:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #25: GFLOPs: 0.6809. Time: 8.2602 ms. Best GFLOPs: 5.6802
[02:01:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #26: GFLOPs: 1.1004. Time: 5.1110 ms. Best GFLOPs: 5.6802
[02:01:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #27: GFLOPs: 0.6657. Time: 8.4482 ms. Best GFLOPs: 5.6802
[02:01:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #28: GFLOPs: 0.8097. Time: 6.9465 ms. Best GFLOPs: 5.6802
[02:01:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #29: GFLOPs: 0.3246. Time: 17.3274 ms. Best GFLOPs: 5.6802
[02:01:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #30: GFLOPs: 0.4310. Time: 13.0507 ms. Best GFLOPs: 5.6802
[02:01:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"] Trial #31: GFLOPs: 0.5198. Time: 10.8211 ms. Best GFLOPs: 5.6802
[02:02:46] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |            
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |            
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |            
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |            
 24 |                          fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 667
Total latency (us): 152165

[02:02:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_concatenate_3"] Trial #0: GFLOPs: 0.0000. Time: 3.6028 ms. Best GFLOPs: 0.0000
[02:02:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_concatenate_3"] Trial #1: GFLOPs: 0.0000. Time: 4.7998 ms. Best GFLOPs: 0.0000
[02:02:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_concatenate_3"] Trial #2: GFLOPs: 0.0000. Time: 5.2799 ms. Best GFLOPs: 0.0000
[02:02:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_concatenate_3"] Trial #3: GFLOPs: 0.0000. Time: 2.3537 ms. Best GFLOPs: 0.0000
[02:02:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_concatenate_3"] Trial #4: GFLOPs: 0.0000. Time: 3.0493 ms. Best GFLOPs: 0.0000
[02:02:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #24: "fused_concatenate_3"] Trial #5: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(1, 64, 13, 13, 4), "float32"], T_concat: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        for i0_i1_fused in T.parallel(128, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i2 in T.serial(13):
                for i3_i4_fused in T.vectorized(52):
                    with T.block("T_concat"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2 = T.axis.remap("SS", [i0_i1_fused, i2])
                        ax3 = T.axis.spatial(13, i3_i4_fused // 4)
                        ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(placeholder_1[ax0, ax1 - 64, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                        T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                        T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(64 <= ax1, placeholder_1[ax0, ax1 - 64, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
sch.enter_postproc()
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit")
b3, = sch.get_child_blocks(b2)
l4, l5, l6, l7, l8 = sch.get_loops(block=b3)
l9 = sch.fuse(l4, l5)
sch.parallel(loop=l9)
l10 = sch.fuse(l7, l8)
sch.vectorize(loop=l10)
sch.annotate(block_or_loop=l9, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l9, ann_key="pragma_unroll_explicit", ann_val=1)
[02:02:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_concatenate_3"] Trial #6: GFLOPs: 0.0000. Time: 37.3175 ms. Best GFLOPs: 0.0000
[02:02:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_concatenate_3"] Trial #7: GFLOPs: 0.0000. Time: 9.5370 ms. Best GFLOPs: 0.0000
[02:03:35] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #24: "fused_concatenate_3"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |            
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |            
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |            
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |            
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 675
Total latency (us): 156873

[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #0: GFLOPs: 2.9377. Time: 59.0231 ms. Best GFLOPs: 2.9377
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #1: GFLOPs: 1.1212. Time: 154.6530 ms. Best GFLOPs: 2.9377
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #2: GFLOPs: 5.5361. Time: 31.3206 ms. Best GFLOPs: 5.5361
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #3: GFLOPs: 3.6165. Time: 47.9457 ms. Best GFLOPs: 5.5361
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #4: GFLOPs: 4.6488. Time: 37.2990 ms. Best GFLOPs: 5.5361
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #5: GFLOPs: 2.2045. Time: 78.6558 ms. Best GFLOPs: 5.5361
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #6: GFLOPs: 4.3812. Time: 39.5765 ms. Best GFLOPs: 5.5361
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #7: GFLOPs: 1.9130. Time: 90.6405 ms. Best GFLOPs: 5.5361
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #8: GFLOPs: 1.1873. Time: 146.0466 ms. Best GFLOPs: 5.5361
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #9: GFLOPs: 1.9185. Time: 90.3782 ms. Best GFLOPs: 5.5361
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #10: GFLOPs: 0.5042. Time: 343.9095 ms. Best GFLOPs: 5.5361
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #11: GFLOPs: 3.5512. Time: 48.8263 ms. Best GFLOPs: 5.5361
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #12: GFLOPs: 4.2874. Time: 40.4426 ms. Best GFLOPs: 5.5361
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #13: GFLOPs: 1.9240. Time: 90.1227 ms. Best GFLOPs: 5.5361
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #14: GFLOPs: 1.7114. Time: 101.3193 ms. Best GFLOPs: 5.5361
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #15: GFLOPs: 3.7163. Time: 46.6579 ms. Best GFLOPs: 5.5361
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #16: GFLOPs: 5.4197. Time: 31.9934 ms. Best GFLOPs: 5.5361
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #17: GFLOPs: 10.4695. Time: 16.5619 ms. Best GFLOPs: 10.4695
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #18: GFLOPs: 4.7317. Time: 36.6454 ms. Best GFLOPs: 10.4695
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #19: GFLOPs: 1.3196. Time: 131.3986 ms. Best GFLOPs: 10.4695
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #20: GFLOPs: 1.4359. Time: 120.7538 ms. Best GFLOPs: 10.4695
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #21: GFLOPs: 3.2180. Time: 53.8826 ms. Best GFLOPs: 10.4695
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #22: GFLOPs: 3.9411. Time: 43.9961 ms. Best GFLOPs: 10.4695
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #23: GFLOPs: 3.3397. Time: 51.9183 ms. Best GFLOPs: 10.4695
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #24: GFLOPs: 2.6544. Time: 65.3242 ms. Best GFLOPs: 10.4695
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(250, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 250, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 250, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 250, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(130, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_1, i3_1, i4_1 in T.grid(1, 1, 2):
                for i2_2_init, i1_3_init in T.grid(13, 25):
                    for i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(250, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 65 * 125 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 5 * 25 + i1_3_init)
                            oh = T.axis.spatial(13, i2_2_init)
                            ow = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 65 // 5)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [250, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3 in T.grid(64, 1, 1, 1, 1, 13, 1, 1, 8, 1, 1, 1, 25):
                    for i2_3_i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(250, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 65 * 125 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 5 * 25 + i1_3)
                            oh = T.axis.spatial(13, i2_2)
                            ow = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 65 // 5)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i2_3_i3_3_i4_3_fused)
                            ic = T.axis.reduce(512, i5_0 * 8 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [250, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2 in T.grid(1, 25, 13):
                    for ax3_ax4_fused in T.vectorized(2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(250, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 65 * 125 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 5 * 25 + ax1)
                            ax2_1 = T.axis.spatial(13, ax2)
                            ax3 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 65 // 5)
                            ax4 = T.axis.spatial(4, i4_1 * 2 + ax3_ax4_fused)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 5, 1, 25])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l94)
l95 = sch.fuse(l91, l92, l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b67)
l105 = sch.fuse(l103, l104)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b106)
b125 = sch.decompose_reduction(block=b106, loop=l111)
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #26: GFLOPs: 4.4079. Time: 39.3368 ms. Best GFLOPs: 10.4695
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #27: GFLOPs: 9.6352. Time: 17.9959 ms. Best GFLOPs: 10.4695
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #28: GFLOPs: 1.4763. Time: 117.4544 ms. Best GFLOPs: 10.4695
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #29: GFLOPs: 2.1679. Time: 79.9817 ms. Best GFLOPs: 10.4695
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #30: GFLOPs: 1.2627. Time: 137.3201 ms. Best GFLOPs: 10.4695
[02:03:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"] Trial #31: GFLOPs: 4.3234. Time: 40.1057 ms. Best GFLOPs: 10.4695
[02:04:25] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |            
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |            
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |            
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |            
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 707
Total latency (us): 173435

[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #0: GFLOPs: 0.0385. Time: 4.9361 ms. Best GFLOPs: 0.0385
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #1: GFLOPs: 0.0550. Time: 3.4569 ms. Best GFLOPs: 0.0550
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #2: GFLOPs: 0.0196. Time: 9.6893 ms. Best GFLOPs: 0.0550
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #3: GFLOPs: 0.0677. Time: 2.8059 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #4: GFLOPs: 0.0212. Time: 8.9471 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #5: GFLOPs: 0.0334. Time: 5.6919 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #6: GFLOPs: 0.0149. Time: 12.7492 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #7: GFLOPs: 0.0233. Time: 8.1671 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #8: GFLOPs: 0.0246. Time: 7.7327 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #9: GFLOPs: 0.0215. Time: 8.8396 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #10: GFLOPs: 0.0309. Time: 6.1530 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #11: GFLOPs: 0.0166. Time: 11.4408 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #12: GFLOPs: 0.0322. Time: 5.8936 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #13: GFLOPs: 0.0475. Time: 4.0011 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #14: GFLOPs: 0.0675. Time: 2.8149 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #15: GFLOPs: 0.0485. Time: 3.9168 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #16: GFLOPs: 0.0665. Time: 2.8571 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #17: GFLOPs: 0.0223. Time: 8.5393 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #18: GFLOPs: 0.0136. Time: 14.0004 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #19: GFLOPs: 0.0200. Time: 9.4986 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #20: GFLOPs: 0.0248. Time: 7.6710 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #26: "fused_nn_avg_pool2d"] Trial #21: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 250, 13, 13, 4), "float32"], tensor: T.Buffer[(1, 250, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 250, 1, 1, 4], dtype="float32")
        tensor_rf = T.alloc_buffer([1, 250, 1, 1, 4, 13], dtype="float32")
        for i0_i1_fused in T.parallel(250, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2, i3, i4 in T.grid(1, 1, 4):
                for i5_i6_fused_1_fused_init in T.vectorized(13):
                    with T.block("tensor_rf_init"):
                        vi5_i6_fused_1 = T.axis.spatial(13, i5_i6_fused_1_fused_init)
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(250, i0_i1_fused)
                        ax2 = T.axis.spatial(1, 0)
                        ax3 = T.axis.spatial(1, 0)
                        ax4 = T.axis.spatial(4, i4)
                        T.reads()
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.float32(0)
                for i5_i6_fused_0 in T.serial(13):
                    for i5_i6_fused_1_fused in T.vectorized(13):
                        with T.block("tensor_rf_update"):
                            vi5_i6_fused_1 = T.axis.spatial(13, i5_i6_fused_1_fused)
                            ax0 = T.axis.spatial(1, 0)
                            ax1 = T.axis.spatial(250, i0_i1_fused)
                            ax2 = T.axis.spatial(1, 0)
                            ax3 = T.axis.spatial(1, 0)
                            ax4, vi5_i6_fused_0 = T.axis.remap("SR", [i4, i5_i6_fused_0])
                            T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1], placeholder[ax0, ax1, ax2 * 13 + (vi5_i6_fused_0 * 13 + vi5_i6_fused_1) // 13, ax3 * 13 + (vi5_i6_fused_0 * 13 + vi5_i6_fused_1) % 13, ax4])
                            T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                            tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] + placeholder[ax0, ax1, ax2 * 13 + (vi5_i6_fused_0 * 13 + vi5_i6_fused_1) // 13, ax3 * 13 + (vi5_i6_fused_0 * 13 + vi5_i6_fused_1) % 13, ax4]
        for i0_i1_fused in T.parallel(250, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax5_init in T.serial(4):
                with T.block("tensor_init"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(250, i0_i1_fused)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    ax4 = T.axis.spatial(4, ax5_init)
                    T.reads()
                    T.writes(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    tensor_1[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
            for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(13, 1, 1, 1, 1, 4):
                with T.block("tensor_update"):
                    vi5_i6_fused_1 = T.axis.reduce(13, ax0)
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(250, i0_i1_fused)
                    ax2_1 = T.axis.spatial(1, 0)
                    ax3_1 = T.axis.spatial(1, 0)
                    ax4_1 = T.axis.spatial(4, ax5)
                    T.reads(tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_1])
                    T.writes(tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_1]
            for i2_i3_i4_fused in T.vectorized(4):
                with T.block("tensor_1"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(250, i0_i1_fused)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    ax4 = T.axis.spatial(4, i2_i3_i4_fused)
                    T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] / T.cast(T.max((T.min(ax2, 0) * 13 + T.min(ax2 * 13 + 12, 12) + 1 - ax2 * 13) * (T.min(ax3, 0) * 13 + T.min(ax3 * 13 + 12, 12) + 1 - ax3 * 13), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[13, 13])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
b16, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l17 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b16, decision=-1)
sch.compute_at(block=b16, loop=l18, preserve_unit_loops=True)
sch.enter_postproc()
b19 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b19, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b19, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b19, ann_key="meta_schedule.unroll_explicit")
b20, b21, b22 = sch.get_child_blocks(b19)
l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b20)
l30 = sch.fuse(l23, l24)
sch.parallel(loop=l30)
l31 = sch.fuse(l29)
sch.vectorize(loop=l31)
sch.annotate(block_or_loop=l30, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l30, ann_key="pragma_unroll_explicit", ann_val=1)
l32, l33, l34, l35, l36, l37, l38, l39 = sch.get_loops(block=b21)
l40 = sch.fuse(l32, l33)
sch.parallel(loop=l40)
sch.annotate(block_or_loop=l40, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l40, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42, l43, l44 = sch.get_loops(block=b22)
l45 = sch.fuse(l42, l43, l44)
sch.vectorize(loop=l45)
sch.annotate(block_or_loop=l41, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l41, ann_key="pragma_unroll_explicit", ann_val=1)
b46 = sch.get_block(name="tensor_rf", func_name="main")
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
b53 = sch.decompose_reduction(block=b46, loop=l51)
b54 = sch.get_block(name="tensor", func_name="main")
l55, l56, l57, l58, l59, l60, l61 = sch.get_loops(block=b54)
b62 = sch.decompose_reduction(block=b54, loop=l56)
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #22: GFLOPs: 0.0509. Time: 3.7346 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #23: GFLOPs: 0.0226. Time: 8.4126 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #24: GFLOPs: 0.0252. Time: 7.5291 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #25: GFLOPs: 0.0224. Time: 8.4815 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #26: GFLOPs: 0.0163. Time: 11.6641 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #27: GFLOPs: 0.0311. Time: 6.1050 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #28: GFLOPs: 0.0153. Time: 12.3977 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #29: GFLOPs: 0.0246. Time: 7.7173 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #30: GFLOPs: 0.0290. Time: 6.5445 ms. Best GFLOPs: 0.0677
[02:04:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_avg_pool2d"] Trial #31: GFLOPs: 0.0228. Time: 8.3191 ms. Best GFLOPs: 0.0677
[02:05:04] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #26: "fused_nn_avg_pool2d"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |            
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |            
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |            
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |            
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 739
Total latency (us): 176241

[02:05:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #27: "fused_layout_transform_reshape"] Trial #0: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 250, 1, 1, 4), "float32"], T_reshape: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        for i0_i1_fused in T.parallel(1000):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(1, 0)
                ax1 = T.axis.spatial(1000, i0_i1_fused)
                T.reads(placeholder[0, ax1 % 1000 // 4, 0, 0, ax1 % 4])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = T.if_then_else(0 < 1 and ax1 % 1000 < 1000 and 0 < 1 and 0 < 1, placeholder[0, ax1 % 1000 // 4, 0, 0, ax1 % 1000 % 4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
sch.enter_postproc()
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit")
b5, = sch.get_child_blocks(b4)
l6, l7 = sch.get_loops(block=b5)
l8 = sch.fuse(l6, l7)
sch.parallel(loop=l8)
[02:05:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape"] Trial #1: GFLOPs: 0.0000. Time: 39.9835 ms. Best GFLOPs: 0.0000
[02:05:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape"] Trial #2: GFLOPs: 0.0000. Time: 37.3311 ms. Best GFLOPs: 0.0000
[02:05:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape"] Trial #3: GFLOPs: 0.0000. Time: 10.1530 ms. Best GFLOPs: 0.0000
[02:05:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape"] Trial #4: GFLOPs: 0.0000. Time: 6.2946 ms. Best GFLOPs: 0.0000
[02:05:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape"] Trial #5: GFLOPs: 0.0000. Time: 4.5819 ms. Best GFLOPs: 0.0000
[02:05:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape"] Trial #6: GFLOPs: 0.0000. Time: 7.3937 ms. Best GFLOPs: 0.0000
[02:05:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape"] Trial #7: GFLOPs: 0.0000. Time: 4.8418 ms. Best GFLOPs: 0.0000
[02:05:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape"] Trial #8: GFLOPs: 0.0000. Time: 9.8515 ms. Best GFLOPs: 0.0000
[02:05:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape"] Trial #9: GFLOPs: 0.0000. Time: 4.8253 ms. Best GFLOPs: 0.0000
[02:05:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape"] Trial #10: GFLOPs: 0.0000. Time: 6.3160 ms. Best GFLOPs: 0.0000
[02:05:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #27: "fused_layout_transform_reshape"] Trial #11: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 250, 1, 1, 4), "float32"], T_reshape: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 1000, 1, 1], dtype="float32")
        for i0_i1_fused in T.parallel(1000, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                with T.block("T_layout_trans"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(1000, i0_i1_fused)
                    ax2_1 = T.axis.spatial(1, 0)
                    ax3_1 = T.axis.spatial(1, 0)
                    T.reads(placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4])
                    T.writes(T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1])
                    T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 1000 and ax2_1 < 1 and ax3_1 < 1, placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4], T.float32(0), dtype="float32")
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(1, 0)
                ax1 = T.axis.spatial(1000, i0_i1_fused)
                T.reads(T_layout_trans[0, ax1 % 1000, 0, 0])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = T_layout_trans[0, ax1 % 1000, 0, 0]
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
sch.enter_postproc()
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit")
b5, b6 = sch.get_child_blocks(b4)
l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b5)
l13 = sch.fuse(l7, l8)
sch.parallel(loop=l13)
sch.annotate(block_or_loop=l13, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l13, ann_key="pragma_unroll_explicit", ann_val=1)
l14, = sch.get_loops(block=b6)
sch.annotate(block_or_loop=l14, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l14, ann_key="pragma_unroll_explicit", ann_val=1)
[02:05:56] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #27: "fused_layout_transform_reshape"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |            
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |            
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |            
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |            
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |            
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |            
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |            
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |            
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 751
Total latency (us): 180823

[02:05:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"
[02:05:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #3 has finished. Remaining task(s): 27
[02:05:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"
[02:05:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #2 has finished. Remaining task(s): 26
[02:05:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"
[02:05:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #25 has finished. Remaining task(s): 25
[02:05:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"
[02:05:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #9 has finished. Remaining task(s): 24
[02:05:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"
[02:05:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #0 has finished. Remaining task(s): 23
[02:05:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"
[02:05:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #14 has finished. Remaining task(s): 22
[02:05:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_concatenate"
[02:05:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:05:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 8 candidate(s) from database
[02:06:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:06:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2040 candidate(s)
[02:06:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:07:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:07:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:07:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:08:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:08:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:08:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:08:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:08:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:08:29] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_concatenate"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |            
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |            
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |            
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 751
Total latency (us): 180823

[02:08:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"
[02:08:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #19 has finished. Remaining task(s): 21
[02:08:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"
[02:08:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #7 has finished. Remaining task(s): 20
[02:08:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_concatenate_1"
[02:08:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:08:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 8 candidate(s) from database
[02:08:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:08:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2040 candidate(s)
[02:09:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:09:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:10:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:10:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:10:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:10:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:10:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:10:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:10:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:10:58] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_concatenate_1"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |            
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |            
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 751
Total latency (us): 180823

[02:10:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"
[02:10:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #5 has finished. Remaining task(s): 19
[02:10:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #24: "fused_concatenate_3"
[02:10:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:10:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 8 candidate(s) from database
[02:11:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[02:11:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2040 candidate(s)
[02:11:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[02:11:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[02:12:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[02:12:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[02:12:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:12:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:12:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:12:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:12:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:12:56] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #24: "fused_concatenate_3"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |            
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |            
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 751
Total latency (us): 180823

[02:12:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"
[02:12:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #13 has finished. Remaining task(s): 18
[02:12:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_concatenate"
[02:12:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:12:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 8 candidate(s) from database
[02:13:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:13:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2040 candidate(s)
[02:13:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:13:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:14:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:14:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:15:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:15:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:15:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:15:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:15:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:15:17] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_concatenate"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |          Y 
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |            
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 751
Total latency (us): 180823

[02:15:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #27: "fused_layout_transform_reshape"
[02:15:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:15:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 12 candidate(s) from database
[02:15:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[02:15:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2036 candidate(s)
[02:15:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[02:16:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[02:16:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[02:17:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[02:17:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:17:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:17:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:17:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:17:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:17:52] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #27: "fused_layout_transform_reshape"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |          Y 
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |            
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 751
Total latency (us): 180823

[02:17:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_layout_transform"
[02:17:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:17:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[02:17:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[02:17:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[02:18:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[02:18:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[02:19:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[02:19:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[02:19:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:19:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:19:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:19:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:19:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:19:54] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_layout_transform"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |            
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |          Y 
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |            
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 751
Total latency (us): 180823

[02:19:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #12: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"
[02:19:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #12 has finished. Remaining task(s): 17
[02:19:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #20: "fused_concatenate_2"
[02:19:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:19:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 8 candidate(s) from database
[02:19:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[02:19:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2040 candidate(s)
[02:20:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[02:20:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[02:21:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[02:21:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[02:22:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:22:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:22:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:22:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:22:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:22:07] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #20: "fused_concatenate_2"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     31 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |          Y 
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |          Y 
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |            
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 751
Total latency (us): 180823

[02:22:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_max_pool2d"
[02:22:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:22:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 31 candidate(s) from database
[02:22:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f0c588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c680fa368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6c518)]: 0 failure(s)
[02:22:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2017 candidate(s)
[02:23:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f0c588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c680fa368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6c518)]: 0 failure(s)
[02:23:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f0c588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c680fa368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6c518)]: 0 failure(s)
[02:24:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f0c588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c680fa368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6c518)]: 0 failure(s)
[02:25:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f0c588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c680fa368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6c518)]: 0 failure(s)
[02:26:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 1 candidates:
[1 : 1]:	0.8774
[02:26:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 1 candidate(s) with evolutionary search
[02:26:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 1 candidates(s) for measurement
[02:26:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 1 sample(s) to builder
[02:26:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 1 sample(s) to runner
[02:26:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d"] Trial #31: GFLOPs: 0.0278. Time: 62.6530 ms. Best GFLOPs: 0.4436
[02:27:04] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_nn_max_pool2d"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     32 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |            
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |          Y 
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |          Y 
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |            
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |            
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 752
Total latency (us): 180823

[02:27:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"
[02:27:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #8 has finished. Remaining task(s): 16
[02:27:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #18: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"
[02:27:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #18 has finished. Remaining task(s): 15
[02:27:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_concatenate_1"
[02:27:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:27:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 8 candidate(s) from database
[02:27:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:27:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2040 candidate(s)
[02:27:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:28:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:28:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:29:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:29:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:29:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:29:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:29:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:29:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:29:35] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_concatenate_1"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     32 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |          Y 
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |          Y 
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |          Y 
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |          Y 
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |            
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 752
Total latency (us): 180823

[02:29:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_concatenate"
[02:29:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:29:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 8 candidate(s) from database
[02:29:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:29:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2040 candidate(s)
[02:30:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:30:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:30:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:31:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:31:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:31:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:31:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:31:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:31:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:31:43] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_concatenate"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |            
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     32 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |          Y 
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |          Y 
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |          Y 
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |          Y 
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |            
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |            
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |            
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 752
Total latency (us): 180823

[02:31:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #26: "fused_nn_avg_pool2d"
[02:31:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #26 has finished. Remaining task(s): 14
[02:31:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"
[02:31:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #1 has finished. Remaining task(s): 13
[02:31:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"
[02:31:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #22 has finished. Remaining task(s): 12
[02:31:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #24: "fused_concatenate_3"
[02:31:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:31:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 8 candidate(s) from database
[02:31:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[02:31:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2040 candidate(s)
[02:32:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[02:32:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[02:33:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[02:33:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[02:33:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:33:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:33:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:33:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:33:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:33:41] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #24: "fused_concatenate_3"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |          Y 
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     32 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |          Y 
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |          Y 
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |          Y 
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |          Y 
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |          Y 
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |            
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |          Y 
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 752
Total latency (us): 180823

[02:33:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_concatenate"
[02:33:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:33:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 8 candidate(s) from database
[02:33:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:33:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2040 candidate(s)
[02:34:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:34:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:35:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:35:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:36:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:36:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:36:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:36:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:36:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:36:20] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_concatenate"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |          Y 
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     32 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |          Y 
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |          Y 
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |          Y 
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |          Y 
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |          Y 
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |            
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |          Y 
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 752
Total latency (us): 180823

[02:36:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #27: "fused_layout_transform_reshape"
[02:36:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:36:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 12 candidate(s) from database
[02:36:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[02:36:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2036 candidate(s)
[02:37:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[02:37:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[02:37:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[02:38:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[02:38:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:38:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:38:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:38:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:38:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:38:53] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #27: "fused_layout_transform_reshape"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |          Y 
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     32 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |          Y 
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |          Y 
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |          Y 
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |          Y 
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |          Y 
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |            
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |          Y 
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 752
Total latency (us): 180823

[02:38:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_layout_transform"
[02:38:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:38:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[02:38:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[02:38:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[02:39:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[02:39:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[02:39:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[02:40:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[02:40:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:40:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:40:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:40:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:40:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:40:37] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_layout_transform"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |          Y 
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     32 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |          Y 
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |          Y 
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |          Y 
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |          Y 
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |          Y 
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |            
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |          Y 
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 752
Total latency (us): 180823

[02:40:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_concatenate_1"
[02:40:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:40:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 8 candidate(s) from database
[02:40:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:40:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2040 candidate(s)
[02:40:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:41:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:41:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:42:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:42:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:42:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:42:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:42:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:42:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:42:52] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_concatenate_1"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |          Y 
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     32 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |          Y 
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |          Y 
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |          Y 
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |            
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |          Y 
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |          Y 
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |            
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |          Y 
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 752
Total latency (us): 180823

[02:42:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #16: "fused_nn_max_pool2d_2"
[02:42:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #16 has finished. Remaining task(s): 11
[02:42:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"
[02:42:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #23 has finished. Remaining task(s): 10
[02:42:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #20: "fused_concatenate_2"
[02:42:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:42:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 8 candidate(s) from database
[02:42:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[02:42:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2040 candidate(s)
[02:43:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[02:43:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[02:44:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[02:44:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[02:44:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:44:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:44:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:45:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:45:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:45:00] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #20: "fused_concatenate_2"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |          Y 
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     32 |            
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |          Y 
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |            
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |          Y 
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |          Y 
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |          Y 
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |          Y 
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |          Y 
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |          Y 
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |          Y 
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 752
Total latency (us): 180823

[02:45:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_max_pool2d"
[02:45:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #6 has finished. Remaining task(s): 9
[02:45:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_concatenate"
[02:45:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:45:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 8 candidate(s) from database
[02:45:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:45:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2040 candidate(s)
[02:45:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:45:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:46:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:46:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67f73bd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67d1b048)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67e4efb8)]: 0 failure(s)
[02:46:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:46:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:46:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:47:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #10 has finished. Remaining task(s): 8
[02:47:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_concatenate_1"
[02:47:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:47:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 8 candidate(s) from database
[02:47:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:47:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2040 candidate(s)
[02:47:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:48:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:48:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:48:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:49:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:49:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:49:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:49:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:49:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:49:08] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_concatenate_1"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |          Y 
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     32 |          Y 
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |          Y 
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |          Y 
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |          Y 
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |          Y 
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |          Y 
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |          Y 
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |          Y 
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |          Y 
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |          Y 
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 752
Total latency (us): 180823

[02:49:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #24: "fused_concatenate_3"
[02:49:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:49:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 8 candidate(s) from database
[02:49:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[02:49:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2040 candidate(s)
[02:49:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[02:50:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[02:50:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[02:50:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[02:51:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:51:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:51:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:51:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:51:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:51:27] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #24: "fused_concatenate_3"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |          Y 
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     32 |          Y 
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |          Y 
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |          Y 
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |            
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |          Y 
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |          Y 
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |          Y 
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |          Y 
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |          Y 
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |          Y 
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |          Y 
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 752
Total latency (us): 180823

[02:51:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #11: "fused_nn_max_pool2d_1"
[02:51:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #11 has finished. Remaining task(s): 7
[02:51:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #27: "fused_layout_transform_reshape"
[02:51:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:51:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 12 candidate(s) from database
[02:51:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[02:51:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2036 candidate(s)
[02:52:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[02:52:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[02:53:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[02:53:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[02:53:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:53:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:53:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:53:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:53:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:53:33] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #27: "fused_layout_transform_reshape"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |          Y 
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     32 |          Y 
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |          Y 
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |          Y 
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |          Y 
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |          Y 
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |          Y 
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |          Y 
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |          Y 
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |          Y 
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |          Y 
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |          Y 
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 752
Total latency (us): 180823

[02:53:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_layout_transform"
[02:53:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:53:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[02:53:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[02:53:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[02:53:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[02:54:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[02:54:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[02:55:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[02:55:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:55:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:55:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:55:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:55:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:55:16] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_layout_transform"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |          Y 
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     32 |          Y 
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |          Y 
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |          Y 
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |          Y 
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |          Y 
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |          Y 
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |          Y 
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |          Y 
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |          Y 
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |          Y 
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |          Y 
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 752
Total latency (us): 180823

[02:55:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #20: "fused_concatenate_2"
[02:55:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:55:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 8 candidate(s) from database
[02:55:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[02:55:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2040 candidate(s)
[02:55:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[02:56:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[02:56:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[02:57:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[02:57:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:57:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:57:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:57:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:57:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:57:50] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #20: "fused_concatenate_2"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |          Y 
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     32 |          Y 
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |          Y 
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |          Y 
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |          Y 
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |          Y 
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |          Y 
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |            
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |          Y 
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |          Y 
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |          Y 
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |          Y 
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |          Y 
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 752
Total latency (us): 180823

[02:57:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_concatenate_1"
[02:57:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:57:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 8 candidate(s) from database
[02:57:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:57:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2040 candidate(s)
[02:58:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:58:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:59:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:59:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c6810bd18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f77448)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f6b7e8)]: 0 failure(s)
[02:59:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:59:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:59:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:59:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #15 has finished. Remaining task(s): 6
[02:59:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #24: "fused_concatenate_3"
[02:59:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:59:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 8 candidate(s) from database
[03:00:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[03:00:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2040 candidate(s)
[03:00:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[03:00:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[03:01:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[03:01:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[03:01:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:01:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:01:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:01:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:01:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:01:38] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #24: "fused_concatenate_3"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |          Y 
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     32 |          Y 
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |          Y 
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |          Y 
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |          Y 
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |          Y 
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |          Y 
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |          Y 
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |          Y 
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |          Y 
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |          Y 
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |          Y 
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |          Y 
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 752
Total latency (us): 180823

[03:01:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #27: "fused_layout_transform_reshape"
[03:01:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:01:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 12 candidate(s) from database
[03:01:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[03:01:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2036 candidate(s)
[03:02:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[03:02:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[03:03:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[03:03:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[03:03:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:03:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:03:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:03:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:03:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:03:39] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #27: "fused_layout_transform_reshape"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |          Y 
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     32 |          Y 
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |          Y 
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |          Y 
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |          Y 
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |          Y 
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |          Y 
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |          Y 
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |          Y 
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |          Y 
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |          Y 
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |          Y 
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |          Y 
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 752
Total latency (us): 180823

[03:03:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_layout_transform"
[03:03:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:03:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[03:03:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[03:03:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[03:04:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[03:04:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[03:04:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[03:05:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[03:05:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:05:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:05:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:05:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:05:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:05:20] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_layout_transform"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |          Y 
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     32 |          Y 
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |          Y 
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |          Y 
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |          Y 
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |          Y 
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |          Y 
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |          Y 
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |          Y 
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |          Y 
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |          Y 
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |          Y 
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |          Y 
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 752
Total latency (us): 180823

[03:05:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #20: "fused_concatenate_2"
[03:05:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:05:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 8 candidate(s) from database
[03:05:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[03:05:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2040 candidate(s)
[03:05:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[03:06:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[03:06:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[03:06:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[03:07:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:07:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:07:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:07:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:07:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:07:01] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #20: "fused_concatenate_2"
 ID |                                         Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  49926656 |      2 |         9.9363 |    5024.6548 |            10049.3097 |     32 |          Y 
  1 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  28099968 |      2 |        23.3347 |    1204.2125 |             2408.4250 |     32 |          Y 
  2 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  53934336 |      2 |         5.1246 |   10524.5433 |            21049.0865 |     32 |          Y 
  3 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  56144000 |      2 |         4.5669 |   12293.7873 |            24587.5747 |     32 |          Y 
  4 |                       fused_layout_transform |         1 |      1 |         0.0000 |    4331.5231 |             4331.5231 |      4 |            
  5 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  44158464 |      1 |         7.4139 |    5956.1899 |             5956.1899 |     32 |          Y 
  6 |                          fused_nn_max_pool2d |   1742400 |      1 |         0.4436 |    3927.8187 |             3927.8187 |     32 |          Y 
  7 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |   6292000 |      1 |         0.9535 |    6598.6874 |             6598.6874 |     32 |          Y 
  8 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  12487200 |      1 |         3.3532 |    3724.0143 |             3724.0143 |     32 |          Y 
  9 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   6582400 |      2 |         1.2697 |    5184.1441 |            10368.2881 |     32 |          Y 
 10 |                            fused_concatenate |         1 |      2 |         0.0000 |    4668.2611 |             9336.5222 |      8 |          Y 
 11 |                        fused_nn_max_pool2d_1 |    839808 |      1 |         0.5488 |    1530.1950 |             1530.1950 |     32 |          Y 
 12 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   6018624 |      1 |         1.4472 |    4158.7710 |             4158.7710 |     32 |          Y 
 13 |  fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |  11990592 |      1 |         2.5534 |    4696.0222 |             4696.0222 |     32 |          Y 
 14 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   6158592 |      2 |         1.2832 |    4799.2623 |             9598.5246 |     32 |          Y 
 15 |                          fused_concatenate_1 |         1 |      2 |         0.0000 |    3225.8901 |             6451.7803 |      8 |          Y 
 16 |                        fused_nn_max_pool2d_2 |    389376 |      1 |         0.1947 |    1999.5298 |             1999.5298 |     32 |          Y 
 17 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   4169568 |      1 |        14.3699 |     290.1596 |              290.1596 |     32 |            
 18 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   6246240 |      1 |         1.8890 |    3306.6312 |             3306.6312 |     32 |          Y 
 19 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   3179904 |      2 |         0.7106 |    4474.8125 |             8949.6250 |     32 |          Y 
 20 |                          fused_concatenate_2 |         1 |      2 |         0.0000 |    1966.4151 |             3932.8302 |      8 |            
 21 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   8328320 |      1 |        15.4357 |     539.5505 |              539.5505 |     32 |            
 22 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  11097216 |      1 |         4.6353 |    2394.0765 |             2394.0765 |     32 |          Y 
 23 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |   5624320 |      2 |         5.6802 |     990.1697 |             1980.3395 |     32 |          Y 
 24 |                          fused_concatenate_3 |         1 |      2 |         0.0000 |    2353.7380 |             4707.4760 |      8 |            
 25 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 | 173394000 |      1 |        10.4695 |   16561.8580 |            16561.8580 |     32 |          Y 
 26 |                          fused_nn_avg_pool2d |    190000 |      1 |         0.0677 |    2805.9266 |             2805.9266 |     32 |          Y 
 27 |               fused_layout_transform_reshape |         1 |      1 |         0.0000 |    4581.8627 |             4581.8627 |     12 |            
------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 752
Total latency (us): 180823

[03:07:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #24: "fused_concatenate_3"
[03:07:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:07:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 8 candidate(s) from database
[03:07:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[03:07:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2040 candidate(s)
[03:07:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[03:07:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[03:08:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[03:08:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68186738)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67cd6858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c678cf708)]: 0 failure(s)
[03:08:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:08:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:08:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:08:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #24 has finished. Remaining task(s): 5
[03:08:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #27: "fused_layout_transform_reshape"
[03:08:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:08:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 12 candidate(s) from database
[03:09:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[03:09:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2036 candidate(s)
[03:09:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[03:09:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[03:10:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[03:10:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67cf0208)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6814ae98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67fdc0a8)]: 0 failure(s)
[03:10:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:10:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:10:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:10:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #27 has finished. Remaining task(s): 4
[03:10:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_layout_transform"
[03:10:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:10:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[03:10:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[03:10:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[03:11:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[03:11:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[03:11:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[03:12:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c68002db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c67f79e98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f0e1e8)]: 0 failure(s)
[03:12:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:12:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:12:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:12:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #4 has finished. Remaining task(s): 3
[03:12:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #20: "fused_concatenate_2"
[03:12:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:12:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 8 candidate(s) from database
[03:12:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[03:12:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2040 candidate(s)
[03:12:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[03:13:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[03:13:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[03:13:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557c67d056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x557c6817e148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x557c67f5bac8)]: 0 failure(s)
[03:14:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:14:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:14:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:14:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #20 has finished. Remaining task(s): 2
[03:14:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"
[03:14:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #21 has finished. Remaining task(s): 1
[03:14:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"
[03:14:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #17 has finished. Remaining task(s): 0
Starting to build with relay.
/home/yj/anaconda3/lib/python3.7/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
The result is correct!
