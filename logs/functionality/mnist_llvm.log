nohup: ignoring input
[00:56:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #0: "fused_nn_pad_layout_transform"
[00:56:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 28, 28), "float32"], T_layout_trans: T.Buffer[(1, 1, 32, 32, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        compile_engine_const = T.alloc_buffer([], dtype="float32")
        T_cast = T.alloc_buffer([], dtype="float32")
        T_pad = T.alloc_buffer([1, 1, 32, 32], dtype="float32")
        with T.block("compile_engine_const"):
            T.reads()
            T.writes(compile_engine_const[()])
            compile_engine_const[()] = T.float32(0)
        with T.block("T_cast"):
            T.reads(compile_engine_const[()])
            T.writes(T_cast[()])
            T_cast[()] = compile_engine_const[()]
        for i0, i1, i2, i3 in T.grid(1, 1, 32, 32):
            with T.block("T_pad"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2 - 2, ax3 - 2], T_cast[()])
                T.writes(T_pad[ax0, ax1, ax2, ax3])
                T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(2 <= ax2 and ax2 < 30 and 2 <= ax3 and ax3 < 30, placeholder[ax0, ax1, ax2 - 2, ax3 - 2], T_cast[()], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 32, 32, 1):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_pad[ax0, ax1 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 + ax4 < 1 and ax2 < 32 and ax3 < 32, T_pad[ax0, ax1 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[00:56:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[00:56:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 28, 28), "float32"], T_layout_trans: T.Buffer[(1, 1, 32, 32, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_pad = T.alloc_buffer([1, 1, 32, 32], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 1, 32, 32):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("T_pad"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(32, i2 + ax2)
                        ax3_1 = T.axis.spatial(32, i3 + ax3)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 - 2, ax3_1 - 2])
                        T.writes(T_pad[ax0_1, ax1_1, ax2_1, ax3_1])
                        T_pad[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(2 <= ax2_1 and ax2_1 < 30 and 2 <= ax3_1 and ax3_1 < 30, placeholder[ax0_1, ax1_1, ax2_1 - 2, ax3_1 - 2], T.float32(0), dtype="float32")
                for i4 in T.serial(1):
                    with T.block("T_layout_trans"):
                        ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(T_pad[ax0, ax1 + ax4, ax2, ax3])
                        T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                        T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 + ax4 < 1 and ax2 < 32 and ax3 < 32, T_pad[ax0, ax1 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="compile_engine_const", func_name="main")
b1 = sch.get_block(name="T_cast", func_name="main")
b2 = sch.get_block(name="T_pad", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=3)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
[00:56:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"
[00:56:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 32, 32, 1), "float32"], placeholder_1: T.Buffer[(2, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 2, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 2, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 2, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 2, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 2, 28, 28, 4, 1, 5, 5):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic, oh + kh, ow + kw, 0], placeholder_1[oc_chunk, ic, kh, kw, 0, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 32, 32, 1], "float32"], ["TENSOR", [2, 1, 5, 5, 1, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW1c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic, oh + kh, ow + kw, 0] * placeholder_1[oc_chunk, ic, kh, kw, 0, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 2, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 2, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[00:56:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[00:56:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 32, 32, 1), "float32"], placeholder_1: T.Buffer[(2, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 2, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 2, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 2, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 4, 1, 1, 1, 4, 1, 4, 1, 1, 5, 1, 2, 7, 1, 1, 1, 5, 1, 1, 1, 1, 7, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(2, i1_2)
                    oh = T.axis.spatial(28, i2_1 * 7 + i2_2)
                    ow = T.axis.spatial(28, i3_0 * 7 + i3_3)
                    oc_block = T.axis.spatial(4, i4_1)
                    ic = T.axis.reduce(1, 0)
                    kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                    T.reads(placeholder[n, ic, oh + kh, ow + kw, 0], placeholder_1[oc_chunk, ic, kh, kw, 0, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 32, 32, 1], "float32"], ["TENSOR", [2, 1, 5, 5, 1, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW1c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic, oh + kh, ow + kw, 0] * placeholder_1[oc_chunk, ic, kh, kw, 0, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 2, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 5])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[5, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[00:56:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 32, 32, 1), "float32"], placeholder_1: T.Buffer[(2, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 2, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 2, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 2, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 4, 1, 1, 1, 4, 1, 4):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 5, 1, 2, 7, 1, 1, 1, 5, 1, 1, 1, 1, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(2, i1_2)
                        oh = T.axis.spatial(28, i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(28, i3_0 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(1, 0)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(placeholder[n, ic, oh + kh, ow + kw, 0], placeholder_1[oc_chunk, ic, kh, kw, 0, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 32, 32, 1], "float32"], ["TENSOR", [2, 1, 5, 5, 1, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW1c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic, oh + kh, ow + kw, 0] * placeholder_1[oc_chunk, ic, kh, kw, 0, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 7, 7, 1):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(28, i2_1 * 7 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 5])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[5, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[00:56:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 32, 32, 1), "float32"], placeholder_1: T.Buffer[(2, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 2, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 2, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 2, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 4, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 4, 1, 4, 1, 1, 5, 1, 2, 7, 1, 1, 1, 5, 1, 1, 1, 1, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(2, i1_2)
                        oh = T.axis.spatial(28, i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(28, i3_0 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(1, 0)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(placeholder[n, ic, oh + kh, ow + kw, 0], placeholder_1[oc_chunk, ic, kh, kw, 0, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 32, 32, 1], "float32"], ["TENSOR", [2, 1, 5, 5, 1, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW1c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic, oh + kh, ow + kw, 0] * placeholder_1[oc_chunk, ic, kh, kw, 0, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 28, 7, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(28, i3_0 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 5])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[5, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[00:56:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #2: "fused_nn_max_pool2d"
[00:56:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 2, 28, 28, 4), "float32"], tensor: T.Buffer[(1, 2, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 2, 14, 14, 4, 2, 2):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
    

[00:56:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[00:56:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 2, 28, 28, 4), "float32"], tensor: T.Buffer[(1, 2, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 2, 14, 14, 4, 2, 2):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[00:56:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #3: "fused_nn_pad"
[00:56:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 2, 14, 14, 4), "float32"], T_pad: T.Buffer[(1, 2, 18, 18, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        compile_engine_const = T.alloc_buffer([], dtype="float32")
        T_cast = T.alloc_buffer([], dtype="float32")
        with T.block("compile_engine_const"):
            T.reads()
            T.writes(compile_engine_const[()])
            compile_engine_const[()] = T.float32(0)
        with T.block("T_cast"):
            T.reads(compile_engine_const[()])
            T.writes(T_cast[()])
            T_cast[()] = compile_engine_const[()]
        for i0, i1, i2, i3, i4 in T.grid(1, 2, 18, 18, 4):
            with T.block("T_pad"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2 - 2, ax3 - 2, ax4], T_cast[()])
                T.writes(T_pad[ax0, ax1, ax2, ax3, ax4])
                T_pad[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(2 <= ax2 and ax2 < 16 and 2 <= ax3 and ax3 < 16, placeholder[ax0, ax1, ax2 - 2, ax3 - 2, ax4], T_cast[()], dtype="float32")
    

[00:56:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[00:56:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 2, 14, 14, 4), "float32"], T_pad: T.Buffer[(1, 2, 18, 18, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 2, 18, 18, 4):
                with T.block("T_pad"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2 - 2, ax3 - 2, ax4])
                    T.writes(T_pad[ax0, ax1, ax2, ax3, ax4])
                    T_pad[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(2 <= ax2 and ax2 < 16 and 2 <= ax3 and ax3 < 16, placeholder[ax0, ax1, ax2 - 2, ax3 - 2, ax4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="compile_engine_const", func_name="main")
b1 = sch.get_block(name="T_cast", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[00:56:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"
[00:56:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 2, 18, 18, 4), "float32"], placeholder_1: T.Buffer[(4, 2, 5, 5, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 4, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 4, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 4, 14, 14, 4, 8, 5, 5):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 2, 18, 18, 4], "float32"], ["TENSOR", [4, 2, 5, 5, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 4, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 4, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 2, 18, 18, 4), "float32"], placeholder_1: T.Buffer[(4, 2, 5, 5, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 4, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 7, 7, 4, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 4, 5, 5, 1, 2, 2, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(4, i1_0 * 2 + i1_3)
                    oh = T.axis.spatial(14, i2_0 * 2 + i2_3)
                    ow = T.axis.spatial(14, i3_0 * 2 + i3_2)
                    oc_block = T.axis.spatial(4, i4_0)
                    ic = T.axis.reduce(8, i5_0 * 4 + i5_1)
                    kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 2, 18, 18, 4], "float32"], ["TENSOR", [4, 2, 5, 5, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 4, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 5])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 5])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 2, 18, 18, 4), "float32"], placeholder_1: T.Buffer[(4, 2, 5, 5, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 4, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 7, 7, 4, 1, 1, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 1, 1, 2, 1, 4, 5, 5, 1, 2, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(4, i1_0 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_0 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_0 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(8, i5_0 * 4 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 2, 18, 18, 4], "float32"], ["TENSOR", [4, 2, 5, 5, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 2, 2, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(4, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 * 2 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 5])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 5])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 2, 18, 18, 4), "float32"], placeholder_1: T.Buffer[(4, 2, 5, 5, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 4, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 7, 7, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 4, 5, 5, 1, 2, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(4, i1_0 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_0 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_0 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(8, i5_0 * 4 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 2, 18, 18, 4], "float32"], ["TENSOR", [4, 2, 5, 5, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 2, 2, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(4, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 * 2 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 5])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 5])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #5: "fused_nn_max_pool2d_1"
[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 14, 14, 4), "float32"], tensor: T.Buffer[(1, 4, 4, 4, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 4, 4, 4, 4, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder[ax0, ax1, ax2 * 3 + rv0, ax3 * 3 + rv1, ax4])
                T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], placeholder[ax0, ax1, ax2 * 3 + rv0, ax3 * 3 + rv1, ax4])
    

[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 14, 14, 4), "float32"], tensor: T.Buffer[(1, 4, 4, 4, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            tensor_rf = T.alloc_buffer([1, 4, 4, 4, 4, 3], dtype="float32")
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 4, 4, 4, 4, 3, 3):
                with T.block("tensor_rf"):
                    vi5_i6_fused_0 = T.axis.spatial(3, i5_i6_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4, vi5_i6_fused_1 = T.axis.remap("SSSSR", [i1, i2, i3, i4, i5_i6_fused_1])
                    T.reads(placeholder[ax0, ax1, ax2 * 3 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) // 3, ax3 * 3 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) % 3, ax4])
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    with T.init():
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.float32(-3.4028234663852886e+38)
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0], placeholder[ax0, ax1, ax2 * 3 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) // 3, ax3 * 3 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) % 3, ax4])
            for i0, i1, i2, i3, i4, i5_i6_fused_0 in T.grid(1, 4, 4, 4, 4, 3):
                with T.block("tensor"):
                    vi5_i6_fused_0 = T.axis.reduce(3, i5_i6_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 3])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l12, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 14, 14, 4), "float32"], tensor: T.Buffer[(1, 4, 4, 4, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            tensor_rf = T.alloc_buffer([1, 4, 4, 4, 4, 3], dtype="float32")
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 4, 4, 4, 4, 3, 3):
                with T.block("tensor_rf"):
                    vi5_i6_fused_1 = T.axis.spatial(3, i5_i6_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4, vi5_i6_fused_0 = T.axis.remap("SSSSR", [i1, i2, i3, i4, i5_i6_fused_0])
                    T.reads(placeholder[ax0, ax1, ax2 * 3 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) // 3, ax3 * 3 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) % 3, ax4])
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    with T.init():
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.float32(-3.4028234663852886e+38)
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1], placeholder[ax0, ax1, ax2 * 3 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) // 3, ax3 * 3 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) % 3, ax4])
            for i0, i1, i2, i3, i4, i5_i6_fused_1 in T.grid(1, 4, 4, 4, 4, 3):
                with T.block("tensor"):
                    vi5_i6_fused_1 = T.axis.reduce(3, i5_i6_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 3])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 14, 14, 4), "float32"], tensor: T.Buffer[(1, 4, 4, 4, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 4, 4, 4, 4, 3, 3):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                    T.reads(placeholder[ax0, ax1, ax2 * 3 + rv0, ax3 * 3 + rv1, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], placeholder[ax0, ax1, ax2 * 3 + rv0, ax3 * 3 + rv1, ax4])
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #6: "fused_layout_transform_reshape"
[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 4, 4, 4), "float32"], T_reshape: T.Buffer[(1, 256), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 16, 4, 4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 16, 4, 4):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 16 and ax2 < 4 and ax3 < 4, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1 in T.grid(1, 256):
            with T.block("T_reshape"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_layout_trans[0, ax1 % 256 // 16, ax1 % 16 // 4, ax1 % 4])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = T_layout_trans[0, ax1 % 256 // 16, ax1 % 16 // 4, ax1 % 4]
    

[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 4, 4, 4), "float32"], T_reshape: T.Buffer[(1, 256), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_layout_trans = T.alloc_buffer([1, 16, 4, 4], dtype="float32")
            for i0, i1 in T.grid(1, 256):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("T_layout_trans"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1 // 16 + ax1)
                        ax2_1 = T.axis.spatial(4, i1 % 16 // 4 + ax2)
                        ax3_1 = T.axis.spatial(4, i1 % 4 + ax3)
                        T.reads(placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4])
                        T.writes(T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1])
                        T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 16 and ax2_1 < 4 and ax3_1 < 4, placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4], T.float32(0), dtype="float32")
                with T.block("T_reshape"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(T_layout_trans[0, ax1 % 256 // 16, ax1 % 16 // 4, ax1 % 4])
                    T.writes(T_reshape[ax0, ax1])
                    T_reshape[ax0, ax1] = T_layout_trans[0, ax1 % 256 // 16, ax1 % 16 // 4, ax1 % 4]
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #7: "fused_nn_dense_add"
[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256), "float32"], placeholder_1: T.Buffer[(10, 256), "float32"], placeholder_2: T.Buffer[(1, 10), "float32"], T_add: T.Buffer[(1, 10), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True, "layout_free_placeholders": [1]})
        # body
        # with T.block("root")
        T_matmul_NT = T.alloc_buffer([1, 10], dtype="float32")
        for i0, i1, i2 in T.grid(1, 10, 256):
            with T.block("T_matmul_NT"):
                i, j, k = T.axis.remap("SSR", [i0, i1, i2])
                T.reads(placeholder[i, k], placeholder_1[j, k])
                T.writes(T_matmul_NT[i, j])
                with T.init():
                    T_matmul_NT[i, j] = T.float32(0)
                T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
        for i0, i1 in T.grid(1, 10):
            with T.block("T_add"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                T.writes(T_add[ax0, ax1])
                T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
    

[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 5 design space(s) generated
[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256), "float32"], placeholder_1: T.Buffer[(10, 256), "float32"], placeholder_2: T.Buffer[(1, 10), "float32"], T_add: T.Buffer[(1, 10), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True, "layout_free_placeholders": [1]})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 10], dtype="float32")
            T_matmul_NT_rf = T.alloc_buffer([1, 10, 16], dtype="float32")
            for i0, i1 in T.grid(1, 10):
                for ax0 in T.serial(16):
                    for ax0_1, ax1, ax2, ax3 in T.grid(1, 1, 1, 16):
                        with T.block("T_matmul_NT_rf"):
                            vi2_0 = T.axis.spatial(16, ax0 + ax0_1)
                            i = T.axis.spatial(1, ax1)
                            j = T.axis.spatial(10, i1 + ax2)
                            vi2_1 = T.axis.reduce(16, ax3)
                            T.reads(placeholder[i, vi2_0 * 16 + vi2_1], placeholder_1[j, vi2_0 * 16 + vi2_1])
                            T.writes(T_matmul_NT_rf[i, j, vi2_0])
                            with T.init():
                                T_matmul_NT_rf[i, j, vi2_0] = T.float32(0)
                            T_matmul_NT_rf[i, j, vi2_0] = T_matmul_NT_rf[i, j, vi2_0] + placeholder[i, vi2_0 * 16 + vi2_1] * placeholder_1[j, vi2_0 * 16 + vi2_1]
                    for ax1, ax2 in T.grid(1, 1):
                        with T.block("T_matmul_NT"):
                            vi2_0, i = T.axis.remap("RS", [ax0, ax1])
                            j = T.axis.spatial(10, i1 + ax2)
                            T.reads(T_matmul_NT_rf[i, j, vi2_0])
                            T.writes(T_matmul_NT[i, j])
                            with T.init():
                                T_matmul_NT[i, j] = T.float32(0)
                            T_matmul_NT[i, j] = T_matmul_NT[i, j] + T_matmul_NT_rf[i, j, vi2_0]
                with T.block("T_add"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                    T.writes(T_add[ax0, ax1])
                    T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6 = sch.sample_perfect_tile(loop=l4, n=2, max_innermost_factor=64, decision=[16, 16])
l7, l8 = sch.split(loop=l4, factors=[v5, v6])
b9 = sch.rfactor(loop=l7, factor_axis=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v10 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v10)
b11, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l12 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l12, preserve_unit_loops=True)
l13 = sch.sample_compute_location(block=b11, decision=2)
sch.compute_at(block=b11, loop=l13, preserve_unit_loops=True)
[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256), "float32"], placeholder_1: T.Buffer[(10, 256), "float32"], placeholder_2: T.Buffer[(1, 10), "float32"], T_add: T.Buffer[(1, 10), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True, "layout_free_placeholders": [1]})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 10], dtype="float32")
            T_matmul_NT_rf = T.alloc_buffer([1, 10, 16], dtype="float32")
            for i0, i1, i2_0, i2_1 in T.grid(1, 10, 16, 16):
                with T.block("T_matmul_NT_rf"):
                    vi2_1 = T.axis.spatial(16, i2_1)
                    i = T.axis.spatial(1, 0)
                    j, vi2_0 = T.axis.remap("SR", [i1, i2_0])
                    T.reads(placeholder[i, vi2_0 * 16 + vi2_1], placeholder_1[j, vi2_0 * 16 + vi2_1])
                    T.writes(T_matmul_NT_rf[i, j, vi2_1])
                    with T.init():
                        T_matmul_NT_rf[i, j, vi2_1] = T.float32(0)
                    T_matmul_NT_rf[i, j, vi2_1] = T_matmul_NT_rf[i, j, vi2_1] + placeholder[i, vi2_0 * 16 + vi2_1] * placeholder_1[j, vi2_0 * 16 + vi2_1]
            for i0, i1 in T.grid(1, 10):
                for ax0, ax1, ax2 in T.grid(16, 1, 1):
                    with T.block("T_matmul_NT"):
                        vi2_1, i = T.axis.remap("RS", [ax0, ax1])
                        j = T.axis.spatial(10, i1 + ax2)
                        T.reads(T_matmul_NT_rf[i, j, vi2_1])
                        T.writes(T_matmul_NT[i, j])
                        with T.init():
                            T_matmul_NT[i, j] = T.float32(0)
                        T_matmul_NT[i, j] = T_matmul_NT[i, j] + T_matmul_NT_rf[i, j, vi2_1]
                with T.block("T_add"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                    T.writes(T_add[ax0, ax1])
                    T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6 = sch.sample_perfect_tile(loop=l4, n=2, max_innermost_factor=64, decision=[16, 16])
l7, l8 = sch.split(loop=l4, factors=[v5, v6])
b9 = sch.rfactor(loop=l8, factor_axis=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v10 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v10)
b11, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l12 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l12, preserve_unit_loops=True)
l13 = sch.sample_compute_location(block=b11, decision=-1)
sch.compute_at(block=b11, loop=l13, preserve_unit_loops=True)
[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256), "float32"], placeholder_1: T.Buffer[(10, 256), "float32"], placeholder_2: T.Buffer[(1, 10), "float32"], T_add: T.Buffer[(1, 10), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True, "layout_free_placeholders": [1]})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 10], dtype="float32")
            for i0_0, i1_0, i0_1, i1_1, i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(1, 2, 1, 1, 32, 1, 5, 8, 1, 1):
                with T.block("T_matmul_NT"):
                    i = T.axis.spatial(1, 0)
                    j = T.axis.spatial(10, i1_0 * 5 + i1_2)
                    k = T.axis.reduce(256, i2_0 * 8 + i2_1)
                    T.reads(placeholder[i, k], placeholder_1[j, k])
                    T.writes(T_matmul_NT[i, j])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                    with T.init():
                        T_matmul_NT[i, j] = T.float32(0)
                    T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
            for i0, i1 in T.grid(1, 10):
                with T.block("T_add"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                    T.writes(T_add[ax0, ax1])
                    T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l9, l10, l11, l12 = sch.split(loop=l2, factors=[v5, v6, v7, v8])
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 5, 1])
l17, l18, l19, l20 = sch.split(loop=l3, factors=[v13, v14, v15, v16])
v21, v22 = sch.sample_perfect_tile(loop=l4, n=2, max_innermost_factor=64, decision=[32, 8])
l23, l24 = sch.split(loop=l4, factors=[v21, v22])
sch.reorder(l9, l17, l10, l18, l23, l11, l19, l24, l12, l20)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v25 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v25)
[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #3:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256), "float32"], placeholder_1: T.Buffer[(10, 256), "float32"], placeholder_2: T.Buffer[(1, 10), "float32"], T_add: T.Buffer[(1, 10), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True, "layout_free_placeholders": [1]})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 10], dtype="float32")
            for i0_0, i1_0, i0_1, i1_1 in T.grid(1, 2, 1, 1):
                for i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(32, 1, 5, 8, 1, 1):
                    with T.block("T_matmul_NT"):
                        i = T.axis.spatial(1, 0)
                        j = T.axis.spatial(10, i1_0 * 5 + i1_2)
                        k = T.axis.reduce(256, i2_0 * 8 + i2_1)
                        T.reads(placeholder[i, k], placeholder_1[j, k])
                        T.writes(T_matmul_NT[i, j])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            T_matmul_NT[i, j] = T.float32(0)
                        T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
                for ax0, ax1 in T.grid(1, 5):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(10, i1_0 * 5 + ax1)
                        T.reads(T_matmul_NT[ax0_1, ax1_1], placeholder_2[ax0_1, ax1_1])
                        T.writes(T_add[ax0_1, ax1_1])
                        T_add[ax0_1, ax1_1] = T_matmul_NT[ax0_1, ax1_1] + placeholder_2[ax0_1, ax1_1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l9, l10, l11, l12 = sch.split(loop=l2, factors=[v5, v6, v7, v8])
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 5, 1])
l17, l18, l19, l20 = sch.split(loop=l3, factors=[v13, v14, v15, v16])
v21, v22 = sch.sample_perfect_tile(loop=l4, n=2, max_innermost_factor=64, decision=[32, 8])
l23, l24 = sch.split(loop=l4, factors=[v21, v22])
sch.reorder(l9, l17, l10, l18, l23, l11, l19, l24, l12, l20)
b25, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b25, loop=l18, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #4:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256), "float32"], placeholder_1: T.Buffer[(10, 256), "float32"], placeholder_2: T.Buffer[(1, 10), "float32"], T_add: T.Buffer[(1, 10), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True, "layout_free_placeholders": [1]})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 10], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 2):
                for i0_1, i1_1, i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(1, 1, 32, 1, 5, 8, 1, 1):
                    with T.block("T_matmul_NT"):
                        i = T.axis.spatial(1, 0)
                        j = T.axis.spatial(10, i1_0 * 5 + i1_2)
                        k = T.axis.reduce(256, i2_0 * 8 + i2_1)
                        T.reads(placeholder[i, k], placeholder_1[j, k])
                        T.writes(T_matmul_NT[i, j])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            T_matmul_NT[i, j] = T.float32(0)
                        T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
                for ax0, ax1 in T.grid(1, 5):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(10, i1_0 * 5 + ax1)
                        T.reads(T_matmul_NT[ax0_1, ax1_1], placeholder_2[ax0_1, ax1_1])
                        T.writes(T_add[ax0_1, ax1_1])
                        T_add[ax0_1, ax1_1] = T_matmul_NT[ax0_1, ax1_1] + placeholder_2[ax0_1, ax1_1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l9, l10, l11, l12 = sch.split(loop=l2, factors=[v5, v6, v7, v8])
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 5, 1])
l17, l18, l19, l20 = sch.split(loop=l3, factors=[v13, v14, v15, v16])
v21, v22 = sch.sample_perfect_tile(loop=l4, n=2, max_innermost_factor=64, decision=[32, 8])
l23, l24 = sch.split(loop=l4, factors=[v21, v22])
sch.reorder(l9, l17, l10, l18, l23, l11, l19, l24, l12, l20)
b25, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b25, loop=l17, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |                         fused_nn_max_pool2d |    6272 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                                fused_nn_pad |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |              fused_layout_transform_reshape |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                          fused_nn_dense_add |    5130 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[00:56:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_nn_pad_layout_transform"
[00:56:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:56:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:56:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[00:56:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:57:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[00:57:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[00:57:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[00:57:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[00:57:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 28 candidates:
[1 : 16]:	0.9261  0.9128  0.8830  0.8662  0.8637  0.8534  0.8521  0.8052  0.7796  0.6650  0.6158  0.5990  0.5977  0.5188  0.4930  0.4470
[17 : 28]:	0.4072  0.4071  0.4009  0.3475  0.3435  0.3419  0.3401  0.2605  0.2583  0.1449  0.1440  0.0675
[00:57:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 28 candidate(s) with evolutionary search
[00:57:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 28 candidates(s) for measurement
[00:57:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 28 sample(s) to builder
[00:58:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 28 sample(s) to runner
[00:58:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"
[00:58:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:58:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:58:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea76efd058)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea76ef51e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7719a2a8)]: 0 failure(s)
[00:58:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:58:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea76efd058)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea76ef51e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7719a2a8)]: 0 failure(s)
[00:58:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea76efd058)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea76ef51e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7719a2a8)]: 0 failure(s)
[00:58:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea76efd058)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea76ef51e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7719a2a8)]: 0 failure(s)
[00:58:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea76efd058)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea76ef51e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7719a2a8)]: 0 failure(s)
[00:58:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9997  0.9997  0.9997  0.9997  0.9992  0.9991  0.9989  0.9989  0.9987  0.9985  0.9984  0.9984  0.9983  0.9983
[17 : 32]:	0.9981  0.9980  0.9980  0.9978  0.9978  0.9976  0.9975  0.9975  0.9975  0.9973  0.9973  0.9973  0.9972  0.9971  0.9971  0.9971
[00:58:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:58:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:58:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:59:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:00:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_max_pool2d"
[01:00:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:00:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:00:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[01:00:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:00:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[01:00:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[01:00:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[01:00:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[01:00:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 12 candidates:
[1 : 12]:	0.9417  0.8546  0.8505  0.8125  0.8119  0.7312  0.5763  0.5286  0.4637  0.4299  0.0295  0.0165
[01:00:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 12 candidate(s) with evolutionary search
[01:00:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 12 candidates(s) for measurement
[01:00:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 12 sample(s) to builder
[01:00:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 12 sample(s) to runner
[01:00:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_pad"
[01:00:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:00:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:01:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[01:01:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:01:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[01:01:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[01:02:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[01:02:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[01:03:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 12 candidates:
[1 : 12]:	0.9087  0.7529  0.6571  0.6240  0.5619  0.5491  0.4938  0.4533  0.3401  0.3234  0.1005  0.0382
[01:03:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 12 candidate(s) with evolutionary search
[01:03:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 12 candidates(s) for measurement
[01:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 12 sample(s) to builder
[01:03:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 12 sample(s) to runner
[01:03:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"
[01:03:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:03:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:04:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7731da78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7732dc58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7725a828)]: 0 failure(s)
[01:04:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:04:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7731da78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7732dc58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7725a828)]: 0 failure(s)
[01:05:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7731da78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7732dc58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7725a828)]: 0 failure(s)
[01:06:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7731da78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7732dc58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7725a828)]: 0 failure(s)
[01:07:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7731da78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7732dc58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7725a828)]: 0 failure(s)
[01:07:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9999  0.9998  0.9994  0.9994  0.9988  0.9987  0.9986  0.9985  0.9983  0.9983  0.9982  0.9982  0.9982  0.9981
[17 : 32]:	0.9980  0.9980  0.9980  0.9979  0.9978  0.9977  0.9976  0.9971  0.9970  0.9970  0.9968  0.9966  0.9966  0.9964  0.9962  0.9960
[01:07:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:07:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:08:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:08:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:10:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_max_pool2d_1"
[01:10:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:10:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:10:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea77199458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7728b1a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea76efda18)]: 0 failure(s)
[01:10:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:10:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea77199458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7728b1a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea76efda18)]: 0 failure(s)
[01:11:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea77199458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7728b1a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea76efda18)]: 0 failure(s)
[01:11:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea77199458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7728b1a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea76efda18)]: 0 failure(s)
[01:12:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea77199458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7728b1a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea76efda18)]: 0 failure(s)
[01:12:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9987  0.9954  0.9937  0.9876  0.9253  0.8972  0.8958  0.8958  0.8926  0.8886  0.8838  0.8786  0.8703  0.8671  0.8650  0.8434
[17 : 32]:	0.8413  0.8135  0.8071  0.8056  0.8021  0.7849  0.7780  0.7764  0.7668  0.7637  0.7594  0.7576  0.7472  0.7220  0.7203  0.7188
[01:12:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:12:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 31 candidates(s) for measurement
[01:12:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 31 sample(s) to builder
[01:13:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 31 sample(s) to runner
[01:16:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_layout_transform_reshape"
[01:16:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:16:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:16:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:16:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:16:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:17:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:17:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:18:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:18:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 20 candidates:
[1 : 16]:	0.9792  0.9522  0.9120  0.8790  0.8788  0.8169  0.5945  0.5612  0.5474  0.5192  0.4156  0.3772  0.3690  0.3379  0.2244  0.1608
[17 : 20]:	0.1532  0.0534  0.0495  0.0090
[01:18:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 20 candidate(s) with evolutionary search
[01:18:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 20 candidates(s) for measurement
[01:18:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 20 sample(s) to builder
[01:18:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 20 sample(s) to runner
[01:21:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_dense_add"
[01:21:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:21:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:21:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea77340118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea76efab18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea772d84b8)]: 0 failure(s)
[01:21:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:22:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea77340118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea76efab18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea772d84b8)]: 0 failure(s)
[01:22:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea77340118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea76efab18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea772d84b8)]: 0 failure(s)
[01:23:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea77340118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea76efab18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea772d84b8)]: 0 failure(s)
[01:23:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea77340118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea76efab18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea772d84b8)]: 0 failure(s)
[01:24:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9988  0.9960  0.9954  0.9952  0.9946  0.9945  0.9944  0.9943  0.9935  0.9935  0.9934  0.9932  0.9927  0.9927  0.9915  0.9897
[17 : 32]:	0.9886  0.9882  0.9880  0.9877  0.9868  0.9857  0.9838  0.9826  0.9826  0.9821  0.9816  0.9809  0.9808  0.9797  0.9794  0.9788
[01:24:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:24:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:24:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:24:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #0: GFLOPs: 0.0000. Time: 0.0116 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #1: GFLOPs: 0.0000. Time: 0.0101 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #2: GFLOPs: 0.0000. Time: 0.0125 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #3: GFLOPs: 0.0000. Time: 0.0101 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #4: GFLOPs: 0.0000. Time: 0.0123 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #5: GFLOPs: 0.0000. Time: 0.0120 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #6: GFLOPs: 0.0000. Time: 0.0085 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #7: GFLOPs: 0.0000. Time: 0.0101 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #8: GFLOPs: 0.0000. Time: 0.0235 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #9: GFLOPs: 0.0000. Time: 0.0119 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #10: GFLOPs: 0.0000. Time: 0.0220 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #11: GFLOPs: 0.0000. Time: 0.0214 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #12: GFLOPs: 0.0000. Time: 0.0120 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #13: GFLOPs: 0.0000. Time: 0.0222 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #14: GFLOPs: 0.0000. Time: 0.0133 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #15: GFLOPs: 0.0000. Time: 0.0225 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #16: GFLOPs: 0.0000. Time: 0.0106 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #17: GFLOPs: 0.0000. Time: 0.0115 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #18: GFLOPs: 0.0000. Time: 0.0207 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #19: GFLOPs: 0.0000. Time: 0.0140 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #20: GFLOPs: 0.0000. Time: 0.0065 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #21: GFLOPs: 0.0000. Time: 0.0104 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #22: GFLOPs: 0.0000. Time: 0.0120 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #23: GFLOPs: 0.0000. Time: 0.0104 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #24: GFLOPs: 0.0000. Time: 0.0183 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #25: GFLOPs: 0.0000. Time: 0.0109 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #26: GFLOPs: 0.0000. Time: 0.0104 ms. Best GFLOPs: 0.0000
[01:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad_layout_transform"] Trial #27: GFLOPs: 0.0000. Time: 0.0108 ms. Best GFLOPs: 0.0000
[01:26:28] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_nn_pad_layout_transform"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |                         fused_nn_max_pool2d |    6272 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                                fused_nn_pad |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |              fused_layout_transform_reshape |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                          fused_nn_dense_add |    5130 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 28
Total latency (us): 6.50122

[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #0: GFLOPs: 0.1009. Time: 3.2339 ms. Best GFLOPs: 0.1009
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #1: GFLOPs: 0.0949. Time: 3.4355 ms. Best GFLOPs: 0.1009
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #2: GFLOPs: 0.0349. Time: 9.3328 ms. Best GFLOPs: 0.1009
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #3: GFLOPs: 0.0408. Time: 7.9895 ms. Best GFLOPs: 0.1009
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #4: GFLOPs: 0.1227. Time: 2.6574 ms. Best GFLOPs: 0.1227
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #5: GFLOPs: 0.2603. Time: 1.2530 ms. Best GFLOPs: 0.2603
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #6: GFLOPs: 0.3484. Time: 0.9360 ms. Best GFLOPs: 0.3484
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #7: GFLOPs: 0.1768. Time: 1.8451 ms. Best GFLOPs: 0.3484
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #8: GFLOPs: 0.2968. Time: 1.0988 ms. Best GFLOPs: 0.3484
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #9: GFLOPs: 0.0426. Time: 7.6536 ms. Best GFLOPs: 0.3484
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #10: GFLOPs: 0.1917. Time: 1.7012 ms. Best GFLOPs: 0.3484
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #11: GFLOPs: 1.1499. Time: 0.2836 ms. Best GFLOPs: 1.1499
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #12: GFLOPs: 0.1860. Time: 1.7535 ms. Best GFLOPs: 1.1499
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #13: GFLOPs: 0.1985. Time: 1.6432 ms. Best GFLOPs: 1.1499
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #14: GFLOPs: 0.9651. Time: 0.3379 ms. Best GFLOPs: 1.1499
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #15: GFLOPs: 0.4585. Time: 0.7113 ms. Best GFLOPs: 1.1499
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #16: GFLOPs: 0.6898. Time: 0.4728 ms. Best GFLOPs: 1.1499
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #17: GFLOPs: 0.1921. Time: 1.6982 ms. Best GFLOPs: 1.1499
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #18: GFLOPs: 0.1555. Time: 2.0968 ms. Best GFLOPs: 1.1499
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #19: GFLOPs: 0.4133. Time: 0.7891 ms. Best GFLOPs: 1.1499
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #20: GFLOPs: 3.2027. Time: 0.1018 ms. Best GFLOPs: 3.2027
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #21: GFLOPs: 0.8025. Time: 0.4064 ms. Best GFLOPs: 3.2027
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #22: GFLOPs: 0.3829. Time: 0.8518 ms. Best GFLOPs: 3.2027
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 32, 32, 1), "float32"], placeholder_1: T.Buffer[(2, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 2, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 2, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 2, 28, 28, 4], dtype="float32")
        for i0_0 in T.serial(1, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 1, 2, 1, 1, 1):
                    for i2_2_init, i3_2_init, i1_3_init in T.grid(14, 2, 2):
                        for i2_3_i3_3_i4_3_fused_init in T.vectorized(56):
                            with T.block("conv2d_NCHWc_init"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(2, i1_3_init)
                                oh = T.axis.spatial(28, i2_1 * 14 + i2_2_init)
                                ow = T.axis.spatial(28, i3_2_init * 14 + i2_3_i3_3_i4_3_fused_init // 4)
                                oc_block = T.axis.spatial(4, i2_3_i3_3_i4_3_fused_init % 4)
                                T.reads()
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 32, 32, 1], "float32"], ["TENSOR", [2, 1, 5, 5, 1, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW1c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3 in T.grid(5, 1, 1, 1, 14, 2, 1, 1, 1, 5, 1, 2):
                        for i2_3_i3_3_i4_3_fused in T.vectorized(56):
                            with T.block("conv2d_NCHWc_update"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(2, i1_3)
                                oh = T.axis.spatial(28, i2_1 * 14 + i2_2)
                                ow = T.axis.spatial(28, i3_2 * 14 + i2_3_i3_3_i4_3_fused // 4)
                                oc_block = T.axis.spatial(4, i2_3_i3_3_i4_3_fused % 4)
                                ic = T.axis.reduce(1, 0)
                                kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                                T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic, oh + kh, ow + kw, 0], placeholder_1[oc_chunk, ic, kh, kw, 0, oc_block])
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 32, 32, 1], "float32"], ["TENSOR", [2, 1, 5, 5, 1, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW1c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic, oh + kh, ow + kw, 0] * placeholder_1[oc_chunk, ic, kh, kw, 0, oc_block]
                for ax0, ax1, ax2, ax3 in T.grid(1, 2, 28, 28):
                    for ax4_fused in T.vectorized(4):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1, ax2_1, ax3_1, ax4 = T.axis.remap("SSSS", [ax1, ax2, ax3, ax4_fused])
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[5, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 5])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l91, l92, l93)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l68, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l68, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b67)
l105 = sch.fuse(l104)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b106)
b131 = sch.decompose_reduction(block=b106, loop=l118)
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #24: GFLOPs: 1.0549. Time: 0.3092 ms. Best GFLOPs: 3.2027
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #25: GFLOPs: 3.4928. Time: 0.0934 ms. Best GFLOPs: 3.4928
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #26: GFLOPs: 2.2536. Time: 0.1447 ms. Best GFLOPs: 3.4928
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #27: GFLOPs: 4.2633. Time: 0.0765 ms. Best GFLOPs: 4.2633
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #28: GFLOPs: 4.8139. Time: 0.0678 ms. Best GFLOPs: 4.8139
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #29: GFLOPs: 9.5547. Time: 0.0341 ms. Best GFLOPs: 9.5547
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #30: GFLOPs: 9.5602. Time: 0.0341 ms. Best GFLOPs: 9.5602
[01:26:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #31: GFLOPs: 7.6952. Time: 0.0424 ms. Best GFLOPs: 9.5602
[01:26:42] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |            
  2 |                         fused_nn_max_pool2d |    6272 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                                fused_nn_pad |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |              fused_layout_transform_reshape |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                          fused_nn_dense_add |    5130 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 60
Total latency (us): 40.616

[01:26:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #0: GFLOPs: 0.1532. Time: 0.0409 ms. Best GFLOPs: 0.1532
[01:26:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #1: GFLOPs: 0.1644. Time: 0.0381 ms. Best GFLOPs: 0.1644
[01:26:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #2: GFLOPs: 0.1684. Time: 0.0373 ms. Best GFLOPs: 0.1684
[01:26:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #3: GFLOPs: 0.1684. Time: 0.0372 ms. Best GFLOPs: 0.1684
[01:26:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #4: GFLOPs: 0.1432. Time: 0.0438 ms. Best GFLOPs: 0.1684
[01:26:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #5: GFLOPs: 0.3149. Time: 0.0199 ms. Best GFLOPs: 0.3149
[01:26:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #6: GFLOPs: 0.1421. Time: 0.0442 ms. Best GFLOPs: 0.3149
[01:26:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #7: GFLOPs: 0.1460. Time: 0.0429 ms. Best GFLOPs: 0.3149
[01:26:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #8: GFLOPs: 0.0294. Time: 0.2136 ms. Best GFLOPs: 0.3149
[01:26:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #9: GFLOPs: 0.0153. Time: 0.4101 ms. Best GFLOPs: 0.3149
[01:26:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #10: GFLOPs: 0.0159. Time: 0.3949 ms. Best GFLOPs: 0.3149
[01:26:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #11: GFLOPs: 0.0135. Time: 0.4663 ms. Best GFLOPs: 0.3149
[01:27:11] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_max_pool2d"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |            
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |            
  3 |                                fused_nn_pad |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |              fused_layout_transform_reshape |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                          fused_nn_dense_add |    5130 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 72
Total latency (us): 60.5345

[01:27:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_pad"] Trial #0: GFLOPs: 0.0000. Time: 0.9452 ms. Best GFLOPs: 0.0000
[01:27:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_pad"] Trial #1: GFLOPs: 0.0000. Time: 0.5444 ms. Best GFLOPs: 0.0000
[01:27:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_pad"] Trial #2: GFLOPs: 0.0000. Time: 0.8277 ms. Best GFLOPs: 0.0000
[01:27:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_pad"] Trial #3: GFLOPs: 0.0000. Time: 1.3462 ms. Best GFLOPs: 0.0000
[01:27:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_pad"] Trial #4: GFLOPs: 0.0000. Time: 0.8485 ms. Best GFLOPs: 0.0000
[01:27:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_pad"] Trial #5: GFLOPs: 0.0000. Time: 0.7712 ms. Best GFLOPs: 0.0000
[01:27:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_pad"] Trial #6: GFLOPs: 0.0000. Time: 2.3975 ms. Best GFLOPs: 0.0000
[01:27:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_pad"] Trial #7: GFLOPs: 0.0000. Time: 0.8388 ms. Best GFLOPs: 0.0000
[01:27:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_pad"] Trial #8: GFLOPs: 0.0000. Time: 0.5801 ms. Best GFLOPs: 0.0000
[01:27:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_pad"] Trial #9: GFLOPs: 0.0000. Time: 1.1824 ms. Best GFLOPs: 0.0000
[01:27:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_pad"] Trial #10: GFLOPs: 0.0000. Time: 0.8035 ms. Best GFLOPs: 0.0000
[01:27:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_pad"] Trial #11: GFLOPs: 0.0000. Time: 0.7748 ms. Best GFLOPs: 0.0000
[01:27:44] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_pad"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |            
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |            
  3 |                                fused_nn_pad |       1 |      1 |         0.0000 |     544.3648 |              544.3648 |     12 |            
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |              fused_layout_transform_reshape |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                          fused_nn_dense_add |    5130 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 84
Total latency (us): 604.899

[01:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #0: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 2, 18, 18, 4), "float32"], placeholder_1: T.Buffer[(4, 2, 5, 5, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 4, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(392, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i1_3_init, i2_3_init in T.grid(2, 2, 2):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(4, i1_2_init * 2 + i1_3_init)
                    oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 56 * 2 + i2_3_init)
                    ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 56 // 8 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 2, 18, 18, 4], "float32"], ["TENSOR", [4, 2, 5, 5, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 5, 1, 2, 1, 1, 1, 4, 5, 1, 1, 2, 2, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(4, i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 56 * 2 + i2_3)
                    ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 56 // 8 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    ic = T.axis.reduce(8, i5_0 * 4 + i5_1)
                    kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 2, 18, 18, 4], "float32"], ["TENSOR", [4, 2, 5, 5, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(56):
                with T.block("T_relu"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(4, i0_i1_i2_fused // 14)
                    ax2 = T.axis.spatial(14, i0_i1_i2_fused % 14)
                    ax3 = T.axis.spatial(14, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 5])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[5, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l97, l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[01:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #1: GFLOPs: 5.2040. Time: 0.2423 ms. Best GFLOPs: 5.2040
[01:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 2, 18, 18, 4), "float32"], placeholder_1: T.Buffer[(4, 2, 5, 5, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 4, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(14, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 1, 1):
                for i3_2_init, i2_3_init in T.grid(7, 2):
                    for i3_3_i4_3_fused_init in T.vectorized(4):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(4, i1_1)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 2 + i2_3_init)
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 7 + i3_2_init)
                            oc_block = T.axis.spatial(4, i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 2, 18, 18, 4], "float32"], ["TENSOR", [4, 2, 5, 5, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(8, 5, 5, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 2):
                    for i3_3_i4_3_fused in T.vectorized(4):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(4, i1_1)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 2 + i2_3)
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 7 + i3_2)
                            oc_block, ic, kh, kw = T.axis.remap("SRRR", [i3_3_i4_3_fused, i5_0, i6_0, i7_0])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 2, 18, 18, 4], "float32"], ["TENSOR", [4, 2, 5, 5, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 4, 2):
                for ax3_ax4_fused in T.vectorized(28):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(4, ax1)
                        ax2_1 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 2 + ax2)
                        ax3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 7 + ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[5, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[5, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l92, l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l100, l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b103)
b125 = sch.decompose_reduction(block=b103, loop=l110)
[01:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #3: GFLOPs: 0.1999. Time: 6.3069 ms. Best GFLOPs: 5.2040
[01:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #4: GFLOPs: 0.3028. Time: 4.1635 ms. Best GFLOPs: 5.2040
[01:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #5: GFLOPs: 0.1126. Time: 11.1983 ms. Best GFLOPs: 5.2040
[01:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #6: GFLOPs: 0.2687. Time: 4.6921 ms. Best GFLOPs: 5.2040
[01:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 2, 18, 18, 4), "float32"], placeholder_1: T.Buffer[(4, 2, 5, 5, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 4, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 2, 1, 1):
                for i1_2_init, i3_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 7, 2, 2, 7, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(4, i1_2_init * 2 + i1_3_init)
                        oh = T.axis.spatial(14, i2_1 * 7 + i2_3_init)
                        ow = T.axis.spatial(14, i3_2_init * 2 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 2, 18, 18, 4], "float32"], ["TENSOR", [4, 2, 5, 5, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 5, 5, 1, 2, 1, 7, 2, 2, 1, 1, 1, 2, 7, 2, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(4, i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 7 + i2_3)
                        ow = T.axis.spatial(14, i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + i4_2)
                        ic = T.axis.reduce(8, i5_0 * 2 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 2, 18, 18, 4], "float32"], ["TENSOR", [4, 2, 5, 5, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 14, 14):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1, ax3_1 = T.axis.remap("SSS", [ax1, ax2, ax3])
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[5, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[5, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #8: GFLOPs: 0.2584. Time: 4.8782 ms. Best GFLOPs: 5.2040
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #9: GFLOPs: 0.2244. Time: 5.6174 ms. Best GFLOPs: 5.2040
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #10: GFLOPs: 0.2472. Time: 5.0998 ms. Best GFLOPs: 5.2040
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #11: GFLOPs: 0.0939. Time: 13.4199 ms. Best GFLOPs: 5.2040
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #12: GFLOPs: 0.1866. Time: 6.7571 ms. Best GFLOPs: 5.2040
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #13: GFLOPs: 0.2994. Time: 4.2100 ms. Best GFLOPs: 5.2040
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #14: GFLOPs: 0.1926. Time: 6.5447 ms. Best GFLOPs: 5.2040
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #15: GFLOPs: 0.4237. Time: 2.9754 ms. Best GFLOPs: 5.2040
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #16: GFLOPs: 0.2074. Time: 6.0791 ms. Best GFLOPs: 5.2040
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #17: GFLOPs: 0.2182. Time: 5.7772 ms. Best GFLOPs: 5.2040
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #18: GFLOPs: 5.8938. Time: 0.2139 ms. Best GFLOPs: 5.8938
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #19: GFLOPs: 0.2987. Time: 4.2212 ms. Best GFLOPs: 5.8938
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #20: GFLOPs: 0.1719. Time: 7.3327 ms. Best GFLOPs: 5.8938
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #21: GFLOPs: 0.2395. Time: 5.2636 ms. Best GFLOPs: 5.8938
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #22: GFLOPs: 0.1970. Time: 6.3982 ms. Best GFLOPs: 5.8938
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #23: GFLOPs: 2.4629. Time: 0.5119 ms. Best GFLOPs: 5.8938
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #24: GFLOPs: 0.1669. Time: 7.5548 ms. Best GFLOPs: 5.8938
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 2, 18, 18, 4), "float32"], placeholder_1: T.Buffer[(4, 2, 5, 5, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 4, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 7, 14, 1):
                for i2_2_init in T.serial(2):
                    for i3_2_i4_2_i5_1_i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init in T.vectorized(4):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + i1_1)
                            oh = T.axis.spatial(14, i2_1 * 2 + i2_2_init)
                            ow = T.axis.spatial(14, i3_1)
                            oc_block = T.axis.spatial(4, i3_2_i4_2_i5_1_i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init // 2 * 2 + i3_2_i4_2_i5_1_i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init % 2)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 2, 18, 18, 4], "float32"], ["TENSOR", [4, 2, 5, 5, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2 in T.grid(8, 5, 5, 1, 1, 2):
                    for i3_2_i4_2_i5_1_i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused in T.vectorized(4):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + i1_1)
                            oh = T.axis.spatial(14, i2_1 * 2 + i2_2)
                            ow = T.axis.spatial(14, i3_1)
                            oc_block = T.axis.spatial(4, i3_2_i4_2_i5_1_i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused // 2 * 2 + i3_2_i4_2_i5_1_i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused % 2)
                            ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_0])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 2, 18, 18, 4], "float32"], ["TENSOR", [4, 2, 5, 5, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 2, 14):
                for ax3_ax4_fused in T.vectorized(56):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3 = T.axis.spatial(14, ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[5, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[5, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l84, l85, l86, l87, l88, l89, l90, l91, l92, l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l100, l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b103)
b117 = sch.decompose_reduction(block=b103, loop=l110)
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #26: GFLOPs: 0.2303. Time: 5.4731 ms. Best GFLOPs: 5.8938
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #27: GFLOPs: 0.1912. Time: 6.5926 ms. Best GFLOPs: 5.8938
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #28: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 2, 18, 18, 4), "float32"], placeholder_1: T.Buffer[(4, 2, 5, 5, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 4, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(98, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i1_3_init, i2_3_init, i3_3_init in T.grid(2, 2, 2):
                for i4_3_fused_init in T.vectorized(4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 49 * 2 + i1_3_init)
                        oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 49 // 7 * 2 + i2_3_init)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7 * 2 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 2, 18, 18, 4], "float32"], ["TENSOR", [4, 2, 5, 5, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(2, 5, 1, 1, 1, 1, 1, 1, 4, 1, 5, 1, 2, 2, 2):
                for i4_3_fused in T.vectorized(4):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 49 * 2 + i1_3)
                        oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 49 // 7 * 2 + i2_3)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3_fused)
                        ic = T.axis.reduce(8, i5_0 * 4 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 2, 18, 18, 4], "float32"], ["TENSOR", [4, 2, 5, 5, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(56):
                with T.block("T_relu"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(4, i0_i1_i2_fused // 14)
                    ax2 = T.axis.spatial(14, i0_i1_i2_fused % 14)
                    ax3 = T.axis.spatial(14, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[5, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 5])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l98, l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l104)
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #29: GFLOPs: 0.2157. Time: 5.8450 ms. Best GFLOPs: 5.8938
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 2, 18, 18, 4), "float32"], placeholder_1: T.Buffer[(4, 2, 5, 5, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 4, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 4, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i5_0, i6_0 in T.grid(1, 1):
                for i2_2_init, i3_2_init in T.grid(7, 7):
                    for i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 7 + i2_2_init)
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 7 + i3_2_init)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 2 + i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 2, 18, 18, 4], "float32"], ["TENSOR", [4, 2, 5, 5, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1 in T.grid(5, 1, 1, 7, 7, 1, 8, 5):
                    for i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 7 + i2_2)
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 7 + i3_2)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 2 + i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused)
                            ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_1, i7_0])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 2, 18, 18, 4], "float32"], ["TENSOR", [4, 2, 5, 5, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 7, 7):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                        ax2_1 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 7 + ax2)
                        ax3_1 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 7 + ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 5])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[5, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)
sch.parallel(loop=l94)
l95 = sch.fuse(l88, l89, l90, l91, l92, l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b103)
b116 = sch.decompose_reduction(block=b103, loop=l107)
[01:27:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #31: GFLOPs: 0.1483. Time: 8.5009 ms. Best GFLOPs: 5.8938
[01:28:05] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |            
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |            
  3 |                                fused_nn_pad |       1 |      1 |         0.0000 |     544.3648 |              544.3648 |     12 |            
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |         5.8938 |     213.8992 |              213.8992 |     32 |            
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |              fused_layout_transform_reshape |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                          fused_nn_dense_add |    5130 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 116
Total latency (us): 818.799

[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #0: GFLOPs: 0.0012. Time: 1.9471 ms. Best GFLOPs: 0.0012
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #1: GFLOPs: 0.0009. Time: 2.4738 ms. Best GFLOPs: 0.0012
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #2: GFLOPs: 0.0009. Time: 2.5175 ms. Best GFLOPs: 0.0012
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #3: GFLOPs: 0.0001. Time: 15.9976 ms. Best GFLOPs: 0.0012
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_max_pool2d_1"] Trial #4: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 14, 14, 4), "float32"], tensor: T.Buffer[(1, 4, 4, 4, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        tensor_rf = T.alloc_buffer([1, 4, 4, 4, 4, 9], dtype="float32")
        for i0_i1_fused in T.parallel(4):
            for i2, i3 in T.grid(4, 4):
                for i4_i5_i6_fused_0_i5_i6_fused_1_fused in T.vectorized(36):
                    with T.block("tensor_rf"):
                        vi5_i6_fused_1 = T.axis.spatial(9, i4_i5_i6_fused_0_i5_i6_fused_1_fused % 9)
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3 = T.axis.remap("SSS", [i0_i1_fused, i2, i3])
                        ax4 = T.axis.spatial(4, i4_i5_i6_fused_0_i5_i6_fused_1_fused // 9)
                        T.reads(placeholder[ax0, ax1, ax2 * 3 + vi5_i6_fused_1 // 3, ax3 * 3 + vi5_i6_fused_1 % 3, ax4])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = placeholder[ax0, ax1, ax2 * 3 + vi5_i6_fused_1 // 3, ax3 * 3 + vi5_i6_fused_1 % 3, ax4]
        for i0_i1_fused in T.parallel(4):
            for i2, i3, i4, i5_i6_fused_0 in T.grid(4, 4, 4, 1):
                with T.block("tensor_init"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                    T.reads()
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                for i5_i6_fused_1 in T.serial(9):
                    with T.block("tensor_update"):
                        vi5_i6_fused_1 = T.axis.reduce(9, i5_i6_fused_1)
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                        T.reads(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                        T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                        T.block_attr({"meta_schedule.random_compute_producer":True})
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 9])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
sch.enter_postproc()
b16 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.unroll_explicit")
b17, b18 = sch.get_child_blocks(b16)
l19, l20, l21, l22, l23, l24, l25 = sch.get_loops(block=b17)
l26 = sch.fuse(l19, l20)
sch.parallel(loop=l26)
l27 = sch.fuse(l23, l24, l25)
sch.vectorize(loop=l27)
l28, l29, l30, l31, l32, l33, l34 = sch.get_loops(block=b18)
l35 = sch.fuse(l28, l29)
sch.parallel(loop=l35)
b36 = sch.get_block(name="tensor", func_name="main")
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
b43 = sch.decompose_reduction(block=b36, loop=l42)
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #5: GFLOPs: 0.0002. Time: 12.8427 ms. Best GFLOPs: 0.0012
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #6: GFLOPs: 0.0002. Time: 12.3995 ms. Best GFLOPs: 0.0012
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_max_pool2d_1"] Trial #7: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 14, 14, 4), "float32"], tensor: T.Buffer[(1, 4, 4, 4, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        tensor_rf = T.alloc_buffer([1, 4, 4, 4, 4, 1], dtype="float32")
        for i0_i1_i2_i3_i4_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i5_i6_fused_0 in T.serial(1):
                with T.block("tensor_rf_init"):
                    vi5_i6_fused_0 = T.axis.spatial(1, 0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(4, i0_i1_i2_i3_i4_fused // 64)
                    ax2 = T.axis.spatial(4, i0_i1_i2_i3_i4_fused % 64 // 16)
                    ax3 = T.axis.spatial(4, i0_i1_i2_i3_i4_fused % 16 // 4)
                    ax4 = T.axis.spatial(4, i0_i1_i2_i3_i4_fused % 4)
                    T.reads()
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.float32(-3.4028234663852886e+38)
                for i5_i6_fused_1 in T.serial(9):
                    with T.block("tensor_rf_update"):
                        vi5_i6_fused_0 = T.axis.spatial(1, 0)
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(4, i0_i1_i2_i3_i4_fused // 64)
                        ax2 = T.axis.spatial(4, i0_i1_i2_i3_i4_fused % 64 // 16)
                        ax3 = T.axis.spatial(4, i0_i1_i2_i3_i4_fused % 16 // 4)
                        ax4 = T.axis.spatial(4, i0_i1_i2_i3_i4_fused % 4)
                        rv0 = T.axis.reduce(3, i5_i6_fused_1 // 3)
                        rv1 = T.axis.reduce(3, i5_i6_fused_1 % 3)
                        T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0], placeholder[ax0, ax1, ax2 * 3 + rv0, ax3 * 3 + rv1, ax4])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0], placeholder[ax0, ax1, ax2 * 3 + rv0, ax3 * 3 + rv1, ax4])
        for i0_i1_i2_i3_i4_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i5_i6_fused_0 in T.serial(1):
                with T.block("tensor"):
                    vi5_i6_fused_0 = T.axis.reduce(1, 0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(4, i0_i1_i2_i3_i4_fused // 64)
                    ax2 = T.axis.spatial(4, i0_i1_i2_i3_i4_fused % 64 // 16)
                    ax3 = T.axis.spatial(4, i0_i1_i2_i3_i4_fused % 16 // 4)
                    ax4 = T.axis.spatial(4, i0_i1_i2_i3_i4_fused % 4)
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 9])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l12, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
sch.enter_postproc()
b16 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.unroll_explicit")
b17, b18 = sch.get_child_blocks(b16)
l19, l20, l21, l22, l23, l24, l25 = sch.get_loops(block=b17)
l26 = sch.fuse(l19, l20, l21, l22, l23)
sch.parallel(loop=l26)
sch.annotate(block_or_loop=l26, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l26, ann_key="pragma_unroll_explicit", ann_val=1)
l27, l28, l29, l30, l31, l32 = sch.get_loops(block=b18)
l33 = sch.fuse(l27, l28, l29, l30, l31)
sch.parallel(loop=l33)
sch.annotate(block_or_loop=l33, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l33, ann_key="pragma_unroll_explicit", ann_val=1)
b34 = sch.get_block(name="tensor_rf", func_name="main")
l35, l36, l37 = sch.get_loops(block=b34)
b38 = sch.decompose_reduction(block=b34, loop=l37)
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #8: GFLOPs: 0.0002. Time: 10.4000 ms. Best GFLOPs: 0.0012
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #9: GFLOPs: 0.0005. Time: 4.4209 ms. Best GFLOPs: 0.0012
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #10: GFLOPs: 0.0003. Time: 6.9470 ms. Best GFLOPs: 0.0012
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #11: GFLOPs: 0.0004. Time: 6.3166 ms. Best GFLOPs: 0.0012
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #12: GFLOPs: 0.0002. Time: 12.9980 ms. Best GFLOPs: 0.0012
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #13: GFLOPs: 0.0001. Time: 17.8445 ms. Best GFLOPs: 0.0012
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_max_pool2d_1"] Trial #14: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 14, 14, 4), "float32"], tensor: T.Buffer[(1, 4, 4, 4, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        for i0_i1_i2_i3_fused in T.parallel(64, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i4 in T.serial(4):
                with T.block("tensor_init"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(4, i0_i1_i2_i3_fused // 16)
                    ax2 = T.axis.spatial(4, i0_i1_i2_i3_fused % 16 // 4)
                    ax3 = T.axis.spatial(4, i0_i1_i2_i3_fused % 4)
                    ax4 = T.axis.spatial(4, i4)
                    T.reads()
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                for i5, i6 in T.grid(3, 3):
                    with T.block("tensor_update"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(4, i0_i1_i2_i3_fused // 16)
                        ax2 = T.axis.spatial(4, i0_i1_i2_i3_fused % 16 // 4)
                        ax3 = T.axis.spatial(4, i0_i1_i2_i3_fused % 4)
                        ax4, rv0, rv1 = T.axis.remap("SRR", [i4, i5, i6])
                        T.reads(tensor[ax0, ax1, ax2, ax3, ax4], placeholder[ax0, ax1, ax2 * 3 + rv0, ax3 * 3 + rv1, ax4])
                        T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], placeholder[ax0, ax1, ax2 * 3 + rv0, ax3 * 3 + rv1, ax4])
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=16)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
sch.enter_postproc()
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit")
b3, = sch.get_child_blocks(b2)
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b3)
l11 = sch.fuse(l4, l5, l6, l7)
sch.parallel(loop=l11)
sch.annotate(block_or_loop=l11, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l11, ann_key="pragma_unroll_explicit", ann_val=1)
b12 = sch.get_block(name="tensor", func_name="main")
l13, l14, l15, l16 = sch.get_loops(block=b12)
b17 = sch.decompose_reduction(block=b12, loop=l15)
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #15: GFLOPs: 0.0002. Time: 11.7121 ms. Best GFLOPs: 0.0012
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #16: GFLOPs: 0.0008. Time: 3.0470 ms. Best GFLOPs: 0.0012
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #17: GFLOPs: 0.0003. Time: 8.4611 ms. Best GFLOPs: 0.0012
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #18: GFLOPs: 0.0004. Time: 5.8948 ms. Best GFLOPs: 0.0012
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #19: GFLOPs: 0.0005. Time: 4.8812 ms. Best GFLOPs: 0.0012
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #20: GFLOPs: 0.0010. Time: 2.4166 ms. Best GFLOPs: 0.0012
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #21: GFLOPs: 0.0002. Time: 11.6318 ms. Best GFLOPs: 0.0012
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #22: GFLOPs: 0.0007. Time: 3.0874 ms. Best GFLOPs: 0.0012
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #23: GFLOPs: 0.0010. Time: 2.4054 ms. Best GFLOPs: 0.0012
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #24: GFLOPs: 0.0017. Time: 1.3905 ms. Best GFLOPs: 0.0017
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #25: GFLOPs: 0.0020. Time: 1.1396 ms. Best GFLOPs: 0.0020
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #26: GFLOPs: 0.0004. Time: 5.3515 ms. Best GFLOPs: 0.0020
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_max_pool2d_1"] Trial #27: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 14, 14, 4), "float32"], tensor: T.Buffer[(1, 4, 4, 4, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        tensor_rf = T.alloc_buffer([1, 4, 4, 4, 4, 3], dtype="float32")
        for i0_i1_i2_i3_fused in T.parallel(64, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4, i5_i6_fused_0 in T.grid(4, 3):
                with T.block("tensor_rf_init"):
                    vi5_i6_fused_0 = T.axis.spatial(3, i5_i6_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(4, i0_i1_i2_i3_fused // 16)
                    ax2 = T.axis.spatial(4, i0_i1_i2_i3_fused % 16 // 4)
                    ax3 = T.axis.spatial(4, i0_i1_i2_i3_fused % 4)
                    ax4 = T.axis.spatial(4, i4)
                    T.reads()
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.float32(-3.4028234663852886e+38)
                for i5_i6_fused_1 in T.serial(3):
                    with T.block("tensor_rf_update"):
                        vi5_i6_fused_0 = T.axis.spatial(3, i5_i6_fused_0)
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(4, i0_i1_i2_i3_fused // 16)
                        ax2 = T.axis.spatial(4, i0_i1_i2_i3_fused % 16 // 4)
                        ax3 = T.axis.spatial(4, i0_i1_i2_i3_fused % 4)
                        ax4, vi5_i6_fused_1 = T.axis.remap("SR", [i4, i5_i6_fused_1])
                        T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0], placeholder[ax0, ax1, ax2 * 3 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) // 3, ax3 * 3 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) % 3, ax4])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0], placeholder[ax0, ax1, ax2 * 3 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) // 3, ax3 * 3 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) % 3, ax4])
        for i0_i1_i2_i3_fused in T.parallel(64, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4 in T.serial(4):
                with T.block("tensor_init"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(4, i0_i1_i2_i3_fused // 16)
                    ax2 = T.axis.spatial(4, i0_i1_i2_i3_fused % 16 // 4)
                    ax3 = T.axis.spatial(4, i0_i1_i2_i3_fused % 4)
                    ax4 = T.axis.spatial(4, i4)
                    T.reads()
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                for i5_i6_fused_0 in T.serial(3):
                    with T.block("tensor_update"):
                        vi5_i6_fused_0 = T.axis.reduce(3, i5_i6_fused_0)
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(4, i0_i1_i2_i3_fused // 16)
                        ax2 = T.axis.spatial(4, i0_i1_i2_i3_fused % 16 // 4)
                        ax3 = T.axis.spatial(4, i0_i1_i2_i3_fused % 4)
                        ax4 = T.axis.spatial(4, i4)
                        T.reads(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                        T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                        T.block_attr({"meta_schedule.random_compute_producer":True})
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 3])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l12, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=16)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
sch.enter_postproc()
b16 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b16, ann_key="meta_schedule.unroll_explicit")
b17, b18 = sch.get_child_blocks(b16)
l19, l20, l21, l22, l23, l24, l25 = sch.get_loops(block=b17)
l26 = sch.fuse(l19, l20, l21, l22)
sch.parallel(loop=l26)
sch.annotate(block_or_loop=l26, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l26, ann_key="pragma_unroll_explicit", ann_val=1)
l27, l28, l29, l30, l31, l32 = sch.get_loops(block=b18)
l33 = sch.fuse(l27, l28, l29, l30)
sch.parallel(loop=l33)
sch.annotate(block_or_loop=l33, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l33, ann_key="pragma_unroll_explicit", ann_val=1)
b34 = sch.get_block(name="tensor_rf", func_name="main")
l35, l36, l37, l38 = sch.get_loops(block=b34)
b39 = sch.decompose_reduction(block=b34, loop=l38)
b40 = sch.get_block(name="tensor", func_name="main")
l41, l42, l43 = sch.get_loops(block=b40)
b44 = sch.decompose_reduction(block=b40, loop=l43)
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #28: GFLOPs: 0.0003. Time: 8.6178 ms. Best GFLOPs: 0.0020
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #29: GFLOPs: 0.0001. Time: 21.7244 ms. Best GFLOPs: 0.0020
[01:28:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #30: GFLOPs: 0.0003. Time: 7.4263 ms. Best GFLOPs: 0.0020
[01:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_max_pool2d_1"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |            
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |            
  3 |                                fused_nn_pad |       1 |      1 |         0.0000 |     544.3648 |              544.3648 |     12 |            
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |         5.8938 |     213.8992 |              213.8992 |     32 |            
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |         0.0020 |    1139.6440 |             1139.6440 |     31 |            
  6 |              fused_layout_transform_reshape |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                          fused_nn_dense_add |    5130 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 147
Total latency (us): 1958.44

[01:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_layout_transform_reshape"] Trial #0: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 4, 4, 4), "float32"], T_reshape: T.Buffer[(1, 256), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 16, 4, 4], dtype="float32")
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                with T.block("T_layout_trans"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(16, i0_i1_fused // 16)
                    ax2_1 = T.axis.spatial(4, i0_i1_fused % 16 // 4)
                    ax3_1 = T.axis.spatial(4, i0_i1_fused % 4)
                    T.reads(placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4])
                    T.writes(T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1])
                    T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 16 and ax2_1 < 4 and ax3_1 < 4, placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4], T.float32(0), dtype="float32")
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(1, 0)
                ax1 = T.axis.spatial(256, i0_i1_fused)
                T.reads(T_layout_trans[0, ax1 % 256 // 16, ax1 % 16 // 4, ax1 % 4])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = T_layout_trans[0, ax1 % 256 // 16, ax1 % 16 // 4, ax1 % 4]
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
sch.enter_postproc()
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit")
b5, b6 = sch.get_child_blocks(b4)
l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b5)
l13 = sch.fuse(l7, l8)
sch.parallel(loop=l13)
sch.annotate(block_or_loop=l13, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l13, ann_key="pragma_unroll_explicit", ann_val=1)
l14, = sch.get_loops(block=b6)
sch.annotate(block_or_loop=l14, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l14, ann_key="pragma_unroll_explicit", ann_val=1)
[01:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_reshape"] Trial #1: GFLOPs: 0.0000. Time: 5.3792 ms. Best GFLOPs: 0.0000
[01:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_reshape"] Trial #2: GFLOPs: 0.0000. Time: 5.3731 ms. Best GFLOPs: 0.0000
[01:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_layout_transform_reshape"] Trial #3: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 4, 4, 4), "float32"], T_reshape: T.Buffer[(1, 256), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 16, 4, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(64, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(4):
                with T.block("T_layout_trans"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(16, i0_i1_i2_fused // 4)
                    ax2 = T.axis.spatial(4, i0_i1_i2_fused % 4)
                    ax3 = T.axis.spatial(4, i3)
                    T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 16 and ax2 < 4 and ax3 < 4, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(1, 0)
                ax1 = T.axis.spatial(256, i0_i1_fused)
                T.reads(T_layout_trans[0, ax1 % 256 // 16, ax1 % 16 // 4, ax1 % 4])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = T_layout_trans[0, ax1 % 256 // 16, ax1 % 16 // 4, ax1 % 4]
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=16)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
sch.enter_postproc()
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit")
b5, b6 = sch.get_child_blocks(b4)
l7, l8, l9, l10 = sch.get_loops(block=b5)
l11 = sch.fuse(l7, l8, l9)
sch.parallel(loop=l11)
sch.annotate(block_or_loop=l11, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l11, ann_key="pragma_unroll_explicit", ann_val=1)
l12, l13 = sch.get_loops(block=b6)
l14 = sch.fuse(l12, l13)
sch.parallel(loop=l14)
sch.annotate(block_or_loop=l14, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l14, ann_key="pragma_unroll_explicit", ann_val=1)
[01:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_reshape"] Trial #4: GFLOPs: 0.0000. Time: 4.3645 ms. Best GFLOPs: 0.0000
[01:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_reshape"] Trial #5: GFLOPs: 0.0000. Time: 1.6972 ms. Best GFLOPs: 0.0000
[01:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_reshape"] Trial #6: GFLOPs: 0.0000. Time: 5.6018 ms. Best GFLOPs: 0.0000
[01:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_reshape"] Trial #7: GFLOPs: 0.0000. Time: 0.9988 ms. Best GFLOPs: 0.0000
[01:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_reshape"] Trial #8: GFLOPs: 0.0000. Time: 4.5263 ms. Best GFLOPs: 0.0000
[01:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_reshape"] Trial #9: GFLOPs: 0.0000. Time: 1.8007 ms. Best GFLOPs: 0.0000
[01:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_reshape"] Trial #10: GFLOPs: 0.0000. Time: 2.5929 ms. Best GFLOPs: 0.0000
[01:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_layout_transform_reshape"] Trial #11: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 4, 4, 4), "float32"], T_reshape: T.Buffer[(1, 256), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 16, 4, 4], dtype="float32")
        for i0_i1_fused in T.parallel(16):
            for i2, i3 in T.grid(4, 4):
                with T.block("T_layout_trans"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3 = T.axis.remap("SSS", [i0_i1_fused, i2, i3])
                    T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 16 and ax2 < 4 and ax3 < 4, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0_i1_fused in T.parallel(256):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(1, 0)
                ax1 = T.axis.spatial(256, i0_i1_fused)
                T.reads(T_layout_trans[0, ax1 % 256 // 16, ax1 % 16 // 4, ax1 % 4])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = T_layout_trans[0, ax1 % 256 // 16, ax1 % 16 // 4, ax1 % 4]
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
sch.enter_postproc()
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit")
b5, b6 = sch.get_child_blocks(b4)
l7, l8, l9, l10 = sch.get_loops(block=b5)
l11 = sch.fuse(l7, l8)
sch.parallel(loop=l11)
l12, l13 = sch.get_loops(block=b6)
l14 = sch.fuse(l12, l13)
sch.parallel(loop=l14)
[01:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_reshape"] Trial #12: GFLOPs: 0.0000. Time: 41.3154 ms. Best GFLOPs: 0.0000
[01:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_layout_transform_reshape"] Trial #13: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 4, 4, 4), "float32"], T_reshape: T.Buffer[(1, 256), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 16, 4, 4], dtype="float32")
        for i0_i1_fused_fused in T.parallel(256):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                with T.block("T_layout_trans"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(16, i0_i1_fused_fused // 16)
                    ax2_1 = T.axis.spatial(4, i0_i1_fused_fused % 16 // 4)
                    ax3_1 = T.axis.spatial(4, i0_i1_fused_fused % 4)
                    T.reads(placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4])
                    T.writes(T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1])
                    T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 16 and ax2_1 < 4 and ax3_1 < 4, placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4], T.float32(0), dtype="float32")
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(1, 0)
                ax1 = T.axis.spatial(256, i0_i1_fused_fused)
                T.reads(T_layout_trans[0, ax1 % 256 // 16, ax1 % 16 // 4, ax1 % 4])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = T_layout_trans[0, ax1 % 256 // 16, ax1 % 16 // 4, ax1 % 4]
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
sch.enter_postproc()
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit")
b5, b6 = sch.get_child_blocks(b4)
l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b5)
l13 = sch.fuse(l7, l8)
sch.parallel(loop=l13)
l14, = sch.get_loops(block=b6)
l15 = sch.fuse(l14)
sch.parallel(loop=l15)
[01:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_reshape"] Trial #14: GFLOPs: 0.0000. Time: 5.2105 ms. Best GFLOPs: 0.0000
[01:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_reshape"] Trial #15: GFLOPs: 0.0000. Time: 8.3071 ms. Best GFLOPs: 0.0000
[01:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_reshape"] Trial #16: GFLOPs: 0.0000. Time: 9.0524 ms. Best GFLOPs: 0.0000
[01:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_reshape"] Trial #17: GFLOPs: 0.0000. Time: 11.9972 ms. Best GFLOPs: 0.0000
[01:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_reshape"] Trial #18: GFLOPs: 0.0000. Time: 9.8132 ms. Best GFLOPs: 0.0000
[01:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_layout_transform_reshape"] Trial #19: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 4, 4, 4), "float32"], T_reshape: T.Buffer[(1, 256), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 16, 4, 4], dtype="float32")
        for i0_i1_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i2, i3 in T.grid(4, 4):
                with T.block("T_layout_trans"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3 = T.axis.remap("SSS", [i0_i1_fused, i2, i3])
                    T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 16 and ax2 < 4 and ax3 < 4, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(1, 0)
                ax1 = T.axis.spatial(256, i0_i1_fused)
                T.reads(T_layout_trans[0, ax1 % 256 // 16, ax1 % 16 // 4, ax1 % 4])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = T_layout_trans[0, ax1 % 256 // 16, ax1 % 16 // 4, ax1 % 4]
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
sch.enter_postproc()
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit")
b5, b6 = sch.get_child_blocks(b4)
l7, l8, l9, l10 = sch.get_loops(block=b5)
l11 = sch.fuse(l7, l8)
sch.parallel(loop=l11)
sch.annotate(block_or_loop=l11, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l11, ann_key="pragma_unroll_explicit", ann_val=1)
l12, l13 = sch.get_loops(block=b6)
l14 = sch.fuse(l12, l13)
sch.parallel(loop=l14)
sch.annotate(block_or_loop=l14, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l14, ann_key="pragma_unroll_explicit", ann_val=1)
[01:29:17] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_layout_transform_reshape"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |            
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |            
  3 |                                fused_nn_pad |       1 |      1 |         0.0000 |     544.3648 |              544.3648 |     12 |            
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |         5.8938 |     213.8992 |              213.8992 |     32 |            
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |         0.0020 |    1139.6440 |             1139.6440 |     31 |            
  6 |              fused_layout_transform_reshape |       1 |      1 |         0.0000 |     998.8433 |              998.8433 |     20 |            
  7 |                          fused_nn_dense_add |    5130 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 167
Total latency (us): 2957.29

[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #0: GFLOPs: 0.9899. Time: 0.0052 ms. Best GFLOPs: 0.9899
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #1: GFLOPs: 0.0001. Time: 43.9871 ms. Best GFLOPs: 0.9899
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #2: GFLOPs: 0.0013. Time: 3.9997 ms. Best GFLOPs: 0.9899
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #3: GFLOPs: 0.0010. Time: 4.9649 ms. Best GFLOPs: 0.9899
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #4: GFLOPs: 5.3482. Time: 0.0010 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_dense_add"] Trial #5: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256), "float32"], placeholder_1: T.Buffer[(10, 256), "float32"], placeholder_2: T.Buffer[(1, 10), "float32"], T_add: T.Buffer[(1, 10), "float32"]) -> None:
        # function attr dict
        T.func_attr({"layout_free_placeholders": [1], "global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_matmul_NT = T.alloc_buffer([1, 10], dtype="float32")
        for i0_0_i1_0_i0_1_i1_1_fused in T.parallel(5, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i1_2_init in T.serial(2):
                with T.block("T_matmul_NT_init"):
                    i = T.axis.spatial(1, 0)
                    j = T.axis.spatial(10, i0_0_i1_0_i0_1_i1_1_fused * 2 + i1_2_init)
                    T.reads()
                    T.writes(T_matmul_NT[i, j])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                    T_matmul_NT[i, j] = T.float32(0)
            for i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(4, 1, 2, 64, 1, 1):
                with T.block("T_matmul_NT_update"):
                    i = T.axis.spatial(1, 0)
                    j = T.axis.spatial(10, i0_0_i1_0_i0_1_i1_1_fused * 2 + i1_2)
                    k = T.axis.reduce(256, i2_0 * 64 + i2_1)
                    T.reads(T_matmul_NT[i, j], placeholder[i, k], placeholder_1[j, k])
                    T.writes(T_matmul_NT[i, j])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                    T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
        for i0_i1_fused in T.vectorized(10, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            with T.block("T_add"):
                ax0 = T.axis.spatial(1, 0)
                ax1 = T.axis.spatial(10, i0_i1_fused)
                T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                T.writes(T_add[ax0, ax1])
                T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l9, l10, l11, l12 = sch.split(loop=l2, factors=[v5, v6, v7, v8])
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 5, 2, 1])
l17, l18, l19, l20 = sch.split(loop=l3, factors=[v13, v14, v15, v16])
v21, v22 = sch.sample_perfect_tile(loop=l4, n=2, max_innermost_factor=64, decision=[4, 64])
l23, l24 = sch.split(loop=l4, factors=[v21, v22])
sch.reorder(l9, l17, l10, l18, l23, l11, l19, l24, l12, l20)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v25 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v25)
sch.enter_postproc()
b26 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b26, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b26, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b26, ann_key="meta_schedule.unroll_explicit")
b27, b28 = sch.get_child_blocks(b26)
l29, l30, l31, l32, l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b27)
l39 = sch.fuse(l29, l30, l31, l32)
sch.parallel(loop=l39)
sch.annotate(block_or_loop=l39, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l39, ann_key="pragma_unroll_explicit", ann_val=1)
l40, l41 = sch.get_loops(block=b28)
l42 = sch.fuse(l40, l41)
sch.vectorize(loop=l42)
sch.annotate(block_or_loop=l42, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l42, ann_key="pragma_unroll_explicit", ann_val=1)
b43 = sch.get_block(name="T_matmul_NT", func_name="main")
l44, l45, l46, l47, l48, l49, l50 = sch.get_loops(block=b43)
b51 = sch.decompose_reduction(block=b43, loop=l45)
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #6: GFLOPs: 0.0059. Time: 0.8759 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #7: GFLOPs: 0.0030. Time: 1.7075 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #8: GFLOPs: 0.0018. Time: 2.7771 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #9: GFLOPs: 0.0014. Time: 3.6221 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #10: GFLOPs: 2.1733. Time: 0.0024 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #11: GFLOPs: 0.0069. Time: 0.7426 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #12: GFLOPs: 0.0044. Time: 1.1775 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #13: GFLOPs: 0.0023. Time: 2.2292 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #14: GFLOPs: 0.0008. Time: 6.6446 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #15: GFLOPs: 0.0012. Time: 4.2087 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #16: GFLOPs: 0.0014. Time: 3.6954 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #17: GFLOPs: 0.0025. Time: 2.0706 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_dense_add"] Trial #18: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256), "float32"], placeholder_1: T.Buffer[(10, 256), "float32"], placeholder_2: T.Buffer[(1, 10), "float32"], T_add: T.Buffer[(1, 10), "float32"]) -> None:
        # function attr dict
        T.func_attr({"layout_free_placeholders": [1], "global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_matmul_NT = T.alloc_buffer([1, 10], dtype="float32")
        for i0_0_i1_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1 in T.grid(1, 1):
                for i1_3_init in T.serial(5):
                    with T.block("T_matmul_NT_init"):
                        i = T.axis.spatial(1, 0)
                        j = T.axis.spatial(10, i0_0_i1_0_fused * 5 + i1_3_init)
                        T.reads()
                        T.writes(T_matmul_NT[i, j])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        T_matmul_NT[i, j] = T.float32(0)
                for i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(8, 1, 1, 32, 1, 5):
                    with T.block("T_matmul_NT_update"):
                        i = T.axis.spatial(1, 0)
                        j = T.axis.spatial(10, i0_0_i1_0_fused * 5 + i1_3)
                        k = T.axis.reduce(256, i2_0 * 32 + i2_1)
                        T.reads(T_matmul_NT[i, j], placeholder[i, k], placeholder_1[j, k])
                        T.writes(T_matmul_NT[i, j])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
            for ax0_ax1_fused in T.vectorized(5):
                with T.block("T_add"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(10, i0_0_i1_0_fused * 5 + ax0_ax1_fused)
                    T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                    T.writes(T_add[ax0, ax1])
                    T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l9, l10, l11, l12 = sch.split(loop=l2, factors=[v5, v6, v7, v8])
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 1, 5])
l17, l18, l19, l20 = sch.split(loop=l3, factors=[v13, v14, v15, v16])
v21, v22 = sch.sample_perfect_tile(loop=l4, n=2, max_innermost_factor=64, decision=[8, 32])
l23, l24 = sch.split(loop=l4, factors=[v21, v22])
sch.reorder(l9, l17, l10, l18, l23, l11, l19, l24, l12, l20)
b25, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b25, loop=l17, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
sch.enter_postproc()
b27 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b27, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b27, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b27, ann_key="meta_schedule.unroll_explicit")
b28, b29 = sch.get_child_blocks(b27)
l30, l31, l32, l33, l34, l35, l36, l37, l38, l39 = sch.get_loops(block=b28)
l40 = sch.fuse(l30, l31)
sch.parallel(loop=l40)
sch.annotate(block_or_loop=l40, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l40, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42, l43 = sch.get_loops(block=b29)
l44 = sch.fuse(l42, l43)
sch.vectorize(loop=l44)
sch.annotate(block_or_loop=l41, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l41, ann_key="pragma_unroll_explicit", ann_val=1)
b45 = sch.get_block(name="T_matmul_NT", func_name="main")
l46, l47, l48, l49, l50, l51, l52, l53, l54 = sch.get_loops(block=b45)
b55 = sch.decompose_reduction(block=b45, loop=l49)
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #19: GFLOPs: 0.0001. Time: 37.3192 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #20: GFLOPs: 0.0014. Time: 3.5423 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #21: GFLOPs: 3.8656. Time: 0.0013 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #22: GFLOPs: 0.0011. Time: 4.6396 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #23: GFLOPs: 2.1983. Time: 0.0023 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #24: GFLOPs: 0.0023. Time: 2.2641 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #25: GFLOPs: 0.0002. Time: 33.7637 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #26: GFLOPs: 0.0014. Time: 3.7833 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #27: GFLOPs: 0.1933. Time: 0.0265 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_dense_add"] Trial #28: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256), "float32"], placeholder_1: T.Buffer[(10, 256), "float32"], placeholder_2: T.Buffer[(1, 10), "float32"], T_add: T.Buffer[(1, 10), "float32"]) -> None:
        # function attr dict
        T.func_attr({"layout_free_placeholders": [1], "global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_matmul_NT = T.alloc_buffer([1, 10], dtype="float32")
        T_matmul_NT_rf = T.alloc_buffer([1, 10, 2], dtype="float32")
        for i0_i1_fused in T.parallel(10, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0_init in T.serial(2):
                with T.block("T_matmul_NT_rf_init"):
                    vi2_1 = T.axis.spatial(2, ax0_init)
                    i = T.axis.spatial(1, 0)
                    j = T.axis.spatial(10, i0_i1_fused)
                    T.reads()
                    T.writes(T_matmul_NT_rf[i, j, vi2_1])
                    T_matmul_NT_rf[i, j, vi2_1] = T.float32(0)
            with T.block("T_matmul_NT_init"):
                i = T.axis.spatial(1, 0)
                j = T.axis.spatial(10, i0_i1_fused)
                T.reads()
                T.writes(T_matmul_NT[i, j])
                T_matmul_NT[i, j] = T.float32(0)
            for ax0 in T.serial(2):
                for ax0_1, ax1, ax2, ax3 in T.grid(1, 1, 1, 128):
                    with T.block("T_matmul_NT_rf_update"):
                        vi2_1 = T.axis.spatial(2, ax0)
                        i = T.axis.spatial(1, 0)
                        j, vi2_0 = T.axis.remap("SR", [i0_i1_fused, ax3])
                        T.reads(T_matmul_NT_rf[i, j, vi2_1], placeholder[i, vi2_0 * 2 + vi2_1], placeholder_1[j, vi2_0 * 2 + vi2_1])
                        T.writes(T_matmul_NT_rf[i, j, vi2_1])
                        T_matmul_NT_rf[i, j, vi2_1] = T_matmul_NT_rf[i, j, vi2_1] + placeholder[i, vi2_0 * 2 + vi2_1] * placeholder_1[j, vi2_0 * 2 + vi2_1]
                for ax1, ax2 in T.grid(1, 1):
                    with T.block("T_matmul_NT_update"):
                        vi2_1 = T.axis.reduce(2, ax0)
                        i = T.axis.spatial(1, 0)
                        j = T.axis.spatial(10, i0_i1_fused)
                        T.reads(T_matmul_NT[i, j], T_matmul_NT_rf[i, j, vi2_1])
                        T.writes(T_matmul_NT[i, j])
                        T_matmul_NT[i, j] = T_matmul_NT[i, j] + T_matmul_NT_rf[i, j, vi2_1]
            with T.block("T_add"):
                ax0 = T.axis.spatial(1, 0)
                ax1 = T.axis.spatial(10, i0_i1_fused)
                T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                T.writes(T_add[ax0, ax1])
                T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6 = sch.sample_perfect_tile(loop=l4, n=2, max_innermost_factor=64, decision=[128, 2])
l7, l8 = sch.split(loop=l4, factors=[v5, v6])
b9 = sch.rfactor(loop=l8, factor_axis=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v10 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v10)
b11, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l12 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l12, preserve_unit_loops=True)
l13 = sch.sample_compute_location(block=b11, decision=2)
sch.compute_at(block=b11, loop=l13, preserve_unit_loops=True)
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, b16, b17 = sch.get_child_blocks(b14)
l18, l19, l20, l21, l22, l23, l24 = sch.get_loops(block=b15)
l25 = sch.fuse(l18, l19)
sch.parallel(loop=l25)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l26, l27, l28, l29 = sch.get_loops(block=b16)
sch.annotate(block_or_loop=l26, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l26, ann_key="pragma_unroll_explicit", ann_val=1)
l30, = sch.get_loops(block=b17)
sch.annotate(block_or_loop=l30, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l30, ann_key="pragma_unroll_explicit", ann_val=1)
b31 = sch.get_block(name="T_matmul_NT_rf", func_name="main")
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b31)
b38 = sch.decompose_reduction(block=b31, loop=l33)
b39 = sch.get_block(name="T_matmul_NT", func_name="main")
l40, l41, l42, l43 = sch.get_loops(block=b39)
b44 = sch.decompose_reduction(block=b39, loop=l41)
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #29: GFLOPs: 0.0020. Time: 2.5713 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #30: GFLOPs: 0.0010. Time: 4.9977 ms. Best GFLOPs: 5.3482
[01:29:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #31: GFLOPs: 0.0018. Time: 2.8551 ms. Best GFLOPs: 5.3482
[01:29:34] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_dense_add"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |            
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |            
  3 |                                fused_nn_pad |       1 |      1 |         0.0000 |     544.3648 |              544.3648 |     12 |            
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |         5.8938 |     213.8992 |              213.8992 |     32 |            
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |         0.0020 |    1139.6440 |             1139.6440 |     31 |            
  6 |              fused_layout_transform_reshape |       1 |      1 |         0.0000 |     998.8433 |              998.8433 |     20 |            
  7 |                          fused_nn_dense_add |    5130 |      1 |         5.3482 |       0.9592 |                0.9592 |     32 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 199
Total latency (us): 2958.25

[01:29:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_max_pool2d_1"
[01:29:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:29:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 31 candidate(s) from database
[01:29:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea77199458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7728b1a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea76efda18)]: 0 failure(s)
[01:29:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2017 candidate(s)
[01:30:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea77199458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7728b1a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea76efda18)]: 0 failure(s)
[01:31:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea77199458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7728b1a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea76efda18)]: 0 failure(s)
[01:32:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea77199458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7728b1a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea76efda18)]: 0 failure(s)
[01:32:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea77199458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7728b1a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea76efda18)]: 0 failure(s)
[01:33:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 1 candidates:
[1 : 1]:	0.8990
[01:33:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 1 candidate(s) with evolutionary search
[01:33:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 1 candidates(s) for measurement
[01:33:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 1 sample(s) to builder
[01:33:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 1 sample(s) to runner
[01:34:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_max_pool2d_1"] Trial #31: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 14, 14, 4), "float32"], tensor: T.Buffer[(1, 4, 4, 4, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        for i0_i1_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2, i3, i4 in T.grid(4, 4, 4):
                with T.block("tensor_init"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                    T.reads()
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                for i5, i6 in T.grid(3, 3):
                    with T.block("tensor_update"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSRR", [i0_i1_fused, i2, i3, i4, i5, i6])
                        T.reads(tensor[ax0, ax1, ax2, ax3, ax4], placeholder[ax0, ax1, ax2 * 3 + rv0, ax3 * 3 + rv1, ax4])
                        T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], placeholder[ax0, ax1, ax2 * 3 + rv0, ax3 * 3 + rv1, ax4])
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
sch.enter_postproc()
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit")
b3, = sch.get_child_blocks(b2)
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b3)
l11 = sch.fuse(l4, l5)
sch.parallel(loop=l11)
sch.annotate(block_or_loop=l11, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l11, ann_key="pragma_unroll_explicit", ann_val=1)
b12 = sch.get_block(name="tensor", func_name="main")
l13, l14, l15, l16, l17, l18 = sch.get_loops(block=b12)
b19 = sch.decompose_reduction(block=b12, loop=l17)
[01:34:21] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_max_pool2d_1"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |            
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |            
  3 |                                fused_nn_pad |       1 |      1 |         0.0000 |     544.3648 |              544.3648 |     12 |            
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |         5.8938 |     213.8992 |              213.8992 |     32 |            
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |         0.0020 |    1139.6440 |             1139.6440 |     32 |            
  6 |              fused_layout_transform_reshape |       1 |      1 |         0.0000 |     998.8433 |              998.8433 |     20 |            
  7 |                          fused_nn_dense_add |    5130 |      1 |         5.3482 |       0.9592 |                0.9592 |     32 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 200
Total latency (us): 2958.25

[01:34:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_layout_transform_reshape"
[01:34:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:34:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 20 candidate(s) from database
[01:34:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:34:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2028 candidate(s)
[01:35:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:36:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:36:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:37:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:37:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[01:37:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[01:37:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[01:37:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[01:37:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[01:37:39] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_layout_transform_reshape"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |            
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |            
  3 |                                fused_nn_pad |       1 |      1 |         0.0000 |     544.3648 |              544.3648 |     12 |            
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |         5.8938 |     213.8992 |              213.8992 |     32 |            
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |         0.0020 |    1139.6440 |             1139.6440 |     32 |            
  6 |              fused_layout_transform_reshape |       1 |      1 |         0.0000 |     998.8433 |              998.8433 |     20 |            
  7 |                          fused_nn_dense_add |    5130 |      1 |         5.3482 |       0.9592 |                0.9592 |     32 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 200
Total latency (us): 2958.25

[01:37:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_max_pool2d_1"
[01:37:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #5 has finished. Remaining task(s): 7
[01:37:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_pad"
[01:37:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:37:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 12 candidate(s) from database
[01:38:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[01:38:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2036 candidate(s)
[01:39:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[01:40:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[01:41:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[01:42:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[01:43:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[01:43:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[01:43:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[01:43:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[01:43:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[01:43:19] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_pad"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |            
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |            
  3 |                                fused_nn_pad |       1 |      1 |         0.0000 |     544.3648 |              544.3648 |     12 |            
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |         5.8938 |     213.8992 |              213.8992 |     32 |            
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |         0.0020 |    1139.6440 |             1139.6440 |     32 |          Y 
  6 |              fused_layout_transform_reshape |       1 |      1 |         0.0000 |     998.8433 |              998.8433 |     20 |            
  7 |                          fused_nn_dense_add |    5130 |      1 |         5.3482 |       0.9592 |                0.9592 |     32 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 200
Total latency (us): 2958.25

[01:43:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_layout_transform_reshape"
[01:43:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:43:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 20 candidate(s) from database
[01:43:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:43:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2028 candidate(s)
[01:44:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:44:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:45:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:46:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:46:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[01:46:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[01:46:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[01:46:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[01:46:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[01:46:41] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_layout_transform_reshape"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |            
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |            
  3 |                                fused_nn_pad |       1 |      1 |         0.0000 |     544.3648 |              544.3648 |     12 |            
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |         5.8938 |     213.8992 |              213.8992 |     32 |            
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |         0.0020 |    1139.6440 |             1139.6440 |     32 |          Y 
  6 |              fused_layout_transform_reshape |       1 |      1 |         0.0000 |     998.8433 |              998.8433 |     20 |            
  7 |                          fused_nn_dense_add |    5130 |      1 |         5.3482 |       0.9592 |                0.9592 |     32 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 200
Total latency (us): 2958.25

[01:46:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_layout_transform_reshape"
[01:46:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:46:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 20 candidate(s) from database
[01:46:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:46:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2028 candidate(s)
[01:47:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:48:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:48:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:49:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:50:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[01:50:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[01:50:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[01:50:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[01:50:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[01:50:14] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_layout_transform_reshape"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |            
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |            
  3 |                                fused_nn_pad |       1 |      1 |         0.0000 |     544.3648 |              544.3648 |     12 |            
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |         5.8938 |     213.8992 |              213.8992 |     32 |            
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |         0.0020 |    1139.6440 |             1139.6440 |     32 |          Y 
  6 |              fused_layout_transform_reshape |       1 |      1 |         0.0000 |     998.8433 |              998.8433 |     20 |            
  7 |                          fused_nn_dense_add |    5130 |      1 |         5.3482 |       0.9592 |                0.9592 |     32 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 200
Total latency (us): 2958.25

[01:50:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_pad"
[01:50:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:50:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 12 candidate(s) from database
[01:50:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[01:50:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2036 candidate(s)
[01:51:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[01:53:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[01:54:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[01:55:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[01:56:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[01:56:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[01:56:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[01:56:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[01:56:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[01:56:29] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_pad"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |            
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |            
  3 |                                fused_nn_pad |       1 |      1 |         0.0000 |     544.3648 |              544.3648 |     12 |            
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |         5.8938 |     213.8992 |              213.8992 |     32 |            
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |         0.0020 |    1139.6440 |             1139.6440 |     32 |          Y 
  6 |              fused_layout_transform_reshape |       1 |      1 |         0.0000 |     998.8433 |              998.8433 |     20 |            
  7 |                          fused_nn_dense_add |    5130 |      1 |         5.3482 |       0.9592 |                0.9592 |     32 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 200
Total latency (us): 2958.25

[01:56:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_layout_transform_reshape"
[01:56:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:56:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 20 candidate(s) from database
[01:56:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:56:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2028 candidate(s)
[01:57:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:58:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:58:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:59:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[01:59:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[01:59:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[01:59:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[01:59:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[01:59:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[01:59:45] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_layout_transform_reshape"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |            
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |            
  3 |                                fused_nn_pad |       1 |      1 |         0.0000 |     544.3648 |              544.3648 |     12 |            
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |         5.8938 |     213.8992 |              213.8992 |     32 |            
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |         0.0020 |    1139.6440 |             1139.6440 |     32 |          Y 
  6 |              fused_layout_transform_reshape |       1 |      1 |         0.0000 |     998.8433 |              998.8433 |     20 |            
  7 |                          fused_nn_dense_add |    5130 |      1 |         5.3482 |       0.9592 |                0.9592 |     32 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 200
Total latency (us): 2958.25

[01:59:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"
[01:59:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #4 has finished. Remaining task(s): 6
[01:59:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_layout_transform_reshape"
[01:59:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:59:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 20 candidate(s) from database
[02:00:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[02:00:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2028 candidate(s)
[02:00:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[02:01:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[02:02:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[02:02:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771d1ba8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771af768)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771c2ac8)]: 0 failure(s)
[02:03:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:03:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:03:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:03:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #6 has finished. Remaining task(s): 5
[02:03:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_pad"
[02:03:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:03:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 12 candidate(s) from database
[02:03:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[02:03:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2036 candidate(s)
[02:04:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[02:05:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[02:06:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[02:08:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[02:09:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:09:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:09:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:09:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:09:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:09:11] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_pad"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |            
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |            
  3 |                                fused_nn_pad |       1 |      1 |         0.0000 |     544.3648 |              544.3648 |     12 |            
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |         5.8938 |     213.8992 |              213.8992 |     32 |          Y 
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |         0.0020 |    1139.6440 |             1139.6440 |     32 |          Y 
  6 |              fused_layout_transform_reshape |       1 |      1 |         0.0000 |     998.8433 |              998.8433 |     20 |          Y 
  7 |                          fused_nn_dense_add |    5130 |      1 |         5.3482 |       0.9592 |                0.9592 |     32 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 200
Total latency (us): 2958.25

[02:09:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_pad"
[02:09:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:09:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 12 candidate(s) from database
[02:09:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[02:09:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2036 candidate(s)
[02:10:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[02:11:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[02:13:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[02:14:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[02:14:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:14:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:14:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:14:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:14:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:14:54] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_pad"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |            
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |            
  3 |                                fused_nn_pad |       1 |      1 |         0.0000 |     544.3648 |              544.3648 |     12 |            
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |         5.8938 |     213.8992 |              213.8992 |     32 |          Y 
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |         0.0020 |    1139.6440 |             1139.6440 |     32 |          Y 
  6 |              fused_layout_transform_reshape |       1 |      1 |         0.0000 |     998.8433 |              998.8433 |     20 |          Y 
  7 |                          fused_nn_dense_add |    5130 |      1 |         5.3482 |       0.9592 |                0.9592 |     32 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 200
Total latency (us): 2958.25

[02:14:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_pad"
[02:14:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:14:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 12 candidate(s) from database
[02:15:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[02:15:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2036 candidate(s)
[02:16:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[02:17:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[02:19:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[02:20:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea771f0388)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771c6c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea771e7e08)]: 0 failure(s)
[02:21:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:21:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:21:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:21:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #3 has finished. Remaining task(s): 4
[02:21:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"
[02:21:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #1 has finished. Remaining task(s): 3
[02:21:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_max_pool2d"
[02:21:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:21:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 12 candidate(s) from database
[02:21:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:21:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2036 candidate(s)
[02:22:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:22:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:23:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:23:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:24:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:24:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:24:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:24:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:24:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:24:11] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_max_pool2d"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |          Y 
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |            
  3 |                                fused_nn_pad |       1 |      1 |         0.0000 |     544.3648 |              544.3648 |     12 |          Y 
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |         5.8938 |     213.8992 |              213.8992 |     32 |          Y 
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |         0.0020 |    1139.6440 |             1139.6440 |     32 |          Y 
  6 |              fused_layout_transform_reshape |       1 |      1 |         0.0000 |     998.8433 |              998.8433 |     20 |          Y 
  7 |                          fused_nn_dense_add |    5130 |      1 |         5.3482 |       0.9592 |                0.9592 |     32 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 200
Total latency (us): 2958.25

[02:24:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_max_pool2d"
[02:24:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:24:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 12 candidate(s) from database
[02:24:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:24:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2036 candidate(s)
[02:24:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:25:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:26:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:26:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:27:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:27:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:27:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:27:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:27:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:27:28] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_max_pool2d"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |          Y 
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |            
  3 |                                fused_nn_pad |       1 |      1 |         0.0000 |     544.3648 |              544.3648 |     12 |          Y 
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |         5.8938 |     213.8992 |              213.8992 |     32 |          Y 
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |         0.0020 |    1139.6440 |             1139.6440 |     32 |          Y 
  6 |              fused_layout_transform_reshape |       1 |      1 |         0.0000 |     998.8433 |              998.8433 |     20 |          Y 
  7 |                          fused_nn_dense_add |    5130 |      1 |         5.3482 |       0.9592 |                0.9592 |     32 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 200
Total latency (us): 2958.25

[02:27:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_max_pool2d"
[02:27:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:27:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 12 candidate(s) from database
[02:27:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:27:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2036 candidate(s)
[02:28:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:28:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:29:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:29:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:30:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:30:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:30:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:30:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:30:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:30:04] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_max_pool2d"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |          Y 
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |            
  3 |                                fused_nn_pad |       1 |      1 |         0.0000 |     544.3648 |              544.3648 |     12 |          Y 
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |         5.8938 |     213.8992 |              213.8992 |     32 |          Y 
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |         0.0020 |    1139.6440 |             1139.6440 |     32 |          Y 
  6 |              fused_layout_transform_reshape |       1 |      1 |         0.0000 |     998.8433 |              998.8433 |     20 |          Y 
  7 |                          fused_nn_dense_add |    5130 |      1 |         5.3482 |       0.9592 |                0.9592 |     32 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 200
Total latency (us): 2958.25

[02:30:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_nn_pad_layout_transform"
[02:30:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:30:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 28 candidate(s) from database
[02:31:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[02:31:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2020 candidate(s)
[02:33:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[02:35:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[02:37:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[02:39:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[02:40:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:40:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:40:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:40:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:40:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:40:38] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_nn_pad_layout_transform"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |          Y 
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |            
  3 |                                fused_nn_pad |       1 |      1 |         0.0000 |     544.3648 |              544.3648 |     12 |          Y 
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |         5.8938 |     213.8992 |              213.8992 |     32 |          Y 
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |         0.0020 |    1139.6440 |             1139.6440 |     32 |          Y 
  6 |              fused_layout_transform_reshape |       1 |      1 |         0.0000 |     998.8433 |              998.8433 |     20 |          Y 
  7 |                          fused_nn_dense_add |    5130 |      1 |         5.3482 |       0.9592 |                0.9592 |     32 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 200
Total latency (us): 2958.25

[02:40:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_max_pool2d"
[02:40:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:40:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 12 candidate(s) from database
[02:40:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:40:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2036 candidate(s)
[02:41:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:41:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:42:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:43:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:43:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:43:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:43:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:43:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:43:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:43:36] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_max_pool2d"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |          Y 
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |            
  3 |                                fused_nn_pad |       1 |      1 |         0.0000 |     544.3648 |              544.3648 |     12 |          Y 
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |         5.8938 |     213.8992 |              213.8992 |     32 |          Y 
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |         0.0020 |    1139.6440 |             1139.6440 |     32 |          Y 
  6 |              fused_layout_transform_reshape |       1 |      1 |         0.0000 |     998.8433 |              998.8433 |     20 |          Y 
  7 |                          fused_nn_dense_add |    5130 |      1 |         5.3482 |       0.9592 |                0.9592 |     32 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 200
Total latency (us): 2958.25

[02:43:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_max_pool2d"
[02:43:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:43:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 12 candidate(s) from database
[02:43:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:43:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2036 candidate(s)
[02:44:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:44:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:44:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:45:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea772ed2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea7739e318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7736c408)]: 0 failure(s)
[02:46:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:46:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:46:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:46:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #2 has finished. Remaining task(s): 2
[02:46:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_nn_pad_layout_transform"
[02:46:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:46:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 28 candidate(s) from database
[02:47:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[02:47:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2020 candidate(s)
[02:49:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[02:52:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[02:54:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[02:56:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[02:57:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:57:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:57:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:57:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:57:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:57:07] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_nn_pad_layout_transform"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |          Y 
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |          Y 
  3 |                                fused_nn_pad |       1 |      1 |         0.0000 |     544.3648 |              544.3648 |     12 |          Y 
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |         5.8938 |     213.8992 |              213.8992 |     32 |          Y 
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |         0.0020 |    1139.6440 |             1139.6440 |     32 |          Y 
  6 |              fused_layout_transform_reshape |       1 |      1 |         0.0000 |     998.8433 |              998.8433 |     20 |          Y 
  7 |                          fused_nn_dense_add |    5130 |      1 |         5.3482 |       0.9592 |                0.9592 |     32 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 200
Total latency (us): 2958.25

[02:57:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_nn_pad_layout_transform"
[02:57:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:57:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 28 candidate(s) from database
[02:58:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[02:58:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2020 candidate(s)
[03:00:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[03:02:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[03:04:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[03:06:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[03:07:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:07:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:07:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:07:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:07:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:07:06] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_nn_pad_layout_transform"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |          Y 
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |          Y 
  3 |                                fused_nn_pad |       1 |      1 |         0.0000 |     544.3648 |              544.3648 |     12 |          Y 
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |         5.8938 |     213.8992 |              213.8992 |     32 |          Y 
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |         0.0020 |    1139.6440 |             1139.6440 |     32 |          Y 
  6 |              fused_layout_transform_reshape |       1 |      1 |         0.0000 |     998.8433 |              998.8433 |     20 |          Y 
  7 |                          fused_nn_dense_add |    5130 |      1 |         5.3482 |       0.9592 |                0.9592 |     32 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 200
Total latency (us): 2958.25

[03:07:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_nn_pad_layout_transform"
[03:07:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:07:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 28 candidate(s) from database
[03:08:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[03:08:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2020 candidate(s)
[03:10:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[03:12:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[03:14:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[03:16:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[03:17:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:17:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:17:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:17:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:17:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:17:50] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_nn_pad_layout_transform"
 ID |                                        Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |               fused_nn_pad_layout_transform |       1 |      1 |         0.0002 |       6.5012 |                6.5012 |     28 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  326144 |      1 |         9.5602 |      34.1148 |               34.1148 |     32 |          Y 
  2 |                         fused_nn_max_pool2d |    6272 |      1 |         0.3149 |      19.9185 |               19.9185 |     12 |          Y 
  3 |                                fused_nn_pad |       1 |      1 |         0.0000 |     544.3648 |              544.3648 |     12 |          Y 
  4 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 1260672 |      1 |         5.8938 |     213.8992 |              213.8992 |     32 |          Y 
  5 |                       fused_nn_max_pool2d_1 |    2304 |      1 |         0.0020 |    1139.6440 |             1139.6440 |     32 |          Y 
  6 |              fused_layout_transform_reshape |       1 |      1 |         0.0000 |     998.8433 |              998.8433 |     20 |          Y 
  7 |                          fused_nn_dense_add |    5130 |      1 |         5.3482 |       0.9592 |                0.9592 |     32 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 200
Total latency (us): 2958.25

[03:17:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_nn_pad_layout_transform"
[03:17:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:17:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 28 candidate(s) from database
[03:19:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[03:19:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2020 candidate(s)
[03:21:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[03:23:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[03:25:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[03:27:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55ea7734db48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55ea771a3398)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55ea7728c8b8)]: 0 failure(s)
[03:28:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:28:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:28:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:28:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #0 has finished. Remaining task(s): 1
[03:28:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_dense_add"
[03:28:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #7 has finished. Remaining task(s): 0
/home/yj/anaconda3/lib/python3.7/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
[[-0.40726173 -0.6145911   0.42801306 -2.7619433   2.3159206   1.255761
  -3.1834373   0.3802811   1.7520946  -0.6257868 ]]
None
Traceback (most recent call last):
  File "python/tvm/testing/test_meta_schedule.py", line 118, in <module>
    assert np.allclose(actual_output, expected_output, rtol=1e-4, atol=2e-4)
  File "<__array_function__ internals>", line 6, in allclose
  File "/home/yj/anaconda3/lib/python3.7/site-packages/numpy/core/numeric.py", line 2256, in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
  File "<__array_function__ internals>", line 6, in isclose
  File "/home/yj/anaconda3/lib/python3.7/site-packages/numpy/core/numeric.py", line 2363, in isclose
    yfin = isfinite(y)
TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
