nohup: ignoring input
[14:39:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #0: "fused_nn_conv2d_add_nn_relu"
[14:39:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(256, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 1024, 54, 54], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 256, 52, 52], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1024, 54, 54):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 53 and 1 <= i3_1 and i3_1 < 53, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 52, 52, 1024, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 256, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 52, 52):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:39:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(256, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 256, 52, 52], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 1024, 54, 54], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([256, 1024, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(52, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(13, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(172032):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, ax0_ax1_ax2_ax3_fused % 172032 // 168)
                                    v2 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 4 + ax0_ax1_ax2_ax3_fused % 168 // 28)
                                    v3 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 53 and 1 <= v3 and v3 < 53, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(1179648):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 26 * 128 + ax0_ax1_ax2_ax3_fused // 9216)
                                    v1 = T.axis.spatial(1024, ax0_ax1_ax2_ax3_fused % 9216 // 9)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(128, 3, 3, 1, 2, 4, 1, 8, 1, 1, 1, 1, 1, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 26 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3)
                                    yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 4 + i2_3)
                                    xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_4)
                                    rc = T.axis.reduce(1024, i4_1 * 8 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_1, i6_1])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 4, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 26 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                                v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 4 + ax2)
                                v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 64, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 1, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 13, 1, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 128, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:39:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #1: "fused_nn_conv2d_add"
[14:39:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52), "float32"], placeholder_1: T.Buffer[(21, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 21, 1, 1), "float32"], T_add: T.Buffer[(1, 21, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 52, 52], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 21, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 52, 52):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 21, 52, 52, 256, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 52, 52], "float32"], ["TENSOR", [21, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 21, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:39:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52), "float32"], placeholder_1: T.Buffer[(21, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 21, 1, 1), "float32"], T_add: T.Buffer[(1, 21, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 21, 52, 52], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 256, 52, 52], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([21, 256, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(52, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(86528):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 64 + ax0_ax1_ax2_ax3_fused // 1352)
                                    v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 26 + ax0_ax1_ax2_ax3_fused % 1352 // 52)
                                    v3 = T.axis.spatial(52, ax0_ax1_ax2_ax3_fused % 52)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(1344):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(21, ax0_ax1_ax2_ax3_fused // 64)
                                    v1 = T.axis.spatial(256, i4_0 * 64 + ax0_ax1_ax2_ax3_fused % 64)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 1, 1, 13, 8, 1, 1, 1, 21, 2, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(21, i1_4)
                                    yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 26 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 2 + i2_4)
                                    xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 4 * 13 + i3_3)
                                    rc = T.axis.reduce(256, i4_0 * 64 + i4_1 * 8 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 52, 52], "float32"], ["TENSOR", [21, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 21, 2, 13):
                            with T.block("conv2d_nchw_local"):
                                v0, v1 = T.axis.remap("SS", [ax0, ax1])
                                v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 26 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 2 + ax2)
                                v3 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 4 * 13 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 21])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 13, 1, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 1, 13, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[4, 8, 8])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:39:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #2: "fused_nn_conv2d_add_1"
[14:39:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(2048, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 2048, 1, 1), "float32"], T_add: T.Buffer[(1, 2048, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 1024, 52, 52], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 2048, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1024, 52, 52):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 2048, 52, 52, 1024, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 2048, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:39:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(2048, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 2048, 1, 1), "float32"], T_add: T.Buffer[(1, 2048, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 2048, 52, 52], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([2048, 1024, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(416, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(692224):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i4_0 * 256 + ax0_ax1_ax2_ax3_fused // 2704)
                                    v2 = T.axis.spatial(52, ax0_ax1_ax2_ax3_fused % 2704 // 52)
                                    v3 = T.axis.spatial(52, ax0_ax1_ax2_ax3_fused % 52)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(16384):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused * 64 + ax0_ax1_ax2_ax3_fused // 256)
                                    v1 = T.axis.spatial(1024, i4_0 * 256 + ax0_ax1_ax2_ax3_fused % 256)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 8, 13, 1, 16, 1, 1, 1, 2, 1, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 104 * 16 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 104 // 26 * 13 + i2_3)
                                    xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + i3_4)
                                    rc = T.axis.reduce(1024, i4_0 * 256 + i4_1 * 16 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 16, 13, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 104 * 16 + ax1)
                                v2 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 104 // 26 * 13 + ax2)
                                v3 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 1, 4, 8, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 13, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 26, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[4, 16, 16])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:39:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #3: "fused_nn_conv2d_add_2"
[14:39:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(1024, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_add: T.Buffer[(1, 1024, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 512, 52, 52], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 1024, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 52, 52):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 1024, 52, 52, 512, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 1024, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:39:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(1024, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_add: T.Buffer[(1, 1024, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([1024, 512, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(416, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(8, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1664):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 16 + ax0_ax1_ax2_ax3_fused // 104)
                                    v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 4 + ax0_ax1_ax2_ax3_fused % 104 // 26)
                                    v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + ax0_ax1_ax2_ax3_fused % 26)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(1024):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 26 * 64 + ax0_ax1_ax2_ax3_fused // 16)
                                    v1 = T.axis.spatial(512, i4_0 * 16 + ax0_ax1_ax2_ax3_fused % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 4, 1, 13, 2, 1, 1, 1, 1, 2, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 26 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3)
                                    yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + i2_4)
                                    xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 13 + i3_3)
                                    rc = T.axis.reduce(512, i4_0 * 16 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 13):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 26 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                                v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + ax2)
                                v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 13 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[16, 2, 8, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[13, 2, 1, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 1, 13, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[32, 8, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:39:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #4: "fused_nn_conv2d_add_3"
[14:39:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 104, 104), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_add: T.Buffer[(1, 512, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 104, 104], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 512, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 104, 104):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 512, 52, 52, 256, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 104, 104], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 512, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:39:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 104, 104), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_add: T.Buffer[(1, 512, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 256, 104, 104], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([512, 256, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(416, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(39168):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, ax0_ax1_ax2_ax3_fused % 39168 // 153)
                                    v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 52 // 26 * 52 + ax0_ax1_ax2_ax3_fused % 153 // 3)
                                    v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 26 * 4 + ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(16384):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 52 * 64 + ax0_ax1_ax2_ax3_fused // 256)
                                    v1 = T.axis.spatial(256, ax0_ax1_ax2_ax3_fused % 256)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 8, 2, 1, 64, 1, 1, 1, 2, 1, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 52 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 52 // 26 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + i2_3)
                                    xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 * 2 + i3_4)
                                    rc = T.axis.reduce(256, i4_1 * 64 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 104, 104], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 16, 2, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 52 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + ax1)
                                v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 52 // 26 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + ax2)
                                v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 1, 4, 8, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 13, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[26, 1, 1, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 4, 64])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:39:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #5: "fused_nn_conv2d_add_4"
[14:39:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], placeholder_1: T.Buffer[(256, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 256, 104, 104], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 104, 104, 64, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 256, 104, 104):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:39:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], placeholder_1: T.Buffer[(256, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 256, 104, 104], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([256, 64, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(43264):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 4 + ax0_ax1_ax2_ax3_fused // 10816)
                                    v2 = T.axis.spatial(104, ax0_ax1_ax2_ax3_fused % 10816 // 104)
                                    v3 = T.axis.spatial(104, ax0_ax1_ax2_ax3_fused % 104)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(128):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 32 + ax0_ax1_ax2_ax3_fused // 4)
                                    v1 = T.axis.spatial(64, i4_0 * 4 + ax0_ax1_ax2_ax3_fused % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 13, 26, 2, 1, 1, 1, 1, 2, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                                    yy = T.axis.spatial(104, i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 26 + i2_3 * 2 + i2_4)
                                    xx = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 2 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 26 + i3_3)
                                    rc = T.axis.reduce(64, i4_0 * 4 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 26, 26):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 + ax1)
                                v2 = T.axis.spatial(104, i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 26 + ax2)
                                v3 = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 2 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 26 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 2, 16, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 13, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 2, 26, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 2, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:39:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #6: "fused_nn_conv2d_add_nn_relu_1"
[14:39:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 416, 416), "float32"], placeholder_1: T.Buffer[(64, 3, 7, 7), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 3, 422, 422], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 64, 208, 208], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 208, 208], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 3, 422, 422):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 3, i3_1 - 3])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(3 <= i2_1 and i2_1 < 419 and 3 <= i3_1 and i3_1 < 419, placeholder[i0_1, i1_1, i2_1 - 3, i3_1 - 3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 208, 208, 3, 7, 7):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 64, 208, 208):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 64, 208, 208):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:39:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 416, 416), "float32"], placeholder_1: T.Buffer[(64, 3, 7, 7), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 64, 208, 208], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 3, 422, 422], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([64, 3, 7, 7], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(5408, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 7, 7):
                            for ax0_ax1_ax2_ax3_fused in T.serial(128235):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 128235 // 42745)
                                    v2 = T.axis.spatial(422, i5_0 + ax0_ax1_ax2_ax3_fused % 42745 // 103)
                                    v3 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused * 104 + i6_0 + ax0_ax1_ax2_ax3_fused % 103)
                                    T.reads(placeholder[v0, v1, v2 - 3, v3 - 3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(3 <= v2 and v2 < 419 and 3 <= v3 and v3 < 419, placeholder[v0, v1, v2 - 3, v3 - 3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(192):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused // 3)
                                    v1 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 4, 4, 1, 1, 1, 1, 1, 2, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 338)
                                    yy = T.axis.spatial(208, i0_2_i1_2_i2_2_i3_2_fused % 338 // 13 * 8 + i2_3 * 2 + i2_4)
                                    xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 52 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_3)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_1, i5_0, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 8, 4):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 338 + ax1)
                                v2 = T.axis.spatial(208, i0_2_i1_2_i2_2_i3_2_fused % 338 // 13 * 8 + ax2)
                                v3 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 52 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 16, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 26, 4, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 13, 4, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[7, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[7, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:39:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #7: "fused_nn_max_pool2d"
[14:39:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 208, 208), "float32"], tensor: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 210, 210], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 210, 210):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1])
                T.writes(pad_temp[ax0, ax1, ax2, ax3])
                pad_temp[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 209 and 1 <= ax3 and ax3 < 209, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T.float32(-3.4028234663852886e+38), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 64, 104, 104, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

[14:39:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 208, 208), "float32"], tensor: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 64, 104, 104, 3, 3):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0 - 1, ax3 * 2 + rv1 - 1])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], T.if_then_else(1 <= ax2 * 2 + rv0 and ax2 * 2 + rv0 < 209 and 1 <= ax3 * 2 + rv1 and ax3 * 2 + rv1 < 209, placeholder[ax0, ax1, ax2 * 2 + rv0 - 1, ax3 * 2 + rv1 - 1], T.float32(-3.4028234663852886e+38), dtype="float32"))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b0)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
[14:39:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #8: "fused_nn_conv2d_add_nn_relu_2"
[14:39:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 104, 104, 64, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [64, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:39:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([64, 64, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(8, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(5408):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 8 + ax0_ax1_ax2_ax3_fused // 676)
                                    v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 4 * 26 + ax0_ax1_ax2_ax3_fused % 676 // 26)
                                    v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 4 * 26 + ax0_ax1_ax2_ax3_fused % 26)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(512):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused // 8)
                                    v1 = T.axis.spatial(64, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 8, 1, 1, 1, 1, 13, 13):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3)
                                    yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 4 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 13 + i2_4)
                                    xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 4 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 13 + i3_4)
                                    rc = T.axis.reduce(64, i4_0 * 8 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [64, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 13, 13):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                                v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 4 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 13 + ax2)
                                v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 4 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 13 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 4, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 2, 1, 1, 13])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 2, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:39:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #9: "fused_nn_conv2d_add_nn_relu_3"
[14:39:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 104, 104), "float32"], placeholder_1: T.Buffer[(64, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 104, 104], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 104, 104):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 104, 104, 256, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 104, 104], "float32"], ["TENSOR", [64, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:39:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 104, 104), "float32"], placeholder_1: T.Buffer[(64, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 256, 104, 104], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([64, 256, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(338, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(346112):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 64 + ax0_ax1_ax2_ax3_fused // 5408)
                                    v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + ax0_ax1_ax2_ax3_fused % 5408 // 104)
                                    v3 = T.axis.spatial(104, ax0_ax1_ax2_ax3_fused % 104)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(128):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + ax0_ax1_ax2_ax3_fused // 64)
                                    v1 = T.axis.spatial(256, i4_0 * 64 + ax0_ax1_ax2_ax3_fused % 64)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(64, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i1_4)
                                    yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused // 26 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i2_3)
                                    xx = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 26 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3)
                                    rc = T.axis.reduce(256, i4_0 * 64 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 104, 104], "float32"], ["TENSOR", [64, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + ax1)
                                v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused // 26 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax2)
                                v3 = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 26 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 1, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 13, 2, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 26, 2, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 64, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:39:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #10: "fused_nn_conv2d_add_nn_relu_4"
[14:39:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 106, 106], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 106, 106):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 105 and 1 <= i3_1 and i3_1 < 105, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 104, 104, 64, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:39:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 64, 106, 106], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([64, 64, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(52, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(13, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(359552):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 32 + ax0_ax1_ax2_ax3_fused // 11236)
                                    v2 = T.axis.spatial(106, ax0_ax1_ax2_ax3_fused % 11236 // 106)
                                    v3 = T.axis.spatial(106, ax0_ax1_ax2_ax3_fused % 106)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 105 and 1 <= v3 and v3 < 105, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(9216):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 32 + ax0_ax1_ax2_ax3_fused // 288)
                                    v1 = T.axis.spatial(64, i4_0 * 32 + ax0_ax1_ax2_ax3_fused % 288 // 9)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 8, 1, 4, 32, 1, 3, 1, 2, 8, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 26 * 16 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(104, i0_2_i1_2_i2_2_i3_2_fused * 8 + i2_4)
                                    xx = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 26 * 4 + i3_3)
                                    rc = T.axis.reduce(64, i4_0 * 32 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_1, i6_2])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 16, 8, 4):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 26 * 16 + ax1)
                                v2 = T.axis.spatial(104, i0_2_i1_2_i2_2_i3_2_fused * 8 + ax2)
                                v3 = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 26 * 4 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 1, 8, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 8])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 26, 1, 4, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 1, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:39:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #11: "fused_nn_conv2d_add_add_nn_relu"
[14:39:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], placeholder_1: T.Buffer[(256, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 256, 104, 104), "float32"], T_relu: T.Buffer[(1, 256, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 256, 104, 104], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 104, 104], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 256, 104, 104], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 104, 104, 64, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 256, 104, 104):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 104, 104):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 256, 104, 104):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:39:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], placeholder_1: T.Buffer[(256, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 256, 104, 104), "float32"], T_relu: T.Buffer[(1, 256, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 256, 104, 104], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([256, 64, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(86528):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 16 + ax0_ax1_ax2_ax3_fused // 5408)
                                    v2 = T.axis.spatial(104, ax0_ax1_ax2_ax3_fused % 5408 // 52)
                                    v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + ax0_ax1_ax2_ax3_fused % 52)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(256):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + ax0_ax1_ax2_ax3_fused // 16)
                                    v1 = T.axis.spatial(64, i4_0 * 16 + ax0_ax1_ax2_ax3_fused % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 13, 4, 8, 1, 1, 1, 2, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused * 13 + i2_3)
                                    xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_3)
                                    rc = T.axis.reduce(64, i4_0 * 16 + i4_1 * 8 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 13, 4):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + ax1)
                                v2 = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused * 13 + ax2)
                                v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17])
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[16, 1, 4, 2, 2])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27])
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 8, 1, 13, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37])
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[2, 1, 13, 4, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47])
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[4, 2, 8])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55])
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61])
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67])
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
[14:39:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #12: "fused_nn_conv2d_add_nn_relu_5"
[14:39:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 104, 104), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 104, 104], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 104, 104], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 104, 104], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 104, 104):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 104, 104, 256, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 104, 104], "float32"], ["TENSOR", [128, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 104, 104):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 104, 104):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:39:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 104, 104), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 104, 104], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 256, 104, 104], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 256, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(208, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(53248):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 128 + ax0_ax1_ax2_ax3_fused // 416)
                                    v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 26 // 13 * 52 + ax0_ax1_ax2_ax3_fused % 416 // 8)
                                    v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + ax0_ax1_ax2_ax3_fused % 8)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(2048):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 26 * 16 + ax0_ax1_ax2_ax3_fused // 128)
                                    v1 = T.axis.spatial(256, i4_0 * 128 + ax0_ax1_ax2_ax3_fused % 128)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(128, 1, 1, 1, 8, 2, 1, 1, 1, 1, 1, 1, 13, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 26 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 8 + i1_3)
                                    yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 26 // 13 * 52 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 26 + i2_3 * 13 + i2_4)
                                    xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    rc = T.axis.reduce(256, i4_0 * 128 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 104, 104], "float32"], ["TENSOR", [128, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 26, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 26 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 8 + ax1)
                                v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 26 // 13 * 52 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 26 + ax2)
                                v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 1, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 2, 2, 13])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 4, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 128, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:39:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #13: "fused_nn_conv2d_add_nn_relu_6"
[14:39:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 104, 104), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 106, 106], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 106, 106):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 105 and 1 <= i3_1 and i3_1 < 105, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 52, 52, 128, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 104, 104], "float32"], ["TENSOR", [128, 128, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:39:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 104, 104), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 52, 52], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 128, 106, 106], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 128, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(32, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(26, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(352800):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 32 + ax0_ax1_ax2_ax3_fused // 11025)
                                    v2 = T.axis.spatial(106, ax0_ax1_ax2_ax3_fused % 11025 // 105)
                                    v3 = T.axis.spatial(106, ax0_ax1_ax2_ax3_fused % 105)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 105 and 1 <= v3 and v3 < 105, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(9216):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 32 + ax0_ax1_ax2_ax3_fused // 288)
                                    v1 = T.axis.spatial(128, i4_0 * 32 + ax0_ax1_ax2_ax3_fused % 288 // 9)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 3, 1, 1, 1, 1, 2, 2, 1, 3, 1, 1, 26, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 13)
                                    yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i2_4)
                                    xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_3 * 2 + i3_4)
                                    rc = T.axis.reduce(128, i4_0 * 32 + i4_1 * 2 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_1, i6_2])
                                    T.reads(pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 104, 104], "float32"], ["TENSOR", [128, 128, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 26, 4):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 13 + ax1)
                                v2 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + ax2)
                                v3 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 16, 2, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 26])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 16, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:39:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #14: "fused_nn_conv2d_add_nn_relu_7"
[14:39:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(128, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 512, 52, 52], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 52, 52):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 52, 52, 512, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [128, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:39:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(128, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 52, 52], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 512, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(64, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1384448):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, ax0_ax1_ax2_ax3_fused // 2704)
                                    v2 = T.axis.spatial(52, ax0_ax1_ax2_ax3_fused % 2704 // 52)
                                    v3 = T.axis.spatial(52, ax0_ax1_ax2_ax3_fused % 52)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(32768):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + ax0_ax1_ax2_ax3_fused // 512)
                                    v1 = T.axis.spatial(512, ax0_ax1_ax2_ax3_fused % 512)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(256, 1, 1, 1, 8, 13, 13, 2, 1, 1, 1, 1, 1, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused // 8 * 8 + i1_3)
                                    yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 8 // 2 * 13 + i2_3)
                                    xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i3_3 * 2 + i3_4)
                                    rc = T.axis.reduce(512, i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [128, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 13, 26):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused // 8 * 8 + ax1)
                                v2 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 8 // 2 * 13 + ax2)
                                v3 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 8, 1, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 1, 13, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 256, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:39:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #15: "fused_nn_conv2d_add_nn_relu_8"
[14:39:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 54, 54], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 54, 54):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 53 and 1 <= i3_1 and i3_1 < 53, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 52, 52, 128, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 52, 52], "float32"], ["TENSOR", [128, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:39:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 52, 52], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 128, 54, 54], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 128, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(416, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(16, 3, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(6240):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 6240 // 780)
                                    v2 = T.axis.spatial(54, i5_0 + ax0_ax1_ax2_ax3_fused % 780 // 15)
                                    v3 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused * 13 + ax0_ax1_ax2_ax3_fused % 15)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 53 and 1 <= v3 and v3 < 53, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(3072):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 24)
                                    v1 = T.axis.spatial(128, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 24 // 3)
                                    v2 = T.axis.spatial(3, i5_0)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 2, 1, 4, 1, 3, 1, 2, 1, 13):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + i1_4)
                                    yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + i2_3)
                                    xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 13 + i3_4)
                                    rc = T.axis.reduce(128, i4_0 * 8 + i4_1 * 4 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 52, 52], "float32"], ["TENSOR", [128, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 13):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + ax1)
                                v2 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + ax2)
                                v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 13 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 16, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 26, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:39:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #16: "fused_nn_conv2d_add_add_nn_relu_1"
[14:39:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52), "float32"], placeholder_1: T.Buffer[(512, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 512, 52, 52), "float32"], T_relu: T.Buffer[(1, 512, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 512, 52, 52], dtype="float32")
        T_add = T.alloc_buffer([1, 512, 52, 52], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 512, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 512, 52, 52, 128, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 52, 52], "float32"], ["TENSOR", [512, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 512, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 52, 52):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 512, 52, 52):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:39:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52), "float32"], placeholder_1: T.Buffer[(512, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 512, 52, 52), "float32"], T_relu: T.Buffer[(1, 512, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 128, 52, 52], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([512, 128, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(16, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1664):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 8 + ax0_ax1_ax2_ax3_fused // 208)
                                    v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + ax0_ax1_ax2_ax3_fused % 208 // 52)
                                    v3 = T.axis.spatial(52, ax0_ax1_ax2_ax3_fused % 52)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(512):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 13 * 64 + ax0_ax1_ax2_ax3_fused // 8)
                                    v1 = T.axis.spatial(128, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 13, 8, 1, 1, 1, 1, 2, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 13 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused)
                                    yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i2_3 * 2 + i2_4)
                                    xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 4 * 13 + i3_3)
                                    rc = T.axis.reduce(128, i4_0 * 8 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 52, 52], "float32"], ["TENSOR", [512, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 4, 13):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 13 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                                v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + ax2)
                                v3 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 4 * 13 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17])
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 4, 16, 1, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27])
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 1, 2, 2])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37])
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 4, 1, 13, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47])
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[16, 1, 8])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55])
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61])
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67])
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
[14:39:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #17: "fused_nn_conv2d_add_nn_relu_9"
[14:39:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(256, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 512, 52, 52], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 256, 52, 52], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 52, 52):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 52, 52, 512, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [256, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 256, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 52, 52):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:39:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(256, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 256, 52, 52], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([256, 512, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1352, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1024):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 256 + ax0_ax1_ax2_ax3_fused // 4)
                                    v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 676 // 26 * 2 + ax0_ax1_ax2_ax3_fused % 4 // 2)
                                    v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 * 2 + ax0_ax1_ax2_ax3_fused % 2)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(32768):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 676 * 128 + ax0_ax1_ax2_ax3_fused // 256)
                                    v1 = T.axis.spatial(512, i4_0 * 256 + ax0_ax1_ax2_ax3_fused % 256)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 4, 1, 1, 32, 1, 1, 1, 2, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 676 * 128 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 676 // 26 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2)
                                    xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    rc = T.axis.reduce(512, i4_0 * 256 + i4_1 * 32 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [256, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 676 * 128 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + ax1)
                                v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 676 // 26 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2 + ax2)
                                v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 8, 2, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[26, 2, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[26, 1, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 8, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:39:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #18: "fused_nn_conv2d_add_nn_relu_10"
[14:39:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52), "float32"], placeholder_1: T.Buffer[(256, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 54, 54], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 256, 52, 52], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 54, 54):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 53 and 1 <= i3_1 and i3_1 < 53, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 52, 52, 256, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 52, 52], "float32"], ["TENSOR", [256, 256, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 256, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 52, 52):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:39:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52), "float32"], placeholder_1: T.Buffer[(256, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 256, 52, 52], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 256, 54, 54], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([256, 256, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(13, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(373248):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 128 + ax0_ax1_ax2_ax3_fused // 2916)
                                    v2 = T.axis.spatial(54, ax0_ax1_ax2_ax3_fused % 2916 // 54)
                                    v3 = T.axis.spatial(54, ax0_ax1_ax2_ax3_fused % 54)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 53 and 1 <= v3 and v3 < 53, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(73728):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + ax0_ax1_ax2_ax3_fused // 1152)
                                    v1 = T.axis.spatial(256, i4_0 * 128 + ax0_ax1_ax2_ax3_fused % 1152 // 9)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(32, 1, 1, 1, 1, 1, 1, 4, 3, 3, 1, 4, 2, 26):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_4)
                                    yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + i2_4)
                                    xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 2 * 26 + i3_4)
                                    rc = T.axis.reduce(256, i4_0 * 128 + i4_1 * 4 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_2, i6_2])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 52, 52], "float32"], ["TENSOR", [256, 256, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 26):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + ax1)
                                v2 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + ax2)
                                v3 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 2 * 26 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 16, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 13, 2, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 26])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 32, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:39:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #19: "fused_nn_conv2d_add_nn_relu_11"
[14:39:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(256, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 1024, 52, 52], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 256, 52, 52], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1024, 52, 52):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 52, 52, 1024, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 256, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 52, 52):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:39:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(256, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 256, 52, 52], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([256, 1024, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(416, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(416, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(173056):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i4_0 * 128 + ax0_ax1_ax2_ax3_fused // 1352)
                                    v2 = T.axis.spatial(52, ax0_ax1_ax2_ax3_fused % 1352 // 26)
                                    v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + ax0_ax1_ax2_ax3_fused % 26)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(16384):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + ax0_ax1_ax2_ax3_fused // 128)
                                    v1 = T.axis.spatial(1024, i4_0 * 128 + ax0_ax1_ax2_ax3_fused % 128)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 64, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_1_i1_1_i2_1_i3_1_fused // 26 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 52)
                                    yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 26 // 13 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 52 // 2)
                                    xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    rc = T.axis.reduce(1024, i4_0 * 128 + i4_1 * 64 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_1_i1_1_i2_1_i3_1_fused // 26 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 52 + ax1)
                                v2 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 26 // 13 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 52 // 2 + ax2)
                                v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 16, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 26, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 13, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 64])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:39:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #20: "fused_nn_conv2d_add_nn_relu_12"
[14:39:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52), "float32"], placeholder_1: T.Buffer[(256, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 56, 56], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 256, 52, 52], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 56, 56):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(2 <= i2_1 and i2_1 < 54 and 2 <= i3_1 and i3_1 < 54, placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 52, 52, 256, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, ry * 2 + yy, rx * 2 + xx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 52, 52], "float32"], ["TENSOR", [256, 256, 3, 3], "float32"], [1, 1], [2, 2, 2, 2], [2, 2], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, ry * 2 + yy, rx * 2 + xx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 256, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 52, 52):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:39:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52), "float32"], placeholder_1: T.Buffer[(256, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 256, 52, 52], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 256, 56, 56], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([256, 256, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(676, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(128, 3, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(2912):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 2 + ax0_ax1_ax2_ax3_fused % 2912 // 1456)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 26 + i5_0 * 2 + ax0_ax1_ax2_ax3_fused % 1456 // 56)
                                    v3 = T.axis.spatial(56, ax0_ax1_ax2_ax3_fused % 56)
                                    T.reads(placeholder[v0, v1, v2 - 2, v3 - 2])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(2 <= v2 and v2 < 54 and 2 <= v3 and v3 < 54, placeholder[v0, v1, v2 - 2, v3 - 2], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(1536):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, ax0_ax1_ax2_ax3_fused // 6)
                                    v1 = T.axis.spatial(256, i4_0 * 2 + ax0_ax1_ax2_ax3_fused % 6 // 3)
                                    v2 = T.axis.spatial(3, i5_0)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 16, 2, 2, 2, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 169 * 16 + i1_3)
                                    yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 26 + i0_2_i1_2_i2_2_i3_2_fused % 169 // 13 * 2 + i2_3)
                                    xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + i3_3)
                                    rc = T.axis.reduce(256, i4_0 * 2 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                    T.reads(pad_temp_shared[nn, rc, ry * 2 + yy, rx * 2 + xx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 52, 52], "float32"], ["TENSOR", [256, 256, 3, 3], "float32"], [1, 1], [2, 2, 2, 2], [2, 2], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, ry * 2 + yy, rx * 2 + xx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 16, 2, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 169 * 16 + ax1)
                                v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 26 + i0_2_i1_2_i2_2_i3_2_fused % 169 // 13 * 2 + ax2)
                                v3 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 4, 16, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 13, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 13, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:39:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #21: "fused_nn_conv2d_add_add_nn_relu_2"
[14:39:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52), "float32"], placeholder_1: T.Buffer[(1024, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 1024, 52, 52), "float32"], T_relu: T.Buffer[(1, 1024, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 52, 52], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 1024, 52, 52], dtype="float32")
        T_add = T.alloc_buffer([1, 1024, 52, 52], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 1024, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 52, 52):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 1024, 52, 52, 256, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 52, 52], "float32"], ["TENSOR", [1024, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 1024, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 1024, 52, 52):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 1024, 52, 52):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:39:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52), "float32"], placeholder_1: T.Buffer[(1024, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 1024, 52, 52), "float32"], T_relu: T.Buffer[(1, 1024, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 256, 52, 52], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([1024, 256, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(338, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(32, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(256):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 32 + ax0_ax1_ax2_ax3_fused // 8)
                                    v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 13 * 2 + ax0_ax1_ax2_ax3_fused % 8 // 4)
                                    v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + ax0_ax1_ax2_ax3_fused % 4)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(32768):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, ax0_ax1_ax2_ax3_fused // 32)
                                    v1 = T.axis.spatial(256, i4_0 * 32 + ax0_ax1_ax2_ax3_fused % 32)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 4, 1, 1, 4, 1, 1, 1, 2, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused // 4 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 13 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2)
                                    xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    rc = T.axis.reduce(256, i4_0 * 32 + i4_1 * 4 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 52, 52], "float32"], ["TENSOR", [1024, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused // 4 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + ax1)
                                v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 13 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 + ax2)
                                v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17])
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 8, 16, 4, 2])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27])
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[26, 2, 1, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37])
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[13, 2, 2, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47])
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[8, 8, 4])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55])
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61])
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67])
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
[14:39:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #22: "fused_nn_conv2d_add_nn_relu_13"
[14:39:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(512, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_relu: T.Buffer[(1, 512, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 1024, 52, 52], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 512, 52, 52], dtype="float32")
        T_add = T.alloc_buffer([1, 512, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1024, 52, 52):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 512, 52, 52, 1024, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [512, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 512, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 52, 52):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:39:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(512, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_relu: T.Buffer[(1, 512, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([512, 1024, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(26, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(106496):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i4_0 * 512 + ax0_ax1_ax2_ax3_fused // 208)
                                    v2 = T.axis.spatial(52, ax0_ax1_ax2_ax3_fused % 208 // 4)
                                    v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + ax0_ax1_ax2_ax3_fused % 4)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(131072):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 13 * 256 + ax0_ax1_ax2_ax3_fused // 512)
                                    v1 = T.axis.spatial(1024, i4_0 * 512 + ax0_ax1_ax2_ax3_fused % 512)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(128, 1, 1, 1, 2, 13, 2, 4, 1, 1, 1, 2, 2, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 13 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 2 * 26 + i2_3 * 2 + i2_4)
                                    xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i3_3 * 2 + i3_4)
                                    rc = T.axis.reduce(1024, i4_0 * 512 + i4_1 * 4 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [512, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 26, 4):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 13 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + ax1)
                                v2 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 2 * 26 + ax2)
                                v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 32, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 13, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 1, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 128, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:39:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #23: "fused_nn_conv2d_add_nn_relu_14"
[14:39:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(512, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_relu: T.Buffer[(1, 512, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 512, 56, 56], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 512, 52, 52], dtype="float32")
        T_add = T.alloc_buffer([1, 512, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 56, 56):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(2 <= i2_1 and i2_1 < 54 and 2 <= i3_1 and i3_1 < 54, placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 512, 52, 52, 512, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, ry * 2 + yy, rx * 2 + xx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [1, 1], [2, 2, 2, 2], [2, 2], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, ry * 2 + yy, rx * 2 + xx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 512, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 52, 52):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:39:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(512, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_relu: T.Buffer[(1, 512, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 512, 56, 56], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([512, 512, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(26, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(745472):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, ax0_ax1_ax2_ax3_fused % 745472 // 1456)
                                    v2 = T.axis.spatial(56, ax0_ax1_ax2_ax3_fused % 1456 // 26)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i6_0 * 2 + ax0_ax1_ax2_ax3_fused % 26)
                                    T.reads(placeholder[v0, v1, v2 - 2, v3 - 2])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(2 <= v2 and v2 < 54 and 2 <= v3 and v3 < 54, placeholder[v0, v1, v2 - 2, v3 - 2], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(196608):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + ax0_ax1_ax2_ax3_fused // 1536)
                                    v1 = T.axis.spatial(512, ax0_ax1_ax2_ax3_fused % 1536 // 3)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(128, 1, 1, 1, 8, 2, 13, 4, 3, 1, 1, 4, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i2_3)
                                    xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 13 + i3_3)
                                    rc = T.axis.reduce(512, i4_1 * 4 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, ry * 2 + yy, rx * 2 + xx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [1, 1], [2, 2, 2, 2], [2, 2], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, ry * 2 + yy, rx * 2 + xx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 2, 13):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + ax1)
                                v2 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax2)
                                v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 13 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 4, 1, 8, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 13, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 2, 13, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 128, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:39:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #24: "fused_nn_conv2d_add_nn_relu_15"
[14:39:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 2048, 52, 52), "float32"], placeholder_1: T.Buffer[(512, 2048, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_relu: T.Buffer[(1, 512, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 2048, 52, 52], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 512, 52, 52], dtype="float32")
        T_add = T.alloc_buffer([1, 512, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 2048, 52, 52):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 512, 52, 52, 2048, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 2048, 52, 52], "float32"], ["TENSOR", [512, 2048, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 512, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 52, 52):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:39:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 2048, 52, 52), "float32"], placeholder_1: T.Buffer[(512, 2048, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_relu: T.Buffer[(1, 512, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 2048, 52, 52], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([512, 2048, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(8, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(256, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(21632):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(2048, i4_0 * 8 + ax0_ax1_ax2_ax3_fused // 2704)
                                    v2 = T.axis.spatial(52, ax0_ax1_ax2_ax3_fused % 2704 // 52)
                                    v3 = T.axis.spatial(52, ax0_ax1_ax2_ax3_fused % 52)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(1024):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 128 + ax0_ax1_ax2_ax3_fused // 8)
                                    v1 = T.axis.spatial(2048, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 1, 1, 13, 1, 1, 1, 1, 8, 52, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 8 + i1_4)
                                    yy = T.axis.spatial(52, i2_4)
                                    xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 4 * 13 + i3_3)
                                    rc = T.axis.reduce(2048, i4_0 * 8 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 2048, 52, 52], "float32"], ["TENSOR", [512, 2048, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 52, 13):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 8 + ax1)
                                v2 = T.axis.spatial(52, ax2)
                                v3 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 4 * 13 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 8, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 52])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 4, 1, 13, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[256, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:39:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #25: "fused_nn_conv2d_add_nn_relu_16"
[14:39:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(512, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_relu: T.Buffer[(1, 512, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 512, 60, 60], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 512, 52, 52], dtype="float32")
        T_add = T.alloc_buffer([1, 512, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 60, 60):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 4, i3_1 - 4])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(4 <= i2_1 and i2_1 < 56 and 4 <= i3_1 and i3_1 < 56, placeholder[i0_1, i1_1, i2_1 - 4, i3_1 - 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 512, 52, 52, 512, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, ry * 4 + yy, rx * 4 + xx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [1, 1], [4, 4, 4, 4], [4, 4], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, ry * 4 + yy, rx * 4 + xx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 512, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 52, 52):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:39:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(512, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_relu: T.Buffer[(1, 512, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 512, 60, 60], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([512, 512, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1352, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(452608):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 256 + ax0_ax1_ax2_ax3_fused % 452608 // 1768)
                                    v2 = T.axis.spatial(60, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + ax0_ax1_ax2_ax3_fused % 1768 // 52)
                                    v3 = T.axis.spatial(60, i6_0 * 4 + ax0_ax1_ax2_ax3_fused % 52)
                                    T.reads(placeholder[v0, v1, v2 - 4, v3 - 4])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(4 <= v2 and v2 < 56 and 4 <= v3 and v3 < 56, placeholder[v0, v1, v2 - 4, v3 - 4], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(196608):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 256 + ax0_ax1_ax2_ax3_fused // 768)
                                    v1 = T.axis.spatial(512, i4_0 * 256 + ax0_ax1_ax2_ax3_fused % 768 // 3)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 3, 1, 1, 4, 2, 1, 16, 1, 1, 1, 8, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 256 + i0_1_i1_1_i2_1_i3_1_fused // 338 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 32 + i1_3 * 8 + i1_4)
                                    yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 338 // 26 * 2 + i2_3)
                                    xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 26 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    rc = T.axis.reduce(512, i4_0 * 256 + i4_1 * 16 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, ry * 4 + yy, rx * 4 + xx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [1, 1], [4, 4, 4, 4], [4, 4], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, ry * 4 + yy, rx * 4 + xx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 2, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 256 + i0_1_i1_1_i2_1_i3_1_fused // 338 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 32 + ax1)
                                v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 338 // 26 * 2 + ax2)
                                v3 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 26 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 2, 4, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 13, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 26, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 16, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:39:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #26: "fused_nn_conv2d_add_add_nn_relu_3"
[14:39:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(2048, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 2048, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 2048, 52, 52), "float32"], T_relu: T.Buffer[(1, 2048, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 512, 52, 52], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 2048, 52, 52], dtype="float32")
        T_add = T.alloc_buffer([1, 2048, 52, 52], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 2048, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 52, 52):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 2048, 52, 52, 512, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [2048, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 2048, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 2048, 52, 52):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 2048, 52, 52):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:39:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(2048, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 2048, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 2048, 52, 52), "float32"], T_relu: T.Buffer[(1, 2048, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 2048, 52, 52], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([2048, 512, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(3328, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(8, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(3328):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 32 + ax0_ax1_ax2_ax3_fused // 104)
                                    v2 = T.axis.spatial(52, ax0_ax1_ax2_ax3_fused % 104 // 2)
                                    v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 * 2 + ax0_ax1_ax2_ax3_fused % 2)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(512):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused // 26 * 16 + ax0_ax1_ax2_ax3_fused // 32)
                                    v1 = T.axis.spatial(512, i4_0 * 32 + ax0_ax1_ax2_ax3_fused % 32)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 13, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused // 26 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_4)
                                    yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 13 + i2_4)
                                    xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    rc = T.axis.reduce(512, i4_0 * 32 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [2048, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 13, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused // 26 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + ax1)
                                v2 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 13 + ax2)
                                v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17])
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[128, 2, 2, 1, 4])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27])
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 2, 1, 13])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37])
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[26, 1, 2, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47])
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[16, 32, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55])
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61])
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67])
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
[14:39:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #27: "fused_nn_conv2d_add_nn_relu_17"
[14:39:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 2048, 52, 52), "float32"], placeholder_1: T.Buffer[(512, 2048, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_relu: T.Buffer[(1, 512, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 2048, 54, 54], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 512, 52, 52], dtype="float32")
        T_add = T.alloc_buffer([1, 512, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 2048, 54, 54):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 53 and 1 <= i3_1 and i3_1 < 53, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 512, 52, 52, 2048, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 2048, 52, 52], "float32"], ["TENSOR", [512, 2048, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 512, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 52, 52):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:39:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 2048, 52, 52), "float32"], placeholder_1: T.Buffer[(512, 2048, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_relu: T.Buffer[(1, 512, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 2048, 54, 54], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([512, 2048, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(128, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(46656):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(2048, i4_0 * 16 + ax0_ax1_ax2_ax3_fused // 2916)
                                    v2 = T.axis.spatial(54, ax0_ax1_ax2_ax3_fused % 2916 // 54)
                                    v3 = T.axis.spatial(54, ax0_ax1_ax2_ax3_fused % 54)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 53 and 1 <= v3 and v3 < 53, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(4608):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + ax0_ax1_ax2_ax3_fused // 144)
                                    v1 = T.axis.spatial(2048, i4_0 * 16 + ax0_ax1_ax2_ax3_fused % 144 // 9)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 1, 13, 1, 2, 3, 3, 1, 4, 4, 13):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_4)
                                    yy = T.axis.spatial(52, i2_3 * 4 + i2_4)
                                    xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 4 * 13 + i3_4)
                                    rc = T.axis.reduce(2048, i4_0 * 16 + i4_1 * 2 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_2, i6_2])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 2048, 52, 52], "float32"], ["TENSOR", [512, 2048, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 52, 13):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                                v2 = T.axis.spatial(52, ax2)
                                v3 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 4 * 13 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 2, 4, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 4, 1, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 8, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:39:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #28: "fused_nn_conv2d_add_5"
[14:39:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(21, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 21, 1, 1), "float32"], T_add: T.Buffer[(1, 21, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 512, 52, 52], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 21, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 52, 52):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 21, 52, 52, 512, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [21, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 21, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:39:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(21, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 21, 1, 1), "float32"], T_add: T.Buffer[(1, 21, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 21, 52, 52], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([21, 512, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(26, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(13, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(3328):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 32 + ax0_ax1_ax2_ax3_fused // 104)
                                    v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 2 + ax0_ax1_ax2_ax3_fused % 104 // 52)
                                    v3 = T.axis.spatial(52, ax0_ax1_ax2_ax3_fused % 52)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(672):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(21, ax0_ax1_ax2_ax3_fused // 32)
                                    v1 = T.axis.spatial(512, i4_0 * 32 + ax0_ax1_ax2_ax3_fused % 32)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 21, 1, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(21, i1_4)
                                    yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused)
                                    xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused * 4 + i3_3 * 2 + i3_4)
                                    rc = T.axis.reduce(512, i4_0 * 32 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [21, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 21, 1, 4):
                            with T.block("conv2d_nchw_local"):
                                v0, v1 = T.axis.remap("SS", [ax0, ax1])
                                v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused + ax2)
                                v3 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused * 4 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 21])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[26, 1, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 13, 1, 2, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 16, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:39:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #29: "fused_image_resize2d"
[14:39:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 21, 52, 52), "float32"], resize: T.Buffer[(1, 21, 416, 416), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 21, 416, 416):
            with T.block("resize"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, 0 : 52, 0 : 52])
                T.writes(resize[i0_1, i1_1, i2_1, i3_1])
                resize[i0_1, i1_1, i2_1, i3_1] = (placeholder[i0_1, i1_1, T.max(T.min(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), 51), 0), T.max(T.min(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), 51), 0)] * (T.float32(1) - ((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))) + placeholder[i0_1, i1_1, T.max(T.min(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), 51), 0), T.max(T.min(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32") + 1, 51), 0)] * ((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))) * (T.float32(1) - ((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))) + (placeholder[i0_1, i1_1, T.max(T.min(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32") + 1, 51), 0), T.max(T.min(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), 51), 0)] * (T.float32(1) - ((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))) + placeholder[i0_1, i1_1, T.max(T.min(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32") + 1, 51), 0), T.max(T.min(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32") + 1, 51), 0)] * ((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))) * ((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))
    

[14:39:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 21, 52, 52), "float32"], resize: T.Buffer[(1, 21, 416, 416), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 21, 416, 416):
                with T.block("resize"):
                    i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[i0_1, i1_1, 0 : 52, 0 : 52])
                    T.writes(resize[i0_1, i1_1, i2_1, i3_1])
                    resize[i0_1, i1_1, i2_1, i3_1] = (placeholder[i0_1, i1_1, T.max(T.min(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), 51), 0), T.max(T.min(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), 51), 0)] * (T.float32(1) - ((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))) + placeholder[i0_1, i1_1, T.max(T.min(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), 51), 0), T.max(T.min(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32") + 1, 51), 0)] * ((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))) * (T.float32(1) - ((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))) + (placeholder[i0_1, i1_1, T.max(T.min(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32") + 1, 51), 0), T.max(T.min(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), 51), 0)] * (T.float32(1) - ((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))) + placeholder[i0_1, i1_1, T.max(T.min(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32") + 1, 51), 0), T.max(T.min(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32") + 1, 51), 0)] * ((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))) * ((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5) - T.cast(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.125) - T.float32(0.5), dtype="float32"), "int32"), "float32"))
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:39:40] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
 ID |                              Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------
  0 |       fused_nn_conv2d_add_nn_relu | 12760457216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  1 |               fused_nn_conv2d_add |    29130192 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |             fused_nn_conv2d_add_1 | 11346935808 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |             fused_nn_conv2d_add_2 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |             fused_nn_conv2d_add_3 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |             fused_nn_conv2d_add_4 |   357187584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |     fused_nn_conv2d_add_nn_relu_1 |   819593216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |               fused_nn_max_pool2d |     6230016 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_conv2d_add_nn_relu_2 |    89989120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |     fused_nn_conv2d_add_nn_relu_3 |   355803136 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |     fused_nn_conv2d_add_nn_relu_4 |   798826496 |      3 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_add_add_nn_relu |   362725376 |      3 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_conv2d_add_nn_relu_5 |   711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |     fused_nn_conv2d_add_nn_relu_6 |   798134272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |     fused_nn_conv2d_add_nn_relu_7 |   355110912 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_nn_conv2d_add_nn_relu_8 |   798134272 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_add_nn_relu_1 |   358572032 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |     fused_nn_conv2d_add_nn_relu_9 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |    fused_nn_conv2d_add_nn_relu_10 |  3191152640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |    fused_nn_conv2d_add_nn_relu_11 |  1419059200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 20 |    fused_nn_conv2d_add_nn_relu_12 |  3191152640 |      5 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_add_nn_relu_2 |  1425981440 |      6 |            N/A |          N/A |                   N/A |      0 |            
 22 |    fused_nn_conv2d_add_nn_relu_13 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |    fused_nn_conv2d_add_nn_relu_14 | 12761841664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |    fused_nn_conv2d_add_nn_relu_15 |  5673467904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 |    fused_nn_conv2d_add_nn_relu_16 | 12761841664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_nn_conv2d_add_add_nn_relu_3 |  5687312384 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 |    fused_nn_conv2d_add_nn_relu_17 | 51039059968 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |             fused_nn_conv2d_add_5 |    58203600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |              fused_image_resize2d |   196245504 |      2 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[14:39:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_nn_conv2d_add_nn_relu"
[14:39:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[14:39:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[14:40:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55716278bb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571627eef08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x557160eb0998)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5571627ed278)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5571628bd758)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160ed4e98)]: 1955 failure(s)
[14:40:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 93 candidate(s)
[14:41:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55716278bb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571627eef08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x557160eb0998)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5571627ed278)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5571628bd758)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160ed4e98)]: 407 failure(s)
[14:42:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55716278bb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571627eef08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x557160eb0998)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5571627ed278)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5571628bd758)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160ed4e98)]: 383 failure(s)
[14:43:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55716278bb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571627eef08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x557160eb0998)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5571627ed278)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5571628bd758)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160ed4e98)]: 343 failure(s)
[14:44:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55716278bb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571627eef08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x557160eb0998)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5571627ed278)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5571628bd758)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160ed4e98)]: 313 failure(s)
[14:44:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9994  0.9993  0.9990  0.9989  0.9987  0.9983  0.9982  0.9976  0.9975  0.9973  0.9973  0.9972  0.9969  0.9968  0.9967
[17 : 32]:	0.9966  0.9965  0.9965  0.9962  0.9958  0.9957  0.9954  0.9953  0.9953  0.9953  0.9952  0.9950  0.9949  0.9948  0.9948  0.9946
[14:44:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[14:44:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[14:44:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[14:45:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[14:46:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_conv2d_add"
[14:46:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[14:46:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[14:46:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5571627ee138)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571627ecf78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x557162734908)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5571627a6188)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557160f1d468)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160ec92e8)]: 1922 failure(s)
[14:46:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 126 candidate(s)
[14:46:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5571627ee138)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571627ecf78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x557162734908)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5571627a6188)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557160f1d468)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160ec92e8)]: 584 failure(s)
[14:47:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5571627ee138)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571627ecf78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x557162734908)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5571627a6188)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557160f1d468)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160ec92e8)]: 434 failure(s)
[14:48:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5571627ee138)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571627ecf78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x557162734908)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5571627a6188)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557160f1d468)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160ec92e8)]: 456 failure(s)
[14:49:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5571627ee138)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571627ecf78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x557162734908)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5571627a6188)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557160f1d468)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160ec92e8)]: 439 failure(s)
[14:49:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9998  0.9997  0.9997  0.9995  0.9993  0.9991  0.9990  0.9989  0.9986  0.9985  0.9983  0.9981  0.9978  0.9978
[17 : 32]:	0.9970  0.9970  0.9965  0.9962  0.9961  0.9959  0.9958  0.9957  0.9956  0.9951  0.9951  0.9949  0.9949  0.9948  0.9948  0.9948
[14:49:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[14:49:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[14:49:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[14:49:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[14:50:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_conv2d_add_1"
[14:50:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[14:50:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[14:50:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557160eca4c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571627f3608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x557160f215d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55716282faa8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557160f4af18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5571627b80d8)]: 1938 failure(s)
[14:50:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 110 candidate(s)
[14:51:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557160eca4c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571627f3608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x557160f215d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55716282faa8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557160f4af18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5571627b80d8)]: 407 failure(s)
[14:51:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557160eca4c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571627f3608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x557160f215d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55716282faa8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557160f4af18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5571627b80d8)]: 372 failure(s)
[14:52:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557160eca4c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571627f3608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x557160f215d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55716282faa8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557160f4af18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5571627b80d8)]: 374 failure(s)
[14:53:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557160eca4c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571627f3608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x557160f215d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55716282faa8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557160f4af18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5571627b80d8)]: 370 failure(s)
[14:53:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9994  0.9992  0.9992  0.9991  0.9991  0.9990  0.9987  0.9987  0.9986  0.9985  0.9984  0.9982  0.9981  0.9980  0.9979
[17 : 32]:	0.9977  0.9977  0.9975  0.9973  0.9972  0.9971  0.9969  0.9968  0.9966  0.9963  0.9957  0.9957  0.9954  0.9953  0.9951  0.9951
[14:53:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[14:53:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[14:53:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[14:54:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[14:54:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_conv2d_add_2"
[14:54:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[14:54:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[14:55:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5571627b4068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x557162752c68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55716283ecf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5571628d6f38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55716271ff78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160f38608)]: 1913 failure(s)
[14:55:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 135 candidate(s)
[14:55:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5571627b4068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x557162752c68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55716283ecf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5571628d6f38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55716271ff78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160f38608)]: 466 failure(s)
[14:56:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5571627b4068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x557162752c68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55716283ecf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5571628d6f38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55716271ff78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160f38608)]: 406 failure(s)
[14:57:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5571627b4068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x557162752c68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55716283ecf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5571628d6f38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55716271ff78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160f38608)]: 435 failure(s)
[14:58:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5571627b4068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x557162752c68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55716283ecf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5571628d6f38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55716271ff78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160f38608)]: 355 failure(s)
[14:58:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9996  0.9996  0.9995  0.9990  0.9985  0.9984  0.9984  0.9982  0.9981  0.9980  0.9978  0.9974  0.9970  0.9969  0.9967
[17 : 32]:	0.9963  0.9962  0.9961  0.9960  0.9960  0.9960  0.9959  0.9951  0.9951  0.9948  0.9940  0.9939  0.9939  0.9937  0.9933  0.9930
[14:58:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[14:58:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[14:58:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[14:58:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[14:59:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_conv2d_add_3"
[14:59:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[14:59:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[14:59:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557160f144a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55716278d6b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55716277fab8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55716278c878)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55716284dbb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160f199d8)]: 1950 failure(s)
[14:59:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 98 candidate(s)
[15:00:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557160f144a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55716278d6b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55716277fab8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55716278c878)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55716284dbb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160f199d8)]: 483 failure(s)
[15:01:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557160f144a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55716278d6b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55716277fab8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55716278c878)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55716284dbb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160f199d8)]: 457 failure(s)
[15:02:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557160f144a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55716278d6b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55716277fab8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55716278c878)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55716284dbb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160f199d8)]: 391 failure(s)
[15:02:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557160f144a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55716278d6b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55716277fab8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55716278c878)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55716284dbb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160f199d8)]: 376 failure(s)
[15:02:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9994  0.9991  0.9988  0.9984  0.9984  0.9984  0.9982  0.9981  0.9980  0.9977  0.9976  0.9971  0.9967  0.9966  0.9966
[17 : 32]:	0.9966  0.9963  0.9963  0.9951  0.9951  0.9943  0.9940  0.9938  0.9933  0.9932  0.9932  0.9929  0.9928  0.9926  0.9926  0.9925
[15:02:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[15:02:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[15:02:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:03:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[15:03:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_conv2d_add_4"
[15:03:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[15:03:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[15:03:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5571627e8b38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571628733a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x557160eb65a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x557160ed9f48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55715eefda08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55716010e0e8)]: 1837 failure(s)
[15:03:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 211 candidate(s)
[15:04:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5571627e8b38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571628733a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x557160eb65a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x557160ed9f48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55715eefda08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55716010e0e8)]: 419 failure(s)
[15:04:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5571627e8b38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571628733a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x557160eb65a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x557160ed9f48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55715eefda08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55716010e0e8)]: 410 failure(s)
[15:05:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5571627e8b38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571628733a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x557160eb65a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x557160ed9f48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55715eefda08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55716010e0e8)]: 347 failure(s)
[15:05:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5571627e8b38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571628733a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x557160eb65a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x557160ed9f48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55715eefda08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55716010e0e8)]: 333 failure(s)
[15:06:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9997  0.9997  0.9997  0.9995  0.9995  0.9992  0.9991  0.9989  0.9985  0.9983  0.9983  0.9982  0.9981  0.9981
[17 : 32]:	0.9978  0.9976  0.9973  0.9972  0.9972  0.9971  0.9968  0.9965  0.9964  0.9963  0.9959  0.9958  0.9957  0.9955  0.9954  0.9952
[15:06:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[15:06:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[15:06:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:06:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[15:07:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_conv2d_add_nn_relu_1"
[15:07:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[15:07:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[15:07:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55716283c458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x557162782ec8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5571628512f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x557160eb1238)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557162853a28)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5571627a8be8)]: 1956 failure(s)
[15:07:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 92 candidate(s)
[15:08:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55716283c458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x557162782ec8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5571628512f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x557160eb1238)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557162853a28)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5571627a8be8)]: 408 failure(s)
[15:09:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55716283c458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x557162782ec8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5571628512f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x557160eb1238)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557162853a28)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5571627a8be8)]: 383 failure(s)
[15:10:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55716283c458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x557162782ec8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5571628512f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x557160eb1238)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557162853a28)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5571627a8be8)]: 330 failure(s)
[15:12:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55716283c458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x557162782ec8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5571628512f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x557160eb1238)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557162853a28)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5571627a8be8)]: 345 failure(s)
[15:12:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9998  0.9998  0.9995  0.9992  0.9990  0.9988  0.9985  0.9983  0.9983  0.9981  0.9980  0.9980  0.9980  0.9978  0.9977
[17 : 32]:	0.9976  0.9974  0.9974  0.9973  0.9972  0.9971  0.9971  0.9970  0.9970  0.9969  0.9965  0.9962  0.9961  0.9959  0.9957  0.9952
[15:12:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[15:12:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[15:12:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:13:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [15:13:12] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/callproc.c:921
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/callproc.c:1264
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:159
  25: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:125
  26: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  27: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3469
  28: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  29: function_code_fastcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:284
  30: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:411
  31: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  32: _PyObject_FastCall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:147
  33: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:1465
  34: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:6838
  35: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:310
  36: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:328
  37: subtype_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:1221
  38: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  39: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  40: frame_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/frameobject.c:430
  41: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  42: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  43: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  44: tb_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/traceback.c:168
  45: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  46: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  47: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  48: tb_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/traceback.c:167
  49: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  50: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  51: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:2119
  52: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  53: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  54: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  55: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  56: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  57: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  58: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  59: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  60: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4327
  61: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:718
  62: builtin_exec_impl.isra.15
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/bltinmodule.c:1025
  63: builtin_exec
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/clinic/bltinmodule.c.h:396
  64: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/methodobject.c:426
  65: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  66: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  67: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  68: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  69: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  70: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  71: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  72: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  73: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  74: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  75: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  76: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  77: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:200
  78: PyObject_Call
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:228
  79: pymain_run_module
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:309
  80: pymain_run_python
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:610
  81: Py_RunMain
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:695
  82: Py_BytesMain
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:1127
  83: __libc_start_main
  84: 0x00005642b3a91426
  85: 0xffffffffffffffff


[15:13:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_max_pool2d"
[15:13:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[15:13:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[15:13:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557160f3a408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571628de358)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55716275b128)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5571628ceb88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557160ea3eb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5571627bb478)]: 0 failure(s)
[15:13:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[15:13:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557160f3a408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571628de358)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55716275b128)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5571628ceb88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557160ea3eb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5571627bb478)]: 0 failure(s)
[15:14:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557160f3a408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571628de358)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55716275b128)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5571628ceb88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557160ea3eb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5571627bb478)]: 0 failure(s)
[15:14:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557160f3a408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571628de358)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55716275b128)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5571628ceb88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557160ea3eb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5571627bb478)]: 0 failure(s)
[15:14:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557160f3a408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5571628de358)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55716275b128)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5571628ceb88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557160ea3eb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5571627bb478)]: 0 failure(s)
[15:14:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.7141  0.7042  0.2601  0.0747  0.0545
[15:14:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[15:14:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[15:14:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[15:15:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[15:15:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_nn_conv2d_add_nn_relu_2"
[15:15:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[15:15:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[15:15:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557160f2cbb8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x557160eb0948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55716278eb08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x557160edaf88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557160ec8b88)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160ea0798)]: 1886 failure(s)
[15:15:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 162 candidate(s)
[15:16:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557160f2cbb8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x557160eb0948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55716278eb08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x557160edaf88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557160ec8b88)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160ea0798)]: 471 failure(s)
[15:16:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557160f2cbb8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x557160eb0948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55716278eb08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x557160edaf88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557160ec8b88)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160ea0798)]: 398 failure(s)
[15:17:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557160f2cbb8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x557160eb0948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55716278eb08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x557160edaf88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557160ec8b88)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160ea0798)]: 327 failure(s)
[15:17:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x557160f2cbb8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x557160eb0948)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55716278eb08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x557160edaf88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x557160ec8b88)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x557160ea0798)]: 419 failure(s)
[15:17:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9995  0.9995  0.9995  0.9992  0.9991  0.9988  0.9987  0.9984  0.9982  0.9978  0.9977  0.9974  0.9966  0.9963  0.9963
[17 : 32]:	0.9963  0.9962  0.9958  0.9956  0.9953  0.9952  0.9949  0.9949  0.9946  0.9944  0.9941  0.9940  0.9940  0.9939  0.9939  0.9938
[15:17:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[15:17:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[15:17:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:18:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(256, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1024, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(8, 13, 2, 2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 16 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 26 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 4 * 13 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(512, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(92):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 2916)
                                        v2 = T.axis.spatial(54, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 2916 // 54)
                                        v3 = T.axis.spatial(54, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 54)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 5832)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 53 and 1 <= v3 and v3 < 53, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 18)
                                        v1 = T.axis.spatial(1024, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 18 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 8, 13, 1, 2, 1, 3, 1, 2, 2, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 16 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 26 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 4 * 13 + i3_4)
                                rc = T.axis.reduce(1024, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 26, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 16 + ax1)
                            v2 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 26 + ax2)
                            v3 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 4 * 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 4, 8, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 13, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 4, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[512, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #1: GFLOPs: 1382.3267. Time: 9.2311 ms. Best GFLOPs: 1382.3267
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(256, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1024, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i2_3_init, i2_4_init, i3_4_init in T.grid(13, 2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 13)
                            yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(256, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 5616 // 1404)
                                        v2 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 1404 // 54)
                                        v3 = T.axis.spatial(54, ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 54)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 5616)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 53 and 1 <= v3 and v3 < 53, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(1024, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 12 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1536)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 3, 1, 1, 13, 1, 2, 1, 1, 1, 1, 2, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 13)
                                yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_4)
                                rc = T.axis.reduce(1024, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 26, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 13 + ax1)
                            v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + ax2)
                            v3 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 16, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1, 13, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[256, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 104, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 104, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #3: GFLOPs: 477.8773. Time: 26.7024 ms. Best GFLOPs: 1382.3267
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #4: GFLOPs: 12.3489. Time: 1033.3237 ms. Best GFLOPs: 1382.3267
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(256, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1024, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(8, 13, 2, 2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 16 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 26 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 4 * 13 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(512, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(46):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 2916)
                                        v2 = T.axis.spatial(54, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 2916 // 54)
                                        v3 = T.axis.spatial(54, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 54)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 5832)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 53 and 1 <= v3 and v3 < 53, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 18)
                                        v1 = T.axis.spatial(1024, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 18 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 8, 13, 1, 2, 1, 3, 1, 2, 2, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 16 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 26 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 4 * 13 + i3_4)
                                rc = T.axis.reduce(1024, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 26, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 16 + ax1)
                            v2 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 26 + ax2)
                            v3 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 4 * 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 4, 8, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 13, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 4, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[512, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #6: GFLOPs: 613.5561. Time: 20.7975 ms. Best GFLOPs: 1382.3267
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(256, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1024, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(52, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(52, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init in T.grid(2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 26 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4_init)
                            yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 26 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i2_3_init)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(52):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i4_0 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 4992 // 312)
                                        v2 = T.axis.spatial(54, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 312 // 6)
                                        v3 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 6)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 53 and 1 <= v3 and v3 < 53, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(48):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 48)
                                        v1 = T.axis.spatial(1024, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 48 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 3, 1, 1, 2, 2, 4, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 26 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4)
                                yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 26 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i2_3)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_3)
                                rc = T.axis.reduce(1024, i4_0 * 16 + i4_1 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 26 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 26 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 16, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 13, 2, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 2, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 4, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(256, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1024, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(104, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init in T.grid(2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 26 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + i1_4_init)
                            yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 26 // 13 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 2 * 2 + i2_3_init)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(256, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 5824 // 1456)
                                        v2 = T.axis.spatial(54, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 1456 // 28)
                                        v3 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 53 and 1 <= v3 and v3 < 53, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 12)
                                    v1 = T.axis.spatial(1024, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 12 // 3)
                                    v2 = T.axis.spatial(3, i5_0)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 384)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 1, 2, 1, 4, 1, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 26 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + i1_4)
                                yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 26 // 13 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 2 * 2 + i2_3)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                rc = T.axis.reduce(1024, i4_0 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 26 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + ax1)
                            v2 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 26 // 13 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 4, 2, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 13, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 13, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[256, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 52])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #9: GFLOPs: 99.7121. Time: 127.9730 ms. Best GFLOPs: 1382.3267
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #10: GFLOPs: 157.1268. Time: 81.2112 ms. Best GFLOPs: 1382.3267
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #11: GFLOPs: 598.2002. Time: 21.3314 ms. Best GFLOPs: 1382.3267
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(256, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1024, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(104, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i3_4_init in T.grid(13, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 2 * 13 + i2_3_init)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(43):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i4_0 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 2704 // 338)
                                        v2 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused // 2 * 13 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 338 // 26)
                                        v3 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 26)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 2704)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 53 and 1 <= v3 and v3 < 53, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(1024, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 1, 13, 1, 1, 1, 1, 1, 1, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 2 * 13 + i2_3)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i3_4)
                                rc = T.axis.reduce(1024, i4_0 * 8 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 13, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 2 * 13 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 13, 1, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 32])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #13: GFLOPs: 1153.8109. Time: 11.0594 ms. Best GFLOPs: 1382.3267
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #14: GFLOPs: 132.0913. Time: 96.6033 ms. Best GFLOPs: 1382.3267
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #15: GFLOPs: 345.8228. Time: 36.8988 ms. Best GFLOPs: 1382.3267
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #16: GFLOPs: 735.3983. Time: 17.3518 ms. Best GFLOPs: 1382.3267
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #17: GFLOPs: 4243.9381. Time: 3.0067 ms. Best GFLOPs: 4243.9381
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #18: GFLOPs: 797.5850. Time: 15.9989 ms. Best GFLOPs: 4243.9381
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #19: GFLOPs: 7941.8943. Time: 1.6067 ms. Best GFLOPs: 7941.8943
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #20: GFLOPs: 154.5148. Time: 82.5840 ms. Best GFLOPs: 7941.8943
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #21: GFLOPs: 3639.4889. Time: 3.5061 ms. Best GFLOPs: 7941.8943
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(256, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1024, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i2_4_init, i3_4_init in T.grid(26, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 13)
                            yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i2_4_init)
                            xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(108):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i4_0 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 11232 // 1404)
                                        v2 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 1404 // 54)
                                        v3 = T.axis.spatial(54, ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 54)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 53 and 1 <= v3 and v3 < 53, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 24)
                                    v1 = T.axis.spatial(1024, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 24 // 3)
                                    v2 = T.axis.spatial(3, i5_0)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 768)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 26, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 13)
                                yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i2_4)
                                xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_4)
                                rc = T.axis.reduce(1024, i4_0 * 8 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 26, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 13 + ax1)
                            v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + ax2)
                            v3 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 8, 4, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 26])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 52])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #23: GFLOPs: 674.7281. Time: 18.9120 ms. Best GFLOPs: 7941.8943
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #24: GFLOPs: 3762.2409. Time: 3.3917 ms. Best GFLOPs: 7941.8943
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(256, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1024, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i2_4_init in T.grid(2, 2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + i1_3_init)
                            yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 // 2 * 4 + i2_4_init)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(17):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i4_0 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 3456 // 216)
                                        v2 = T.axis.spatial(54, ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 216 // 4)
                                        v3 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3456)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 53 and 1 <= v3 and v3 < 53, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 48)
                                        v1 = T.axis.spatial(1024, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 48 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1536)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 3, 1, 1, 2, 1, 2, 8, 1, 1, 1, 1, 4, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + i1_3)
                                yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 // 2 * 4 + i2_4)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3)
                                rc = T.axis.reduce(1024, i4_0 * 16 + i4_1 * 8 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + ax1)
                            v2 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 // 2 * 4 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 8, 2, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 2, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 52, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #26: GFLOPs: 1778.4630. Time: 7.1750 ms. Best GFLOPs: 7941.8943
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #27: GFLOPs: 245.4133. Time: 51.9958 ms. Best GFLOPs: 7941.8943
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #28: GFLOPs: 37.3837. Time: 341.3373 ms. Best GFLOPs: 7941.8943
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #29: GFLOPs: 116.3111. Time: 109.7098 ms. Best GFLOPs: 7941.8943
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(256, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1024, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(26, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init in T.grid(2, 2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3_init)
                            yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_3_init)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 13 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(512, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 2808 // 1404)
                                        v2 = T.axis.spatial(54, ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 1404 // 26)
                                        v3 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 26)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 2808)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 53 and 1 <= v3 and v3 < 53, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(1024, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 6 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 384)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 3, 1, 1, 2, 2, 13, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3)
                                yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_3)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 13 + i3_3)
                                rc = T.axis.reduce(1024, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused * 2 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 32, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 26, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 2, 13, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[512, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 64, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #31: GFLOPs: 6093.6965. Time: 2.0940 ms. Best GFLOPs: 7941.8943
[15:18:28] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_nn_conv2d_add_nn_relu"
 ID |                              Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------
  0 |       fused_nn_conv2d_add_nn_relu | 12760457216 |      1 |      7941.8943 |    1606.7272 |             1606.7272 |     32 |            
  1 |               fused_nn_conv2d_add |    29130192 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |             fused_nn_conv2d_add_1 | 11346935808 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |             fused_nn_conv2d_add_2 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |             fused_nn_conv2d_add_3 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |             fused_nn_conv2d_add_4 |   357187584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |     fused_nn_conv2d_add_nn_relu_1 |   819593216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |               fused_nn_max_pool2d |     6230016 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_conv2d_add_nn_relu_2 |    89989120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |     fused_nn_conv2d_add_nn_relu_3 |   355803136 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |     fused_nn_conv2d_add_nn_relu_4 |   798826496 |      3 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_add_add_nn_relu |   362725376 |      3 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_conv2d_add_nn_relu_5 |   711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |     fused_nn_conv2d_add_nn_relu_6 |   798134272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |     fused_nn_conv2d_add_nn_relu_7 |   355110912 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_nn_conv2d_add_nn_relu_8 |   798134272 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_add_nn_relu_1 |   358572032 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |     fused_nn_conv2d_add_nn_relu_9 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |    fused_nn_conv2d_add_nn_relu_10 |  3191152640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |    fused_nn_conv2d_add_nn_relu_11 |  1419059200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 20 |    fused_nn_conv2d_add_nn_relu_12 |  3191152640 |      5 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_add_nn_relu_2 |  1425981440 |      6 |            N/A |          N/A |                   N/A |      0 |            
 22 |    fused_nn_conv2d_add_nn_relu_13 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |    fused_nn_conv2d_add_nn_relu_14 | 12761841664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |    fused_nn_conv2d_add_nn_relu_15 |  5673467904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 |    fused_nn_conv2d_add_nn_relu_16 | 12761841664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_nn_conv2d_add_add_nn_relu_3 |  5687312384 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 |    fused_nn_conv2d_add_nn_relu_17 | 51039059968 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |             fused_nn_conv2d_add_5 |    58203600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |              fused_image_resize2d |   196245504 |      2 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 32
Total latency (us): 1606.73

[15:18:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #0 has finished. Remaining task(s): 29
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #0: GFLOPs: 21.9701. Time: 1.3259 ms. Best GFLOPs: 21.9701
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #1: GFLOPs: 1890.8568. Time: 0.0154 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #2: GFLOPs: 628.0553. Time: 0.0464 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #3: GFLOPs: 19.0221. Time: 1.5314 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #4: GFLOPs: 216.0804. Time: 0.1348 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #5: GFLOPs: 179.5557. Time: 0.1622 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #6: GFLOPs: 379.6187. Time: 0.0767 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #7: GFLOPs: 4.4786. Time: 6.5043 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #8: GFLOPs: 25.2736. Time: 1.1526 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #9: GFLOPs: 462.6677. Time: 0.0630 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #10: GFLOPs: 94.5866. Time: 0.3080 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #11: GFLOPs: 299.8801. Time: 0.0971 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #12: GFLOPs: 216.4548. Time: 0.1346 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #13: GFLOPs: 260.4683. Time: 0.1118 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #14: GFLOPs: 13.6700. Time: 2.1310 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #15: GFLOPs: 34.7105. Time: 0.8392 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #16: GFLOPs: 224.8170. Time: 0.1296 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #17: GFLOPs: 245.4150. Time: 0.1187 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #18: GFLOPs: 23.8166. Time: 1.2231 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #19: GFLOPs: 21.4439. Time: 1.3584 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #20: GFLOPs: 45.6730. Time: 0.6378 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #21: GFLOPs: 21.2112. Time: 1.3733 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #22: GFLOPs: 80.5721. Time: 0.3615 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #23: GFLOPs: 137.5610. Time: 0.2118 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #24: GFLOPs: 244.6750. Time: 0.1191 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #25: GFLOPs: 201.1417. Time: 0.1448 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #26: GFLOPs: 373.1037. Time: 0.0781 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #27: GFLOPs: 299.6276. Time: 0.0972 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #28: GFLOPs: 3.5228. Time: 8.2691 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52), "float32"], placeholder_1: T.Buffer[(21, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 21, 1, 1), "float32"], T_add: T.Buffer[(1, 21, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 21, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 52, 52], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([21, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(91, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_4_init in T.grid(3, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(21, i0_1_i1_1_i2_1_i3_1_fused // 13 * 3 + i1_3_init)
                            yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + i2_3_init)
                            xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 52, 52], "float32"], ["TENSOR", [21, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(104):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 2704)
                                    v2 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 2704 // 52)
                                    v3 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 52)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(21, ax0_ax1_ax2_ax3_fused_1 // 2)
                                    v1 = T.axis.spatial(256, i4_0 * 2 + ax0_ax1_ax2_ax3_fused_1 % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 42)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 3, 2, 1, 2, 1, 1, 1, 1, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(21, i0_1_i1_1_i2_1_i3_1_fused // 13 * 3 + i1_3)
                                yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + i2_3)
                                xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + i3_4)
                                rc = T.axis.reduce(256, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 52, 52], "float32"], ["TENSOR", [21, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(21, i0_1_i1_1_i2_1_i3_1_fused // 13 * 3 + ax1)
                            v2 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + ax2)
                            v3 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 7, 1, 3, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 13, 2, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 26, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[128, 1, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 52])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117 = sch.split(loop=l115, factors=[None, 52])
sch.bind(loop=l117, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b119)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l139, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l139, ann_key="pragma_unroll_explicit", ann_val=1)
l159, l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l159, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l159, ann_key="pragma_unroll_explicit", ann_val=1)
b166 = sch.get_block(name="conv2d_nchw", func_name="main")
l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b166)
b187 = sch.decompose_reduction(block=b166, loop=l170)
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #30: GFLOPs: 497.9381. Time: 0.0585 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add"] Trial #31: GFLOPs: 12.6439. Time: 2.3039 ms. Best GFLOPs: 1890.8568
[15:18:29] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_nn_conv2d_add"
 ID |                              Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------
  0 |       fused_nn_conv2d_add_nn_relu | 12760457216 |      1 |      7941.8943 |    1606.7272 |             1606.7272 |     32 |          Y 
  1 |               fused_nn_conv2d_add |    29130192 |      1 |      1890.8568 |      15.4058 |               15.4058 |     32 |            
  2 |             fused_nn_conv2d_add_1 | 11346935808 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |             fused_nn_conv2d_add_2 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |             fused_nn_conv2d_add_3 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |             fused_nn_conv2d_add_4 |   357187584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |     fused_nn_conv2d_add_nn_relu_1 |   819593216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |               fused_nn_max_pool2d |     6230016 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_conv2d_add_nn_relu_2 |    89989120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |     fused_nn_conv2d_add_nn_relu_3 |   355803136 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |     fused_nn_conv2d_add_nn_relu_4 |   798826496 |      3 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_add_add_nn_relu |   362725376 |      3 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_conv2d_add_nn_relu_5 |   711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |     fused_nn_conv2d_add_nn_relu_6 |   798134272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |     fused_nn_conv2d_add_nn_relu_7 |   355110912 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_nn_conv2d_add_nn_relu_8 |   798134272 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_add_nn_relu_1 |   358572032 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |     fused_nn_conv2d_add_nn_relu_9 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |    fused_nn_conv2d_add_nn_relu_10 |  3191152640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |    fused_nn_conv2d_add_nn_relu_11 |  1419059200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 20 |    fused_nn_conv2d_add_nn_relu_12 |  3191152640 |      5 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_add_nn_relu_2 |  1425981440 |      6 |            N/A |          N/A |                   N/A |      0 |            
 22 |    fused_nn_conv2d_add_nn_relu_13 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |    fused_nn_conv2d_add_nn_relu_14 | 12761841664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |    fused_nn_conv2d_add_nn_relu_15 |  5673467904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 |    fused_nn_conv2d_add_nn_relu_16 | 12761841664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_nn_conv2d_add_add_nn_relu_3 |  5687312384 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 |    fused_nn_conv2d_add_nn_relu_17 | 51039059968 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |             fused_nn_conv2d_add_5 |    58203600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |              fused_image_resize2d |   196245504 |      2 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 64
Total latency (us): 1622.13

[15:18:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #1 has finished. Remaining task(s): 28
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_1"] Trial #0: GFLOPs: 728.4633. Time: 15.5765 ms. Best GFLOPs: 728.4633
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_1"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(2048, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 2048, 1, 1), "float32"], T_add: T.Buffer[(1, 2048, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 2048, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([2048, 1024, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 4, 13, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused // 4 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_4_init)
                            yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 2 * 26 + i2_3_init * 13 + i2_4_init)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 4 * 13 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(43):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 676)
                                        v2 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 676 // 13)
                                        v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 4 * 13 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 5408)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused // 4 * 256 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(1024, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 4, 13, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused // 4 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_4)
                                yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 2 * 26 + i2_3 * 13 + i2_4)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 4 * 13 + i3_4)
                                rc = T.axis.reduce(1024, i4_0 * 8 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 26, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused // 4 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + ax1)
                            v2 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 2 * 26 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 4 * 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 2, 32, 1, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 2, 2, 13])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 13])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[128, 4, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 64, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 64, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_1"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(2048, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 2048, 1, 1), "float32"], T_add: T.Buffer[(1, 2048, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 2048, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([2048, 1024, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i2_4_init in T.grid(13, 8, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused * 512 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + i1_4_init)
                            yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 4 * 13 + i2_4_init)
                            xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 4 * 13 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(1024, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(43):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i4_0)
                                    v2 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 52)
                                    v3 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 52)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 < 2704)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused * 512 + ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(1024, i4_0)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 13, 1, 1, 1, 1, 8, 13, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused * 512 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + i1_4)
                                yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 4 * 13 + i2_4)
                                xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 4 * 13 + i3_3)
                                rc = T.axis.reduce(1024, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 13, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused * 512 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + ax1)
                            v2 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 4 * 13 + ax2)
                            v3 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 4 * 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 4, 16, 1, 8])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 1, 13])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 1, 13, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1024, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 64])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117 = sch.split(loop=l115, factors=[None, 64])
sch.bind(loop=l117, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b119)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l139, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l139, ann_key="pragma_unroll_explicit", ann_val=1)
l159, l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l159, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l159, ann_key="pragma_unroll_explicit", ann_val=1)
b166 = sch.get_block(name="conv2d_nchw", func_name="main")
l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b166)
b187 = sch.decompose_reduction(block=b166, loop=l170)
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_1"] Trial #3: GFLOPs: 91.5127. Time: 123.9930 ms. Best GFLOPs: 728.4633
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_1"] Trial #4: GFLOPs: 3414.0489. Time: 3.3236 ms. Best GFLOPs: 3414.0489
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_1"] Trial #5: Error in building: LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 153, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 379, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/yj/tvm/python/tvm/runtime/module.py", line 297, in evaluator
    blob = feval(*args)
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  7: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  5: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  4: operator()
        at /home/yj/tvm/src/runtime/rpc/rpc_module.cc:375
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1221
...
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/library_module.cc:80
  4: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  3: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::detail::PackFuncVoidAddr_<4, tvm::runtime::CUDAWrappedFunc>(tvm::runtime::CUDAWrappedFunc, std::vector<tvm::runtime::detail::ArgConvertCode, std::allocator<tvm::runtime::detail::ArgConvertCode> > const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  1: tvm::runtime::detail::PackFuncVoidAddr_<4, tvm::runtime::CUDAWrappedFunc>(tvm::runtime::CUDAWrappedFunc, std::vector<tvm::runtime::detail::ArgConvertCode, std::allocator<tvm::runtime::detail::ArgConvertCode> > const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/src/runtime/cuda/../pack_args.h:183
  0: tvm::runtime::CUDAWrappedFunc::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*, void**) const
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:190
  File "/home/yj/tvm/src/runtime/cuda/cuda_module.cc", line 190
  File "/home/yj/tvm/src/runtime/library_module.cc", line 80
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: ret == 0 (-1 vs. 0) : TVMError: CUDALaunch Error: CUDA_ERROR_OUT_OF_MEMORY
 grid=(1,1,1),  block=(64,1,1)

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(2048, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 2048, 1, 1), "float32"], T_add: T.Buffer[(1, 2048, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 2048, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([2048, 1024, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(4, 32, 13, 52):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(2048, i0_2_i1_2_i2_2_i3_2_fused * 32 + i1_4_init)
                            yy = T.axis.spatial(52, i2_3_init * 13 + i2_4_init)
                            xx = T.axis.spatial(52, i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(512, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(22):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 2704)
                                        v2 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 2704 // 52)
                                        v3 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 52)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 5408)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(2048, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(1024, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 4, 1, 2, 1, 1, 1, 32, 13, 52):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(2048, i0_2_i1_2_i2_2_i3_2_fused * 32 + i1_4)
                                yy = T.axis.spatial(52, i2_3 * 13 + i2_4)
                                xx = T.axis.spatial(52, i3_4)
                                rc = T.axis.reduce(1024, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 52, 52):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(2048, i0_2_i1_2_i2_2_i3_2_fused * 32 + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 64, 1, 32])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 4, 13])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 52])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[512, 1, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 64, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 64, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_1"] Trial #6: GFLOPs: 201.3943. Time: 56.3419 ms. Best GFLOPs: 3414.0489
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_1"] Trial #7: GFLOPs: 1100.1574. Time: 10.3139 ms. Best GFLOPs: 3414.0489
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_1"] Trial #8: GFLOPs: 75.5157. Time: 150.2594 ms. Best GFLOPs: 3414.0489
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_1"] Trial #9: GFLOPs: 65.1825. Time: 174.0795 ms. Best GFLOPs: 3414.0489
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_1"] Trial #10: GFLOPs: 17.4385. Time: 650.6833 ms. Best GFLOPs: 3414.0489
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_1"] Trial #11: GFLOPs: 25.0645. Time: 452.7092 ms. Best GFLOPs: 3414.0489
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_1"] Trial #12: GFLOPs: 63.9880. Time: 177.3291 ms. Best GFLOPs: 3414.0489
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_1"] Trial #13: GFLOPs: 90.3664. Time: 125.5659 ms. Best GFLOPs: 3414.0489
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_1"] Trial #14: GFLOPs: 4442.9361. Time: 2.5539 ms. Best GFLOPs: 4442.9361
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_1"] Trial #15: Error in building: LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 153, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 379, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/yj/tvm/python/tvm/runtime/module.py", line 297, in evaluator
    blob = feval(*args)
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  7: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  5: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  4: operator()
        at /home/yj/tvm/src/runtime/rpc/rpc_module.cc:375
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1221
...
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/library_module.cc:80
  4: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  3: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::detail::PackFuncVoidAddr_<4, tvm::runtime::CUDAWrappedFunc>(tvm::runtime::CUDAWrappedFunc, std::vector<tvm::runtime::detail::ArgConvertCode, std::allocator<tvm::runtime::detail::ArgConvertCode> > const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  1: tvm::runtime::detail::PackFuncVoidAddr_<4, tvm::runtime::CUDAWrappedFunc>(tvm::runtime::CUDAWrappedFunc, std::vector<tvm::runtime::detail::ArgConvertCode, std::allocator<tvm::runtime::detail::ArgConvertCode> > const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/src/runtime/cuda/../pack_args.h:183
  0: tvm::runtime::CUDAWrappedFunc::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*, void**) const
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:190
  File "/home/yj/tvm/src/runtime/cuda/cuda_module.cc", line 190
  File "/home/yj/tvm/src/runtime/library_module.cc", line 80
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: ret == 0 (-1 vs. 0) : TVMError: CUDALaunch Error: CUDA_ERROR_OUT_OF_MEMORY
 grid=(1,1,1),  block=(52,1,1)

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(2048, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 2048, 1, 1), "float32"], T_add: T.Buffer[(1, 2048, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 2048, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([2048, 1024, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3328, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init in T.grid(8, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(2048, i0_1_i1_1_i2_1_i3_1_fused // 13 * 8 + i1_3_init)
                            yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + i2_3_init)
                            xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(512, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(52):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 2704)
                                        v2 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 2704 // 52)
                                        v3 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 52)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(40):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(2048, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(1024, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 4096)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 8, 4, 1, 2, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(2048, i0_1_i1_1_i2_1_i3_1_fused // 13 * 8 + i1_3)
                                yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + i2_3)
                                xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused)
                                rc = T.axis.reduce(1024, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 4, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(2048, i0_1_i1_1_i2_1_i3_1_fused // 13 * 8 + ax1)
                            v2 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + ax2)
                            v3 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 256, 1, 8, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 13, 1, 4, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 52, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[512, 1, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 52, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 52, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_1"] Trial #16: GFLOPs: 66.2268. Time: 171.3345 ms. Best GFLOPs: 4442.9361
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_1"] Trial #17: GFLOPs: 74.0214. Time: 153.2926 ms. Best GFLOPs: 4442.9361
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_1"] Trial #18: GFLOPs: 1960.0106. Time: 5.7892 ms. Best GFLOPs: 4442.9361
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_1"] Trial #19: Error in building: LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 153, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 379, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/yj/tvm/python/tvm/runtime/module.py", line 297, in evaluator
    blob = feval(*args)
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  7: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  5: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  4: operator()
        at /home/yj/tvm/src/runtime/rpc/rpc_module.cc:375
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1221
...
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/library_module.cc:80
  4: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  3: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::detail::PackFuncVoidAddr_<4, tvm::runtime::CUDAWrappedFunc>(tvm::runtime::CUDAWrappedFunc, std::vector<tvm::runtime::detail::ArgConvertCode, std::allocator<tvm::runtime::detail::ArgConvertCode> > const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  1: tvm::runtime::detail::PackFuncVoidAddr_<4, tvm::runtime::CUDAWrappedFunc>(tvm::runtime::CUDAWrappedFunc, std::vector<tvm::runtime::detail::ArgConvertCode, std::allocator<tvm::runtime::detail::ArgConvertCode> > const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/src/runtime/cuda/../pack_args.h:183
  0: tvm::runtime::CUDAWrappedFunc::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*, void**) const
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:190
  File "/home/yj/tvm/src/runtime/cuda/cuda_module.cc", line 190
  File "/home/yj/tvm/src/runtime/library_module.cc", line 80
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: ret == 0 (-1 vs. 0) : TVMError: CUDALaunch Error: CUDA_ERROR_OUT_OF_MEMORY
 grid=(1,1,1),  block=(52,1,1)

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(2048, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 2048, 1, 1), "float32"], T_add: T.Buffer[(1, 2048, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 2048, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([2048, 1024, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1664, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i2_4_init, i3_4_init in T.grid(4, 2, 4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(2048, i0_1_i1_1_i2_1_i3_1_fused // 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3_init)
                            yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i2_4_init)
                            xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(1024, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(26):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i4_0)
                                        v2 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 52)
                                        v3 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 52)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(2048, ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(1024, i4_0)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 2048)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 1, 2, 1, 1, 1, 1, 1, 4, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(2048, i0_1_i1_1_i2_1_i3_1_fused // 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3)
                                yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i2_4)
                                xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(1024, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(2048, i0_1_i1_1_i2_1_i3_1_fused // 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + ax1)
                            v2 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + ax2)
                            v3 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 128, 4, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 4])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 13, 1, 2, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1024, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 52, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 52, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_1"] Trial #20: GFLOPs: 20.5103. Time: 553.2306 ms. Best GFLOPs: 4442.9361
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_1"] Trial #21: GFLOPs: 176.6355. Time: 64.2393 ms. Best GFLOPs: 4442.9361
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_1"] Trial #22: Error in building: LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 153, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 379, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/yj/tvm/python/tvm/runtime/module.py", line 297, in evaluator
    blob = feval(*args)
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  7: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  5: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  4: operator()
        at /home/yj/tvm/src/runtime/rpc/rpc_module.cc:375
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1221
...
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/library_module.cc:80
  4: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  3: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::detail::PackFuncVoidAddr_<4, tvm::runtime::CUDAWrappedFunc>(tvm::runtime::CUDAWrappedFunc, std::vector<tvm::runtime::detail::ArgConvertCode, std::allocator<tvm::runtime::detail::ArgConvertCode> > const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  1: tvm::runtime::detail::PackFuncVoidAddr_<4, tvm::runtime::CUDAWrappedFunc>(tvm::runtime::CUDAWrappedFunc, std::vector<tvm::runtime::detail::ArgConvertCode, std::allocator<tvm::runtime::detail::ArgConvertCode> > const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/src/runtime/cuda/../pack_args.h:183
  0: tvm::runtime::CUDAWrappedFunc::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*, void**) const
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:190
  File "/home/yj/tvm/src/runtime/cuda/cuda_module.cc", line 190
  File "/home/yj/tvm/src/runtime/library_module.cc", line 80
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: ret == 0 (-1 vs. 0) : TVMError: CUDALaunch Error: CUDA_ERROR_OUT_OF_MEMORY
 grid=(4,1,1),  block=(52,1,1)

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(2048, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 2048, 1, 1), "float32"], T_add: T.Buffer[(1, 2048, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 2048, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([2048, 1024, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(8, 2, 16, 26, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused * 512 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 128 + i1_3_init * 16 + i1_4_init)
                            yy = T.axis.spatial(52, i2_3_init * 26 + i2_4_init)
                            xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(1024, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(26):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i4_0)
                                        v2 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 52)
                                        v3 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 52)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused * 512 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1))
                                    v1 = T.axis.spatial(1024, i4_0)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 512)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 8, 2, 1, 1, 1, 1, 1, 16, 26, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused * 512 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 128 + i1_3 * 16 + i1_4)
                                yy = T.axis.spatial(52, i2_3 * 26 + i2_4)
                                xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_4)
                                rc = T.axis.reduce(1024, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 128, 52, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused * 512 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 128 + ax1)
                            v2 = T.axis.spatial(52, ax2)
                            v3 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 1, 4, 8, 16])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 26])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 4])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1024, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 52, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118 = sch.split(loop=l116, factors=[None, 52])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_1"] Trial #23: GFLOPs: 92.7707. Time: 122.3116 ms. Best GFLOPs: 4442.9361
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_1"] Trial #24: GFLOPs: 82.4528. Time: 137.6174 ms. Best GFLOPs: 4442.9361
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_1"] Trial #25: GFLOPs: 182.2197. Time: 62.2706 ms. Best GFLOPs: 4442.9361
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_1"] Trial #26: Error in building: LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 153, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 379, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/yj/tvm/python/tvm/runtime/module.py", line 297, in evaluator
    blob = feval(*args)
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  7: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  5: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  4: operator()
        at /home/yj/tvm/src/runtime/rpc/rpc_module.cc:375
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1221
...
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/library_module.cc:80
  4: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  3: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::detail::PackFuncVoidAddr_<4, tvm::runtime::CUDAWrappedFunc>(tvm::runtime::CUDAWrappedFunc, std::vector<tvm::runtime::detail::ArgConvertCode, std::allocator<tvm::runtime::detail::ArgConvertCode> > const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  1: tvm::runtime::detail::PackFuncVoidAddr_<4, tvm::runtime::CUDAWrappedFunc>(tvm::runtime::CUDAWrappedFunc, std::vector<tvm::runtime::detail::ArgConvertCode, std::allocator<tvm::runtime::detail::ArgConvertCode> > const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/src/runtime/cuda/../pack_args.h:183
  0: tvm::runtime::CUDAWrappedFunc::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*, void**) const
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:190
  File "/home/yj/tvm/src/runtime/cuda/cuda_module.cc", line 190
  File "/home/yj/tvm/src/runtime/library_module.cc", line 80
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: ret == 0 (-1 vs. 0) : TVMError: CUDALaunch Error: CUDA_ERROR_OUT_OF_MEMORY
 grid=(1,1,1),  block=(52,1,1)

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(2048, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 2048, 1, 1), "float32"], T_add: T.Buffer[(1, 2048, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 2048, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([2048, 1024, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3328, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init in T.grid(8, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(2048, i0_1_i1_1_i2_1_i3_1_fused // 13 * 8 + i1_3_init)
                            yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + i2_3_init)
                            xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(512, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(104):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 2704)
                                    v2 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 2704 // 52)
                                    v3 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 52)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(79):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(2048, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 2)
                                    v1 = T.axis.spatial(1024, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 4096)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 8, 4, 1, 2, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(2048, i0_1_i1_1_i2_1_i3_1_fused // 13 * 8 + i1_3)
                                yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + i2_3)
                                xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused)
                                rc = T.axis.reduce(1024, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 4, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(2048, i0_1_i1_1_i2_1_i3_1_fused // 13 * 8 + ax1)
                            v2 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + ax2)
                            v3 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 256, 1, 8, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 13, 1, 4, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 52, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[512, 1, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 52])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117 = sch.split(loop=l115, factors=[None, 52])
sch.bind(loop=l117, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b119)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l139, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l139, ann_key="pragma_unroll_explicit", ann_val=1)
l159, l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l159, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l159, ann_key="pragma_unroll_explicit", ann_val=1)
b166 = sch.get_block(name="conv2d_nchw", func_name="main")
l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b166)
b187 = sch.decompose_reduction(block=b166, loop=l170)
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_1"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(2048, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 2048, 1, 1), "float32"], T_add: T.Buffer[(1, 2048, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 2048, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([2048, 1024, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(52, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(128, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_4_init, i3_4_init in T.grid(2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused // 26 * 1024 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + i2_4_init)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 13 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(256, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 104)
                                    v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 104 // 26)
                                    v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 26)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused // 26 * 1024 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(1024, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused // 26 * 1024 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + i2_4)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 13 + i3_4)
                                rc = T.axis.reduce(1024, i4_0 * 4 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused // 26 * 1024 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 32, 32, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[13, 2, 1, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 1, 1, 13])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[256, 4, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 32])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 32, 2])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_1"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(2048, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 2048, 1, 1), "float32"], T_add: T.Buffer[(1, 2048, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 2048, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([2048, 1024, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(208, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(13, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(2, 2, 8):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused // 26 * 256 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 16 + i1_3_init * 8 + i1_4_init)
                            yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + i2_3_init)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(52):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 104)
                                    v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 104 // 26)
                                    v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 26)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused // 26 * 256 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 32)
                                        v1 = T.axis.spatial(1024, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 32)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 2, 1, 32, 1, 1, 1, 8, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused // 26 * 256 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 16 + i1_3 * 8 + i1_4)
                                yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + i2_3)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                rc = T.axis.reduce(1024, i4_0 * 32 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused // 26 * 256 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 16 + ax1)
                            v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 1, 16, 2, 8])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[13, 1, 2, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 13, 2, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[32, 1, 32])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 64])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 64, 2])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_1"] Trial #29: GFLOPs: 757.1880. Time: 14.9856 ms. Best GFLOPs: 4442.9361
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_1"] Trial #30: GFLOPs: 766.8379. Time: 14.7970 ms. Best GFLOPs: 4442.9361
[15:18:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_1"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(2048, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 2048, 1, 1), "float32"], T_add: T.Buffer[(1, 2048, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 2048, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([2048, 1024, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(208, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(2, 2, 13, 4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused * 13 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(1024, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(208, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i4_0)
                                        v2 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 832 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 52)
                                        v3 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 832 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 52)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 2704)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(208, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused * 128 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2))
                                        v1 = T.axis.spatial(1024, i4_0)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 128)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 2, 13, 1, 1, 1, 1, 4, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused * 13 + i3_3)
                                rc = T.axis.reduce(1024, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 4, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + ax1)
                            v2 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + ax2)
                            v3 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused * 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[16, 1, 16, 2, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 13, 2, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 1, 13, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1024, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 208, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 208, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[15:18:31] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_conv2d_add_1"
 ID |                              Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------
  0 |       fused_nn_conv2d_add_nn_relu | 12760457216 |      1 |      7941.8943 |    1606.7272 |             1606.7272 |     32 |          Y 
  1 |               fused_nn_conv2d_add |    29130192 |      1 |      1890.8568 |      15.4058 |               15.4058 |     32 |          Y 
  2 |             fused_nn_conv2d_add_1 | 11346935808 |      1 |      4442.9361 |    2553.9273 |             2553.9273 |     32 |            
  3 |             fused_nn_conv2d_add_2 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |             fused_nn_conv2d_add_3 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |             fused_nn_conv2d_add_4 |   357187584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |     fused_nn_conv2d_add_nn_relu_1 |   819593216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |               fused_nn_max_pool2d |     6230016 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_conv2d_add_nn_relu_2 |    89989120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |     fused_nn_conv2d_add_nn_relu_3 |   355803136 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |     fused_nn_conv2d_add_nn_relu_4 |   798826496 |      3 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_add_add_nn_relu |   362725376 |      3 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_conv2d_add_nn_relu_5 |   711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |     fused_nn_conv2d_add_nn_relu_6 |   798134272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |     fused_nn_conv2d_add_nn_relu_7 |   355110912 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_nn_conv2d_add_nn_relu_8 |   798134272 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_add_nn_relu_1 |   358572032 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |     fused_nn_conv2d_add_nn_relu_9 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |    fused_nn_conv2d_add_nn_relu_10 |  3191152640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |    fused_nn_conv2d_add_nn_relu_11 |  1419059200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 20 |    fused_nn_conv2d_add_nn_relu_12 |  3191152640 |      5 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_add_nn_relu_2 |  1425981440 |      6 |            N/A |          N/A |                   N/A |      0 |            
 22 |    fused_nn_conv2d_add_nn_relu_13 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |    fused_nn_conv2d_add_nn_relu_14 | 12761841664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |    fused_nn_conv2d_add_nn_relu_15 |  5673467904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 |    fused_nn_conv2d_add_nn_relu_16 | 12761841664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_nn_conv2d_add_add_nn_relu_3 |  5687312384 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 |    fused_nn_conv2d_add_nn_relu_17 | 51039059968 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |             fused_nn_conv2d_add_5 |    58203600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |              fused_image_resize2d |   196245504 |      2 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 96
Total latency (us): 4176.06

[15:18:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #2 has finished. Remaining task(s): 27
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #0: GFLOPs: 2401.3634. Time: 1.1819 ms. Best GFLOPs: 2401.3634
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #1: GFLOPs: 3666.2123. Time: 0.7741 ms. Best GFLOPs: 3666.2123
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #2: GFLOPs: 126.7131. Time: 22.3980 ms. Best GFLOPs: 3666.2123
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #3: GFLOPs: 342.5634. Time: 8.2849 ms. Best GFLOPs: 3666.2123
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #4: GFLOPs: 4285.9760. Time: 0.6622 ms. Best GFLOPs: 4285.9760
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_2"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(1024, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_add: T.Buffer[(1, 1024, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init in T.grid(2, 2, 32):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 26 * 256 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 64 + i1_3_init * 32 + i1_4_init)
                            yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 13)
                            xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 104)
                                        v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 104 // 52)
                                        v3 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 52)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(40):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 26 * 256 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 2048)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 32, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 26 * 256 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 64 + i1_3 * 32 + i1_4)
                                yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 13)
                                xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + i3_3)
                                rc = T.axis.reduce(512, i4_0 * 8 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 64, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 26 * 256 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 64 + ax1)
                            v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 13 + ax2)
                            v3 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 2, 2, 2, 32])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[26, 1, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 13, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[64, 8, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 52, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118 = sch.split(loop=l116, factors=[None, 52])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #6: GFLOPs: 4493.8294. Time: 0.6316 ms. Best GFLOPs: 4493.8294
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_2"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(1024, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_add: T.Buffer[(1, 1024, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(26, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(512, thread="threadIdx.x"):
                    for i2_3_init, i2_4_init, i3_4_init in T.grid(2, 26, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 512 + i0_1_i1_1_i2_1_i3_1_fused * 256 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                            yy = T.axis.spatial(52, i2_3_init * 26 + i2_4_init)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(512, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1) // 208)
                                    v2 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1) % 208 // 4)
                                    v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 < 832)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(512, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 512 + (ax0_ax1_ax2_ax3_fused_0 * 1024 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(512, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 1024 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 1, 4, 1, 1, 1, 1, 26, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 512 + i0_1_i1_1_i2_1_i3_1_fused * 256 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                yy = T.axis.spatial(52, i2_3 * 26 + i2_4)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_4)
                                rc = T.axis.reduce(512, i4_0 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 52, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 512 + i0_1_i1_1_i2_1_i3_1_fused * 256 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                            v2 = T.axis.spatial(52, ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 2, 256, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 26])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 2, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[128, 1, 4])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 512])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 512, 2])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_2"] Trial #8: Error in building: LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 153, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 379, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/yj/tvm/python/tvm/runtime/module.py", line 297, in evaluator
    blob = feval(*args)
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  7: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  5: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  4: operator()
        at /home/yj/tvm/src/runtime/rpc/rpc_module.cc:375
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1221
...
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/library_module.cc:80
  4: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  3: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::detail::PackFuncVoidAddr_<4, tvm::runtime::CUDAWrappedFunc>(tvm::runtime::CUDAWrappedFunc, std::vector<tvm::runtime::detail::ArgConvertCode, std::allocator<tvm::runtime::detail::ArgConvertCode> > const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  1: tvm::runtime::detail::PackFuncVoidAddr_<4, tvm::runtime::CUDAWrappedFunc>(tvm::runtime::CUDAWrappedFunc, std::vector<tvm::runtime::detail::ArgConvertCode, std::allocator<tvm::runtime::detail::ArgConvertCode> > const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/src/runtime/cuda/../pack_args.h:183
  0: tvm::runtime::CUDAWrappedFunc::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*, void**) const
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:190
  File "/home/yj/tvm/src/runtime/cuda/cuda_module.cc", line 190
  File "/home/yj/tvm/src/runtime/library_module.cc", line 80
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: ret == 0 (-1 vs. 0) : TVMError: CUDALaunch Error: CUDA_ERROR_OUT_OF_MEMORY
 grid=(2,1,1),  block=(32,1,1)

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(1024, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_add: T.Buffer[(1, 1024, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(13, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(16, 26, 2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_2_i1_2_i2_2_i3_2_fused * 32 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(52, i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 26 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(512, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(22):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i4_0)
                                        v2 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 26)
                                        v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 26 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 26)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1352)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(512, i4_0)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 16, 26, 1, 1, 1, 1, 1, 2, 2, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_2_i1_2_i2_2_i3_2_fused * 32 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(52, i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 26 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_4)
                                rc = T.axis.reduce(512, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 52, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_2_i1_2_i2_2_i3_2_fused * 32 + ax1)
                            v2 = T.axis.spatial(52, ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 26 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 32, 16, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 26, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 13, 1, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #9: GFLOPs: 72.6497. Time: 39.0658 ms. Best GFLOPs: 4493.8294
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #10: GFLOPs: 79.0953. Time: 35.8823 ms. Best GFLOPs: 4493.8294
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #11: GFLOPs: 78.8381. Time: 35.9993 ms. Best GFLOPs: 4493.8294
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #12: GFLOPs: 408.3444. Time: 6.9503 ms. Best GFLOPs: 4493.8294
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #13: GFLOPs: 91.5634. Time: 30.9962 ms. Best GFLOPs: 4493.8294
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #14: GFLOPs: 213.3535. Time: 13.3024 ms. Best GFLOPs: 4493.8294
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_2"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(1024, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_add: T.Buffer[(1, 1024, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(26, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(832, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init, i3_4_init in T.grid(8, 4, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i2_3_init)
                            xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(512, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(832, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0)
                                    v2 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 832 + ax0_ax1_ax2_ax3_fused_1) // 52)
                                    v3 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 832 + ax0_ax1_ax2_ax3_fused_1) % 52)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 832 + ax0_ax1_ax2_ax3_fused_1 < 2704)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(832, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, ax0_ax1_ax2_ax3_fused_0 * 832 + ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(512, i4_0)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 832 + ax0_ax1_ax2_ax3_fused_1 < 1024)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 8, 4, 1, 1, 1, 1, 1, 2, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i2_3)
                                xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_4)
                                rc = T.axis.reduce(512, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + ax1)
                            v2 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + ax2)
                            v3 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 64, 8, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 13, 4, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 26, 1, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 832])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117 = sch.split(loop=l115, factors=[None, 832])
sch.bind(loop=l117, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b119)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l139, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l139, ann_key="pragma_unroll_explicit", ann_val=1)
l159, l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l159, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l159, ann_key="pragma_unroll_explicit", ann_val=1)
b166 = sch.get_block(name="conv2d_nchw", func_name="main")
l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b166)
b187 = sch.decompose_reduction(block=b166, loop=l170)
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #16: GFLOPs: 75.9172. Time: 37.3844 ms. Best GFLOPs: 4493.8294
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_2"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(1024, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_add: T.Buffer[(1, 1024, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(52, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(4, 16, 2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_2_i1_2_i2_2_i3_2_fused // 2 * 64 + i1_3_init * 16 + i1_4_init)
                            yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i2_4_init)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 4 * 13 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 52)
                                    v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 52 // 13)
                                    v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 4 * 13 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 1, 1, 4, 1, 1, 1, 16, 2, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_2_i1_2_i2_2_i3_2_fused // 2 * 64 + i1_3 * 16 + i1_4)
                                yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i2_4)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 4 * 13 + i3_4)
                                rc = T.axis.reduce(512, i4_0 * 8 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 64, 2, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_2_i1_2_i2_2_i3_2_fused // 2 * 64 + ax1)
                            v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 4 * 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 16, 4, 16])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[13, 1, 2, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 13])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 32])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 32, 4])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #18: GFLOPs: 896.7347. Time: 3.1649 ms. Best GFLOPs: 4493.8294
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #19: GFLOPs: 291.9651. Time: 9.7207 ms. Best GFLOPs: 4493.8294
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #20: GFLOPs: 164.3364. Time: 17.2702 ms. Best GFLOPs: 4493.8294
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #21: GFLOPs: 1177.0286. Time: 2.4113 ms. Best GFLOPs: 4493.8294
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_2"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(1024, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_add: T.Buffer[(1, 1024, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(13, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init in T.grid(4, 2, 16):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused // 4 * 256 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 64 + i1_3_init * 16 + i1_4_init)
                            yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 26)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) // 208)
                                    v2 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 208 // 4)
                                    v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 4 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(20):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 8192)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 4, 1, 2, 2, 1, 1, 1, 16, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused // 4 * 256 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 64 + i1_3 * 16 + i1_4)
                                yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 26)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_3)
                                rc = T.axis.reduce(512, i4_0 * 8 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 64, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused // 4 * 256 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 64 + ax1)
                            v2 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 26 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 4, 4, 4, 16])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 26, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 2, 1, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[64, 4, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 104])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 104, 4])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #23: GFLOPs: 3643.9470. Time: 0.7789 ms. Best GFLOPs: 4493.8294
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #24: GFLOPs: 2838.5622. Time: 0.9998 ms. Best GFLOPs: 4493.8294
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #25: GFLOPs: 35.0318. Time: 81.0155 ms. Best GFLOPs: 4493.8294
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #26: GFLOPs: 3084.8976. Time: 0.9200 ms. Best GFLOPs: 4493.8294
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_2"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(1024, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_add: T.Buffer[(1, 1024, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(52, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(8, 2, 8, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i1_3_init * 8 + i1_4_init)
                            yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + i2_4_init)
                            xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 208)
                                    v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 208 // 52)
                                    v3 = T.axis.spatial(52, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 52)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(40):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 256 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 2048)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 8, 1, 2, 1, 1, 1, 1, 8, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i1_3 * 8 + i1_4)
                                yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + i2_4)
                                xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + i3_3)
                                rc = T.axis.reduce(512, i4_0 * 8 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 64, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 64 + ax1)
                            v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + ax2)
                            v3 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 4, 1, 8, 8])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[13, 1, 2, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 26, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[64, 8, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 52])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117 = sch.split(loop=l115, factors=[None, 52])
sch.bind(loop=l117, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b119)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l139, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l139, ann_key="pragma_unroll_explicit", ann_val=1)
l159, l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l159, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l159, ann_key="pragma_unroll_explicit", ann_val=1)
b166 = sch.get_block(name="conv2d_nchw", func_name="main")
l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b166)
b187 = sch.decompose_reduction(block=b166, loop=l170)
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #28: GFLOPs: 438.8520. Time: 6.4671 ms. Best GFLOPs: 4493.8294
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #29: GFLOPs: 182.4601. Time: 15.5547 ms. Best GFLOPs: 4493.8294
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #30: GFLOPs: 306.9533. Time: 9.2461 ms. Best GFLOPs: 4493.8294
[15:18:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_2"] Trial #31: GFLOPs: 1470.2460. Time: 1.9304 ms. Best GFLOPs: 4493.8294
[15:18:33] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_conv2d_add_2"
 ID |                              Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------
  0 |       fused_nn_conv2d_add_nn_relu | 12760457216 |      1 |      7941.8943 |    1606.7272 |             1606.7272 |     32 |          Y 
  1 |               fused_nn_conv2d_add |    29130192 |      1 |      1890.8568 |      15.4058 |               15.4058 |     32 |          Y 
  2 |             fused_nn_conv2d_add_1 | 11346935808 |      1 |      4442.9361 |    2553.9273 |             2553.9273 |     32 |          Y 
  3 |             fused_nn_conv2d_add_2 |  2838118400 |      1 |      4493.8294 |     631.5590 |              631.5590 |     32 |            
  4 |             fused_nn_conv2d_add_3 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |             fused_nn_conv2d_add_4 |   357187584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |     fused_nn_conv2d_add_nn_relu_1 |   819593216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |               fused_nn_max_pool2d |     6230016 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_conv2d_add_nn_relu_2 |    89989120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |     fused_nn_conv2d_add_nn_relu_3 |   355803136 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |     fused_nn_conv2d_add_nn_relu_4 |   798826496 |      3 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_add_add_nn_relu |   362725376 |      3 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_conv2d_add_nn_relu_5 |   711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |     fused_nn_conv2d_add_nn_relu_6 |   798134272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |     fused_nn_conv2d_add_nn_relu_7 |   355110912 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_nn_conv2d_add_nn_relu_8 |   798134272 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_add_nn_relu_1 |   358572032 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |     fused_nn_conv2d_add_nn_relu_9 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |    fused_nn_conv2d_add_nn_relu_10 |  3191152640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |    fused_nn_conv2d_add_nn_relu_11 |  1419059200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 20 |    fused_nn_conv2d_add_nn_relu_12 |  3191152640 |      5 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_add_nn_relu_2 |  1425981440 |      6 |            N/A |          N/A |                   N/A |      0 |            
 22 |    fused_nn_conv2d_add_nn_relu_13 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |    fused_nn_conv2d_add_nn_relu_14 | 12761841664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |    fused_nn_conv2d_add_nn_relu_15 |  5673467904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 |    fused_nn_conv2d_add_nn_relu_16 | 12761841664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_nn_conv2d_add_add_nn_relu_3 |  5687312384 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 |    fused_nn_conv2d_add_nn_relu_17 | 51039059968 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |             fused_nn_conv2d_add_5 |    58203600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |              fused_image_resize2d |   196245504 |      2 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 128
Total latency (us): 4807.62

[15:18:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #3 has finished. Remaining task(s): 26
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #0: GFLOPs: 148.1334. Time: 4.7945 ms. Best GFLOPs: 148.1334
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #1: GFLOPs: 89.9318. Time: 7.8973 ms. Best GFLOPs: 148.1334
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_3"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 104, 104), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_add: T.Buffer[(1, 512, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 104, 104], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(13, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(8, 4, 2, 26):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused // 2 * 32 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_4_init)
                            xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 2 * 26 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 104, 104], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(91):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 2884 // 721)
                                    v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 721 // 103)
                                    v3 = T.axis.spatial(104, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 103)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 2884)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 4)
                                    v1 = T.axis.spatial(256, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1, 4, 2, 26):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused // 2 * 32 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_4)
                                xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 2 * 26 + i3_4)
                                rc = T.axis.reduce(256, i4_0 * 4 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 104, 104], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 2, 26):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused // 2 * 32 + ax1)
                            v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax2)
                            v3 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 2 * 26 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 16, 8, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[13, 2, 1, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 26])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[64, 4, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 32])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117 = sch.split(loop=l115, factors=[None, 32])
sch.bind(loop=l117, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b119)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l139, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l139, ann_key="pragma_unroll_explicit", ann_val=1)
l159, l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l159, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l159, ann_key="pragma_unroll_explicit", ann_val=1)
b166 = sch.get_block(name="conv2d_nchw", func_name="main")
l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b166)
b187 = sch.decompose_reduction(block=b166, loop=l170)
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #3: GFLOPs: 72.5405. Time: 9.7907 ms. Best GFLOPs: 148.1334
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #4: GFLOPs: 109.4654. Time: 6.4881 ms. Best GFLOPs: 148.1334
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #5: GFLOPs: 140.6217. Time: 5.0506 ms. Best GFLOPs: 148.1334
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #6: GFLOPs: 3675.9960. Time: 0.1932 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #7: GFLOPs: 2395.7268. Time: 0.2965 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #8: GFLOPs: 232.8327. Time: 3.0504 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #9: GFLOPs: 106.2426. Time: 6.6849 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #10: GFLOPs: 17.5493. Time: 40.4700 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_3"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 104, 104), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_add: T.Buffer[(1, 512, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 104, 104], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(26, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i3_4_init in T.grid(8, 8, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 13 * 256 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 64 + i1_3_init * 8 + i1_4_init)
                            yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 26)
                            xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 104, 104], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 5768 // 721)
                                        v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 721 // 103)
                                        v3 = T.axis.spatial(104, ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 103)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 5768)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 13 * 256 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(256, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 2048)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 8, 1, 1, 2, 1, 1, 1, 8, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 13 * 256 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 64 + i1_3 * 8 + i1_4)
                                yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 26)
                                xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + i3_4)
                                rc = T.axis.reduce(256, i4_0 * 8 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 104, 104], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 64, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 13 * 256 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 64 + ax1)
                            v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 26 + ax2)
                            v3 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 4, 1, 8, 8])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[13, 2, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 26, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[32, 4, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 52, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 52, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #12: GFLOPs: 699.9667. Time: 1.0147 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #13: GFLOPs: 20.2154. Time: 35.1327 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #14: GFLOPs: 1310.3205. Time: 0.5420 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #15: GFLOPs: 352.3349. Time: 2.0158 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #16: GFLOPs: 744.7285. Time: 0.9537 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #17: GFLOPs: 2136.4172. Time: 0.3324 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #18: GFLOPs: 284.1468. Time: 2.4995 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #19: GFLOPs: 184.4581. Time: 3.8503 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #20: GFLOPs: 3323.0567. Time: 0.2137 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #21: GFLOPs: 255.6713. Time: 2.7779 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #22: GFLOPs: 3098.7533. Time: 0.2292 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #23: GFLOPs: 2549.1437. Time: 0.2786 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #24: GFLOPs: 845.6841. Time: 0.8398 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #25: GFLOPs: 1246.1029. Time: 0.5700 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #26: GFLOPs: 41.9924. Time: 16.9131 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #27: GFLOPs: 1992.2028. Time: 0.3565 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #28: GFLOPs: 254.7062. Time: 2.7884 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #29: GFLOPs: 3388.8294. Time: 0.2096 ms. Best GFLOPs: 3675.9960
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_3"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 104, 104), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_add: T.Buffer[(1, 512, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 104, 104], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(52, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init in T.grid(4, 16):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 16 + i1_4_init)
                            yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + i2_3_init)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 26)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 104, 104], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(256, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(102):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 + 0)
                                    v2 = T.axis.spatial(104, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 5253 // 51)
                                    v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 51)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 5253)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2))
                                        v1 = T.axis.spatial(256, i4_0)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 128)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 16, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 16 + i1_4)
                                yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + i2_3)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 26)
                                rc = T.axis.reduce(256, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 104, 104], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 4, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 16 + ax1)
                            v2 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 26 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 4, 2, 1, 16])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 13, 1, 4, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 26, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[256, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 52])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 52, 4])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[15:18:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_3"] Trial #31: GFLOPs: 154.9569. Time: 4.5834 ms. Best GFLOPs: 3675.9960
[15:18:35] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_nn_conv2d_add_3"
 ID |                              Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------
  0 |       fused_nn_conv2d_add_nn_relu | 12760457216 |      1 |      7941.8943 |    1606.7272 |             1606.7272 |     32 |          Y 
  1 |               fused_nn_conv2d_add |    29130192 |      1 |      1890.8568 |      15.4058 |               15.4058 |     32 |          Y 
  2 |             fused_nn_conv2d_add_1 | 11346935808 |      1 |      4442.9361 |    2553.9273 |             2553.9273 |     32 |          Y 
  3 |             fused_nn_conv2d_add_2 |  2838118400 |      1 |      4493.8294 |     631.5590 |              631.5590 |     32 |          Y 
  4 |             fused_nn_conv2d_add_3 |   710221824 |      1 |      3675.9960 |     193.2053 |              193.2053 |     32 |            
  5 |             fused_nn_conv2d_add_4 |   357187584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |     fused_nn_conv2d_add_nn_relu_1 |   819593216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |               fused_nn_max_pool2d |     6230016 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_conv2d_add_nn_relu_2 |    89989120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |     fused_nn_conv2d_add_nn_relu_3 |   355803136 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |     fused_nn_conv2d_add_nn_relu_4 |   798826496 |      3 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_add_add_nn_relu |   362725376 |      3 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_conv2d_add_nn_relu_5 |   711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |     fused_nn_conv2d_add_nn_relu_6 |   798134272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |     fused_nn_conv2d_add_nn_relu_7 |   355110912 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_nn_conv2d_add_nn_relu_8 |   798134272 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_add_nn_relu_1 |   358572032 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |     fused_nn_conv2d_add_nn_relu_9 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |    fused_nn_conv2d_add_nn_relu_10 |  3191152640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |    fused_nn_conv2d_add_nn_relu_11 |  1419059200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 20 |    fused_nn_conv2d_add_nn_relu_12 |  3191152640 |      5 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_add_nn_relu_2 |  1425981440 |      6 |            N/A |          N/A |                   N/A |      0 |            
 22 |    fused_nn_conv2d_add_nn_relu_13 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |    fused_nn_conv2d_add_nn_relu_14 | 12761841664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |    fused_nn_conv2d_add_nn_relu_15 |  5673467904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 |    fused_nn_conv2d_add_nn_relu_16 | 12761841664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_nn_conv2d_add_add_nn_relu_3 |  5687312384 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 |    fused_nn_conv2d_add_nn_relu_17 | 51039059968 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |             fused_nn_conv2d_add_5 |    58203600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |              fused_image_resize2d |   196245504 |      2 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 160
Total latency (us): 5000.82

[15:18:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #4 has finished. Remaining task(s): 25
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #0: GFLOPs: 175.3589. Time: 2.0369 ms. Best GFLOPs: 175.3589
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #1: GFLOPs: 286.7884. Time: 1.2455 ms. Best GFLOPs: 286.7884
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #2: GFLOPs: 189.2796. Time: 1.8871 ms. Best GFLOPs: 286.7884
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #3: GFLOPs: 1240.1536. Time: 0.2880 ms. Best GFLOPs: 1240.1536
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #4: GFLOPs: 92.7622. Time: 3.8506 ms. Best GFLOPs: 1240.1536
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #5: GFLOPs: 1556.8573. Time: 0.2294 ms. Best GFLOPs: 1556.8573
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #6: GFLOPs: 1935.5482. Time: 0.1845 ms. Best GFLOPs: 1935.5482
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_4"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], placeholder_1: T.Buffer[(256, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i2_4_init in T.grid(2, 13, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 8 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + i1_3_init)
                            yy = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 8 // 2 * 26 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 4 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(52):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 2704)
                                        v2 = T.axis.spatial(104, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 2704 // 26)
                                        v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 4 * 26 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 26)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + ax0_ax1_ax2_ax3_fused_1 // 2)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + ax0_ax1_ax2_ax3_fused_1 % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 32)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 13, 1, 1, 1, 1, 1, 1, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 8 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + i1_3)
                                yy = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 8 // 2 * 26 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 4 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 26, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 8 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + ax1)
                            v2 = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 8 // 2 * 26 + ax2)
                            v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 4 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[16, 2, 4, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 1, 13, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 2, 13, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[32, 2, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 52, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118 = sch.split(loop=l116, factors=[None, 52])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_4"] Trial #8: Error in building: LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 153, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 379, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/yj/tvm/python/tvm/runtime/module.py", line 297, in evaluator
    blob = feval(*args)
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  7: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  5: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  4: operator()
        at /home/yj/tvm/src/runtime/rpc/rpc_module.cc:375
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1221
...
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/library_module.cc:80
  4: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  3: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::detail::PackFuncVoidAddr_<4, tvm::runtime::CUDAWrappedFunc>(tvm::runtime::CUDAWrappedFunc, std::vector<tvm::runtime::detail::ArgConvertCode, std::allocator<tvm::runtime::detail::ArgConvertCode> > const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  1: tvm::runtime::detail::PackFuncVoidAddr_<4, tvm::runtime::CUDAWrappedFunc>(tvm::runtime::CUDAWrappedFunc, std::vector<tvm::runtime::detail::ArgConvertCode, std::allocator<tvm::runtime::detail::ArgConvertCode> > const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/src/runtime/cuda/../pack_args.h:183
  0: tvm::runtime::CUDAWrappedFunc::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*, void**) const
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:190
  File "/home/yj/tvm/src/runtime/cuda/cuda_module.cc", line 190
  File "/home/yj/tvm/src/runtime/library_module.cc", line 80
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: ret == 0 (-1 vs. 0) : TVMError: CUDALaunch Error: CUDA_ERROR_OUT_OF_MEMORY
 grid=(4,1,1),  block=(32,1,1)

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], placeholder_1: T.Buffer[(256, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(208, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init in T.grid(2, 26, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused // 4)
                            xx = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 4 * 26 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(85):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 5408)
                                        v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 5408 // 104)
                                        v3 = T.axis.spatial(104, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 104)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 10816)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 2)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 1, 26, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused // 4)
                                xx = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 4 * 26 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 26):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused // 4 + ax2)
                            v3 = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 4 * 26 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 1, 32, 2, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 52, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 1, 26, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[32, 2, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b120)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_4"] Trial #9: Error in building: LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 153, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 379, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/yj/tvm/python/tvm/runtime/module.py", line 297, in evaluator
    blob = feval(*args)
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  7: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  5: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  4: operator()
        at /home/yj/tvm/src/runtime/rpc/rpc_module.cc:375
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1221
...
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/library_module.cc:80
  4: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  3: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::detail::PackFuncVoidAddr_<4, tvm::runtime::CUDAWrappedFunc>(tvm::runtime::CUDAWrappedFunc, std::vector<tvm::runtime::detail::ArgConvertCode, std::allocator<tvm::runtime::detail::ArgConvertCode> > const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  1: tvm::runtime::detail::PackFuncVoidAddr_<4, tvm::runtime::CUDAWrappedFunc>(tvm::runtime::CUDAWrappedFunc, std::vector<tvm::runtime::detail::ArgConvertCode, std::allocator<tvm::runtime::detail::ArgConvertCode> > const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/src/runtime/cuda/../pack_args.h:183
  0: tvm::runtime::CUDAWrappedFunc::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*, void**) const
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:190
  File "/home/yj/tvm/src/runtime/cuda/cuda_module.cc", line 190
  File "/home/yj/tvm/src/runtime/library_module.cc", line 80
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: ret == 0 (-1 vs. 0) : TVMError: CUDALaunch Error: CUDA_ERROR_OUT_OF_MEMORY
 grid=(4,1,1),  block=(32,1,1)

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], placeholder_1: T.Buffer[(256, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(4, 13, 2, 26):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 4 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 4 * 26 + i2_4_init)
                            xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused * 26 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 13 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(43):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0)
                                        v2 = T.axis.spatial(104, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 26)
                                        v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused * 26 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 26)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 2704)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(64, i4_0)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 1, 13, 1, 1, 1, 1, 2, 26, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 4 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 4 * 26 + i2_4)
                                xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused * 26 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 13 + i3_3)
                                rc = T.axis.reduce(64, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 26, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 4 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + ax1)
                            v2 = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 4 * 26 + ax2)
                            v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused * 26 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 16, 4, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 1, 1, 26])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 2, 13, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[64, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #10: GFLOPs: 739.5846. Time: 0.4830 ms. Best GFLOPs: 1935.5482
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #11: GFLOPs: 74.8433. Time: 4.7725 ms. Best GFLOPs: 1935.5482
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #12: GFLOPs: 176.9309. Time: 2.0188 ms. Best GFLOPs: 1935.5482
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #13: GFLOPs: 10.0123. Time: 35.6749 ms. Best GFLOPs: 1935.5482
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #14: GFLOPs: 3691.2334. Time: 0.0968 ms. Best GFLOPs: 3691.2334
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #15: GFLOPs: 160.2522. Time: 2.2289 ms. Best GFLOPs: 3691.2334
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #16: GFLOPs: 36.1712. Time: 9.8749 ms. Best GFLOPs: 3691.2334
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_4"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], placeholder_1: T.Buffer[(256, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(8, 2, 2, 26):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 26 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 16 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(104, i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 26 + i2_4_init)
                            xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 26 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(26):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 416)
                                        v2 = T.axis.spatial(104, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 416 // 4)
                                        v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 26 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 26 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 8, 1, 2, 1, 1, 1, 1, 2, 26, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 26 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 16 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(104, i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 26 + i2_4)
                                xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 26 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 8 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 26, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 26 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 16 + ax1)
                            v2 = T.axis.spatial(104, i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 26 + ax2)
                            v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 26 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 1, 4, 8, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 1, 26])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[26, 1, 2, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[8, 8, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #18: GFLOPs: 45.1295. Time: 7.9147 ms. Best GFLOPs: 3691.2334
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #19: GFLOPs: 44.8715. Time: 7.9602 ms. Best GFLOPs: 3691.2334
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_4"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], placeholder_1: T.Buffer[(256, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(52, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i2_4_init, i3_4_init in T.grid(2, 2, 2, 13, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_3_init)
                            yy = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 2 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 26 + i2_3_init * 13 + i2_4_init)
                            xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0)
                                        v2 = T.axis.spatial(104, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 64 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2))
                                        v1 = T.axis.spatial(64, i4_0)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 64)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 13, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_3)
                                yy = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 2 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 26 + i2_3 * 13 + i2_4)
                                xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(64, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 26, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + ax1)
                            v2 = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 2 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 26 + ax2)
                            v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 4, 8, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 2, 2, 13])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 2, 2, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[64, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #21: GFLOPs: 716.4091. Time: 0.4986 ms. Best GFLOPs: 3691.2334
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #22: GFLOPs: 108.5421. Time: 3.2908 ms. Best GFLOPs: 3691.2334
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #23: GFLOPs: 633.3829. Time: 0.5639 ms. Best GFLOPs: 3691.2334
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #24: GFLOPs: 534.5961. Time: 0.6681 ms. Best GFLOPs: 3691.2334
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #25: GFLOPs: 34.8565. Time: 10.2474 ms. Best GFLOPs: 3691.2334
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #26: GFLOPs: 109.1677. Time: 3.2719 ms. Best GFLOPs: 3691.2334
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #27: GFLOPs: 88.7340. Time: 4.0254 ms. Best GFLOPs: 3691.2334
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #28: GFLOPs: 161.3049. Time: 2.2144 ms. Best GFLOPs: 3691.2334
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #29: GFLOPs: 294.1383. Time: 1.2144 ms. Best GFLOPs: 3691.2334
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #30: GFLOPs: 5278.6328. Time: 0.0677 ms. Best GFLOPs: 5278.6328
[15:18:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #31: GFLOPs: 93.1095. Time: 3.8362 ms. Best GFLOPs: 5278.6328
[15:18:38] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_conv2d_add_4"
 ID |                              Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------
  0 |       fused_nn_conv2d_add_nn_relu | 12760457216 |      1 |      7941.8943 |    1606.7272 |             1606.7272 |     32 |          Y 
  1 |               fused_nn_conv2d_add |    29130192 |      1 |      1890.8568 |      15.4058 |               15.4058 |     32 |          Y 
  2 |             fused_nn_conv2d_add_1 | 11346935808 |      1 |      4442.9361 |    2553.9273 |             2553.9273 |     32 |          Y 
  3 |             fused_nn_conv2d_add_2 |  2838118400 |      1 |      4493.8294 |     631.5590 |              631.5590 |     32 |          Y 
  4 |             fused_nn_conv2d_add_3 |   710221824 |      1 |      3675.9960 |     193.2053 |              193.2053 |     32 |          Y 
  5 |             fused_nn_conv2d_add_4 |   357187584 |      1 |      5278.6328 |      67.6667 |               67.6667 |     32 |            
  6 |     fused_nn_conv2d_add_nn_relu_1 |   819593216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |               fused_nn_max_pool2d |     6230016 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_conv2d_add_nn_relu_2 |    89989120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |     fused_nn_conv2d_add_nn_relu_3 |   355803136 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |     fused_nn_conv2d_add_nn_relu_4 |   798826496 |      3 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_add_add_nn_relu |   362725376 |      3 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_conv2d_add_nn_relu_5 |   711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |     fused_nn_conv2d_add_nn_relu_6 |   798134272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |     fused_nn_conv2d_add_nn_relu_7 |   355110912 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_nn_conv2d_add_nn_relu_8 |   798134272 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_add_nn_relu_1 |   358572032 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |     fused_nn_conv2d_add_nn_relu_9 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |    fused_nn_conv2d_add_nn_relu_10 |  3191152640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |    fused_nn_conv2d_add_nn_relu_11 |  1419059200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 20 |    fused_nn_conv2d_add_nn_relu_12 |  3191152640 |      5 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_add_nn_relu_2 |  1425981440 |      6 |            N/A |          N/A |                   N/A |      0 |            
 22 |    fused_nn_conv2d_add_nn_relu_13 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |    fused_nn_conv2d_add_nn_relu_14 | 12761841664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |    fused_nn_conv2d_add_nn_relu_15 |  5673467904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 |    fused_nn_conv2d_add_nn_relu_16 | 12761841664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_nn_conv2d_add_add_nn_relu_3 |  5687312384 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 |    fused_nn_conv2d_add_nn_relu_17 | 51039059968 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |             fused_nn_conv2d_add_5 |    58203600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |              fused_image_resize2d |   196245504 |      2 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 192
Total latency (us): 5068.49

[15:18:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #5 has finished. Remaining task(s): 24
[15:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #0: GFLOPs: 87.0260. Time: 9.4178 ms. Best GFLOPs: 87.0260
[15:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #1: GFLOPs: 992.7981. Time: 0.8255 ms. Best GFLOPs: 992.7981
[15:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #2: GFLOPs: 101.3968. Time: 8.0830 ms. Best GFLOPs: 992.7981
[15:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #3: GFLOPs: 138.7538. Time: 5.9068 ms. Best GFLOPs: 992.7981
[15:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #4: GFLOPs: 499.6668. Time: 1.6403 ms. Best GFLOPs: 992.7981
[15:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #5: GFLOPs: 609.3430. Time: 1.3450 ms. Best GFLOPs: 992.7981
[15:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #6: GFLOPs: 1314.9836. Time: 0.6233 ms. Best GFLOPs: 1314.9836
[15:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #7: GFLOPs: 6151.0772. Time: 0.1332 ms. Best GFLOPs: 6151.0772
[15:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #8: GFLOPs: 484.0854. Time: 1.6931 ms. Best GFLOPs: 6151.0772
[15:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 416, 416), "float32"], placeholder_1: T.Buffer[(64, 3, 7, 7), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 422, 422], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 3, 7, 7], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(52, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init in T.grid(2, 32):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 26 * 32 + i1_4_init)
                            yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 // 13 * 104 + i0_1_i1_1_i2_1_i3_1_fused // 8 * 52 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i2_3_init)
                            xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 13 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 8 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 7):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(127):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused % 26 // 13 * 208 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 6603 // 31)
                                    v3 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused % 13 * 32 + i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 31)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 6603)
                                    T.reads(placeholder[v0, v1, v2 - 3, v3 - 3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(3 <= v2 and v2 < 419 and 3 <= v3 and v3 < 419, placeholder[v0, v1, v2 - 3, v3 - 3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 26 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 7)
                                        v1 = T.axis.spatial(3, i4_0)
                                        v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        v3 = T.axis.spatial(7, i6_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 224)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 1, 1, 7, 1, 1, 32, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 26 * 32 + i1_4)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 // 13 * 104 + i0_1_i1_1_i2_1_i3_1_fused // 8 * 52 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i2_3)
                                xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 13 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 8 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 26 * 32 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 // 13 * 104 + i0_1_i1_1_i2_1_i3_1_fused // 8 * 52 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax2)
                            v3 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 13 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 8 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 32])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 26, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 8, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 7])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[7, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 52])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 52, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 416, 416), "float32"], placeholder_1: T.Buffer[(64, 3, 7, 7), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 422, 422], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 3, 7, 7], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(52, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init, i3_4_init in T.grid(13, 2, 4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_4_init)
                            yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 26 * 104 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 13 + i2_3_init)
                            xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(47):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, i4_0 + 0)
                                        v2 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused // 26 * 208 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 4473 // 21)
                                        v3 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused % 26 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 21)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 4473)
                                        T.reads(placeholder[v0, v1, v2 - 3, v3 - 3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(3 <= v2 and v2 < 419 and 3 <= v3 and v3 < 419, placeholder[v0, v1, v2 - 3, v3 - 3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(25):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 49)
                                        v1 = T.axis.spatial(3, i4_0)
                                        v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 49 // 7)
                                        v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3136)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 7, 1, 1, 13, 2, 1, 7, 1, 1, 4, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_4)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 26 * 104 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 13 + i2_3)
                                xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3 * 2 + i3_4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 13, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 26 * 104 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 13 + ax2)
                            v3 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 4, 2, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[26, 1, 2, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 7])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 7, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #11: GFLOPs: 4359.4427. Time: 0.1880 ms. Best GFLOPs: 6151.0772
[15:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 416, 416), "float32"], placeholder_1: T.Buffer[(64, 3, 7, 7), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 422, 422], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 3, 7, 7], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(104, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i3_3_init, i1_4_init in T.grid(2, 4):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 52 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_4_init)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 13 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 52 // 2)
                                xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 13 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_3_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i5_0, i6_0 in T.grid(7, 7):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(50):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("pad_temp_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(3, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 4743 // 1581)
                                            v2 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused // 13 * 52 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1581 // 31)
                                            v3 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused % 13 * 32 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 31)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 4743)
                                            T.reads(placeholder[v0, v1, v2 - 3, v3 - 3])
                                            T.writes(pad_temp_shared[v0, v1, v2, v3])
                                            pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(3 <= v2 and v2 < 419 and 3 <= v3 and v3 < 419, placeholder[v0, v1, v2 - 3, v3 - 3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 192)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 4, 1, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 52 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_4)
                                    yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 13 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 52 // 2)
                                    xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 13 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_3)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_1, i5_0, i6_0])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 52 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 13 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 52 // 2 + ax2)
                            v3 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 13 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 26, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 2, 4, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[7, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[7, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l176)
[15:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #13: GFLOPs: 4254.1519. Time: 0.1927 ms. Best GFLOPs: 6151.0772
[15:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #14: GFLOPs: 3752.9028. Time: 0.2184 ms. Best GFLOPs: 6151.0772
[15:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #15: GFLOPs: 4144.9253. Time: 0.1977 ms. Best GFLOPs: 6151.0772
[15:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #16: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 416, 416), "float32"], placeholder_1: T.Buffer[(64, 3, 7, 7), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 422, 422], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 3, 7, 7], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i3_4_init in T.grid(16, 2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 16 * 32 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 2)
                            xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 4 * 52 + i0_1_i1_1_i2_1_i3_1_fused * 26 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 13 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(72):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 2289 // 109)
                                    v3 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused % 4 * 104 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 109)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 2289)
                                    T.reads(placeholder[v0, v1, v2 - 3, v3 - 3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(3 <= v2 and v2 < 419 and 3 <= v3 and v3 < 419, placeholder[v0, v1, v2 - 3, v3 - 3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(49):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 49)
                                        v1 = T.axis.spatial(3, i4_0)
                                        v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 49 // 7)
                                        v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 7, 1, 16, 1, 1, 1, 7, 1, 1, 2, 1, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 16 * 32 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 2)
                                xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 4 * 52 + i0_1_i1_1_i2_1_i3_1_fused * 26 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 13 + i3_4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 16 * 32 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 2 + ax2)
                            v3 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 4 * 52 + i0_1_i1_1_i2_1_i3_1_fused * 26 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 2, 16, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[26, 1, 8, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 2, 2, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 7])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 7, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 416, 416), "float32"], placeholder_1: T.Buffer[(64, 3, 7, 7), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 422, 422], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 3, 7, 7], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(26, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init, i3_4_init in T.grid(13, 2, 4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 8 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_4_init)
                            yy = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused % 8 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 13 + i2_3_init)
                            xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(93):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, i4_0 + 0)
                                        v2 = T.axis.spatial(422, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 8841 // 21)
                                        v3 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 21)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 8841)
                                        T.reads(placeholder[v0, v1, v2 - 3, v3 - 3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(3 <= v2 and v2 < 419 and 3 <= v3 and v3 < 419, placeholder[v0, v1, v2 - 3, v3 - 3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(98):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 49)
                                    v1 = T.axis.spatial(3, i4_0)
                                    v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 49 // 7)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 7, 1, 1, 13, 2, 1, 7, 1, 1, 4, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 8 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_4)
                                yy = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused % 8 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 13 + i2_3)
                                xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3 * 2 + i3_4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 13, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 8 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + ax1)
                            v2 = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused % 8 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 13 + ax2)
                            v3 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 8, 2, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[26, 1, 2, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 7])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 7, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 32])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:18:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 416, 416), "float32"], placeholder_1: T.Buffer[(64, 3, 7, 7), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 422, 422], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 3, 7, 7], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(208, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init in T.grid(2, 8):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 104 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i1_4_init)
                            yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 104 // 26 * 52 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused // 4)
                            xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, i4_0 + 0)
                                        v2 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused % 104 // 26 * 104 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 2289 // 21)
                                        v3 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused % 26 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 21)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 2289)
                                        T.reads(placeholder[v0, v1, v2 - 3, v3 - 3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(3 <= v2 and v2 < 419 and 3 <= v3 and v3 < 419, placeholder[v0, v1, v2 - 3, v3 - 3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 104 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 49)
                                        v1 = T.axis.spatial(3, i4_0)
                                        v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 49 // 7)
                                        v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1568)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 7, 7, 1, 1, 1, 2, 1, 1, 1, 1, 8, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 104 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i1_4)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 104 // 26 * 52 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused // 4)
                                xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_3)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 104 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 104 // 26 * 52 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused // 4 + ax2)
                            v3 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 1, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 2, 26, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[26, 1, 4, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 7, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 7, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 104, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 104, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:18:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 416, 416), "float32"], placeholder_1: T.Buffer[(64, 3, 7, 7), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 422, 422], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 3, 7, 7], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(676, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(73):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 2331 // 777)
                                    v2 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused % 338 // 13 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 777 // 37)
                                    v3 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused % 13 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 37)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 2331)
                                    T.reads(placeholder[v0, v1, v2 - 3, v3 - 3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(3 <= v2 and v2 < 419 and 3 <= v3 and v3 < 419, placeholder[v0, v1, v2 - 3, v3 - 3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(49):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 338 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 147)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 147 // 49)
                                        v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 49 // 7)
                                        v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i3_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 2, 4, 2):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 338 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_4_init)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 338 // 13 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i2_4_init)
                                xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + i3_3_init * 2 + i3_4_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 7, 1, 1, 1, 2, 1, 7, 1, 1, 2, 4, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 338 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_4)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 338 // 13 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i2_4)
                                xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + i3_3 * 2 + i3_4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_1, i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 4, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 338 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 338 // 13 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + ax2)
                            v3 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[26, 2, 1, 1, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 4, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 7])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 7, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l176)
[15:18:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #20: GFLOPs: 470.5700. Time: 1.7417 ms. Best GFLOPs: 6151.0772
[15:18:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 416, 416), "float32"], placeholder_1: T.Buffer[(64, 3, 7, 7), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 422, 422], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 3, 7, 7], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(26, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(64, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_3_init in T.serial(52):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                            yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2)
                            xx = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused % 2 * 104 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 52 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(277):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 8841 // 421)
                                    v3 = T.axis.spatial(422, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 421)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 8841)
                                    T.reads(placeholder[v0, v1, v2 - 3, v3 - 3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(3 <= v2 and v2 < 419 and 3 <= v3 and v3 < 419, placeholder[v0, v1, v2 - 3, v3 - 3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(25):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 49)
                                        v1 = T.axis.spatial(3, i4_0)
                                        v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 49 // 7)
                                        v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3136)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 7, 7, 1, 1, 1, 52, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2)
                                xx = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused % 2 * 104 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 52 + i3_3)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 52):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 8 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 + ax2)
                            v3 = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused % 2 * 104 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 52 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 16, 4, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[26, 2, 4, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 2, 52, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 7, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 7, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:18:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #22: GFLOPs: 123.7222. Time: 6.6245 ms. Best GFLOPs: 6151.0772
[15:18:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 416, 416), "float32"], placeholder_1: T.Buffer[(64, 3, 7, 7), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 422, 422], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 3, 7, 7], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(104, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i2_3_init, i3_4_init in T.grid(2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 26 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 26 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 16)
                            yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 26 // 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4 * 2 + i2_3_init)
                            xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 2 * 104 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 7, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(104):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 32 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 6603 // 213)
                                    v3 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused % 2 * 208 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 213)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 < 6603)
                                    T.reads(placeholder[v0, v1, v2 - 3, v3 - 3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(3 <= v2 and v2 < 419 and 3 <= v3 and v3 < 419, placeholder[v0, v1, v2 - 3, v3 - 3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 26 * 16 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 7)
                                        v1, v2 = T.axis.remap("SS", [i4_0, i5_0])
                                        v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 112)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 1, 1, 1, 7, 1, 1, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 26 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 26 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 16)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 26 // 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4 * 2 + i2_3)
                                xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 2 * 104 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 26 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 26 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 16 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 26 // 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4 * 2 + ax2)
                            v3 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 2 * 104 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 4, 4, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 2, 4, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 13, 4, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[7, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 7])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 64])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 64, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:18:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #24: GFLOPs: 33.4254. Time: 24.5201 ms. Best GFLOPs: 6151.0772
[15:18:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #25: GFLOPs: 1990.4589. Time: 0.4118 ms. Best GFLOPs: 6151.0772
[15:18:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 416, 416), "float32"], placeholder_1: T.Buffer[(64, 3, 7, 7), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 422, 422], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 3, 7, 7], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i2_3_init, i1_4_init, i3_4_init in T.grid(26, 2, 8):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 13 * 26 + i2_3_init)
                                xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 13 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i3_4_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i5_0, i6_0 in T.grid(7, 1):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(59):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("pad_temp_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(3, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 5661 // 1887)
                                            v2 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused // 13 * 52 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1887 // 37)
                                            v3 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused % 13 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 37)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 5661)
                                            T.reads(placeholder[v0, v1, v2 - 3, v3 - 3])
                                            T.writes(pad_temp_shared[v0, v1, v2, v3])
                                            pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(3 <= v2 and v2 < 419 and 3 <= v3 and v3 < 419, placeholder[v0, v1, v2 - 3, v3 - 3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(42):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 21)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 21 // 7)
                                        v2 = T.axis.spatial(7, i5_0)
                                        v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 26, 1, 1, 1, 7, 1, 2, 1, 8):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                    yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 13 * 26 + i2_3)
                                    xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 13 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i3_4)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_1, i5_0, i6_2])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 26, 8):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 13 * 26 + ax2)
                            v3 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 13 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 1, 1, 26, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 2, 1, 1, 8])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[7, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 7])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 32])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l174)
[15:18:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 416, 416), "float32"], placeholder_1: T.Buffer[(64, 3, 7, 7), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 422, 422], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 3, 7, 7], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(208, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i3_4_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 26 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 104 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                            yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                            xx = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused % 104 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(47):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, i4_0 + 0)
                                        v2 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused % 26 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 8841 // 421)
                                        v3 = T.axis.spatial(422, ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 421)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 8841)
                                        T.reads(placeholder[v0, v1, v2 - 3, v3 - 3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(3 <= v2 and v2 < 419 and 3 <= v3 and v3 < 419, placeholder[v0, v1, v2 - 3, v3 - 3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 26 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 49)
                                    v1 = T.axis.spatial(3, i4_0)
                                    v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 49 // 7)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 < 784)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 7, 7, 1, 1, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 26 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 104 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                xx = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused % 104 * 2 + i3_4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 26 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 104 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 8 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax2)
                            v3 = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused % 104 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[26, 1, 8, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 104, 1, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 7])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 7])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 64])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:18:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #28: GFLOPs: 3802.8333. Time: 0.2155 ms. Best GFLOPs: 6151.0772
[15:18:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 416, 416), "float32"], placeholder_1: T.Buffer[(64, 3, 7, 7), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 422, 422], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 3, 7, 7], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(26, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(416, thread="threadIdx.x"):
                    for i1_3_init, i2_4_init, i3_4_init in T.grid(4, 8, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + i1_3_init)
                            yy = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused // 2 * 104 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 2 * 8 + i2_4_init)
                            xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(22):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(416, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(422, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1) % 8841 // 21)
                                    v3 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused * 16 + (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1) % 21)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 < 8841)
                                    T.reads(placeholder[v0, v1, v2 - 3, v3 - 3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(3 <= v2 and v2 < 419 and 3 <= v3 and v3 < 419, placeholder[v0, v1, v2 - 3, v3 - 3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(416, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1) // 49)
                                    v1 = T.axis.spatial(3, i4_0)
                                    v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1) % 49 // 7)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 < 3136)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 1, 1, 1, 7, 7, 1, 1, 8, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + i1_3)
                                yy = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused // 2 * 104 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 2 * 8 + i2_4)
                                xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 8, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + ax1)
                            v2 = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused // 2 * 104 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 2 * 8 + ax2)
                            v3 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 16, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 13, 1, 8])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[26, 2, 2, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 7])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 7])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 416])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 416])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[15:18:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #30: GFLOPs: 1521.9808. Time: 0.5385 ms. Best GFLOPs: 6151.0772
[15:18:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_1"] Trial #31: GFLOPs: 148.8865. Time: 5.5048 ms. Best GFLOPs: 6151.0772
[15:18:40] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_nn_conv2d_add_nn_relu_1"
 ID |                              Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------
  0 |       fused_nn_conv2d_add_nn_relu | 12760457216 |      1 |      7941.8943 |    1606.7272 |             1606.7272 |     32 |          Y 
  1 |               fused_nn_conv2d_add |    29130192 |      1 |      1890.8568 |      15.4058 |               15.4058 |     32 |          Y 
  2 |             fused_nn_conv2d_add_1 | 11346935808 |      1 |      4442.9361 |    2553.9273 |             2553.9273 |     32 |          Y 
  3 |             fused_nn_conv2d_add_2 |  2838118400 |      1 |      4493.8294 |     631.5590 |              631.5590 |     32 |          Y 
  4 |             fused_nn_conv2d_add_3 |   710221824 |      1 |      3675.9960 |     193.2053 |              193.2053 |     32 |          Y 
  5 |             fused_nn_conv2d_add_4 |   357187584 |      1 |      5278.6328 |      67.6667 |               67.6667 |     32 |          Y 
  6 |     fused_nn_conv2d_add_nn_relu_1 |   819593216 |      1 |      6151.0772 |     133.2439 |              133.2439 |     32 |            
  7 |               fused_nn_max_pool2d |     6230016 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_conv2d_add_nn_relu_2 |    89989120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |     fused_nn_conv2d_add_nn_relu_3 |   355803136 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |     fused_nn_conv2d_add_nn_relu_4 |   798826496 |      3 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_add_add_nn_relu |   362725376 |      3 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_conv2d_add_nn_relu_5 |   711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |     fused_nn_conv2d_add_nn_relu_6 |   798134272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |     fused_nn_conv2d_add_nn_relu_7 |   355110912 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_nn_conv2d_add_nn_relu_8 |   798134272 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_add_nn_relu_1 |   358572032 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |     fused_nn_conv2d_add_nn_relu_9 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |    fused_nn_conv2d_add_nn_relu_10 |  3191152640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |    fused_nn_conv2d_add_nn_relu_11 |  1419059200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 20 |    fused_nn_conv2d_add_nn_relu_12 |  3191152640 |      5 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_add_nn_relu_2 |  1425981440 |      6 |            N/A |          N/A |                   N/A |      0 |            
 22 |    fused_nn_conv2d_add_nn_relu_13 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |    fused_nn_conv2d_add_nn_relu_14 | 12761841664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |    fused_nn_conv2d_add_nn_relu_15 |  5673467904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 |    fused_nn_conv2d_add_nn_relu_16 | 12761841664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_nn_conv2d_add_add_nn_relu_3 |  5687312384 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 |    fused_nn_conv2d_add_nn_relu_17 | 51039059968 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |             fused_nn_conv2d_add_5 |    58203600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |              fused_image_resize2d |   196245504 |      2 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 224
Total latency (us): 5201.74

[15:18:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #6 has finished. Remaining task(s): 23
[15:18:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_max_pool2d"] Trial #0: GFLOPs: 151.6214. Time: 0.0411 ms. Best GFLOPs: 151.6214
[15:18:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_max_pool2d"] Trial #1: GFLOPs: 159.8632. Time: 0.0390 ms. Best GFLOPs: 159.8632
[15:18:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_max_pool2d"] Trial #2: GFLOPs: 148.1379. Time: 0.0421 ms. Best GFLOPs: 159.8632
[15:18:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_max_pool2d"] Trial #3: GFLOPs: 150.9219. Time: 0.0413 ms. Best GFLOPs: 159.8632
[15:18:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_max_pool2d"] Trial #4: GFLOPs: 153.5234. Time: 0.0406 ms. Best GFLOPs: 159.8632
[15:18:42] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_max_pool2d"
 ID |                              Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------
  0 |       fused_nn_conv2d_add_nn_relu | 12760457216 |      1 |      7941.8943 |    1606.7272 |             1606.7272 |     32 |          Y 
  1 |               fused_nn_conv2d_add |    29130192 |      1 |      1890.8568 |      15.4058 |               15.4058 |     32 |          Y 
  2 |             fused_nn_conv2d_add_1 | 11346935808 |      1 |      4442.9361 |    2553.9273 |             2553.9273 |     32 |          Y 
  3 |             fused_nn_conv2d_add_2 |  2838118400 |      1 |      4493.8294 |     631.5590 |              631.5590 |     32 |          Y 
  4 |             fused_nn_conv2d_add_3 |   710221824 |      1 |      3675.9960 |     193.2053 |              193.2053 |     32 |          Y 
  5 |             fused_nn_conv2d_add_4 |   357187584 |      1 |      5278.6328 |      67.6667 |               67.6667 |     32 |          Y 
  6 |     fused_nn_conv2d_add_nn_relu_1 |   819593216 |      1 |      6151.0772 |     133.2439 |              133.2439 |     32 |          Y 
  7 |               fused_nn_max_pool2d |     6230016 |      1 |       159.8632 |      38.9709 |               38.9709 |      5 |            
  8 |     fused_nn_conv2d_add_nn_relu_2 |    89989120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |     fused_nn_conv2d_add_nn_relu_3 |   355803136 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |     fused_nn_conv2d_add_nn_relu_4 |   798826496 |      3 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_add_add_nn_relu |   362725376 |      3 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_conv2d_add_nn_relu_5 |   711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |     fused_nn_conv2d_add_nn_relu_6 |   798134272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |     fused_nn_conv2d_add_nn_relu_7 |   355110912 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_nn_conv2d_add_nn_relu_8 |   798134272 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_add_nn_relu_1 |   358572032 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |     fused_nn_conv2d_add_nn_relu_9 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |    fused_nn_conv2d_add_nn_relu_10 |  3191152640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |    fused_nn_conv2d_add_nn_relu_11 |  1419059200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 20 |    fused_nn_conv2d_add_nn_relu_12 |  3191152640 |      5 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_add_nn_relu_2 |  1425981440 |      6 |            N/A |          N/A |                   N/A |      0 |            
 22 |    fused_nn_conv2d_add_nn_relu_13 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |    fused_nn_conv2d_add_nn_relu_14 | 12761841664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |    fused_nn_conv2d_add_nn_relu_15 |  5673467904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 |    fused_nn_conv2d_add_nn_relu_16 | 12761841664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_nn_conv2d_add_add_nn_relu_3 |  5687312384 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 |    fused_nn_conv2d_add_nn_relu_17 | 51039059968 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |             fused_nn_conv2d_add_5 |    58203600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |              fused_image_resize2d |   196245504 |      2 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 229
Total latency (us): 5240.71

[15:18:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #7 has finished. Remaining task(s): 22
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(26, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(4, 2, 2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_4_init)
                            yy = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 13 * 8 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 13 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [64, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(338):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 5408)
                                    v2 = T.axis.spatial(104, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 5408 // 52)
                                    v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 52)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 2)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 2, 2, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_4)
                                yy = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 13 * 8 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 13 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [64, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 8, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + ax1)
                            v2 = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 13 * 8 + ax2)
                            v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 13, 1, 4, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 4, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #1: GFLOPs: 69.2199. Time: 1.3000 ms. Best GFLOPs: 69.2199
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #2: GFLOPs: 1041.0975. Time: 0.0864 ms. Best GFLOPs: 1041.0975
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #3: GFLOPs: 232.2318. Time: 0.3875 ms. Best GFLOPs: 1041.0975
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #4: GFLOPs: 282.5114. Time: 0.3185 ms. Best GFLOPs: 1041.0975
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #5: GFLOPs: 881.8547. Time: 0.1020 ms. Best GFLOPs: 1041.0975
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #6: GFLOPs: 928.1068. Time: 0.0970 ms. Best GFLOPs: 1041.0975
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #7: GFLOPs: 1020.6065. Time: 0.0882 ms. Best GFLOPs: 1041.0975
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #8: GFLOPs: 2900.7763. Time: 0.0310 ms. Best GFLOPs: 2900.7763
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #9: GFLOPs: 746.5580. Time: 0.1205 ms. Best GFLOPs: 2900.7763
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #10: GFLOPs: 18.6770. Time: 4.8182 ms. Best GFLOPs: 2900.7763
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(52, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i2_4_init in T.grid(2, 13, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_3_init)
                            yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + i2_4_init)
                            xx = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 4 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 13 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [64, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(169):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 5408)
                                        v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 5408 // 104)
                                        v3 = T.axis.spatial(104, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 104)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 32)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 13, 2, 1, 1, 1, 1, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_3)
                                yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + i2_4)
                                xx = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 4 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 13 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [64, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + ax1)
                            v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 4 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 8, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 13, 2, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 4, 2, 13, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #12: GFLOPs: 167.5565. Time: 0.5371 ms. Best GFLOPs: 2900.7763
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #13: GFLOPs: 141.7939. Time: 0.6346 ms. Best GFLOPs: 2900.7763
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #14: GFLOPs: 129.4941. Time: 0.6949 ms. Best GFLOPs: 2900.7763
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #15: GFLOPs: 1463.0418. Time: 0.0615 ms. Best GFLOPs: 2900.7763
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #16: GFLOPs: 1103.3521. Time: 0.0816 ms. Best GFLOPs: 2900.7763
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #17: GFLOPs: 3497.5101. Time: 0.0257 ms. Best GFLOPs: 3497.5101
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #18: GFLOPs: 1364.0993. Time: 0.0660 ms. Best GFLOPs: 3497.5101
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #19: GFLOPs: 36.9597. Time: 2.4348 ms. Best GFLOPs: 3497.5101
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #20: GFLOPs: 2826.1968. Time: 0.0318 ms. Best GFLOPs: 3497.5101
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #21: GFLOPs: 161.7468. Time: 0.5564 ms. Best GFLOPs: 3497.5101
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #22: GFLOPs: 1386.6105. Time: 0.0649 ms. Best GFLOPs: 3497.5101
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #23: GFLOPs: 1050.1816. Time: 0.0857 ms. Best GFLOPs: 3497.5101
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #24: GFLOPs: 1454.3471. Time: 0.0619 ms. Best GFLOPs: 3497.5101
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #25: GFLOPs: 344.3684. Time: 0.2613 ms. Best GFLOPs: 3497.5101
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #26: GFLOPs: 1472.7911. Time: 0.0611 ms. Best GFLOPs: 3497.5101
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #27: GFLOPs: 1186.4368. Time: 0.0758 ms. Best GFLOPs: 3497.5101
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #28: GFLOPs: 210.5876. Time: 0.4273 ms. Best GFLOPs: 3497.5101
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #29: GFLOPs: 709.2712. Time: 0.1269 ms. Best GFLOPs: 3497.5101
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #30: GFLOPs: 1444.1121. Time: 0.0623 ms. Best GFLOPs: 3497.5101
[15:18:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_2"] Trial #31: GFLOPs: 100.1538. Time: 0.8985 ms. Best GFLOPs: 3497.5101
[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_nn_conv2d_add_nn_relu_2"
 ID |                              Name |        FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------
  0 |       fused_nn_conv2d_add_nn_relu | 12760457216 |      1 |      7941.8943 |    1606.7272 |             1606.7272 |     32 |          Y 
  1 |               fused_nn_conv2d_add |    29130192 |      1 |      1890.8568 |      15.4058 |               15.4058 |     32 |          Y 
  2 |             fused_nn_conv2d_add_1 | 11346935808 |      1 |      4442.9361 |    2553.9273 |             2553.9273 |     32 |          Y 
  3 |             fused_nn_conv2d_add_2 |  2838118400 |      1 |      4493.8294 |     631.5590 |              631.5590 |     32 |          Y 
  4 |             fused_nn_conv2d_add_3 |   710221824 |      1 |      3675.9960 |     193.2053 |              193.2053 |     32 |          Y 
  5 |             fused_nn_conv2d_add_4 |   357187584 |      1 |      5278.6328 |      67.6667 |               67.6667 |     32 |          Y 
  6 |     fused_nn_conv2d_add_nn_relu_1 |   819593216 |      1 |      6151.0772 |     133.2439 |              133.2439 |     32 |          Y 
  7 |               fused_nn_max_pool2d |     6230016 |      1 |       159.8632 |      38.9709 |               38.9709 |      5 |          Y 
  8 |     fused_nn_conv2d_add_nn_relu_2 |    89989120 |      1 |      3497.5101 |      25.7295 |               25.7295 |     32 |            
  9 |     fused_nn_conv2d_add_nn_relu_3 |   355803136 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |     fused_nn_conv2d_add_nn_relu_4 |   798826496 |      3 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_add_add_nn_relu |   362725376 |      3 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_conv2d_add_nn_relu_5 |   711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |     fused_nn_conv2d_add_nn_relu_6 |   798134272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |     fused_nn_conv2d_add_nn_relu_7 |   355110912 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_nn_conv2d_add_nn_relu_8 |   798134272 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_add_nn_relu_1 |   358572032 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |     fused_nn_conv2d_add_nn_relu_9 |   710221824 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |    fused_nn_conv2d_add_nn_relu_10 |  3191152640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |    fused_nn_conv2d_add_nn_relu_11 |  1419059200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 20 |    fused_nn_conv2d_add_nn_relu_12 |  3191152640 |      5 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_add_nn_relu_2 |  1425981440 |      6 |            N/A |          N/A |                   N/A |      0 |            
 22 |    fused_nn_conv2d_add_nn_relu_13 |  2838118400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |    fused_nn_conv2d_add_nn_relu_14 | 12761841664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |    fused_nn_conv2d_add_nn_relu_15 |  5673467904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 25 |    fused_nn_conv2d_add_nn_relu_16 | 12761841664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_nn_conv2d_add_add_nn_relu_3 |  5687312384 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 |    fused_nn_conv2d_add_nn_relu_17 | 51039059968 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |             fused_nn_conv2d_add_5 |    58203600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |              fused_image_resize2d |   196245504 |      2 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 261
Total latency (us): 5266.44

[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #8 has finished. Remaining task(s): 21
[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #9 has finished. Remaining task(s): 20
[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #10 has finished. Remaining task(s): 19
[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #11 has finished. Remaining task(s): 18
[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #12 has finished. Remaining task(s): 17
[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #13 has finished. Remaining task(s): 16
[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #14 has finished. Remaining task(s): 15
[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #15 has finished. Remaining task(s): 14
[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #16 has finished. Remaining task(s): 13
[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #17 has finished. Remaining task(s): 12
[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #18 has finished. Remaining task(s): 11
[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #19 has finished. Remaining task(s): 10
[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #20 has finished. Remaining task(s): 9
[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #21 has finished. Remaining task(s): 8
[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #22 has finished. Remaining task(s): 7
[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #23 has finished. Remaining task(s): 6
[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #24 has finished. Remaining task(s): 5
[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #25 has finished. Remaining task(s): 4
[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #26 has finished. Remaining task(s): 3
[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #27 has finished. Remaining task(s): 2
[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #28 has finished. Remaining task(s): 1
[15:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #29 has finished. Remaining task(s): 0
[15:19:06] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 416, 416), "float32"], placeholder_1: T.Buffer[(64, 3, 7, 7), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 3, 422, 422], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 64, 208, 208], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 208, 208], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 3, 422, 422):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 3, i3_1 - 3])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(3 <= i2_1 and i2_1 < 419 and 3 <= i3_1 and i3_1 < 419, placeholder[i0_1, i1_1, i2_1 - 3, i3_1 - 3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 208, 208, 3, 7, 7):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 64, 208, 208):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 64, 208, 208):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[15:19:06] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:19:06] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add_nn_relu(placeholder: T.Buffer[(1, 3, 416, 416), "float32"], placeholder_1: T.Buffer[(64, 3, 7, 7), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 422, 422], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 3, 7, 7], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2704, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for i1_3_init, i3_3_init, i1_4_init in T.grid(2, 2, 4):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 1352 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 8 + i1_3_init * 4 + i1_4_init)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 1352 // 13 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8)
                                xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i3_3_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i6_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("pad_temp_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(3, ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 837 // 279)
                                            v2 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused % 1352 // 13 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 279 // 31)
                                            v3 = T.axis.spatial(422, i0_0_i1_0_i2_0_i3_0_fused % 13 * 32 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 31)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 837)
                                            T.reads(placeholder[v0, v1, v2 - 3, v3 - 3])
                                            T.writes(pad_temp_shared[v0, v1, v2, v3])
                                            pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(3 <= v2 and v2 < 419 and 3 <= v3 and v3 < 419, placeholder[v0, v1, v2 - 3, v3 - 3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 1352 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 21)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 21 // 7)
                                        v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                        v3 = T.axis.spatial(7, i6_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 < 672)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 2, 3, 7, 1, 1, 4, 1, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 1352 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 8 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 1352 // 13 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8)
                                    xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i3_3)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_2, i6_0])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 416, 416], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 1352 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 8 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 1352 // 13 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 + ax2)
                            v3 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

[15:19:06] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:19:06] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_relu\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)", "v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])", "v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 4, 2, 4])", "l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])", "v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[104, 1, 2, 1, 1])", "l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])", "v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 8, 2, 1])", "l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])", "v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])", "l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])", "v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 7])", "l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])", "v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[7, 1, 1])", "l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])", "sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)", "l70 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l70, thread_axis=\"blockIdx.x\")", "l71 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l71, thread_axis=\"vthread.x\")", "l72 = sch.fuse(l19, l29, l39, l49)", "sch.bind(loop=l72, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)", "b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)", "l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)", "l85 = sch.fuse(l81, l82, l83, l84)", "v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v86)", "b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)", "l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)", "l98 = sch.fuse(l94, l95, l96, l97)", "v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)", "sch.annotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v99)", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v100)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\")", "l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)", "l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 3])", "sch.vectorize(loop=l110)", "sch.bind(loop=l109, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\")", "l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)", "l118, l119 = sch.split(loop=l117, factors=[None, 64])", "sch.bind(loop=l119, thread_axis=\"threadIdx.x\")", "b120 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b120, ann_key=\"meta_schedule.unroll_explicit\")", "b121, b122, b123, b124 = sch.get_child_blocks(b120)", "l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)", "sch.annotate(block_or_loop=l125, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l125, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)", "sch.annotate(block_or_loop=l134, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l134, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)", "sch.annotate(block_or_loop=l142, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l142, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)", "sch.annotate(block_or_loop=l162, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l162, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b169 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)", "b190 = sch.decompose_reduction(block=b169, loop=l175)"]
[15:19:07] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 208, 208), "float32"], tensor: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 210, 210], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 210, 210):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1])
                T.writes(pad_temp[ax0, ax1, ax2, ax3])
                pad_temp[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 209 and 1 <= ax3 and ax3 < 209, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T.float32(-3.4028234663852886e+38), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 64, 104, 104, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

[15:19:07] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:19:07] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_max_pool2d(placeholder: T.Buffer[(1, 64, 208, 208), "float32"], tensor: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0_i1_i2_i3_fused_1 in T.thread_binding(256, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(1024, thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in T.serial(3):
                    with T.block("tensor_init"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(64, (i0_i1_i2_i3_fused_0 * 262144 + i0_i1_i2_i3_fused_1 * 1024 + i0_i1_i2_i3_fused_2) // 10816)
                        ax2 = T.axis.spatial(104, (i0_i1_i2_i3_fused_0 * 262144 + i0_i1_i2_i3_fused_1 * 1024 + i0_i1_i2_i3_fused_2) % 10816 // 104)
                        ax3 = T.axis.spatial(104, (i0_i1_i2_i3_fused_0 * 262144 + i0_i1_i2_i3_fused_1 * 1024 + i0_i1_i2_i3_fused_2) % 104)
                        T.where((i0_i1_i2_i3_fused_0 * 256 + i0_i1_i2_i3_fused_1) * 1024 + i0_i1_i2_i3_fused_2 < 692224)
                        T.reads()
                        T.writes(tensor[ax0, ax1, ax2, ax3])
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    for i4, i5 in T.grid(3, 3):
                        with T.block("tensor_update"):
                            ax0 = T.axis.spatial(1, 0)
                            ax1 = T.axis.spatial(64, (i0_i1_i2_i3_fused_0 * 262144 + i0_i1_i2_i3_fused_1 * 1024 + i0_i1_i2_i3_fused_2) // 10816)
                            ax2 = T.axis.spatial(104, (i0_i1_i2_i3_fused_0 * 262144 + i0_i1_i2_i3_fused_1 * 1024 + i0_i1_i2_i3_fused_2) % 10816 // 104)
                            ax3 = T.axis.spatial(104, (i0_i1_i2_i3_fused_0 * 262144 + i0_i1_i2_i3_fused_1 * 1024 + i0_i1_i2_i3_fused_2) % 104)
                            rv0, rv1 = T.axis.remap("RR", [i4, i5])
                            T.where((i0_i1_i2_i3_fused_0 * 256 + i0_i1_i2_i3_fused_1) * 1024 + i0_i1_i2_i3_fused_2 < 692224)
                            T.reads(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 * 2 + rv0 - 1, ax3 * 2 + rv1 - 1])
                            T.writes(tensor[ax0, ax1, ax2, ax3])
                            tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], T.if_then_else(1 <= ax2 * 2 + rv0 and ax2 * 2 + rv0 < 209 and 1 <= ax3 * 2 + rv1 and ax3 * 2 + rv1 < 209, placeholder[ax0, ax1, ax2 * 2 + rv0 - 1, ax3 * 2 + rv1 - 1], T.float32(-3.4028234663852886e+38), dtype="float32"))
    

[15:19:07] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:19:07] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b0)", "v2 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v2)", "sch.enter_postproc()", "b3 = sch.get_block(name=\"tensor\", func_name=\"main\")", "l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b3)", "l10 = sch.fuse(l4, l5, l6, l7)", "l11, l12, l13 = sch.split(loop=l10, factors=[None, 256, 1024])", "sch.reorder(l12, l13, l11)", "sch.bind(loop=l12, thread_axis=\"blockIdx.x\")", "sch.bind(loop=l13, thread_axis=\"threadIdx.x\")", "b14 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b14, ann_key=\"meta_schedule.unroll_explicit\")", "b15, = sch.get_child_blocks(b14)", "l16, l17, l18, l19, l20 = sch.get_loops(block=b15)", "sch.annotate(block_or_loop=l16, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l16, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b21 = sch.get_block(name=\"tensor\", func_name=\"main\")", "l22, l23, l24, l25, l26 = sch.get_loops(block=b21)", "b27 = sch.decompose_reduction(block=b21, loop=l25)"]
[15:19:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 104, 104, 64, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [64, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[15:19:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:19:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add_nn_relu_1(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init in T.grid(4, 16):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 16 + i1_4_init)
                            yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused)
                            xx = T.axis.spatial(104, i0_2_i1_2_i2_2_i3_2_fused % 26 * 4 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [64, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 104)
                                    v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused)
                                    v3 = T.axis.spatial(104, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 104)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(40):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 32)
                                    v1 = T.axis.spatial(64, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 32)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 2048)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 1, 1, 4, 2, 1, 1, 1, 16, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 16 + i1_4)
                                yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused)
                                xx = T.axis.spatial(104, i0_2_i1_2_i2_2_i3_2_fused % 26 * 4 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 32 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [64, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 16 + ax1)
                            v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused + ax2)
                            v3 = T.axis.spatial(104, i0_2_i1_2_i2_2_i3_2_fused % 26 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

[15:19:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:19:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_relu\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)", "v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])", "v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 2, 1, 16])", "l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])", "v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[104, 1, 1, 1, 1])", "l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])", "v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 26, 4, 1])", "l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])", "v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 16, 2])", "l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])", "v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])", "v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])", "sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)", "l70 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l70, thread_axis=\"blockIdx.x\")", "l71 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l71, thread_axis=\"vthread.x\")", "l72 = sch.fuse(l19, l29, l39, l49)", "sch.bind(loop=l72, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)", "b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)", "l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)", "l85 = sch.fuse(l81, l82, l83, l84)", "v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v86)", "b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)", "l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)", "l98 = sch.fuse(l94, l95, l96, l97)", "v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)", "sch.annotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v99)", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v100)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\")", "l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)", "l108, l109 = sch.split(loop=l107, factors=[None, 52])", "sch.bind(loop=l109, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\")", "l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)", "l117, l118 = sch.split(loop=l116, factors=[None, 52])", "sch.bind(loop=l118, thread_axis=\"threadIdx.x\")", "b119 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b119, ann_key=\"meta_schedule.unroll_explicit\")", "b120, b121, b122, b123 = sch.get_child_blocks(b119)", "l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)", "sch.annotate(block_or_loop=l124, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l124, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)", "sch.annotate(block_or_loop=l132, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l132, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)", "sch.annotate(block_or_loop=l140, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l140, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)", "sch.annotate(block_or_loop=l160, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l160, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b167 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)", "b188 = sch.decompose_reduction(block=b167, loop=l171)"]
[15:19:09] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_2
[15:19:10] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], placeholder_1: T.Buffer[(256, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 256, 104, 104], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 104, 104, 64, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 256, 104, 104):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[15:19:10] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:19:10] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], placeholder_1: T.Buffer[(256, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(676, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init in T.serial(8):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + i1_3_init)
                            yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 13 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2)
                            xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 13 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 16 // 8)
                                    v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(64, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 8, 1, 1, 4, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + i1_3)
                                yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 13 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2)
                                xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                rc = T.axis.reduce(64, i4_0 * 16 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 104, 104], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + ax1)
                            v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 13 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2 + ax2)
                            v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

[15:19:10] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:19:10] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)", "v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])", "v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 4, 8, 8, 1])", "l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])", "v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[52, 2, 1, 1, 1])", "l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])", "v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 8, 1, 1])", "l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])", "v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[4, 4, 4])", "l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])", "v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])", "v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])", "sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)", "l69 = sch.fuse(l16, l26, l36, l46)", "sch.bind(loop=l69, thread_axis=\"blockIdx.x\")", "l70 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l70, thread_axis=\"vthread.x\")", "l71 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l71, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)", "b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)", "l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)", "l84 = sch.fuse(l80, l81, l82, l83)", "v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b73, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v85)", "b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)", "l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)", "l97 = sch.fuse(l93, l94, l95, l96)", "v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b86, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v98)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v99)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b73, ann_key=\"meta_schedule.cooperative_fetch\")", "l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)", "l107, l108 = sch.split(loop=l106, factors=[None, 64])", "sch.bind(loop=l108, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b86, ann_key=\"meta_schedule.cooperative_fetch\")", "l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)", "l116, l117 = sch.split(loop=l115, factors=[None, 64])", "sch.bind(loop=l117, thread_axis=\"threadIdx.x\")", "b118 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b118, ann_key=\"meta_schedule.unroll_explicit\")", "b119, b120, b121, b122 = sch.get_child_blocks(b118)", "l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b119)", "sch.annotate(block_or_loop=l123, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l123, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b120)", "sch.annotate(block_or_loop=l131, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l131, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158 = sch.get_loops(block=b121)", "sch.annotate(block_or_loop=l139, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l139, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l159, l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b122)", "sch.annotate(block_or_loop=l159, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l159, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b166 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b166)", "b187 = sch.decompose_reduction(block=b166, loop=l170)"]
[15:19:13] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_add_nn_relu
[15:19:13] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_3
[15:19:14] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_4
[15:19:14] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_5
[15:19:17] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 104, 104), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_add: T.Buffer[(1, 512, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 104, 104], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 512, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 104, 104):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 512, 52, 52, 256, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 104, 104], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 512, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[15:19:17] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:19:17] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add_1(placeholder: T.Buffer[(1, 256, 104, 104), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_add: T.Buffer[(1, 512, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 104, 104], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init in T.grid(8, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 52 * 256 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + i1_3_init)
                            yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 52 // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 4 * 13 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 104, 104], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(22):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 2800 // 175)
                                    v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 52 // 4 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 175 // 25)
                                    v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 4 * 26 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 25)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 < 2800)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 52 * 256 + (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(256, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 8, 1, 13, 4, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 52 * 256 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + i1_3)
                                yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 52 // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 4 * 13 + i3_3)
                                rc = T.axis.reduce(256, i4_0 * 16 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 104, 104], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 52 * 256 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + ax1)
                            v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 52 // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 4 * 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

[15:19:17] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:19:17] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)", "v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])", "v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 1, 32, 8, 1])", "l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])", "v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[13, 1, 4, 1, 1])", "l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])", "v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 1, 13, 1])", "l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])", "v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 4, 4])", "l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])", "v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])", "v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])", "sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)", "l69 = sch.fuse(l16, l26, l36, l46)", "sch.bind(loop=l69, thread_axis=\"blockIdx.x\")", "l70 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l70, thread_axis=\"vthread.x\")", "l71 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l71, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)", "b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)", "l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)", "l84 = sch.fuse(l80, l81, l82, l83)", "v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)", "sch.annotate(block_or_loop=b73, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v85)", "b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)", "l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)", "l97 = sch.fuse(l93, l94, l95, l96)", "v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b86, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v98)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v99)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b73, ann_key=\"meta_schedule.cooperative_fetch\")", "l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)", "l107, l108 = sch.split(loop=l106, factors=[None, 128])", "sch.bind(loop=l108, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b86, ann_key=\"meta_schedule.cooperative_fetch\")", "l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)", "l116, l117, l118 = sch.split(loop=l115, factors=[None, 128, 4])", "sch.vectorize(loop=l118)", "sch.bind(loop=l117, thread_axis=\"threadIdx.x\")", "b119 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b119, ann_key=\"meta_schedule.unroll_explicit\")", "b120, b121, b122, b123 = sch.get_child_blocks(b119)", "l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)", "sch.annotate(block_or_loop=l124, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l124, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)", "sch.annotate(block_or_loop=l132, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l132, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)", "sch.annotate(block_or_loop=l141, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l141, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)", "sch.annotate(block_or_loop=l161, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l161, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b168 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)", "b189 = sch.decompose_reduction(block=b168, loop=l172)"]
[15:19:17] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_add_nn_relu_1
[15:19:18] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_6
[15:19:18] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_7
[15:19:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_8
[15:19:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_9
[15:19:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(1024, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_add: T.Buffer[(1, 1024, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 512, 52, 52], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 1024, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 52, 52):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 1024, 52, 52, 512, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 1024, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[15:19:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:19:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add_2(placeholder: T.Buffer[(1, 512, 52, 52), "float32"], placeholder_1: T.Buffer[(1024, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_add: T.Buffer[(1, 1024, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 52, 52], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(338, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(8, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 169 * 512 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 32 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 16 // 4)
                                    v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 169 * 512 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(512, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 8, 1, 1, 16, 1, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 169 * 512 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 32 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                rc = T.axis.reduce(512, i4_0 * 16 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 52, 52], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 169 * 512 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 32 + ax1)
                            v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

[15:19:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:19:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)", "v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])", "v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 1, 16, 8, 4])", "l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])", "v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[13, 2, 2, 1, 1])", "l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])", "v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 4, 1, 1])", "l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])", "v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[32, 1, 16])", "l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])", "v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])", "v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])", "sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)", "l69 = sch.fuse(l16, l26, l36, l46)", "sch.bind(loop=l69, thread_axis=\"blockIdx.x\")", "l70 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l70, thread_axis=\"vthread.x\")", "l71 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l71, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)", "b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)", "l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)", "l84 = sch.fuse(l80, l81, l82, l83)", "v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b73, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v85)", "b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)", "l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)", "l97 = sch.fuse(l93, l94, l95, l96)", "v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)", "sch.annotate(block_or_loop=b86, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v98)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v99)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b73, ann_key=\"meta_schedule.cooperative_fetch\")", "l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)", "l107, l108 = sch.split(loop=l106, factors=[None, 128])", "sch.bind(loop=l108, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b86, ann_key=\"meta_schedule.cooperative_fetch\")", "l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)", "l116, l117 = sch.split(loop=l115, factors=[None, 128])", "sch.bind(loop=l117, thread_axis=\"threadIdx.x\")", "b118 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b118, ann_key=\"meta_schedule.unroll_explicit\")", "b119, b120, b121, b122 = sch.get_child_blocks(b118)", "l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b119)", "sch.annotate(block_or_loop=l123, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l123, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b120)", "sch.annotate(block_or_loop=l131, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l131, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158 = sch.get_loops(block=b121)", "sch.annotate(block_or_loop=l139, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l139, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l159, l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b122)", "sch.annotate(block_or_loop=l159, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l159, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b166 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b166)", "b187 = sch.decompose_reduction(block=b166, loop=l170)"]
[15:19:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_add_nn_relu_2
[15:19:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_10
[15:19:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_11
[15:19:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_12
[15:19:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_13
[15:19:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(2048, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 2048, 1, 1), "float32"], T_add: T.Buffer[(1, 2048, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 1024, 52, 52], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 2048, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1024, 52, 52):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 2048, 52, 52, 1024, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 2048, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[15:19:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:19:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add_3(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(2048, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 2048, 1, 1), "float32"], T_add: T.Buffer[(1, 2048, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 2048, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 52, 52], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([2048, 1024, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(6656, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(2, 8):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused // 104 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 16 + i1_3_init * 8 + i1_4_init)
                            yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 104 // 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 13)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 4 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 26)
                                        v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 104 // 4 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 26 // 13)
                                        v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 4 * 13 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused // 104 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(1024, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 512)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 2, 1, 1, 4, 1, 1, 1, 8, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused // 104 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 16 + i1_3 * 8 + i1_4)
                                yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 104 // 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 13)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 4 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(1024, i4_0 * 16 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [2048, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(2048, i0_0_i1_0_i2_0_i3_0_fused // 104 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 16 + ax1)
                            v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 104 // 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 13 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 4 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

[15:19:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:19:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)", "v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])", "v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 2, 2, 8])", "l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])", "v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[26, 1, 2, 1, 1])", "l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])", "v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 13, 1, 1])", "l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])", "v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[64, 4, 4])", "l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])", "v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])", "v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])", "sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)", "l69 = sch.fuse(l16, l26, l36, l46)", "sch.bind(loop=l69, thread_axis=\"blockIdx.x\")", "l70 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l70, thread_axis=\"vthread.x\")", "l71 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l71, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)", "b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)", "l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)", "l84 = sch.fuse(l80, l81, l82, l83)", "v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)", "sch.annotate(block_or_loop=b73, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v85)", "b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)", "l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)", "l97 = sch.fuse(l93, l94, l95, l96)", "v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)", "sch.annotate(block_or_loop=b86, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v98)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v99)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b73, ann_key=\"meta_schedule.cooperative_fetch\")", "l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)", "l107, l108, l109 = sch.split(loop=l106, factors=[None, 52, 2])", "sch.vectorize(loop=l109)", "sch.bind(loop=l108, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b86, ann_key=\"meta_schedule.cooperative_fetch\")", "l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)", "l117, l118 = sch.split(loop=l116, factors=[None, 52])", "sch.bind(loop=l118, thread_axis=\"threadIdx.x\")", "b119 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b119, ann_key=\"meta_schedule.unroll_explicit\")", "b120, b121, b122, b123 = sch.get_child_blocks(b119)", "l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b120)", "sch.annotate(block_or_loop=l124, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l124, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)", "sch.annotate(block_or_loop=l133, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l133, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)", "sch.annotate(block_or_loop=l141, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l141, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)", "sch.annotate(block_or_loop=l161, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l161, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b168 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)", "b189 = sch.decompose_reduction(block=b168, loop=l172)"]
[15:19:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_add_nn_relu_3
[15:19:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_14
[15:19:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_15
[15:19:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_16
[15:19:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_4
[15:19:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_image_resize2d
[15:19:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(256, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 1024, 54, 54], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 256, 52, 52], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1024, 54, 54):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 53 and 1 <= i3_1 and i3_1 < 53, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 52, 52, 1024, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 256, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 52, 52):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[15:19:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:19:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add_nn_relu_17(placeholder: T.Buffer[(1, 1024, 52, 52), "float32"], placeholder_1: T.Buffer[(256, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1024, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i3_3_init, i2_4_init in T.grid(2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4)
                            yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused * 26 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 13 + i2_4_init)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(21):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i4_0 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 5184 // 324)
                                        v2 = T.axis.spatial(54, ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 324 // 6)
                                        v3 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 6)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 5184)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 53 and 1 <= v3 and v3 < 53, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 144)
                                        v1 = T.axis.spatial(1024, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 144 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 3, 1, 1, 1, 1, 2, 2, 1, 3, 1, 1, 13, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4)
                                yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused * 26 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 13 + i2_4)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3)
                                rc = T.axis.reduce(1024, i4_0 * 16 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 52, 52], "float32"], ["TENSOR", [256, 1024, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 13, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 + ax1)
                            v2 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused * 26 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 13 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

[15:19:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:19:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_relu\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)", "v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])", "v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 32, 1, 1])", "l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])", "v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 2, 1, 13])", "l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])", "v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 2, 2, 1])", "l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])", "v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 8, 2])", "l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])", "v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])", "l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])", "v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])", "l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])", "sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)", "l70 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l70, thread_axis=\"blockIdx.x\")", "l71 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l71, thread_axis=\"vthread.x\")", "l72 = sch.fuse(l19, l29, l39, l49)", "sch.bind(loop=l72, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)", "b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)", "l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)", "l85 = sch.fuse(l81, l82, l83, l84)", "v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)", "sch.annotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v86)", "b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)", "l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)", "l98 = sch.fuse(l94, l95, l96, l97)", "v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)", "sch.annotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v99)", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v100)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\")", "l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)", "l108, l109, l110 = sch.split(loop=l107, factors=[None, 128, 2])", "sch.vectorize(loop=l110)", "sch.bind(loop=l109, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\")", "l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)", "l118, l119, l120 = sch.split(loop=l117, factors=[None, 128, 2])", "sch.vectorize(loop=l120)", "sch.bind(loop=l119, thread_axis=\"threadIdx.x\")", "b121 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b121, ann_key=\"meta_schedule.unroll_explicit\")", "b122, b123, b124, b125 = sch.get_child_blocks(b121)", "l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)", "sch.annotate(block_or_loop=l126, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l126, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)", "sch.annotate(block_or_loop=l135, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l135, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)", "sch.annotate(block_or_loop=l144, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l144, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)", "sch.annotate(block_or_loop=l164, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l164, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b171 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)", "b192 = sch.decompose_reduction(block=b171, loop=l175)"]
[15:19:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 52, 52), "float32"], placeholder_1: T.Buffer[(21, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 21, 1, 1), "float32"], T_add: T.Buffer[(1, 21, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 52, 52], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 21, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 52, 52):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 21, 52, 52, 256, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 52, 52], "float32"], ["TENSOR", [21, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 21, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[15:19:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:19:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add_5(placeholder: T.Buffer[(1, 256, 52, 52), "float32"], placeholder_1: T.Buffer[(21, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 21, 1, 1), "float32"], T_add: T.Buffer[(1, 21, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 21, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 52, 52], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([21, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(52, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(91, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init in T.grid(2, 2, 3):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(21, i0_2_i1_2_i2_2_i3_2_fused // 13 * 3 + i1_4_init)
                            yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i2_3_init)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 52, 52], "float32"], ["TENSOR", [21, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(91, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 364 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 52)
                                        v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 364 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 52 // 26)
                                        v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + (ax0_ax1_ax2_ax3_fused_0 * 364 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 26)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 91 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 832)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(91, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(21, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(256, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 336)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 2, 16, 1, 1, 1, 3, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(21, i0_2_i1_2_i2_2_i3_2_fused // 13 * 3 + i1_4)
                                yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i2_3)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + i3_3)
                                rc = T.axis.reduce(256, i4_0 * 16 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 52, 52], "float32"], ["TENSOR", [21, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(21, i0_2_i1_2_i2_2_i3_2_fused // 13 * 3 + ax1)
                            v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

[15:19:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:19:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)", "v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])", "v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 3])", "l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])", "v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[26, 1, 1, 2, 1])", "l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])", "v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 13, 2, 1])", "l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])", "v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 1, 16])", "l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])", "v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])", "v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])", "sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)", "l69 = sch.fuse(l16, l26, l36, l46)", "sch.bind(loop=l69, thread_axis=\"blockIdx.x\")", "l70 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l70, thread_axis=\"vthread.x\")", "l71 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l71, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)", "b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)", "l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)", "l84 = sch.fuse(l80, l81, l82, l83)", "v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b73, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v85)", "b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)", "l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)", "l97 = sch.fuse(l93, l94, l95, l96)", "v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b86, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v98)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v99)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b73, ann_key=\"meta_schedule.cooperative_fetch\")", "l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)", "l107, l108, l109 = sch.split(loop=l106, factors=[None, 91, 4])", "sch.vectorize(loop=l109)", "sch.bind(loop=l108, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b86, ann_key=\"meta_schedule.cooperative_fetch\")", "l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)", "l117, l118, l119 = sch.split(loop=l116, factors=[None, 91, 4])", "sch.vectorize(loop=l119)", "sch.bind(loop=l118, thread_axis=\"threadIdx.x\")", "b120 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b120, ann_key=\"meta_schedule.unroll_explicit\")", "b121, b122, b123, b124 = sch.get_child_blocks(b120)", "l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)", "sch.annotate(block_or_loop=l125, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l125, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)", "sch.annotate(block_or_loop=l134, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l134, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)", "sch.annotate(block_or_loop=l143, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l143, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)", "sch.annotate(block_or_loop=l163, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l163, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b170 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)", "b191 = sch.decompose_reduction(block=b170, loop=l174)"]
https://storage.cloud.google.com/octoml-aquarium-models/onnx_model_zoo/vision_object_detection_segmentation_fcn-resnet50-11.onnx
Downloading object from gcp. Bucket name: octoml-aquarium-models, file path: onnx_model_zoo/vision_object_detection_segmentation_fcn-resnet50-11.onnx
/home/yj/models/fcn-resnet50.onnx
Starting to build with relay.
/home/yj/anaconda3/lib/python3.8/site-packages/google/auth/_default.py:79: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a "quota exceeded" or "API not enabled" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/
  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)
/home/yj/anaconda3/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
The result is correct!
