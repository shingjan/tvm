nohup: ignoring input
https://storage.cloud.google.com/octoml-aquarium-models/onnx_model_zoo/amd_3d_unet.onnx
file existed. Skipping downloading.
/home/yj/models/amd-3d-unet.onnx
2022-05-09 14:35:17.467 INFO Logging directory: /tmp/tmp_l76u12e/logs
2022-05-09 14:35:17.467 INFO Working directory: /tmp/tmp_l76u12e
2022-05-09 14:35:17.467 INFO LocalBuilder: max_workers = 24
2022-05-09 14:35:17.800 INFO LocalRunner: max_workers = 1
2022-05-09 14:35:35.977 INFO 
 ID |                                                        Name |         FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                             fused_nn_conv3d |      5017600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  1 |                                           fused_nn_conv3d_1 |     32112640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |                                           fused_nn_conv3d_2 |    128450560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                                           fused_nn_conv3d_3 |    513802240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                                              fused_variance |       235520 |      2 |            N/A |          N/A |                   N/A |      0 |            
  5 |                                                  fused_mean |        78720 |      2 |            N/A |          N/A |                   N/A |      0 |            
  6 |                                            fused_variance_1 |      1881920 |      4 |            N/A |          N/A |                   N/A |      0 |            
  7 |                                                fused_mean_1 |       627520 |      4 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                            fused_variance_2 |     12042496 |      4 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                                fused_mean_2 |      4014336 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                            fused_variance_3 |     48169088 |      4 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                                fused_mean_3 |     16056448 |      4 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                            fused_variance_4 |    192675904 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                fused_mean_4 |     64225344 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                            fused_variance_5 |    770703392 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                fused_mean_5 |    256901152 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                         fused_nn_conv3d_add |  55747543040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu |   1284505632 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                       fused_nn_conv3d_add_1 | 111045509120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                       fused_nn_conv3d_add_2 |  55506698240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                       fused_nn_conv3d_add_3 |  27749335040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |            fused_subtract_add_sqrt_divide_add_nn_leaky_relu |     16056576 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                       fused_nn_conv3d_add_4 |   8671040000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                       fused_nn_conv3d_add_5 |   1354830400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_1 |       313920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                       fused_nn_conv3d_add_6 |   1354830400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_1 |       392320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                   fused_nn_conv3d_transpose |   3211264000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                           fused_concatenate |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                       fused_nn_conv3d_add_7 |  21676659200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_2 |      2509120 |      2 |            N/A |          N/A |                   N/A |      0 |            
 31 |                                       fused_nn_conv3d_add_8 |  10838643200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_2 |      3136320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |                                 fused_nn_conv3d_transpose_1 |  20552089600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                         fused_concatenate_1 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                       fused_nn_conv3d_add_9 | 110985297920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |                                      fused_nn_conv3d_add_10 |  55494656000 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_3 |     20070656 |      3 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_conv3d_transpose_2 |  65766686720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                         fused_concatenate_2 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                      fused_nn_conv3d_add_11 | 221978624000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_3 |     64225408 |      2 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                      fused_nn_conv3d_add_12 | 110997340160 |      2 |            N/A |          N/A |                   N/A |      0 |            
 43 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_4 |     80281728 |      2 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                 fused_nn_conv3d_transpose_3 | 131533373440 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                         fused_concatenate_3 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                      fused_nn_conv3d_add_13 | 443989360640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_4 |    256901184 |      2 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                      fused_nn_conv3d_add_14 | 222026792960 |      2 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_5 |    321126464 |      2 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                 fused_nn_conv3d_transpose_4 | 263066746880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                         fused_concatenate_4 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                      fused_nn_conv3d_add_15 | 888107171840 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                      fused_nn_conv3d_add_16 | 444182036480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_5 |   1027604512 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                           fused_nn_conv3d_4 |   2055208960 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

2022-05-09 14:35:35.977 INFO Scheduler picks Task #0: "fused_nn_conv3d"
2022-05-09 14:37:00.158 INFO Sending 32 sample(s) to builder
2022-05-09 14:37:09.939 INFO Sending 32 sample(s) to runner
2022-05-09 14:37:19.570 INFO Scheduler picks Task #1: "fused_nn_conv3d_1"
2022-05-09 14:39:19.375 INFO Sending 32 sample(s) to builder
2022-05-09 14:39:23.472 INFO Sending 32 sample(s) to runner
2022-05-09 14:39:37.711 INFO Scheduler picks Task #2: "fused_nn_conv3d_2"
2022-05-09 14:41:43.464 INFO Sending 32 sample(s) to builder
2022-05-09 14:42:11.569 INFO Sending 32 sample(s) to runner
2022-05-09 14:42:45.154 INFO Scheduler picks Task #3: "fused_nn_conv3d_3"
2022-05-09 14:45:16.766 INFO Sending 32 sample(s) to builder
2022-05-09 14:45:25.414 INFO Sending 32 sample(s) to runner
2022-05-09 14:46:56.179 INFO Scheduler picks Task #4: "fused_variance"
2022-05-09 14:47:19.209 INFO Sending 32 sample(s) to builder
2022-05-09 14:47:21.777 INFO Sending 32 sample(s) to runner
2022-05-09 14:47:32.081 INFO Scheduler picks Task #5: "fused_mean"
2022-05-09 14:47:50.533 INFO Sending 31 sample(s) to builder
2022-05-09 14:47:52.826 INFO Sending 31 sample(s) to runner
2022-05-09 14:48:02.217 INFO Scheduler picks Task #6: "fused_variance_1"
2022-05-09 14:48:27.073 INFO Sending 31 sample(s) to builder
2022-05-09 14:48:32.256 INFO Sending 31 sample(s) to runner
2022-05-09 14:48:41.560 INFO Scheduler picks Task #7: "fused_mean_1"
2022-05-09 14:49:00.201 INFO Sending 32 sample(s) to builder
2022-05-09 14:49:04.428 INFO Sending 32 sample(s) to runner
2022-05-09 14:49:14.801 INFO Scheduler picks Task #8: "fused_variance_2"
2022-05-09 14:49:37.568 INFO Sending 31 sample(s) to builder
2022-05-09 14:49:45.391 INFO Sending 31 sample(s) to runner
2022-05-09 14:50:00.533 INFO [Updated] Task #0: "fused_nn_conv3d"
 ID |                                                        Name |         FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                             fused_nn_conv3d |      5017600 |      1 |       415.8214 |      12.0667 |               12.0667 |     32 |            
  1 |                                           fused_nn_conv3d_1 |     32112640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |                                           fused_nn_conv3d_2 |    128450560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                                           fused_nn_conv3d_3 |    513802240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                                              fused_variance |       235520 |      2 |            N/A |          N/A |                   N/A |      0 |            
  5 |                                                  fused_mean |        78720 |      2 |            N/A |          N/A |                   N/A |      0 |            
  6 |                                            fused_variance_1 |      1881920 |      4 |            N/A |          N/A |                   N/A |      0 |            
  7 |                                                fused_mean_1 |       627520 |      4 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                            fused_variance_2 |     12042496 |      4 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                                fused_mean_2 |      4014336 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                            fused_variance_3 |     48169088 |      4 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                                fused_mean_3 |     16056448 |      4 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                            fused_variance_4 |    192675904 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                fused_mean_4 |     64225344 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                            fused_variance_5 |    770703392 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                fused_mean_5 |    256901152 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                         fused_nn_conv3d_add |  55747543040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu |   1284505632 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                       fused_nn_conv3d_add_1 | 111045509120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                       fused_nn_conv3d_add_2 |  55506698240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                       fused_nn_conv3d_add_3 |  27749335040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |            fused_subtract_add_sqrt_divide_add_nn_leaky_relu |     16056576 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                       fused_nn_conv3d_add_4 |   8671040000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                       fused_nn_conv3d_add_5 |   1354830400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_1 |       313920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                       fused_nn_conv3d_add_6 |   1354830400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_1 |       392320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                   fused_nn_conv3d_transpose |   3211264000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                           fused_concatenate |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                       fused_nn_conv3d_add_7 |  21676659200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_2 |      2509120 |      2 |            N/A |          N/A |                   N/A |      0 |            
 31 |                                       fused_nn_conv3d_add_8 |  10838643200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_2 |      3136320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |                                 fused_nn_conv3d_transpose_1 |  20552089600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                         fused_concatenate_1 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                       fused_nn_conv3d_add_9 | 110985297920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |                                      fused_nn_conv3d_add_10 |  55494656000 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_3 |     20070656 |      3 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_conv3d_transpose_2 |  65766686720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                         fused_concatenate_2 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                      fused_nn_conv3d_add_11 | 221978624000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_3 |     64225408 |      2 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                      fused_nn_conv3d_add_12 | 110997340160 |      2 |            N/A |          N/A |                   N/A |      0 |            
 43 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_4 |     80281728 |      2 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                 fused_nn_conv3d_transpose_3 | 131533373440 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                         fused_concatenate_3 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                      fused_nn_conv3d_add_13 | 443989360640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_4 |    256901184 |      2 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                      fused_nn_conv3d_add_14 | 222026792960 |      2 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_5 |    321126464 |      2 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                 fused_nn_conv3d_transpose_4 | 263066746880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                         fused_concatenate_4 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                      fused_nn_conv3d_add_15 | 888107171840 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                      fused_nn_conv3d_add_16 | 444182036480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_5 |   1027604512 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                           fused_nn_conv3d_4 |   2055208960 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 32
Total latency (us): 12.0667

2022-05-09 14:50:00.533 INFO Task #0 has finished. Remaining task(s): 55
2022-05-09 14:50:01.479 INFO [Updated] Task #1: "fused_nn_conv3d_1"
 ID |                                                        Name |         FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                             fused_nn_conv3d |      5017600 |      1 |       415.8214 |      12.0667 |               12.0667 |     32 |          Y 
  1 |                                           fused_nn_conv3d_1 |     32112640 |      1 |       663.6067 |      48.3911 |               48.3911 |     32 |            
  2 |                                           fused_nn_conv3d_2 |    128450560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                                           fused_nn_conv3d_3 |    513802240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                                              fused_variance |       235520 |      2 |            N/A |          N/A |                   N/A |      0 |            
  5 |                                                  fused_mean |        78720 |      2 |            N/A |          N/A |                   N/A |      0 |            
  6 |                                            fused_variance_1 |      1881920 |      4 |            N/A |          N/A |                   N/A |      0 |            
  7 |                                                fused_mean_1 |       627520 |      4 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                            fused_variance_2 |     12042496 |      4 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                                fused_mean_2 |      4014336 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                            fused_variance_3 |     48169088 |      4 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                                fused_mean_3 |     16056448 |      4 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                            fused_variance_4 |    192675904 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                fused_mean_4 |     64225344 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                            fused_variance_5 |    770703392 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                fused_mean_5 |    256901152 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                         fused_nn_conv3d_add |  55747543040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu |   1284505632 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                       fused_nn_conv3d_add_1 | 111045509120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                       fused_nn_conv3d_add_2 |  55506698240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                       fused_nn_conv3d_add_3 |  27749335040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |            fused_subtract_add_sqrt_divide_add_nn_leaky_relu |     16056576 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                       fused_nn_conv3d_add_4 |   8671040000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                       fused_nn_conv3d_add_5 |   1354830400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_1 |       313920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                       fused_nn_conv3d_add_6 |   1354830400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_1 |       392320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                   fused_nn_conv3d_transpose |   3211264000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                           fused_concatenate |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                       fused_nn_conv3d_add_7 |  21676659200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_2 |      2509120 |      2 |            N/A |          N/A |                   N/A |      0 |            
 31 |                                       fused_nn_conv3d_add_8 |  10838643200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_2 |      3136320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |                                 fused_nn_conv3d_transpose_1 |  20552089600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                         fused_concatenate_1 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                       fused_nn_conv3d_add_9 | 110985297920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |                                      fused_nn_conv3d_add_10 |  55494656000 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_3 |     20070656 |      3 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_conv3d_transpose_2 |  65766686720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                         fused_concatenate_2 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                      fused_nn_conv3d_add_11 | 221978624000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_3 |     64225408 |      2 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                      fused_nn_conv3d_add_12 | 110997340160 |      2 |            N/A |          N/A |                   N/A |      0 |            
 43 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_4 |     80281728 |      2 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                 fused_nn_conv3d_transpose_3 | 131533373440 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                         fused_concatenate_3 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                      fused_nn_conv3d_add_13 | 443989360640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_4 |    256901184 |      2 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                      fused_nn_conv3d_add_14 | 222026792960 |      2 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_5 |    321126464 |      2 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                 fused_nn_conv3d_transpose_4 | 263066746880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                         fused_concatenate_4 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                      fused_nn_conv3d_add_15 | 888107171840 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                      fused_nn_conv3d_add_16 | 444182036480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_5 |   1027604512 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                           fused_nn_conv3d_4 |   2055208960 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 64
Total latency (us): 60.4578

2022-05-09 14:50:01.479 INFO Task #1 has finished. Remaining task(s): 54
2022-05-09 14:50:02.636 INFO [Updated] Task #2: "fused_nn_conv3d_2"
 ID |                                                        Name |         FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                             fused_nn_conv3d |      5017600 |      1 |       415.8214 |      12.0667 |               12.0667 |     32 |          Y 
  1 |                                           fused_nn_conv3d_1 |     32112640 |      1 |       663.6067 |      48.3911 |               48.3911 |     32 |          Y 
  2 |                                           fused_nn_conv3d_2 |    128450560 |      1 |       790.1843 |     162.5577 |              162.5577 |     32 |            
  3 |                                           fused_nn_conv3d_3 |    513802240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                                              fused_variance |       235520 |      2 |            N/A |          N/A |                   N/A |      0 |            
  5 |                                                  fused_mean |        78720 |      2 |            N/A |          N/A |                   N/A |      0 |            
  6 |                                            fused_variance_1 |      1881920 |      4 |            N/A |          N/A |                   N/A |      0 |            
  7 |                                                fused_mean_1 |       627520 |      4 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                            fused_variance_2 |     12042496 |      4 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                                fused_mean_2 |      4014336 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                            fused_variance_3 |     48169088 |      4 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                                fused_mean_3 |     16056448 |      4 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                            fused_variance_4 |    192675904 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                fused_mean_4 |     64225344 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                            fused_variance_5 |    770703392 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                fused_mean_5 |    256901152 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                         fused_nn_conv3d_add |  55747543040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu |   1284505632 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                       fused_nn_conv3d_add_1 | 111045509120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                       fused_nn_conv3d_add_2 |  55506698240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                       fused_nn_conv3d_add_3 |  27749335040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |            fused_subtract_add_sqrt_divide_add_nn_leaky_relu |     16056576 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                       fused_nn_conv3d_add_4 |   8671040000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                       fused_nn_conv3d_add_5 |   1354830400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_1 |       313920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                       fused_nn_conv3d_add_6 |   1354830400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_1 |       392320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                   fused_nn_conv3d_transpose |   3211264000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                           fused_concatenate |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                       fused_nn_conv3d_add_7 |  21676659200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_2 |      2509120 |      2 |            N/A |          N/A |                   N/A |      0 |            
 31 |                                       fused_nn_conv3d_add_8 |  10838643200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_2 |      3136320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |                                 fused_nn_conv3d_transpose_1 |  20552089600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                         fused_concatenate_1 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                       fused_nn_conv3d_add_9 | 110985297920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |                                      fused_nn_conv3d_add_10 |  55494656000 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_3 |     20070656 |      3 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_conv3d_transpose_2 |  65766686720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                         fused_concatenate_2 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                      fused_nn_conv3d_add_11 | 221978624000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_3 |     64225408 |      2 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                      fused_nn_conv3d_add_12 | 110997340160 |      2 |            N/A |          N/A |                   N/A |      0 |            
 43 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_4 |     80281728 |      2 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                 fused_nn_conv3d_transpose_3 | 131533373440 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                         fused_concatenate_3 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                      fused_nn_conv3d_add_13 | 443989360640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_4 |    256901184 |      2 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                      fused_nn_conv3d_add_14 | 222026792960 |      2 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_5 |    321126464 |      2 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                 fused_nn_conv3d_transpose_4 | 263066746880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                         fused_concatenate_4 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                      fused_nn_conv3d_add_15 | 888107171840 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                      fused_nn_conv3d_add_16 | 444182036480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_5 |   1027604512 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                           fused_nn_conv3d_4 |   2055208960 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 96
Total latency (us): 223.016

2022-05-09 14:50:02.636 INFO Task #2 has finished. Remaining task(s): 53
2022-05-09 14:50:04.281 INFO [Updated] Task #3: "fused_nn_conv3d_3"
 ID |                                                        Name |         FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                             fused_nn_conv3d |      5017600 |      1 |       415.8214 |      12.0667 |               12.0667 |     32 |          Y 
  1 |                                           fused_nn_conv3d_1 |     32112640 |      1 |       663.6067 |      48.3911 |               48.3911 |     32 |          Y 
  2 |                                           fused_nn_conv3d_2 |    128450560 |      1 |       790.1843 |     162.5577 |              162.5577 |     32 |          Y 
  3 |                                           fused_nn_conv3d_3 |    513802240 |      1 |       753.7148 |     681.6932 |              681.6932 |     32 |            
  4 |                                              fused_variance |       235520 |      2 |            N/A |          N/A |                   N/A |      0 |            
  5 |                                                  fused_mean |        78720 |      2 |            N/A |          N/A |                   N/A |      0 |            
  6 |                                            fused_variance_1 |      1881920 |      4 |            N/A |          N/A |                   N/A |      0 |            
  7 |                                                fused_mean_1 |       627520 |      4 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                            fused_variance_2 |     12042496 |      4 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                                fused_mean_2 |      4014336 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                            fused_variance_3 |     48169088 |      4 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                                fused_mean_3 |     16056448 |      4 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                            fused_variance_4 |    192675904 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                fused_mean_4 |     64225344 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                            fused_variance_5 |    770703392 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                fused_mean_5 |    256901152 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                         fused_nn_conv3d_add |  55747543040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu |   1284505632 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                       fused_nn_conv3d_add_1 | 111045509120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                       fused_nn_conv3d_add_2 |  55506698240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                       fused_nn_conv3d_add_3 |  27749335040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |            fused_subtract_add_sqrt_divide_add_nn_leaky_relu |     16056576 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                       fused_nn_conv3d_add_4 |   8671040000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                       fused_nn_conv3d_add_5 |   1354830400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_1 |       313920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                       fused_nn_conv3d_add_6 |   1354830400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_1 |       392320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                   fused_nn_conv3d_transpose |   3211264000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                           fused_concatenate |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                       fused_nn_conv3d_add_7 |  21676659200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_2 |      2509120 |      2 |            N/A |          N/A |                   N/A |      0 |            
 31 |                                       fused_nn_conv3d_add_8 |  10838643200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_2 |      3136320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |                                 fused_nn_conv3d_transpose_1 |  20552089600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                         fused_concatenate_1 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                       fused_nn_conv3d_add_9 | 110985297920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |                                      fused_nn_conv3d_add_10 |  55494656000 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_3 |     20070656 |      3 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_conv3d_transpose_2 |  65766686720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                         fused_concatenate_2 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                      fused_nn_conv3d_add_11 | 221978624000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_3 |     64225408 |      2 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                      fused_nn_conv3d_add_12 | 110997340160 |      2 |            N/A |          N/A |                   N/A |      0 |            
 43 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_4 |     80281728 |      2 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                 fused_nn_conv3d_transpose_3 | 131533373440 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                         fused_concatenate_3 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                      fused_nn_conv3d_add_13 | 443989360640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_4 |    256901184 |      2 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                      fused_nn_conv3d_add_14 | 222026792960 |      2 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_5 |    321126464 |      2 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                 fused_nn_conv3d_transpose_4 | 263066746880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                         fused_concatenate_4 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                      fused_nn_conv3d_add_15 | 888107171840 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                      fused_nn_conv3d_add_16 | 444182036480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_5 |   1027604512 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                           fused_nn_conv3d_4 |   2055208960 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 128
Total latency (us): 904.709

2022-05-09 14:50:04.281 INFO Task #3 has finished. Remaining task(s): 52
2022-05-09 14:50:05.124 INFO [Updated] Task #4: "fused_variance"
 ID |                                                        Name |         FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                             fused_nn_conv3d |      5017600 |      1 |       415.8214 |      12.0667 |               12.0667 |     32 |          Y 
  1 |                                           fused_nn_conv3d_1 |     32112640 |      1 |       663.6067 |      48.3911 |               48.3911 |     32 |          Y 
  2 |                                           fused_nn_conv3d_2 |    128450560 |      1 |       790.1843 |     162.5577 |              162.5577 |     32 |          Y 
  3 |                                           fused_nn_conv3d_3 |    513802240 |      1 |       753.7148 |     681.6932 |              681.6932 |     32 |          Y 
  4 |                                              fused_variance |       235520 |      2 |       107.3560 |       2.1938 |                4.3876 |     32 |            
  5 |                                                  fused_mean |        78720 |      2 |            N/A |          N/A |                   N/A |      0 |            
  6 |                                            fused_variance_1 |      1881920 |      4 |            N/A |          N/A |                   N/A |      0 |            
  7 |                                                fused_mean_1 |       627520 |      4 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                            fused_variance_2 |     12042496 |      4 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                                fused_mean_2 |      4014336 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                            fused_variance_3 |     48169088 |      4 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                                fused_mean_3 |     16056448 |      4 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                            fused_variance_4 |    192675904 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                fused_mean_4 |     64225344 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                            fused_variance_5 |    770703392 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                fused_mean_5 |    256901152 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                         fused_nn_conv3d_add |  55747543040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu |   1284505632 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                       fused_nn_conv3d_add_1 | 111045509120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                       fused_nn_conv3d_add_2 |  55506698240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                       fused_nn_conv3d_add_3 |  27749335040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |            fused_subtract_add_sqrt_divide_add_nn_leaky_relu |     16056576 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                       fused_nn_conv3d_add_4 |   8671040000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                       fused_nn_conv3d_add_5 |   1354830400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_1 |       313920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                       fused_nn_conv3d_add_6 |   1354830400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_1 |       392320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                   fused_nn_conv3d_transpose |   3211264000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                           fused_concatenate |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                       fused_nn_conv3d_add_7 |  21676659200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_2 |      2509120 |      2 |            N/A |          N/A |                   N/A |      0 |            
 31 |                                       fused_nn_conv3d_add_8 |  10838643200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_2 |      3136320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |                                 fused_nn_conv3d_transpose_1 |  20552089600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                         fused_concatenate_1 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                       fused_nn_conv3d_add_9 | 110985297920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |                                      fused_nn_conv3d_add_10 |  55494656000 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_3 |     20070656 |      3 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_conv3d_transpose_2 |  65766686720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                         fused_concatenate_2 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                      fused_nn_conv3d_add_11 | 221978624000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_3 |     64225408 |      2 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                      fused_nn_conv3d_add_12 | 110997340160 |      2 |            N/A |          N/A |                   N/A |      0 |            
 43 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_4 |     80281728 |      2 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                 fused_nn_conv3d_transpose_3 | 131533373440 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                         fused_concatenate_3 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                      fused_nn_conv3d_add_13 | 443989360640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_4 |    256901184 |      2 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                      fused_nn_conv3d_add_14 | 222026792960 |      2 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_5 |    321126464 |      2 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                 fused_nn_conv3d_transpose_4 | 263066746880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                         fused_concatenate_4 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                      fused_nn_conv3d_add_15 | 888107171840 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                      fused_nn_conv3d_add_16 | 444182036480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_5 |   1027604512 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                           fused_nn_conv3d_4 |   2055208960 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 160
Total latency (us): 909.096

2022-05-09 14:50:05.124 INFO Task #4 has finished. Remaining task(s): 51
2022-05-09 14:50:06.113 INFO [Updated] Task #5: "fused_mean"
 ID |                                                        Name |         FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                             fused_nn_conv3d |      5017600 |      1 |       415.8214 |      12.0667 |               12.0667 |     32 |          Y 
  1 |                                           fused_nn_conv3d_1 |     32112640 |      1 |       663.6067 |      48.3911 |               48.3911 |     32 |          Y 
  2 |                                           fused_nn_conv3d_2 |    128450560 |      1 |       790.1843 |     162.5577 |              162.5577 |     32 |          Y 
  3 |                                           fused_nn_conv3d_3 |    513802240 |      1 |       753.7148 |     681.6932 |              681.6932 |     32 |          Y 
  4 |                                              fused_variance |       235520 |      2 |       107.3560 |       2.1938 |                4.3876 |     32 |          Y 
  5 |                                                  fused_mean |        78720 |      2 |        35.6420 |       2.2086 |                4.4173 |     31 |            
  6 |                                            fused_variance_1 |      1881920 |      4 |            N/A |          N/A |                   N/A |      0 |            
  7 |                                                fused_mean_1 |       627520 |      4 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                            fused_variance_2 |     12042496 |      4 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                                fused_mean_2 |      4014336 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                            fused_variance_3 |     48169088 |      4 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                                fused_mean_3 |     16056448 |      4 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                            fused_variance_4 |    192675904 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                fused_mean_4 |     64225344 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                            fused_variance_5 |    770703392 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                fused_mean_5 |    256901152 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                         fused_nn_conv3d_add |  55747543040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu |   1284505632 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                       fused_nn_conv3d_add_1 | 111045509120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                       fused_nn_conv3d_add_2 |  55506698240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                       fused_nn_conv3d_add_3 |  27749335040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |            fused_subtract_add_sqrt_divide_add_nn_leaky_relu |     16056576 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                       fused_nn_conv3d_add_4 |   8671040000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                       fused_nn_conv3d_add_5 |   1354830400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_1 |       313920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                       fused_nn_conv3d_add_6 |   1354830400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_1 |       392320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                   fused_nn_conv3d_transpose |   3211264000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                           fused_concatenate |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                       fused_nn_conv3d_add_7 |  21676659200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_2 |      2509120 |      2 |            N/A |          N/A |                   N/A |      0 |            
 31 |                                       fused_nn_conv3d_add_8 |  10838643200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_2 |      3136320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |                                 fused_nn_conv3d_transpose_1 |  20552089600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                         fused_concatenate_1 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                       fused_nn_conv3d_add_9 | 110985297920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |                                      fused_nn_conv3d_add_10 |  55494656000 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_3 |     20070656 |      3 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_conv3d_transpose_2 |  65766686720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                         fused_concatenate_2 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                      fused_nn_conv3d_add_11 | 221978624000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_3 |     64225408 |      2 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                      fused_nn_conv3d_add_12 | 110997340160 |      2 |            N/A |          N/A |                   N/A |      0 |            
 43 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_4 |     80281728 |      2 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                 fused_nn_conv3d_transpose_3 | 131533373440 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                         fused_concatenate_3 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                      fused_nn_conv3d_add_13 | 443989360640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_4 |    256901184 |      2 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                      fused_nn_conv3d_add_14 | 222026792960 |      2 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_5 |    321126464 |      2 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                 fused_nn_conv3d_transpose_4 | 263066746880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                         fused_concatenate_4 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                      fused_nn_conv3d_add_15 | 888107171840 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                      fused_nn_conv3d_add_16 | 444182036480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_5 |   1027604512 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                           fused_nn_conv3d_4 |   2055208960 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 191
Total latency (us): 913.514

2022-05-09 14:50:06.113 INFO Task #5 has finished. Remaining task(s): 50
2022-05-09 14:50:07.199 INFO [Updated] Task #6: "fused_variance_1"
 ID |                                                        Name |         FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                             fused_nn_conv3d |      5017600 |      1 |       415.8214 |      12.0667 |               12.0667 |     32 |          Y 
  1 |                                           fused_nn_conv3d_1 |     32112640 |      1 |       663.6067 |      48.3911 |               48.3911 |     32 |          Y 
  2 |                                           fused_nn_conv3d_2 |    128450560 |      1 |       790.1843 |     162.5577 |              162.5577 |     32 |          Y 
  3 |                                           fused_nn_conv3d_3 |    513802240 |      1 |       753.7148 |     681.6932 |              681.6932 |     32 |          Y 
  4 |                                              fused_variance |       235520 |      2 |       107.3560 |       2.1938 |                4.3876 |     32 |          Y 
  5 |                                                  fused_mean |        78720 |      2 |        35.6420 |       2.2086 |                4.4173 |     31 |          Y 
  6 |                                            fused_variance_1 |      1881920 |      4 |       591.9538 |       3.1792 |               12.7167 |     31 |            
  7 |                                                fused_mean_1 |       627520 |      4 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                            fused_variance_2 |     12042496 |      4 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                                fused_mean_2 |      4014336 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                            fused_variance_3 |     48169088 |      4 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                                fused_mean_3 |     16056448 |      4 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                            fused_variance_4 |    192675904 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                fused_mean_4 |     64225344 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                            fused_variance_5 |    770703392 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                fused_mean_5 |    256901152 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                         fused_nn_conv3d_add |  55747543040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu |   1284505632 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                       fused_nn_conv3d_add_1 | 111045509120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                       fused_nn_conv3d_add_2 |  55506698240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                       fused_nn_conv3d_add_3 |  27749335040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |            fused_subtract_add_sqrt_divide_add_nn_leaky_relu |     16056576 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                       fused_nn_conv3d_add_4 |   8671040000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                       fused_nn_conv3d_add_5 |   1354830400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_1 |       313920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                       fused_nn_conv3d_add_6 |   1354830400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_1 |       392320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                   fused_nn_conv3d_transpose |   3211264000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                           fused_concatenate |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                       fused_nn_conv3d_add_7 |  21676659200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_2 |      2509120 |      2 |            N/A |          N/A |                   N/A |      0 |            
 31 |                                       fused_nn_conv3d_add_8 |  10838643200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_2 |      3136320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |                                 fused_nn_conv3d_transpose_1 |  20552089600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                         fused_concatenate_1 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                       fused_nn_conv3d_add_9 | 110985297920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |                                      fused_nn_conv3d_add_10 |  55494656000 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_3 |     20070656 |      3 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_conv3d_transpose_2 |  65766686720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                         fused_concatenate_2 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                      fused_nn_conv3d_add_11 | 221978624000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_3 |     64225408 |      2 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                      fused_nn_conv3d_add_12 | 110997340160 |      2 |            N/A |          N/A |                   N/A |      0 |            
 43 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_4 |     80281728 |      2 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                 fused_nn_conv3d_transpose_3 | 131533373440 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                         fused_concatenate_3 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                      fused_nn_conv3d_add_13 | 443989360640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_4 |    256901184 |      2 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                      fused_nn_conv3d_add_14 | 222026792960 |      2 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_5 |    321126464 |      2 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                 fused_nn_conv3d_transpose_4 | 263066746880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                         fused_concatenate_4 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                      fused_nn_conv3d_add_15 | 888107171840 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                      fused_nn_conv3d_add_16 | 444182036480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_5 |   1027604512 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                           fused_nn_conv3d_4 |   2055208960 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 222
Total latency (us): 926.23

2022-05-09 14:50:07.200 INFO Task #6 has finished. Remaining task(s): 49
2022-05-09 14:50:08.337 INFO [Updated] Task #7: "fused_mean_1"
 ID |                                                        Name |         FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                             fused_nn_conv3d |      5017600 |      1 |       415.8214 |      12.0667 |               12.0667 |     32 |          Y 
  1 |                                           fused_nn_conv3d_1 |     32112640 |      1 |       663.6067 |      48.3911 |               48.3911 |     32 |          Y 
  2 |                                           fused_nn_conv3d_2 |    128450560 |      1 |       790.1843 |     162.5577 |              162.5577 |     32 |          Y 
  3 |                                           fused_nn_conv3d_3 |    513802240 |      1 |       753.7148 |     681.6932 |              681.6932 |     32 |          Y 
  4 |                                              fused_variance |       235520 |      2 |       107.3560 |       2.1938 |                4.3876 |     32 |          Y 
  5 |                                                  fused_mean |        78720 |      2 |        35.6420 |       2.2086 |                4.4173 |     31 |          Y 
  6 |                                            fused_variance_1 |      1881920 |      4 |       591.9538 |       3.1792 |               12.7167 |     31 |          Y 
  7 |                                                fused_mean_1 |       627520 |      4 |       197.4302 |       3.1784 |               12.7138 |     32 |            
  8 |                                            fused_variance_2 |     12042496 |      4 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                                fused_mean_2 |      4014336 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                            fused_variance_3 |     48169088 |      4 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                                fused_mean_3 |     16056448 |      4 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                            fused_variance_4 |    192675904 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                fused_mean_4 |     64225344 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                            fused_variance_5 |    770703392 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                fused_mean_5 |    256901152 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                         fused_nn_conv3d_add |  55747543040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu |   1284505632 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                       fused_nn_conv3d_add_1 | 111045509120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                       fused_nn_conv3d_add_2 |  55506698240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                       fused_nn_conv3d_add_3 |  27749335040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |            fused_subtract_add_sqrt_divide_add_nn_leaky_relu |     16056576 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                       fused_nn_conv3d_add_4 |   8671040000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                       fused_nn_conv3d_add_5 |   1354830400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_1 |       313920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                       fused_nn_conv3d_add_6 |   1354830400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_1 |       392320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                   fused_nn_conv3d_transpose |   3211264000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                           fused_concatenate |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                       fused_nn_conv3d_add_7 |  21676659200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_2 |      2509120 |      2 |            N/A |          N/A |                   N/A |      0 |            
 31 |                                       fused_nn_conv3d_add_8 |  10838643200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_2 |      3136320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |                                 fused_nn_conv3d_transpose_1 |  20552089600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                         fused_concatenate_1 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                       fused_nn_conv3d_add_9 | 110985297920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |                                      fused_nn_conv3d_add_10 |  55494656000 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_3 |     20070656 |      3 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_conv3d_transpose_2 |  65766686720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                         fused_concatenate_2 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                      fused_nn_conv3d_add_11 | 221978624000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_3 |     64225408 |      2 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                      fused_nn_conv3d_add_12 | 110997340160 |      2 |            N/A |          N/A |                   N/A |      0 |            
 43 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_4 |     80281728 |      2 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                 fused_nn_conv3d_transpose_3 | 131533373440 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                         fused_concatenate_3 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                      fused_nn_conv3d_add_13 | 443989360640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_4 |    256901184 |      2 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                      fused_nn_conv3d_add_14 | 222026792960 |      2 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_5 |    321126464 |      2 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                 fused_nn_conv3d_transpose_4 | 263066746880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                         fused_concatenate_4 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                      fused_nn_conv3d_add_15 | 888107171840 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                      fused_nn_conv3d_add_16 | 444182036480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_5 |   1027604512 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                           fused_nn_conv3d_4 |   2055208960 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 254
Total latency (us): 938.944

2022-05-09 14:50:08.337 INFO Task #7 has finished. Remaining task(s): 48
2022-05-09 14:50:09.552 INFO [Updated] Task #8: "fused_variance_2"
 ID |                                                        Name |         FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                             fused_nn_conv3d |      5017600 |      1 |       415.8214 |      12.0667 |               12.0667 |     32 |          Y 
  1 |                                           fused_nn_conv3d_1 |     32112640 |      1 |       663.6067 |      48.3911 |               48.3911 |     32 |          Y 
  2 |                                           fused_nn_conv3d_2 |    128450560 |      1 |       790.1843 |     162.5577 |              162.5577 |     32 |          Y 
  3 |                                           fused_nn_conv3d_3 |    513802240 |      1 |       753.7148 |     681.6932 |              681.6932 |     32 |          Y 
  4 |                                              fused_variance |       235520 |      2 |       107.3560 |       2.1938 |                4.3876 |     32 |          Y 
  5 |                                                  fused_mean |        78720 |      2 |        35.6420 |       2.2086 |                4.4173 |     31 |          Y 
  6 |                                            fused_variance_1 |      1881920 |      4 |       591.9538 |       3.1792 |               12.7167 |     31 |          Y 
  7 |                                                fused_mean_1 |       627520 |      4 |       197.4302 |       3.1784 |               12.7138 |     32 |          Y 
  8 |                                            fused_variance_2 |     12042496 |      4 |       298.0568 |      40.4034 |              161.6135 |     31 |            
  9 |                                                fused_mean_2 |      4014336 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                            fused_variance_3 |     48169088 |      4 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                                fused_mean_3 |     16056448 |      4 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                            fused_variance_4 |    192675904 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                fused_mean_4 |     64225344 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                            fused_variance_5 |    770703392 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                fused_mean_5 |    256901152 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                         fused_nn_conv3d_add |  55747543040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu |   1284505632 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                       fused_nn_conv3d_add_1 | 111045509120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                       fused_nn_conv3d_add_2 |  55506698240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                       fused_nn_conv3d_add_3 |  27749335040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |            fused_subtract_add_sqrt_divide_add_nn_leaky_relu |     16056576 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                       fused_nn_conv3d_add_4 |   8671040000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                       fused_nn_conv3d_add_5 |   1354830400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_1 |       313920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                       fused_nn_conv3d_add_6 |   1354830400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_1 |       392320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                   fused_nn_conv3d_transpose |   3211264000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                           fused_concatenate |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                       fused_nn_conv3d_add_7 |  21676659200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_2 |      2509120 |      2 |            N/A |          N/A |                   N/A |      0 |            
 31 |                                       fused_nn_conv3d_add_8 |  10838643200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_2 |      3136320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |                                 fused_nn_conv3d_transpose_1 |  20552089600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                         fused_concatenate_1 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                       fused_nn_conv3d_add_9 | 110985297920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |                                      fused_nn_conv3d_add_10 |  55494656000 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_3 |     20070656 |      3 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_conv3d_transpose_2 |  65766686720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                         fused_concatenate_2 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                      fused_nn_conv3d_add_11 | 221978624000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_3 |     64225408 |      2 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                      fused_nn_conv3d_add_12 | 110997340160 |      2 |            N/A |          N/A |                   N/A |      0 |            
 43 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_4 |     80281728 |      2 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                 fused_nn_conv3d_transpose_3 | 131533373440 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                         fused_concatenate_3 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                      fused_nn_conv3d_add_13 | 443989360640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_4 |    256901184 |      2 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                      fused_nn_conv3d_add_14 | 222026792960 |      2 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_5 |    321126464 |      2 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                 fused_nn_conv3d_transpose_4 | 263066746880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                         fused_concatenate_4 |            1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                      fused_nn_conv3d_add_15 | 888107171840 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                      fused_nn_conv3d_add_16 | 444182036480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |          fused_subtract_add_sqrt_divide_add_nn_leaky_relu_5 |   1027604512 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                           fused_nn_conv3d_4 |   2055208960 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 285
Total latency (us): 1100.56

2022-05-09 14:50:09.552 INFO Task #8 has finished. Remaining task(s): 47
2022-05-09 14:50:09.552 INFO Task #9 has finished. Remaining task(s): 46
2022-05-09 14:50:09.552 INFO Task #10 has finished. Remaining task(s): 45
2022-05-09 14:50:09.553 INFO Task #11 has finished. Remaining task(s): 44
2022-05-09 14:50:09.553 INFO Task #12 has finished. Remaining task(s): 43
2022-05-09 14:50:09.553 INFO Task #13 has finished. Remaining task(s): 42
2022-05-09 14:50:09.553 INFO Task #14 has finished. Remaining task(s): 41
2022-05-09 14:50:09.553 INFO Task #15 has finished. Remaining task(s): 40
2022-05-09 14:50:09.553 INFO Task #16 has finished. Remaining task(s): 39
2022-05-09 14:50:09.553 INFO Task #17 has finished. Remaining task(s): 38
2022-05-09 14:50:09.553 INFO Task #18 has finished. Remaining task(s): 37
2022-05-09 14:50:09.553 INFO Task #19 has finished. Remaining task(s): 36
2022-05-09 14:50:09.553 INFO Task #20 has finished. Remaining task(s): 35
2022-05-09 14:50:09.554 INFO Task #21 has finished. Remaining task(s): 34
2022-05-09 14:50:09.554 INFO Task #22 has finished. Remaining task(s): 33
2022-05-09 14:50:09.554 INFO Task #23 has finished. Remaining task(s): 32
2022-05-09 14:50:09.554 INFO Task #24 has finished. Remaining task(s): 31
2022-05-09 14:50:09.554 INFO Task #25 has finished. Remaining task(s): 30
2022-05-09 14:50:09.554 INFO Task #26 has finished. Remaining task(s): 29
2022-05-09 14:50:09.554 INFO Task #27 has finished. Remaining task(s): 28
2022-05-09 14:50:09.567 INFO Task #28 has finished. Remaining task(s): 27
2022-05-09 14:50:09.567 INFO Task #29 has finished. Remaining task(s): 26
2022-05-09 14:50:09.568 INFO Task #30 has finished. Remaining task(s): 25
2022-05-09 14:50:09.568 INFO Task #31 has finished. Remaining task(s): 24
2022-05-09 14:50:09.568 INFO Task #32 has finished. Remaining task(s): 23
2022-05-09 14:50:09.568 INFO Task #33 has finished. Remaining task(s): 22
2022-05-09 14:50:09.568 INFO Task #34 has finished. Remaining task(s): 21
2022-05-09 14:50:09.568 INFO Task #35 has finished. Remaining task(s): 20
2022-05-09 14:50:09.568 INFO Task #36 has finished. Remaining task(s): 19
2022-05-09 14:50:09.568 INFO Task #37 has finished. Remaining task(s): 18
2022-05-09 14:50:09.568 INFO Task #38 has finished. Remaining task(s): 17
2022-05-09 14:50:09.569 INFO Task #39 has finished. Remaining task(s): 16
2022-05-09 14:50:09.569 INFO Task #40 has finished. Remaining task(s): 15
2022-05-09 14:50:09.569 INFO Task #41 has finished. Remaining task(s): 14
2022-05-09 14:50:09.569 INFO Task #42 has finished. Remaining task(s): 13
2022-05-09 14:50:09.569 INFO Task #43 has finished. Remaining task(s): 12
2022-05-09 14:50:09.569 INFO Task #44 has finished. Remaining task(s): 11
2022-05-09 14:50:09.569 INFO Task #45 has finished. Remaining task(s): 10
2022-05-09 14:50:09.569 INFO Task #46 has finished. Remaining task(s): 9
2022-05-09 14:50:09.569 INFO Task #47 has finished. Remaining task(s): 8
2022-05-09 14:50:09.569 INFO Task #48 has finished. Remaining task(s): 7
2022-05-09 14:50:09.570 INFO Task #49 has finished. Remaining task(s): 6
2022-05-09 14:50:09.570 INFO Task #50 has finished. Remaining task(s): 5
2022-05-09 14:50:09.570 INFO Task #51 has finished. Remaining task(s): 4
2022-05-09 14:50:09.570 INFO Task #52 has finished. Remaining task(s): 3
2022-05-09 14:50:09.570 INFO Task #53 has finished. Remaining task(s): 2
2022-05-09 14:50:09.570 INFO Task #54 has finished. Remaining task(s): 1
2022-05-09 14:50:09.570 INFO Task #55 has finished. Remaining task(s): 0
2022-05-09 14:50:09.694 INFO Saved XGBModel to /tmp/tmp_l76u12e/cost_model.xgb
2022-05-09 14:50:26.523 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d_add
2022-05-09 14:50:26.941 WARNING Cannot find workload: tvmgen_default_fused_mean
2022-05-09 14:50:26.970 WARNING Cannot find workload: tvmgen_default_fused_variance
2022-05-09 14:50:27.051 WARNING Cannot find workload: tvmgen_default_fused_subtract_add_sqrt_divide_add_nn_leaky_relu
2022-05-09 14:50:27.165 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d_add_1
2022-05-09 14:50:27.331 WARNING Cannot find workload: tvmgen_default_fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu
2022-05-09 14:50:27.441 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d_add_2
2022-05-09 14:50:27.522 WARNING Cannot find workload: tvmgen_default_fused_mean_1
2022-05-09 14:50:27.557 WARNING Cannot find workload: tvmgen_default_fused_variance_1
2022-05-09 14:50:27.646 WARNING Cannot find workload: tvmgen_default_fused_subtract_add_sqrt_divide_add_nn_leaky_relu_1
2022-05-09 14:50:27.761 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d_add_3
2022-05-09 14:50:27.925 WARNING Cannot find workload: tvmgen_default_fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_1
2022-05-09 14:50:28.035 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d_add_4
2022-05-09 14:50:28.116 WARNING Cannot find workload: tvmgen_default_fused_mean_2
2022-05-09 14:50:28.150 WARNING Cannot find workload: tvmgen_default_fused_variance_2
2022-05-09 14:50:28.239 WARNING Cannot find workload: tvmgen_default_fused_subtract_add_sqrt_divide_add_nn_leaky_relu_2
2022-05-09 14:50:28.360 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d_add_5
2022-05-09 14:50:28.524 WARNING Cannot find workload: tvmgen_default_fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_2
2022-05-09 14:50:28.643 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d_add_6
2022-05-09 14:50:28.724 WARNING Cannot find workload: tvmgen_default_fused_mean_3
[14:50:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28, 20), "float32"], placeholder_1: T.Buffer[(1, 256, 1, 1, 1), "float32"], T_divide: T.Buffer[(1, 256, 1, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_subtract = T.alloc_buffer([1, 256, 28, 28, 20], dtype="float32")
        T_multiply = T.alloc_buffer([1, 256, 28, 28, 20], dtype="float32")
        T_multiply_red = T.alloc_buffer([1, 256, 1, 1, 1], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 28, 28, 20):
            with T.block("T_subtract"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[0, ax1, ax2, ax3, ax4], placeholder_1[0, ax1, 0, 0, 0])
                T.writes(T_subtract[ax0, ax1, ax2, ax3, ax4])
                T_subtract[ax0, ax1, ax2, ax3, ax4] = placeholder[0, ax1, ax2, ax3, ax4] - placeholder_1[0, ax1, 0, 0, 0]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 28, 28, 20):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_subtract[0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = T_subtract[0, ax1, ax2, ax3, ax4] * T_subtract[0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 256, 1, 1, 1, 28, 28, 20):
            with T.block("T_multiply_red"):
                ax0, ax1, ax2, ax3, ax4, k2, k3, k4 = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(T_multiply[0, ax1, k2, k3, k4])
                T.writes(T_multiply_red[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    T_multiply_red[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
                T_multiply_red[ax0, ax1, ax2, ax3, ax4] = T_multiply_red[ax0, ax1, ax2, ax3, ax4] + T_multiply[0, ax1, k2, k3, k4]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 1, 1, 1):
            with T.block("T_divide"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_red[0, ax1, 0, 0, 0])
                T.writes(T_divide[ax0, ax1, ax2, ax3, ax4])
                T_divide[ax0, ax1, ax2, ax3, ax4] = T_multiply_red[0, ax1, 0, 0, 0] * T.float32(6.3775510204081638e-05)
    

[14:50:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: best replacement TIR found for the above:

[14:50:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_variance_3(placeholder: T.Buffer[(1, 256, 28, 28, 20), "float32"], placeholder_1: T.Buffer[(1, 256, 1, 1, 1), "float32"], T_divide: T.Buffer[(1, 256, 1, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply_red_shared = T.alloc_buffer([1, 256, 1, 1, 1], dtype="float32", scope="shared")
        for i0_i1_i2_i3_i4_0_fused in T.thread_binding(256, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3, ax4, ax5_ax6_ax7_fused_0 in T.grid(1, 1, 1, 1, 1, 245):
                for ax5_ax6_ax7_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                    with T.block("T_multiply_red"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(256, i0_i1_i2_i3_i4_0_fused)
                        ax2_1 = T.axis.spatial(1, 0)
                        ax3_1 = T.axis.spatial(1, 0)
                        ax4_1 = T.axis.spatial(1, 0)
                        k2 = T.axis.reduce(28, (ax5_ax6_ax7_fused_0 * 64 + ax5_ax6_ax7_fused_1) // 560)
                        k3 = T.axis.reduce(28, (ax5_ax6_ax7_fused_0 * 64 + ax5_ax6_ax7_fused_1) % 560 // 20)
                        k4 = T.axis.reduce(20, (ax5_ax6_ax7_fused_0 * 64 + ax5_ax6_ax7_fused_1) % 20)
                        T.reads(placeholder[0, ax1_1, k2, k3, k4], placeholder_1[0, ax1_1, 0, 0, 0])
                        T.writes(T_multiply_red_shared[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        with T.init():
                            T_multiply_red_shared[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.float32(0)
                        T_multiply_red_shared[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T_multiply_red_shared[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + (placeholder[0, ax1_1, k2, k3, k4] - placeholder_1[0, ax1_1, 0, 0, 0]) * (placeholder[0, ax1_1, k2, k3, k4] - placeholder_1[0, ax1_1, 0, 0, 0])
            for i4_1 in T.thread_binding(64, thread="threadIdx.x"):
                with T.block("T_divide"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(256, i0_i1_i2_i3_i4_0_fused)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    ax4 = T.axis.spatial(1, 0)
                    T.where(i4_1 < 1)
                    T.reads(T_multiply_red_shared[0, ax1, 0, 0, 0])
                    T.writes(T_divide[ax0, ax1, ax2, ax3, ax4])
                    T_divide[ax0, ax1, ax2, ax3, ax4] = T_multiply_red_shared[0, ax1, 0, 0, 0] * T.float32(6.3775510204081638e-05)
    

[14:50:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: Printing out trace for AHB:
[14:50:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:127: ["b0 = sch.get_block(name=\"T_subtract\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_multiply\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_multiply_red\", func_name=\"main\")", "b3 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b1)", "sch.compute_inline(block=b0)", "b4, = sch.get_consumers(block=b2)", "l5, l6, l7, l8, l9 = sch.get_loops(block=b4)", "v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=4)", "l11, l12 = sch.split(loop=l9, factors=[None, v10])", "sch.bind(loop=l12, thread_axis=\"threadIdx.x\")", "sch.compute_at(block=b2, loop=l11, preserve_unit_loops=True)", "sch.set_scope(block=b2, buffer_index=0, storage_scope=\"shared\")", "l13, l14, l15, l16, l17, l18, l19, l20, l21, l22, l23, l24, l25 = sch.get_loops(block=b2)", "l26 = sch.fuse(l23, l24, l25)", "l27, l28 = sch.split(loop=l26, factors=[None, v10])", "sch.bind(loop=l28, thread_axis=\"threadIdx.x\")", "v29 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v29)", "sch.enter_postproc()", "b30 = sch.get_block(name=\"T_multiply_red\", func_name=\"main\")", "l31, l32, l33, l34, l35, l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b30)", "l43 = sch.fuse(l31, l32, l33, l34, l35)", "sch.bind(loop=l43, thread_axis=\"blockIdx.x\")", "b44 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b44, ann_key=\"meta_schedule.unroll_explicit\")", "b45, b46 = sch.get_child_blocks(b44)", "l47, l48, l49, l50, l51, l52, l53, l54 = sch.get_loops(block=b45)", "sch.annotate(block_or_loop=l47, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l47, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l55, l56 = sch.get_loops(block=b46)", "sch.annotate(block_or_loop=l55, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l55, ann_key=\"pragma_unroll_explicit\", ann_val=1)"]
2022-05-09 14:50:29.349 WARNING Cannot find workload: tvmgen_default_fused_subtract_add_sqrt_divide_add_nn_leaky_relu_3
2022-05-09 14:50:29.470 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d_add_7
2022-05-09 14:50:29.637 WARNING Cannot find workload: tvmgen_default_fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_3
2022-05-09 14:50:29.756 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d_add_8
[14:50:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 14, 14, 10), "float32"], T_divide: T.Buffer[(1, 320, 1, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        placeholder_red = T.alloc_buffer([1, 320, 1, 1, 1], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 320, 1, 1, 1, 14, 14, 10):
            with T.block("placeholder_red"):
                ax0, ax1, ax2, ax3, ax4, k2, k3, k4 = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[0, ax1, k2, k3, k4])
                T.writes(placeholder_red[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    placeholder_red[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
                placeholder_red[ax0, ax1, ax2, ax3, ax4] = placeholder_red[ax0, ax1, ax2, ax3, ax4] + placeholder[0, ax1, k2, k3, k4]
        for i0, i1, i2, i3, i4 in T.grid(1, 320, 1, 1, 1):
            with T.block("T_divide"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_red[0, ax1, 0, 0, 0])
                T.writes(T_divide[ax0, ax1, ax2, ax3, ax4])
                T_divide[ax0, ax1, ax2, ax3, ax4] = placeholder_red[0, ax1, 0, 0, 0] * T.float32(0.00051020408163265311)
    

[14:50:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: best replacement TIR found for the above:

[14:50:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_mean_4(placeholder: T.Buffer[(1, 320, 14, 14, 10), "float32"], T_divide: T.Buffer[(1, 320, 1, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        placeholder_red_shared = T.alloc_buffer([1, 320, 1, 1, 1], dtype="float32", scope="shared")
        for i0_i1_i2_i3_i4_0_fused in T.thread_binding(320, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3, ax4, ax5_ax6_ax7_fused_0 in T.grid(1, 1, 1, 1, 1, 31):
                for ax5_ax6_ax7_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                    with T.block("placeholder_red"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(320, i0_i1_i2_i3_i4_0_fused)
                        ax2_1 = T.axis.spatial(1, 0)
                        ax3_1 = T.axis.spatial(1, 0)
                        ax4_1 = T.axis.spatial(1, 0)
                        k2 = T.axis.reduce(14, (ax5_ax6_ax7_fused_0 * 64 + ax5_ax6_ax7_fused_1) // 140)
                        k3 = T.axis.reduce(14, (ax5_ax6_ax7_fused_0 * 64 + ax5_ax6_ax7_fused_1) % 140 // 10)
                        k4 = T.axis.reduce(10, (ax5_ax6_ax7_fused_0 * 64 + ax5_ax6_ax7_fused_1) % 10)
                        T.where(ax5_ax6_ax7_fused_0 * 64 + ax5_ax6_ax7_fused_1 < 1960)
                        T.reads(placeholder[0, ax1_1, k2, k3, k4])
                        T.writes(placeholder_red_shared[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        with T.init():
                            placeholder_red_shared[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.float32(0)
                        placeholder_red_shared[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = placeholder_red_shared[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder[0, ax1_1, k2, k3, k4]
            for i4_1 in T.thread_binding(64, thread="threadIdx.x"):
                with T.block("T_divide"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(320, i0_i1_i2_i3_i4_0_fused)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    ax4 = T.axis.spatial(1, 0)
                    T.where(i4_1 < 1)
                    T.reads(placeholder_red_shared[0, ax1, 0, 0, 0])
                    T.writes(T_divide[ax0, ax1, ax2, ax3, ax4])
                    T_divide[ax0, ax1, ax2, ax3, ax4] = placeholder_red_shared[0, ax1, 0, 0, 0] * T.float32(0.00051020408163265311)
    

[14:50:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: Printing out trace for AHB:
[14:50:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:127: ["b0 = sch.get_block(name=\"placeholder_red\", func_name=\"main\")", "b1 = sch.get_block(name=\"root\", func_name=\"main\")", "b2, = sch.get_consumers(block=b0)", "l3, l4, l5, l6, l7 = sch.get_loops(block=b2)", "v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=4)", "l9, l10 = sch.split(loop=l7, factors=[None, v8])", "sch.bind(loop=l10, thread_axis=\"threadIdx.x\")", "sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True)", "sch.set_scope(block=b0, buffer_index=0, storage_scope=\"shared\")", "l11, l12, l13, l14, l15, l16, l17, l18, l19, l20, l21, l22, l23 = sch.get_loops(block=b0)", "l24 = sch.fuse(l21, l22, l23)", "l25, l26 = sch.split(loop=l24, factors=[None, v8])", "sch.bind(loop=l26, thread_axis=\"threadIdx.x\")", "v27 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v27)", "sch.enter_postproc()", "b28 = sch.get_block(name=\"placeholder_red\", func_name=\"main\")", "l29, l30, l31, l32, l33, l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b28)", "l41 = sch.fuse(l29, l30, l31, l32, l33)", "sch.bind(loop=l41, thread_axis=\"blockIdx.x\")", "b42 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b42, ann_key=\"meta_schedule.unroll_explicit\")", "b43, b44 = sch.get_child_blocks(b42)", "l45, l46, l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b43)", "sch.annotate(block_or_loop=l45, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l45, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l53, l54 = sch.get_loops(block=b44)", "sch.annotate(block_or_loop=l53, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l53, ann_key=\"pragma_unroll_explicit\", ann_val=1)"]
[14:50:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 14, 14, 10), "float32"], placeholder_1: T.Buffer[(1, 320, 1, 1, 1), "float32"], T_divide: T.Buffer[(1, 320, 1, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_subtract = T.alloc_buffer([1, 320, 14, 14, 10], dtype="float32")
        T_multiply = T.alloc_buffer([1, 320, 14, 14, 10], dtype="float32")
        T_multiply_red = T.alloc_buffer([1, 320, 1, 1, 1], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 320, 14, 14, 10):
            with T.block("T_subtract"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[0, ax1, ax2, ax3, ax4], placeholder_1[0, ax1, 0, 0, 0])
                T.writes(T_subtract[ax0, ax1, ax2, ax3, ax4])
                T_subtract[ax0, ax1, ax2, ax3, ax4] = placeholder[0, ax1, ax2, ax3, ax4] - placeholder_1[0, ax1, 0, 0, 0]
        for i0, i1, i2, i3, i4 in T.grid(1, 320, 14, 14, 10):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_subtract[0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = T_subtract[0, ax1, ax2, ax3, ax4] * T_subtract[0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 320, 1, 1, 1, 14, 14, 10):
            with T.block("T_multiply_red"):
                ax0, ax1, ax2, ax3, ax4, k2, k3, k4 = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(T_multiply[0, ax1, k2, k3, k4])
                T.writes(T_multiply_red[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    T_multiply_red[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
                T_multiply_red[ax0, ax1, ax2, ax3, ax4] = T_multiply_red[ax0, ax1, ax2, ax3, ax4] + T_multiply[0, ax1, k2, k3, k4]
        for i0, i1, i2, i3, i4 in T.grid(1, 320, 1, 1, 1):
            with T.block("T_divide"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_red[0, ax1, 0, 0, 0])
                T.writes(T_divide[ax0, ax1, ax2, ax3, ax4])
                T_divide[ax0, ax1, ax2, ax3, ax4] = T_multiply_red[0, ax1, 0, 0, 0] * T.float32(0.00051020408163265311)
    

[14:50:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: best replacement TIR found for the above:

[14:50:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_variance_4(placeholder: T.Buffer[(1, 320, 14, 14, 10), "float32"], placeholder_1: T.Buffer[(1, 320, 1, 1, 1), "float32"], T_divide: T.Buffer[(1, 320, 1, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply_red_shared = T.alloc_buffer([1, 320, 1, 1, 1], dtype="float32", scope="shared")
        for i0_i1_i2_i3_i4_0_fused in T.thread_binding(320, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3, ax4, ax5_ax6_ax7_fused_0 in T.grid(1, 1, 1, 1, 1, 62):
                for ax5_ax6_ax7_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                    with T.block("T_multiply_red"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(320, i0_i1_i2_i3_i4_0_fused)
                        ax2_1 = T.axis.spatial(1, 0)
                        ax3_1 = T.axis.spatial(1, 0)
                        ax4_1 = T.axis.spatial(1, 0)
                        k2 = T.axis.reduce(14, (ax5_ax6_ax7_fused_0 * 32 + ax5_ax6_ax7_fused_1) // 140)
                        k3 = T.axis.reduce(14, (ax5_ax6_ax7_fused_0 * 32 + ax5_ax6_ax7_fused_1) % 140 // 10)
                        k4 = T.axis.reduce(10, (ax5_ax6_ax7_fused_0 * 32 + ax5_ax6_ax7_fused_1) % 10)
                        T.where(ax5_ax6_ax7_fused_0 * 32 + ax5_ax6_ax7_fused_1 < 1960)
                        T.reads(placeholder[0, ax1_1, k2, k3, k4], placeholder_1[0, ax1_1, 0, 0, 0])
                        T.writes(T_multiply_red_shared[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        with T.init():
                            T_multiply_red_shared[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.float32(0)
                        T_multiply_red_shared[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T_multiply_red_shared[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + (placeholder[0, ax1_1, k2, k3, k4] - placeholder_1[0, ax1_1, 0, 0, 0]) * (placeholder[0, ax1_1, k2, k3, k4] - placeholder_1[0, ax1_1, 0, 0, 0])
            for i4_1 in T.thread_binding(32, thread="threadIdx.x"):
                with T.block("T_divide"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(320, i0_i1_i2_i3_i4_0_fused)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    ax4 = T.axis.spatial(1, 0)
                    T.where(i4_1 < 1)
                    T.reads(T_multiply_red_shared[0, ax1, 0, 0, 0])
                    T.writes(T_divide[ax0, ax1, ax2, ax3, ax4])
                    T_divide[ax0, ax1, ax2, ax3, ax4] = T_multiply_red_shared[0, ax1, 0, 0, 0] * T.float32(0.00051020408163265311)
    

[14:50:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: Printing out trace for AHB:
[14:50:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:127: ["b0 = sch.get_block(name=\"T_subtract\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_multiply\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_multiply_red\", func_name=\"main\")", "b3 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b1)", "sch.compute_inline(block=b0)", "b4, = sch.get_consumers(block=b2)", "l5, l6, l7, l8, l9 = sch.get_loops(block=b4)", "v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=3)", "l11, l12 = sch.split(loop=l9, factors=[None, v10])", "sch.bind(loop=l12, thread_axis=\"threadIdx.x\")", "sch.compute_at(block=b2, loop=l11, preserve_unit_loops=True)", "sch.set_scope(block=b2, buffer_index=0, storage_scope=\"shared\")", "l13, l14, l15, l16, l17, l18, l19, l20, l21, l22, l23, l24, l25 = sch.get_loops(block=b2)", "l26 = sch.fuse(l23, l24, l25)", "l27, l28 = sch.split(loop=l26, factors=[None, v10])", "sch.bind(loop=l28, thread_axis=\"threadIdx.x\")", "v29 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v29)", "sch.enter_postproc()", "b30 = sch.get_block(name=\"T_multiply_red\", func_name=\"main\")", "l31, l32, l33, l34, l35, l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b30)", "l43 = sch.fuse(l31, l32, l33, l34, l35)", "sch.bind(loop=l43, thread_axis=\"blockIdx.x\")", "b44 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b44, ann_key=\"meta_schedule.unroll_explicit\")", "b45, b46 = sch.get_child_blocks(b44)", "l47, l48, l49, l50, l51, l52, l53, l54 = sch.get_loops(block=b45)", "sch.annotate(block_or_loop=l47, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l47, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l55, l56 = sch.get_loops(block=b46)", "sch.annotate(block_or_loop=l55, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l55, ann_key=\"pragma_unroll_explicit\", ann_val=1)"]
2022-05-09 14:50:30.119 WARNING Cannot find workload: tvmgen_default_fused_subtract_add_sqrt_divide_add_nn_leaky_relu_4
2022-05-09 14:50:30.242 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d_add_9
2022-05-09 14:50:30.409 WARNING Cannot find workload: tvmgen_default_fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_4
2022-05-09 14:50:30.527 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d_add_10
[14:50:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 7, 7, 5), "float32"], T_divide: T.Buffer[(1, 320, 1, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        placeholder_red = T.alloc_buffer([1, 320, 1, 1, 1], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 320, 1, 1, 1, 7, 7, 5):
            with T.block("placeholder_red"):
                ax0, ax1, ax2, ax3, ax4, k2, k3, k4 = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[0, ax1, k2, k3, k4])
                T.writes(placeholder_red[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    placeholder_red[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
                placeholder_red[ax0, ax1, ax2, ax3, ax4] = placeholder_red[ax0, ax1, ax2, ax3, ax4] + placeholder[0, ax1, k2, k3, k4]
        for i0, i1, i2, i3, i4 in T.grid(1, 320, 1, 1, 1):
            with T.block("T_divide"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_red[0, ax1, 0, 0, 0])
                T.writes(T_divide[ax0, ax1, ax2, ax3, ax4])
                T_divide[ax0, ax1, ax2, ax3, ax4] = placeholder_red[0, ax1, 0, 0, 0] * T.float32(0.0040816326530612249)
    

[14:50:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: best replacement TIR found for the above:

[14:50:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_mean_5(placeholder: T.Buffer[(1, 320, 7, 7, 5), "float32"], T_divide: T.Buffer[(1, 320, 1, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        placeholder_red_shared = T.alloc_buffer([1, 320, 1, 1, 1], dtype="float32", scope="shared")
        for i0_i1_i2_i3_i4_0_fused in T.thread_binding(320, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3, ax4, ax5_ax6_ax7_fused_0 in T.grid(1, 1, 1, 1, 1, 31):
                for ax5_ax6_ax7_fused_1 in T.thread_binding(8, thread="threadIdx.x"):
                    with T.block("placeholder_red"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(320, i0_i1_i2_i3_i4_0_fused)
                        ax2_1 = T.axis.spatial(1, 0)
                        ax3_1 = T.axis.spatial(1, 0)
                        ax4_1 = T.axis.spatial(1, 0)
                        k2 = T.axis.reduce(7, (ax5_ax6_ax7_fused_0 * 8 + ax5_ax6_ax7_fused_1) // 35)
                        k3 = T.axis.reduce(7, (ax5_ax6_ax7_fused_0 * 8 + ax5_ax6_ax7_fused_1) % 35 // 5)
                        k4 = T.axis.reduce(5, (ax5_ax6_ax7_fused_0 * 8 + ax5_ax6_ax7_fused_1) % 5)
                        T.where(ax5_ax6_ax7_fused_0 * 8 + ax5_ax6_ax7_fused_1 < 245)
                        T.reads(placeholder[0, ax1_1, k2, k3, k4])
                        T.writes(placeholder_red_shared[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        with T.init():
                            placeholder_red_shared[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.float32(0)
                        placeholder_red_shared[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = placeholder_red_shared[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder[0, ax1_1, k2, k3, k4]
            for i4_1 in T.thread_binding(8, thread="threadIdx.x"):
                with T.block("T_divide"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(320, i0_i1_i2_i3_i4_0_fused)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    ax4 = T.axis.spatial(1, 0)
                    T.where(i4_1 < 1)
                    T.reads(placeholder_red_shared[0, ax1, 0, 0, 0])
                    T.writes(T_divide[ax0, ax1, ax2, ax3, ax4])
                    T_divide[ax0, ax1, ax2, ax3, ax4] = placeholder_red_shared[0, ax1, 0, 0, 0] * T.float32(0.0040816326530612249)
    

[14:50:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: Printing out trace for AHB:
[14:50:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:127: ["b0 = sch.get_block(name=\"placeholder_red\", func_name=\"main\")", "b1 = sch.get_block(name=\"root\", func_name=\"main\")", "b2, = sch.get_consumers(block=b0)", "l3, l4, l5, l6, l7 = sch.get_loops(block=b2)", "v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=1)", "l9, l10 = sch.split(loop=l7, factors=[None, v8])", "sch.bind(loop=l10, thread_axis=\"threadIdx.x\")", "sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True)", "sch.set_scope(block=b0, buffer_index=0, storage_scope=\"shared\")", "l11, l12, l13, l14, l15, l16, l17, l18, l19, l20, l21, l22, l23 = sch.get_loops(block=b0)", "l24 = sch.fuse(l21, l22, l23)", "l25, l26 = sch.split(loop=l24, factors=[None, v8])", "sch.bind(loop=l26, thread_axis=\"threadIdx.x\")", "v27 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v27)", "sch.enter_postproc()", "b28 = sch.get_block(name=\"placeholder_red\", func_name=\"main\")", "l29, l30, l31, l32, l33, l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b28)", "l41 = sch.fuse(l29, l30, l31, l32, l33)", "sch.bind(loop=l41, thread_axis=\"blockIdx.x\")", "b42 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b42, ann_key=\"meta_schedule.unroll_explicit\")", "b43, b44 = sch.get_child_blocks(b42)", "l45, l46, l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b43)", "sch.annotate(block_or_loop=l45, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l45, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l53, l54 = sch.get_loops(block=b44)", "sch.annotate(block_or_loop=l53, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l53, ann_key=\"pragma_unroll_explicit\", ann_val=1)"]
[14:50:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 7, 7, 5), "float32"], placeholder_1: T.Buffer[(1, 320, 1, 1, 1), "float32"], T_divide: T.Buffer[(1, 320, 1, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_subtract = T.alloc_buffer([1, 320, 7, 7, 5], dtype="float32")
        T_multiply = T.alloc_buffer([1, 320, 7, 7, 5], dtype="float32")
        T_multiply_red = T.alloc_buffer([1, 320, 1, 1, 1], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 320, 7, 7, 5):
            with T.block("T_subtract"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[0, ax1, ax2, ax3, ax4], placeholder_1[0, ax1, 0, 0, 0])
                T.writes(T_subtract[ax0, ax1, ax2, ax3, ax4])
                T_subtract[ax0, ax1, ax2, ax3, ax4] = placeholder[0, ax1, ax2, ax3, ax4] - placeholder_1[0, ax1, 0, 0, 0]
        for i0, i1, i2, i3, i4 in T.grid(1, 320, 7, 7, 5):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_subtract[0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = T_subtract[0, ax1, ax2, ax3, ax4] * T_subtract[0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 320, 1, 1, 1, 7, 7, 5):
            with T.block("T_multiply_red"):
                ax0, ax1, ax2, ax3, ax4, k2, k3, k4 = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(T_multiply[0, ax1, k2, k3, k4])
                T.writes(T_multiply_red[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    T_multiply_red[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
                T_multiply_red[ax0, ax1, ax2, ax3, ax4] = T_multiply_red[ax0, ax1, ax2, ax3, ax4] + T_multiply[0, ax1, k2, k3, k4]
        for i0, i1, i2, i3, i4 in T.grid(1, 320, 1, 1, 1):
            with T.block("T_divide"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_red[0, ax1, 0, 0, 0])
                T.writes(T_divide[ax0, ax1, ax2, ax3, ax4])
                T_divide[ax0, ax1, ax2, ax3, ax4] = T_multiply_red[0, ax1, 0, 0, 0] * T.float32(0.0040816326530612249)
    

[14:50:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: best replacement TIR found for the above:

[14:50:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_variance_5(placeholder: T.Buffer[(1, 320, 7, 7, 5), "float32"], placeholder_1: T.Buffer[(1, 320, 1, 1, 1), "float32"], T_divide: T.Buffer[(1, 320, 1, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply_red_shared = T.alloc_buffer([1, 320, 1, 1, 1], dtype="float32", scope="shared")
        for i0_i1_i2_i3_i4_0_fused in T.thread_binding(320, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3, ax4, ax5_ax6_ax7_fused_0 in T.grid(1, 1, 1, 1, 1, 62):
                for ax5_ax6_ax7_fused_1 in T.thread_binding(4, thread="threadIdx.x"):
                    with T.block("T_multiply_red"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(320, i0_i1_i2_i3_i4_0_fused)
                        ax2_1 = T.axis.spatial(1, 0)
                        ax3_1 = T.axis.spatial(1, 0)
                        ax4_1 = T.axis.spatial(1, 0)
                        k2 = T.axis.reduce(7, (ax5_ax6_ax7_fused_0 * 4 + ax5_ax6_ax7_fused_1) // 35)
                        k3 = T.axis.reduce(7, (ax5_ax6_ax7_fused_0 * 4 + ax5_ax6_ax7_fused_1) % 35 // 5)
                        k4 = T.axis.reduce(5, (ax5_ax6_ax7_fused_0 * 4 + ax5_ax6_ax7_fused_1) % 5)
                        T.where(ax5_ax6_ax7_fused_0 * 4 + ax5_ax6_ax7_fused_1 < 245)
                        T.reads(placeholder[0, ax1_1, k2, k3, k4], placeholder_1[0, ax1_1, 0, 0, 0])
                        T.writes(T_multiply_red_shared[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        with T.init():
                            T_multiply_red_shared[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.float32(0)
                        T_multiply_red_shared[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T_multiply_red_shared[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + (placeholder[0, ax1_1, k2, k3, k4] - placeholder_1[0, ax1_1, 0, 0, 0]) * (placeholder[0, ax1_1, k2, k3, k4] - placeholder_1[0, ax1_1, 0, 0, 0])
            for i4_1 in T.thread_binding(4, thread="threadIdx.x"):
                with T.block("T_divide"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(320, i0_i1_i2_i3_i4_0_fused)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    ax4 = T.axis.spatial(1, 0)
                    T.where(i4_1 < 1)
                    T.reads(T_multiply_red_shared[0, ax1, 0, 0, 0])
                    T.writes(T_divide[ax0, ax1, ax2, ax3, ax4])
                    T_divide[ax0, ax1, ax2, ax3, ax4] = T_multiply_red_shared[0, ax1, 0, 0, 0] * T.float32(0.0040816326530612249)
    

[14:50:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: Printing out trace for AHB:
[14:50:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:127: ["b0 = sch.get_block(name=\"T_subtract\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_multiply\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_multiply_red\", func_name=\"main\")", "b3 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b1)", "sch.compute_inline(block=b0)", "b4, = sch.get_consumers(block=b2)", "l5, l6, l7, l8, l9 = sch.get_loops(block=b4)", "v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=0)", "l11, l12 = sch.split(loop=l9, factors=[None, v10])", "sch.bind(loop=l12, thread_axis=\"threadIdx.x\")", "sch.compute_at(block=b2, loop=l11, preserve_unit_loops=True)", "sch.set_scope(block=b2, buffer_index=0, storage_scope=\"shared\")", "l13, l14, l15, l16, l17, l18, l19, l20, l21, l22, l23, l24, l25 = sch.get_loops(block=b2)", "l26 = sch.fuse(l23, l24, l25)", "l27, l28 = sch.split(loop=l26, factors=[None, v10])", "sch.bind(loop=l28, thread_axis=\"threadIdx.x\")", "v29 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v29)", "sch.enter_postproc()", "b30 = sch.get_block(name=\"T_multiply_red\", func_name=\"main\")", "l31, l32, l33, l34, l35, l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b30)", "l43 = sch.fuse(l31, l32, l33, l34, l35)", "sch.bind(loop=l43, thread_axis=\"blockIdx.x\")", "b44 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b44, ann_key=\"meta_schedule.unroll_explicit\")", "b45, b46 = sch.get_child_blocks(b44)", "l47, l48, l49, l50, l51, l52, l53, l54 = sch.get_loops(block=b45)", "sch.annotate(block_or_loop=l47, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l47, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l55, l56 = sch.get_loops(block=b46)", "sch.annotate(block_or_loop=l55, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l55, ann_key=\"pragma_unroll_explicit\", ann_val=1)"]
2022-05-09 14:50:30.855 WARNING Cannot find workload: tvmgen_default_fused_subtract_add_sqrt_divide_add_nn_leaky_relu_5
2022-05-09 14:50:30.965 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d_add_11
2022-05-09 14:50:31.129 WARNING Cannot find workload: tvmgen_default_fused_subtract_add_sqrt_divide_multiply_add_nn_leaky_relu_5
2022-05-09 14:50:31.228 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d_transpose
2022-05-09 14:50:31.684 WARNING Cannot find workload: tvmgen_default_fused_concatenate
2022-05-09 14:50:31.788 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d_add_12
2022-05-09 14:50:31.947 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d_transpose_1
2022-05-09 14:50:32.087 WARNING Cannot find workload: tvmgen_default_fused_concatenate_1
2022-05-09 14:50:32.193 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d_add_13
2022-05-09 14:50:32.351 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d_transpose_2
2022-05-09 14:50:32.491 WARNING Cannot find workload: tvmgen_default_fused_concatenate_2
2022-05-09 14:50:32.595 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d_add_14
2022-05-09 14:50:32.752 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d_transpose_3
2022-05-09 14:50:32.894 WARNING Cannot find workload: tvmgen_default_fused_concatenate_3
2022-05-09 14:50:32.994 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d_add_15
2022-05-09 14:50:33.152 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d_transpose_4
2022-05-09 14:50:33.294 WARNING Cannot find workload: tvmgen_default_fused_concatenate_4
2022-05-09 14:50:33.395 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d_add_16
2022-05-09 14:50:33.509 WARNING Cannot find workload: tvmgen_default_fused_nn_conv3d
[14:50:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112, 80), "float32"], placeholder_1: T.Buffer[(4, 64, 1, 1, 1), "float32"], conv3d_ncdhw: T.Buffer[(1, 4, 112, 112, 80), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 112, 112, 80], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 112, 112, 80):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[0, i1_1, i2_1, i3_1, i4_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1, i4_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1, i4_1] = placeholder[0, i1_1, i2_1, i3_1, i4_1]
        for i0, i1, i2, i3, i4, i5, i6, i7, i8 in T.grid(1, 4, 112, 112, 80, 64, 1, 1, 1):
            with T.block("conv3d_ncdhw"):
                nn, ff, yy, xx, zz, rc, ry, rx, rz = T.axis.remap("SSSSSRRRR", [i0, i1, i2, i3, i4, i5, i6, i7, i8])
                T.reads(pad_temp[0, rc, yy, xx, zz], placeholder_1[ff, rc, 0, 0, 0])
                T.writes(conv3d_ncdhw[nn, ff, yy, xx, zz])
                T.block_attr({"workload":["conv3d_ncdhw.cuda", ["TENSOR", [1, 64, 112, 112, 80], "float32"], ["TENSOR", [4, 64, 1, 1, 1], "float32"], [1, 1, 1], [0, 0, 0, 0, 0, 0], [1, 1, 1], 1, "float32"]})
                with T.init():
                    conv3d_ncdhw[nn, ff, yy, xx, zz] = T.float32(0)
                conv3d_ncdhw[nn, ff, yy, xx, zz] = conv3d_ncdhw[nn, ff, yy, xx, zz] + pad_temp[0, rc, yy, xx, zz] * placeholder_1[ff, rc, 0, 0, 0]
    

[14:50:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: best replacement TIR found for the above:

[14:50:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv3d_1(placeholder: T.Buffer[(1, 64, 112, 112, 80), "float32"], placeholder_1: T.Buffer[(4, 64, 1, 1, 1), "float32"], conv3d_ncdhw: T.Buffer[(1, 4, 112, 112, 80), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv3d_ncdhw_local = T.alloc_buffer([1, 4, 112, 112, 80], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 112, 112, 80], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([4, 64, 1, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.thread_binding(560, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_i4_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i3_4_init, i4_4_init in T.grid(2, 2, 4, 2):
                        with T.block("conv3d_ncdhw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(4, i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 140 * 28 + i0_2_i1_2_i2_2_i3_2_i4_2_fused // 8)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 140 // 5 * 4 + i3_4_init)
                            zz = T.axis.spatial(80, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 5 * 16 + i0_2_i1_2_i2_2_i3_2_i4_2_fused % 8 * 2 + i4_4_init)
                            T.reads()
                            T.writes(conv3d_ncdhw_local[nn, ff, yy, xx, zz])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv3d_ncdhw.cuda", ["TENSOR", [1, 64, 112, 112, 80], "float32"], ["TENSOR", [4, 64, 1, 1, 1], "float32"], [1, 1, 1], [0, 0, 0, 0, 0, 0], [1, 1, 1], 1, "float32"]})
                            conv3d_ncdhw_local[nn, ff, yy, xx, zz] = T.float32(0)
                    for i5_0, i6_0, i7_0, i8_0 in T.grid(64, 1, 1, 1):
                        for ax0_ax1_ax2_ax3_ax4_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i5_0)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 140 * 28 + (ax0_ax1_ax2_ax3_ax4_fused_0 * 224 + ax0_ax1_ax2_ax3_ax4_fused_1) // 64)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 140 // 5 * 4 + (ax0_ax1_ax2_ax3_ax4_fused_0 * 224 + ax0_ax1_ax2_ax3_ax4_fused_1) % 64 // 16)
                                    v4 = T.axis.spatial(80, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 5 * 16 + (ax0_ax1_ax2_ax3_ax4_fused_0 * 224 + ax0_ax1_ax2_ax3_ax4_fused_1) % 16)
                                    T.reads(placeholder[0, v1, v2, v3, v4])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3, v4])
                                    pad_temp_shared[v0, v1, v2, v3, v4] = placeholder[0, v1, v2, v3, v4]
                        for ax0_ax1_ax2_ax3_ax4_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(4, ax0_ax1_ax2_ax3_ax4_fused_1)
                                    v1 = T.axis.spatial(64, i5_0)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    v4 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_ax4_fused_1 < 4)
                                    T.reads(placeholder_1[v0, v1, v2, v3, v4])
                                    T.writes(placeholder_shared[v0, v1, v2, v3, v4])
                                    placeholder_shared[v0, v1, v2, v3, v4] = placeholder_1[v0, v1, v2, v3, v4]
                        for i5_1, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_2, i6_2, i7_2, i8_2, i0_4, i1_4, i2_4, i3_4, i4_4 in T.grid(1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 4, 2):
                            with T.block("conv3d_ncdhw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(4, i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 140 * 28 + i0_2_i1_2_i2_2_i3_2_i4_2_fused // 8)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 140 // 5 * 4 + i3_4)
                                zz = T.axis.spatial(80, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 5 * 16 + i0_2_i1_2_i2_2_i3_2_i4_2_fused % 8 * 2 + i4_4)
                                rc = T.axis.reduce(64, i5_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                rz = T.axis.reduce(1, 0)
                                T.reads(conv3d_ncdhw_local[nn, ff, yy, xx, zz], pad_temp_shared[0, rc, yy, xx, zz], placeholder_shared[ff, rc, 0, 0, 0])
                                T.writes(conv3d_ncdhw_local[nn, ff, yy, xx, zz])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv3d_ncdhw.cuda", ["TENSOR", [1, 64, 112, 112, 80], "float32"], ["TENSOR", [4, 64, 1, 1, 1], "float32"], [1, 1, 1], [0, 0, 0, 0, 0, 0], [1, 1, 1], 1, "float32"]})
                                conv3d_ncdhw_local[nn, ff, yy, xx, zz] = conv3d_ncdhw_local[nn, ff, yy, xx, zz] + pad_temp_shared[0, rc, yy, xx, zz] * placeholder_shared[ff, rc, 0, 0, 0]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 1, 4, 2):
                        with T.block("conv3d_ncdhw_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 140 * 28 + i0_2_i1_2_i2_2_i3_2_i4_2_fused // 8 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 140 // 5 * 4 + ax3)
                            v4 = T.axis.spatial(80, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 5 * 16 + i0_2_i1_2_i2_2_i3_2_i4_2_fused % 8 * 2 + ax4)
                            T.reads(conv3d_ncdhw_local[v0, v1, v2, v3, v4])
                            T.writes(conv3d_ncdhw[v0, v1, v2, v3, v4])
                            conv3d_ncdhw[v0, v1, v2, v3, v4] = conv3d_ncdhw_local[v0, v1, v2, v3, v4]
    

[14:50:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: Printing out trace for AHB:
[14:50:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:127: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv3d_ncdhw\", func_name=\"main\")", "b2 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l3, l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)", "v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l17, l18, l19, l20, l21 = sch.split(loop=l3, factors=[v12, v13, v14, v15, v16])", "v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 2])", "l27, l28, l29, l30, l31 = sch.split(loop=l4, factors=[v22, v23, v24, v25, v26])", "v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 1, 28, 1, 1])", "l37, l38, l39, l40, l41 = sch.split(loop=l5, factors=[v32, v33, v34, v35, v36])", "v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[28, 1, 1, 1, 4])", "l47, l48, l49, l50, l51 = sch.split(loop=l6, factors=[v42, v43, v44, v45, v46])", "v52, v53, v54, v55, v56 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[5, 1, 8, 1, 2])", "l57, l58, l59, l60, l61 = sch.split(loop=l7, factors=[v52, v53, v54, v55, v56])", "v62, v63, v64 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[64, 1, 1])", "l65, l66, l67 = sch.split(loop=l8, factors=[v62, v63, v64])", "v68, v69, v70 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l71, l72, l73 = sch.split(loop=l9, factors=[v68, v69, v70])", "v74, v75, v76 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l77, l78, l79 = sch.split(loop=l10, factors=[v74, v75, v76])", "v80, v81, v82 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l83, l84, l85 = sch.split(loop=l11, factors=[v80, v81, v82])", "sch.reorder(l17, l27, l37, l47, l57, l18, l28, l38, l48, l58, l19, l29, l39, l49, l59, l65, l71, l77, l83, l66, l72, l78, l84, l20, l30, l40, l50, l60, l67, l73, l79, l85, l21, l31, l41, l51, l61)", "l86 = sch.fuse(l17, l27, l37, l47, l57)", "sch.bind(loop=l86, thread_axis=\"blockIdx.x\")", "l87 = sch.fuse(l18, l28, l38, l48, l58)", "sch.bind(loop=l87, thread_axis=\"vthread.x\")", "l88 = sch.fuse(l19, l29, l39, l49, l59)", "sch.bind(loop=l88, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b89 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b89, loop=l88, preserve_unit_loops=True)", "b90 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b90, loop=l83, preserve_unit_loops=True)", "l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b90)", "l103 = sch.fuse(l98, l99, l100, l101, l102)", "v104 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)", "sch.annotate(block_or_loop=b90, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v104)", "b105 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b105, loop=l83, preserve_unit_loops=True)", "l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b105)", "l118 = sch.fuse(l113, l114, l115, l116, l117)", "v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)", "sch.annotate(block_or_loop=b105, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v119)", "sch.compute_inline(block=b0)", "v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v120)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b90, ann_key=\"meta_schedule.cooperative_fetch\")", "l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b90)", "l129, l130 = sch.split(loop=l128, factors=[None, 224])", "sch.bind(loop=l130, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b105, ann_key=\"meta_schedule.cooperative_fetch\")", "l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b105)", "l139, l140 = sch.split(loop=l138, factors=[None, 224])", "sch.bind(loop=l140, thread_axis=\"threadIdx.x\")", "b141 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b141, ann_key=\"meta_schedule.unroll_explicit\")", "b142, b143, b144, b145 = sch.get_child_blocks(b141)", "l146, l147, l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b142)", "sch.annotate(block_or_loop=l146, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l146, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b143)", "sch.annotate(block_or_loop=l155, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l155, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b144)", "sch.annotate(block_or_loop=l164, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l164, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l189, l190, l191, l192, l193, l194, l195, l196 = sch.get_loops(block=b145)", "sch.annotate(block_or_loop=l189, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l189, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b197 = sch.get_block(name=\"conv3d_ncdhw\", func_name=\"main\")", "l198, l199, l200, l201, l202, l203, l204, l205, l206, l207, l208, l209, l210, l211, l212, l213, l214, l215, l216, l217, l218, l219, l220, l221, l222 = sch.get_loops(block=b197)", "b223 = sch.decompose_reduction(block=b197, loop=l201)"]
[14:50:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56, 40), "float32"], placeholder_1: T.Buffer[(4, 128, 1, 1, 1), "float32"], conv3d_ncdhw: T.Buffer[(1, 4, 56, 56, 40), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 56, 56, 40], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 56, 56, 40):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[0, i1_1, i2_1, i3_1, i4_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1, i4_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1, i4_1] = placeholder[0, i1_1, i2_1, i3_1, i4_1]
        for i0, i1, i2, i3, i4, i5, i6, i7, i8 in T.grid(1, 4, 56, 56, 40, 128, 1, 1, 1):
            with T.block("conv3d_ncdhw"):
                nn, ff, yy, xx, zz, rc, ry, rx, rz = T.axis.remap("SSSSSRRRR", [i0, i1, i2, i3, i4, i5, i6, i7, i8])
                T.reads(pad_temp[0, rc, yy, xx, zz], placeholder_1[ff, rc, 0, 0, 0])
                T.writes(conv3d_ncdhw[nn, ff, yy, xx, zz])
                T.block_attr({"workload":["conv3d_ncdhw.cuda", ["TENSOR", [1, 128, 56, 56, 40], "float32"], ["TENSOR", [4, 128, 1, 1, 1], "float32"], [1, 1, 1], [0, 0, 0, 0, 0, 0], [1, 1, 1], 1, "float32"]})
                with T.init():
                    conv3d_ncdhw[nn, ff, yy, xx, zz] = T.float32(0)
                conv3d_ncdhw[nn, ff, yy, xx, zz] = conv3d_ncdhw[nn, ff, yy, xx, zz] + pad_temp[0, rc, yy, xx, zz] * placeholder_1[ff, rc, 0, 0, 0]
    

[14:50:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: best replacement TIR found for the above:

[14:50:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv3d_2(placeholder: T.Buffer[(1, 128, 56, 56, 40), "float32"], placeholder_1: T.Buffer[(4, 128, 1, 1, 1), "float32"], conv3d_ncdhw: T.Buffer[(1, 4, 56, 56, 40), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv3d_ncdhw_local = T.alloc_buffer([1, 4, 56, 56, 40], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 56, 56, 40], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([4, 128, 1, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.thread_binding(224, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_i4_2_fused in T.thread_binding(40, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i4_3_init, i1_4_init in T.grid(2, 7, 2, 2):
                        with T.block("conv3d_ncdhw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(4, i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 7 + i2_3_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 * 2 + i0_2_i1_2_i2_2_i3_2_i4_2_fused // 20)
                            zz = T.axis.spatial(40, i0_2_i1_2_i2_2_i3_2_i4_2_fused % 20 * 2 + i4_3_init)
                            T.reads()
                            T.writes(conv3d_ncdhw_local[nn, ff, yy, xx, zz])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv3d_ncdhw.cuda", ["TENSOR", [1, 128, 56, 56, 40], "float32"], ["TENSOR", [4, 128, 1, 1, 1], "float32"], [1, 1, 1], [0, 0, 0, 0, 0, 0], [1, 1, 1], 1, "float32"]})
                            conv3d_ncdhw_local[nn, ff, yy, xx, zz] = T.float32(0)
                    for i5_0, i6_0, i7_0, i8_0 in T.grid(16, 1, 1, 1):
                        for ax0_ax1_ax2_ax3_ax4_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(40, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_ax4_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i5_0 * 8 + (ax0_ax1_ax2_ax3_ax4_fused_0 * 80 + ax0_ax1_ax2_ax3_ax4_fused_1 * 2 + ax0_ax1_ax2_ax3_ax4_fused_2) // 560)
                                        v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 7 + (ax0_ax1_ax2_ax3_ax4_fused_0 * 80 + ax0_ax1_ax2_ax3_ax4_fused_1 * 2 + ax0_ax1_ax2_ax3_ax4_fused_2) % 560 // 80)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 * 2 + (ax0_ax1_ax2_ax3_ax4_fused_0 * 80 + ax0_ax1_ax2_ax3_ax4_fused_1 * 2 + ax0_ax1_ax2_ax3_ax4_fused_2) % 80 // 40)
                                        v4 = T.axis.spatial(40, (ax0_ax1_ax2_ax3_ax4_fused_0 * 80 + ax0_ax1_ax2_ax3_ax4_fused_1 * 2 + ax0_ax1_ax2_ax3_ax4_fused_2) % 40)
                                        T.reads(placeholder[0, v1, v2, v3, v4])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3, v4])
                                        pad_temp_shared[v0, v1, v2, v3, v4] = placeholder[0, v1, v2, v3, v4]
                        for ax0_ax1_ax2_ax3_ax4_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(40, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_ax4_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(4, (ax0_ax1_ax2_ax3_ax4_fused_1 * 4 + ax0_ax1_ax2_ax3_ax4_fused_2) // 8)
                                        v1 = T.axis.spatial(128, i5_0 * 8 + (ax0_ax1_ax2_ax3_ax4_fused_1 * 4 + ax0_ax1_ax2_ax3_ax4_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        v4 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_ax4_fused_1 * 4 + ax0_ax1_ax2_ax3_ax4_fused_2 < 32)
                                        T.reads(placeholder_1[v0, v1, v2, v3, v4])
                                        T.writes(placeholder_shared[v0, v1, v2, v3, v4])
                                        placeholder_shared[v0, v1, v2, v3, v4] = placeholder_1[v0, v1, v2, v3, v4]
                        for i5_1, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_2, i6_2, i7_2, i8_2, i0_4, i1_4, i2_4, i3_4, i4_4 in T.grid(4, 1, 1, 1, 1, 2, 7, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1):
                            with T.block("conv3d_ncdhw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(4, i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 7 + i2_3)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 * 2 + i0_2_i1_2_i2_2_i3_2_i4_2_fused // 20)
                                zz = T.axis.spatial(40, i0_2_i1_2_i2_2_i3_2_i4_2_fused % 20 * 2 + i4_3)
                                rc = T.axis.reduce(128, i5_0 * 8 + i5_1 * 2 + i5_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                rz = T.axis.reduce(1, 0)
                                T.reads(conv3d_ncdhw_local[nn, ff, yy, xx, zz], pad_temp_shared[0, rc, yy, xx, zz], placeholder_shared[ff, rc, 0, 0, 0])
                                T.writes(conv3d_ncdhw_local[nn, ff, yy, xx, zz])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv3d_ncdhw.cuda", ["TENSOR", [1, 128, 56, 56, 40], "float32"], ["TENSOR", [4, 128, 1, 1, 1], "float32"], [1, 1, 1], [0, 0, 0, 0, 0, 0], [1, 1, 1], 1, "float32"]})
                                conv3d_ncdhw_local[nn, ff, yy, xx, zz] = conv3d_ncdhw_local[nn, ff, yy, xx, zz] + pad_temp_shared[0, rc, yy, xx, zz] * placeholder_shared[ff, rc, 0, 0, 0]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 7, 1, 2):
                        with T.block("conv3d_ncdhw_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 7 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 * 2 + i0_2_i1_2_i2_2_i3_2_i4_2_fused // 20 + ax3)
                            v4 = T.axis.spatial(40, i0_2_i1_2_i2_2_i3_2_i4_2_fused % 20 * 2 + ax4)
                            T.reads(conv3d_ncdhw_local[v0, v1, v2, v3, v4])
                            T.writes(conv3d_ncdhw[v0, v1, v2, v3, v4])
                            conv3d_ncdhw[v0, v1, v2, v3, v4] = conv3d_ncdhw_local[v0, v1, v2, v3, v4]
    

[14:50:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: Printing out trace for AHB:
[14:50:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:127: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv3d_ncdhw\", func_name=\"main\")", "b2 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l3, l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)", "v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l17, l18, l19, l20, l21 = sch.split(loop=l3, factors=[v12, v13, v14, v15, v16])", "v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 2])", "l27, l28, l29, l30, l31 = sch.split(loop=l4, factors=[v22, v23, v24, v25, v26])", "v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 1, 1, 7, 1])", "l37, l38, l39, l40, l41 = sch.split(loop=l5, factors=[v32, v33, v34, v35, v36])", "v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[28, 1, 2, 1, 1])", "l47, l48, l49, l50, l51 = sch.split(loop=l6, factors=[v42, v43, v44, v45, v46])", "v52, v53, v54, v55, v56 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 20, 2, 1])", "l57, l58, l59, l60, l61 = sch.split(loop=l7, factors=[v52, v53, v54, v55, v56])", "v62, v63, v64 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 4, 2])", "l65, l66, l67 = sch.split(loop=l8, factors=[v62, v63, v64])", "v68, v69, v70 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l71, l72, l73 = sch.split(loop=l9, factors=[v68, v69, v70])", "v74, v75, v76 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l77, l78, l79 = sch.split(loop=l10, factors=[v74, v75, v76])", "v80, v81, v82 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l83, l84, l85 = sch.split(loop=l11, factors=[v80, v81, v82])", "sch.reorder(l17, l27, l37, l47, l57, l18, l28, l38, l48, l58, l19, l29, l39, l49, l59, l65, l71, l77, l83, l66, l72, l78, l84, l20, l30, l40, l50, l60, l67, l73, l79, l85, l21, l31, l41, l51, l61)", "l86 = sch.fuse(l17, l27, l37, l47, l57)", "sch.bind(loop=l86, thread_axis=\"blockIdx.x\")", "l87 = sch.fuse(l18, l28, l38, l48, l58)", "sch.bind(loop=l87, thread_axis=\"vthread.x\")", "l88 = sch.fuse(l19, l29, l39, l49, l59)", "sch.bind(loop=l88, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b89 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b89, loop=l88, preserve_unit_loops=True)", "b90 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b90, loop=l83, preserve_unit_loops=True)", "l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b90)", "l103 = sch.fuse(l98, l99, l100, l101, l102)", "v104 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)", "sch.annotate(block_or_loop=b90, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v104)", "b105 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b105, loop=l83, preserve_unit_loops=True)", "l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b105)", "l118 = sch.fuse(l113, l114, l115, l116, l117)", "v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b105, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v119)", "sch.compute_inline(block=b0)", "v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v120)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b90, ann_key=\"meta_schedule.cooperative_fetch\")", "l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b90)", "l129, l130, l131 = sch.split(loop=l128, factors=[None, 40, 2])", "sch.vectorize(loop=l131)", "sch.bind(loop=l130, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b105, ann_key=\"meta_schedule.cooperative_fetch\")", "l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b105)", "l140, l141, l142 = sch.split(loop=l139, factors=[None, 40, 4])", "sch.vectorize(loop=l142)", "sch.bind(loop=l141, thread_axis=\"threadIdx.x\")", "b143 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b143, ann_key=\"meta_schedule.unroll_explicit\")", "b144, b145, b146, b147 = sch.get_child_blocks(b143)", "l148, l149, l150, l151, l152, l153, l154, l155, l156, l157 = sch.get_loops(block=b144)", "l158, l159, l160, l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b145)", "l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191, l192 = sch.get_loops(block=b146)", "l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b147)", "b201 = sch.get_block(name=\"conv3d_ncdhw\", func_name=\"main\")", "l202, l203, l204, l205, l206, l207, l208, l209, l210, l211, l212, l213, l214, l215, l216, l217, l218, l219, l220, l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b201)", "b227 = sch.decompose_reduction(block=b201, loop=l205)"]
[14:50:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28, 20), "float32"], placeholder_1: T.Buffer[(4, 256, 1, 1, 1), "float32"], conv3d_ncdhw: T.Buffer[(1, 4, 28, 28, 20), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 28, 28, 20], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 28, 28, 20):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[0, i1_1, i2_1, i3_1, i4_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1, i4_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1, i4_1] = placeholder[0, i1_1, i2_1, i3_1, i4_1]
        for i0, i1, i2, i3, i4, i5, i6, i7, i8 in T.grid(1, 4, 28, 28, 20, 256, 1, 1, 1):
            with T.block("conv3d_ncdhw"):
                nn, ff, yy, xx, zz, rc, ry, rx, rz = T.axis.remap("SSSSSRRRR", [i0, i1, i2, i3, i4, i5, i6, i7, i8])
                T.reads(pad_temp[0, rc, yy, xx, zz], placeholder_1[ff, rc, 0, 0, 0])
                T.writes(conv3d_ncdhw[nn, ff, yy, xx, zz])
                T.block_attr({"workload":["conv3d_ncdhw.cuda", ["TENSOR", [1, 256, 28, 28, 20], "float32"], ["TENSOR", [4, 256, 1, 1, 1], "float32"], [1, 1, 1], [0, 0, 0, 0, 0, 0], [1, 1, 1], 1, "float32"]})
                with T.init():
                    conv3d_ncdhw[nn, ff, yy, xx, zz] = T.float32(0)
                conv3d_ncdhw[nn, ff, yy, xx, zz] = conv3d_ncdhw[nn, ff, yy, xx, zz] + pad_temp[0, rc, yy, xx, zz] * placeholder_1[ff, rc, 0, 0, 0]
    

[14:50:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: best replacement TIR found for the above:

[14:50:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv3d_3(placeholder: T.Buffer[(1, 256, 28, 28, 20), "float32"], placeholder_1: T.Buffer[(4, 256, 1, 1, 1), "float32"], conv3d_ncdhw: T.Buffer[(1, 4, 28, 28, 20), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv3d_ncdhw_local = T.alloc_buffer([1, 4, 28, 28, 20], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 28, 28, 20], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([4, 256, 1, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_i4_2_fused in T.thread_binding(140, thread="threadIdx.x"):
                    for i1_4_init in T.serial(2):
                        with T.block("conv3d_ncdhw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 56 * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 56 // 8 * 4 + i0_1_i1_1_i2_1_i3_1_i4_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_i4_2_fused // 70)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 7 + i0_2_i1_2_i2_2_i3_2_i4_2_fused % 70 // 10)
                            zz = T.axis.spatial(20, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 10 + i0_2_i1_2_i2_2_i3_2_i4_2_fused % 10)
                            T.reads()
                            T.writes(conv3d_ncdhw_local[nn, ff, yy, xx, zz])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv3d_ncdhw.cuda", ["TENSOR", [1, 256, 28, 28, 20], "float32"], ["TENSOR", [4, 256, 1, 1, 1], "float32"], [1, 1, 1], [0, 0, 0, 0, 0, 0], [1, 1, 1], 1, "float32"]})
                            conv3d_ncdhw_local[nn, ff, yy, xx, zz] = T.float32(0)
                    for i5_0, i6_0, i7_0, i8_0 in T.grid(64, 1, 1, 1):
                        for ax0_ax1_ax2_ax3_ax4_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(140, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i5_0 * 4 + (ax0_ax1_ax2_ax3_ax4_fused_0 * 140 + ax0_ax1_ax2_ax3_ax4_fused_1) // 280)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 56 // 8 * 4 + (ax0_ax1_ax2_ax3_ax4_fused_0 * 140 + ax0_ax1_ax2_ax3_ax4_fused_1) % 280 // 70)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 7 + (ax0_ax1_ax2_ax3_ax4_fused_0 * 140 + ax0_ax1_ax2_ax3_ax4_fused_1) % 70 // 10)
                                    v4 = T.axis.spatial(20, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 10 + (ax0_ax1_ax2_ax3_ax4_fused_0 * 140 + ax0_ax1_ax2_ax3_ax4_fused_1) % 10)
                                    T.reads(placeholder[0, v1, v2, v3, v4])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3, v4])
                                    pad_temp_shared[v0, v1, v2, v3, v4] = placeholder[0, v1, v2, v3, v4]
                        for ax0_ax1_ax2_ax3_ax4_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(140, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_ax4_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 56 * 2 + (ax0_ax1_ax2_ax3_ax4_fused_1 * 2 + ax0_ax1_ax2_ax3_ax4_fused_2) // 4)
                                        v1 = T.axis.spatial(256, i5_0 * 4 + (ax0_ax1_ax2_ax3_ax4_fused_1 * 2 + ax0_ax1_ax2_ax3_ax4_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        v4 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_ax4_fused_1 * 2 + ax0_ax1_ax2_ax3_ax4_fused_2 < 8)
                                        T.reads(placeholder_1[v0, v1, v2, v3, v4])
                                        T.writes(placeholder_shared[v0, v1, v2, v3, v4])
                                        placeholder_shared[v0, v1, v2, v3, v4] = placeholder_1[v0, v1, v2, v3, v4]
                        for i5_1, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_2, i6_2, i7_2, i8_2, i0_4, i1_4, i2_4, i3_4, i4_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 2, 1, 1, 1):
                            with T.block("conv3d_ncdhw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 56 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 56 // 8 * 4 + i0_1_i1_1_i2_1_i3_1_i4_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_i4_2_fused // 70)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 7 + i0_2_i1_2_i2_2_i3_2_i4_2_fused % 70 // 10)
                                zz = T.axis.spatial(20, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 10 + i0_2_i1_2_i2_2_i3_2_i4_2_fused % 10)
                                rc = T.axis.reduce(256, i5_0 * 4 + i5_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                rz = T.axis.reduce(1, 0)
                                T.reads(conv3d_ncdhw_local[nn, ff, yy, xx, zz], pad_temp_shared[0, rc, yy, xx, zz], placeholder_shared[ff, rc, 0, 0, 0])
                                T.writes(conv3d_ncdhw_local[nn, ff, yy, xx, zz])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv3d_ncdhw.cuda", ["TENSOR", [1, 256, 28, 28, 20], "float32"], ["TENSOR", [4, 256, 1, 1, 1], "float32"], [1, 1, 1], [0, 0, 0, 0, 0, 0], [1, 1, 1], 1, "float32"]})
                                conv3d_ncdhw_local[nn, ff, yy, xx, zz] = conv3d_ncdhw_local[nn, ff, yy, xx, zz] + pad_temp_shared[0, rc, yy, xx, zz] * placeholder_shared[ff, rc, 0, 0, 0]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 1, 1, 1):
                        with T.block("conv3d_ncdhw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 56 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 56 // 8 * 4 + i0_1_i1_1_i2_1_i3_1_i4_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_i4_2_fused // 70 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 7 + i0_2_i1_2_i2_2_i3_2_i4_2_fused % 70 // 10 + ax3)
                            v4 = T.axis.spatial(20, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 10 + i0_2_i1_2_i2_2_i3_2_i4_2_fused % 10 + ax4)
                            T.reads(conv3d_ncdhw_local[v0, v1, v2, v3, v4])
                            T.writes(conv3d_ncdhw[v0, v1, v2, v3, v4])
                            conv3d_ncdhw[v0, v1, v2, v3, v4] = conv3d_ncdhw_local[v0, v1, v2, v3, v4]
    

[14:50:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: Printing out trace for AHB:
[14:50:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:127: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv3d_ncdhw\", func_name=\"main\")", "b2 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l3, l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)", "v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l17, l18, l19, l20, l21 = sch.split(loop=l3, factors=[v12, v13, v14, v15, v16])", "v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 2])", "l27, l28, l29, l30, l31 = sch.split(loop=l4, factors=[v22, v23, v24, v25, v26])", "v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[7, 2, 2, 1, 1])", "l37, l38, l39, l40, l41 = sch.split(loop=l5, factors=[v32, v33, v34, v35, v36])", "v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 1])", "l47, l48, l49, l50, l51 = sch.split(loop=l6, factors=[v42, v43, v44, v45, v46])", "v52, v53, v54, v55, v56 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 10, 1, 1])", "l57, l58, l59, l60, l61 = sch.split(loop=l7, factors=[v52, v53, v54, v55, v56])", "v62, v63, v64 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[64, 1, 4])", "l65, l66, l67 = sch.split(loop=l8, factors=[v62, v63, v64])", "v68, v69, v70 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l71, l72, l73 = sch.split(loop=l9, factors=[v68, v69, v70])", "v74, v75, v76 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l77, l78, l79 = sch.split(loop=l10, factors=[v74, v75, v76])", "v80, v81, v82 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l83, l84, l85 = sch.split(loop=l11, factors=[v80, v81, v82])", "sch.reorder(l17, l27, l37, l47, l57, l18, l28, l38, l48, l58, l19, l29, l39, l49, l59, l65, l71, l77, l83, l66, l72, l78, l84, l20, l30, l40, l50, l60, l67, l73, l79, l85, l21, l31, l41, l51, l61)", "l86 = sch.fuse(l17, l27, l37, l47, l57)", "sch.bind(loop=l86, thread_axis=\"blockIdx.x\")", "l87 = sch.fuse(l18, l28, l38, l48, l58)", "sch.bind(loop=l87, thread_axis=\"vthread.x\")", "l88 = sch.fuse(l19, l29, l39, l49, l59)", "sch.bind(loop=l88, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b89 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b89, loop=l88, preserve_unit_loops=True)", "b90 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b90, loop=l83, preserve_unit_loops=True)", "l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b90)", "l103 = sch.fuse(l98, l99, l100, l101, l102)", "v104 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b90, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v104)", "b105 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b105, loop=l83, preserve_unit_loops=True)", "l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b105)", "l118 = sch.fuse(l113, l114, l115, l116, l117)", "v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)", "sch.annotate(block_or_loop=b105, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v119)", "sch.compute_inline(block=b0)", "v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v120)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b90, ann_key=\"meta_schedule.cooperative_fetch\")", "l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b90)", "l129, l130 = sch.split(loop=l128, factors=[None, 140])", "sch.bind(loop=l130, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b105, ann_key=\"meta_schedule.cooperative_fetch\")", "l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b105)", "l139, l140, l141 = sch.split(loop=l138, factors=[None, 140, 2])", "sch.vectorize(loop=l141)", "sch.bind(loop=l140, thread_axis=\"threadIdx.x\")", "b142 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b142, ann_key=\"meta_schedule.unroll_explicit\")", "b143, b144, b145, b146 = sch.get_child_blocks(b142)", "l147, l148, l149, l150, l151, l152, l153, l154, l155 = sch.get_loops(block=b143)", "sch.annotate(block_or_loop=l147, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l147, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l156, l157, l158, l159, l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b144)", "sch.annotate(block_or_loop=l156, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l156, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b145)", "sch.annotate(block_or_loop=l166, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l166, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l191, l192, l193, l194, l195, l196, l197, l198 = sch.get_loops(block=b146)", "sch.annotate(block_or_loop=l191, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l191, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b199 = sch.get_block(name=\"conv3d_ncdhw\", func_name=\"main\")", "l200, l201, l202, l203, l204, l205, l206, l207, l208, l209, l210, l211, l212, l213, l214, l215, l216, l217, l218, l219, l220, l221, l222, l223, l224 = sch.get_loops(block=b199)", "b225 = sch.decompose_reduction(block=b199, loop=l203)"]
[14:50:34] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 14, 14, 10), "float32"], placeholder_1: T.Buffer[(4, 320, 1, 1, 1), "float32"], conv3d_ncdhw: T.Buffer[(1, 4, 14, 14, 10), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 320, 14, 14, 10], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 320, 14, 14, 10):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[0, i1_1, i2_1, i3_1, i4_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1, i4_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1, i4_1] = placeholder[0, i1_1, i2_1, i3_1, i4_1]
        for i0, i1, i2, i3, i4, i5, i6, i7, i8 in T.grid(1, 4, 14, 14, 10, 320, 1, 1, 1):
            with T.block("conv3d_ncdhw"):
                nn, ff, yy, xx, zz, rc, ry, rx, rz = T.axis.remap("SSSSSRRRR", [i0, i1, i2, i3, i4, i5, i6, i7, i8])
                T.reads(pad_temp[0, rc, yy, xx, zz], placeholder_1[ff, rc, 0, 0, 0])
                T.writes(conv3d_ncdhw[nn, ff, yy, xx, zz])
                T.block_attr({"workload":["conv3d_ncdhw.cuda", ["TENSOR", [1, 320, 14, 14, 10], "float32"], ["TENSOR", [4, 320, 1, 1, 1], "float32"], [1, 1, 1], [0, 0, 0, 0, 0, 0], [1, 1, 1], 1, "float32"]})
                with T.init():
                    conv3d_ncdhw[nn, ff, yy, xx, zz] = T.float32(0)
                conv3d_ncdhw[nn, ff, yy, xx, zz] = conv3d_ncdhw[nn, ff, yy, xx, zz] + pad_temp[0, rc, yy, xx, zz] * placeholder_1[ff, rc, 0, 0, 0]
    

[14:50:34] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: best replacement TIR found for the above:

[14:50:34] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv3d_4(placeholder: T.Buffer[(1, 320, 14, 14, 10), "float32"], placeholder_1: T.Buffer[(4, 320, 1, 1, 1), "float32"], conv3d_ncdhw: T.Buffer[(1, 4, 14, 14, 10), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv3d_ncdhw_local = T.alloc_buffer([1, 4, 14, 14, 10], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 320, 14, 14, 10], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([4, 320, 1, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.thread_binding(56, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_i4_2_fused in T.thread_binding(35, thread="threadIdx.x"):
                    with T.block("conv3d_ncdhw_init"):
                        nn = T.axis.spatial(1, 0)
                        ff = T.axis.spatial(4, i0_1_i1_1_i2_1_i3_1_i4_1_fused)
                        yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4)
                        xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 7 + i0_2_i1_2_i2_2_i3_2_i4_2_fused // 5)
                        zz = T.axis.spatial(10, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 5 + i0_2_i1_2_i2_2_i3_2_i4_2_fused % 5)
                        T.reads()
                        T.writes(conv3d_ncdhw_local[nn, ff, yy, xx, zz])
                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv3d_ncdhw.cuda", ["TENSOR", [1, 320, 14, 14, 10], "float32"], ["TENSOR", [4, 320, 1, 1, 1], "float32"], [1, 1, 1], [0, 0, 0, 0, 0, 0], [1, 1, 1], 1, "float32"]})
                        conv3d_ncdhw_local[nn, ff, yy, xx, zz] = T.float32(0)
                    for i5_0, i6_0, i7_0, i8_0 in T.grid(32, 1, 1, 1):
                        for ax0_ax1_ax2_ax3_ax4_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(35, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_ax4_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(320, i5_0 * 10 + (ax0_ax1_ax2_ax3_ax4_fused_0 * 70 + ax0_ax1_ax2_ax3_ax4_fused_1 * 2 + ax0_ax1_ax2_ax3_ax4_fused_2) // 35)
                                        v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4)
                                        v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 7 + (ax0_ax1_ax2_ax3_ax4_fused_0 * 70 + ax0_ax1_ax2_ax3_ax4_fused_1 * 2 + ax0_ax1_ax2_ax3_ax4_fused_2) % 35 // 5)
                                        v4 = T.axis.spatial(10, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 5 + (ax0_ax1_ax2_ax3_ax4_fused_0 * 70 + ax0_ax1_ax2_ax3_ax4_fused_1 * 2 + ax0_ax1_ax2_ax3_ax4_fused_2) % 5)
                                        T.reads(placeholder[0, v1, v2, v3, v4])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3, v4])
                                        pad_temp_shared[v0, v1, v2, v3, v4] = placeholder[0, v1, v2, v3, v4]
                        for ax0_ax1_ax2_ax3_ax4_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(35, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_ax4_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(4, (ax0_ax1_ax2_ax3_ax4_fused_1 * 2 + ax0_ax1_ax2_ax3_ax4_fused_2) // 10)
                                        v1 = T.axis.spatial(320, i5_0 * 10 + (ax0_ax1_ax2_ax3_ax4_fused_1 * 2 + ax0_ax1_ax2_ax3_ax4_fused_2) % 10)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        v4 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_ax4_fused_1 * 2 + ax0_ax1_ax2_ax3_ax4_fused_2 < 40)
                                        T.reads(placeholder_1[v0, v1, v2, v3, v4])
                                        T.writes(placeholder_shared[v0, v1, v2, v3, v4])
                                        placeholder_shared[v0, v1, v2, v3, v4] = placeholder_1[v0, v1, v2, v3, v4]
                        for i5_1, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_2, i6_2, i7_2, i8_2, i0_4, i1_4, i2_4, i3_4, i4_4 in T.grid(5, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv3d_ncdhw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(4, i0_1_i1_1_i2_1_i3_1_i4_1_fused)
                                yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4)
                                xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 7 + i0_2_i1_2_i2_2_i3_2_i4_2_fused // 5)
                                zz = T.axis.spatial(10, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 5 + i0_2_i1_2_i2_2_i3_2_i4_2_fused % 5)
                                rc = T.axis.reduce(320, i5_0 * 10 + i5_1 * 2 + i5_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                rz = T.axis.reduce(1, 0)
                                T.reads(conv3d_ncdhw_local[nn, ff, yy, xx, zz], pad_temp_shared[0, rc, yy, xx, zz], placeholder_shared[ff, rc, 0, 0, 0])
                                T.writes(conv3d_ncdhw_local[nn, ff, yy, xx, zz])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv3d_ncdhw.cuda", ["TENSOR", [1, 320, 14, 14, 10], "float32"], ["TENSOR", [4, 320, 1, 1, 1], "float32"], [1, 1, 1], [0, 0, 0, 0, 0, 0], [1, 1, 1], 1, "float32"]})
                                conv3d_ncdhw_local[nn, ff, yy, xx, zz] = conv3d_ncdhw_local[nn, ff, yy, xx, zz] + pad_temp_shared[0, rc, yy, xx, zz] * placeholder_shared[ff, rc, 0, 0, 0]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                        with T.block("conv3d_ncdhw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(4, i0_1_i1_1_i2_1_i3_1_i4_1_fused + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 7 + i0_2_i1_2_i2_2_i3_2_i4_2_fused // 5 + ax3)
                            v4 = T.axis.spatial(10, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 5 + i0_2_i1_2_i2_2_i3_2_i4_2_fused % 5 + ax4)
                            T.reads(conv3d_ncdhw_local[v0, v1, v2, v3, v4])
                            T.writes(conv3d_ncdhw[v0, v1, v2, v3, v4])
                            conv3d_ncdhw[v0, v1, v2, v3, v4] = conv3d_ncdhw_local[v0, v1, v2, v3, v4]
    

[14:50:34] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: Printing out trace for AHB:
[14:50:34] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:127: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv3d_ncdhw\", func_name=\"main\")", "b2 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l3, l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)", "v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l17, l18, l19, l20, l21 = sch.split(loop=l3, factors=[v12, v13, v14, v15, v16])", "v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 4, 1, 1, 1])", "l27, l28, l29, l30, l31 = sch.split(loop=l4, factors=[v22, v23, v24, v25, v26])", "v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 1])", "l37, l38, l39, l40, l41 = sch.split(loop=l5, factors=[v32, v33, v34, v35, v36])", "v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 1])", "l47, l48, l49, l50, l51 = sch.split(loop=l6, factors=[v42, v43, v44, v45, v46])", "v52, v53, v54, v55, v56 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 5, 1, 1])", "l57, l58, l59, l60, l61 = sch.split(loop=l7, factors=[v52, v53, v54, v55, v56])", "v62, v63, v64 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[32, 5, 2])", "l65, l66, l67 = sch.split(loop=l8, factors=[v62, v63, v64])", "v68, v69, v70 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l71, l72, l73 = sch.split(loop=l9, factors=[v68, v69, v70])", "v74, v75, v76 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l77, l78, l79 = sch.split(loop=l10, factors=[v74, v75, v76])", "v80, v81, v82 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l83, l84, l85 = sch.split(loop=l11, factors=[v80, v81, v82])", "sch.reorder(l17, l27, l37, l47, l57, l18, l28, l38, l48, l58, l19, l29, l39, l49, l59, l65, l71, l77, l83, l66, l72, l78, l84, l20, l30, l40, l50, l60, l67, l73, l79, l85, l21, l31, l41, l51, l61)", "l86 = sch.fuse(l17, l27, l37, l47, l57)", "sch.bind(loop=l86, thread_axis=\"blockIdx.x\")", "l87 = sch.fuse(l18, l28, l38, l48, l58)", "sch.bind(loop=l87, thread_axis=\"vthread.x\")", "l88 = sch.fuse(l19, l29, l39, l49, l59)", "sch.bind(loop=l88, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b89 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b89, loop=l88, preserve_unit_loops=True)", "b90 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b90, loop=l83, preserve_unit_loops=True)", "l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b90)", "l103 = sch.fuse(l98, l99, l100, l101, l102)", "v104 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)", "sch.annotate(block_or_loop=b90, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v104)", "b105 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b105, loop=l83, preserve_unit_loops=True)", "l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b105)", "l118 = sch.fuse(l113, l114, l115, l116, l117)", "v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)", "sch.annotate(block_or_loop=b105, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v119)", "sch.compute_inline(block=b0)", "v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v120)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b90, ann_key=\"meta_schedule.cooperative_fetch\")", "l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b90)", "l129, l130, l131 = sch.split(loop=l128, factors=[None, 35, 2])", "sch.vectorize(loop=l131)", "sch.bind(loop=l130, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b105, ann_key=\"meta_schedule.cooperative_fetch\")", "l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b105)", "l140, l141, l142 = sch.split(loop=l139, factors=[None, 35, 2])", "sch.vectorize(loop=l142)", "sch.bind(loop=l141, thread_axis=\"threadIdx.x\")", "b143 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b143, ann_key=\"meta_schedule.unroll_explicit\")", "b144, b145, b146, b147 = sch.get_child_blocks(b143)", "l148, l149, l150, l151, l152, l153, l154, l155, l156, l157 = sch.get_loops(block=b144)", "l158, l159, l160, l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b145)", "l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191, l192 = sch.get_loops(block=b146)", "l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b147)", "b201 = sch.get_block(name=\"conv3d_ncdhw\", func_name=\"main\")", "l202, l203, l204, l205, l206, l207, l208, l209, l210, l211, l212, l213, l214, l215, l216, l217, l218, l219, l220, l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b201)", "b227 = sch.decompose_reduction(block=b201, loop=l205)"]
Starting to build with relay.
/home/yj/anaconda3/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
The result is correct!
