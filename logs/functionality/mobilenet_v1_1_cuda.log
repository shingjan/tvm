nohup: ignoring input
[01:02:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #0: "fused_nn_conv2d_add_clip"
[01:02:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 3, 225, 225], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 3, 225, 225):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(0 <= i2_1 and i2_1 < 224 and 0 <= i3_1 and i3_1 < 224, placeholder[i0_1, i1_1, i2_1, i3_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 112, 112, 3, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[01:02:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:02:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 3, 225, 225], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(6272, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(3, 1, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(255):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused % 196 // 14 * 16 + ax0_ax1_ax2_ax3_fused % 255 // 15)
                                    v3 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused % 14 * 16 + i6_0 + ax0_ax1_ax2_ax3_fused % 15)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 224 and 0 <= v3 and v3 < 224, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(3):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 196)
                                    v1, v2, v3 = T.axis.remap("SSS", [i4_0, ax0_ax1_ax2_ax3_fused, i6_0])
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 196)
                                    yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 196 // 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i2_3 * 2 + i2_4)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_1, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 4, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 196 + ax1)
                                v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 196 // 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + ax2)
                                v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 1, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 2, 1, 2, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 2, 4, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[01:02:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #1: "fused_nn_conv2d_add_clip_1"
[01:02:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 32, 114, 114], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 32, 114, 114):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 113 and 1 <= i3_1 and i3_1 < 113, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 32, 112, 112, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[01:02:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:02:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(112, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(16, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(408576):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, ax0_ax1_ax2_ax3_fused % 408576 // 12768)
                                    v2 = T.axis.spatial(114, ax0_ax1_ax2_ax3_fused % 12768 // 112)
                                    v3 = T.axis.spatial(114, i5_0 + ax0_ax1_ax2_ax3_fused % 112)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(96):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, ax0_ax1_ax2_ax3_fused // 3)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    v3 = T.axis.spatial(3, i5_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 8, 2, 7, 3, 1, 1, 1, 2, 1):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + i1_3)
                                    i = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i2_3 * 2 + i2_4)
                                    j = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 4, 7):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + ax1)
                                v2 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + ax2)
                                v3 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 8, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 28, 1, 2, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 4, 4, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[01:02:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #2: "fused_nn_conv2d_add_clip_2"
[01:02:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 64, 112, 112], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 112, 112, 32, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 64, 112, 112):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 64, 112, 112):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[01:02:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:02:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([64, 32, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(392, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(25088):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 4 + ax0_ax1_ax2_ax3_fused // 6272)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + ax0_ax1_ax2_ax3_fused % 6272 // 112)
                                    v3 = T.axis.spatial(112, ax0_ax1_ax2_ax3_fused % 112)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(256):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused // 4)
                                    v1 = T.axis.spatial(32, i4_0 * 4 + ax0_ax1_ax2_ax3_fused % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 2, 1, 8, 1, 1, 1, 1, 16, 1, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused * 32 + i1_3 * 16 + i1_4)
                                    yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + i0_1_i1_1_i2_1_i3_1_fused // 7)
                                    xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 7 * 16 + i3_3 * 2 + i3_4)
                                    rc = T.axis.reduce(32, i4_0 * 4 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 16):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused * 32 + ax1)
                                v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + i0_1_i1_1_i2_1_i3_1_fused // 7 + ax2)
                                v3 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 7 * 16 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 2, 2, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 56, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 8, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[01:02:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #3: "fused_nn_conv2d_add_clip_3"
[01:02:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 64, 113, 113], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 64, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 113, 113):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(0 <= i2_1 and i2_1 < 112 and 0 <= i3_1 and i3_1 < 112, placeholder[i0_1, i1_1, i2_1, i3_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 64, 56, 56, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i * 2 + di, j * 2 + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i * 2 + di, j * 2 + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 64, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 64, 56, 56):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[01:02:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:02:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            DepthwiseConv2d_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 64, 113, 113], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([64, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(32, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(16, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(9792):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused % 9792 // 153)
                                    v2 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused // 7 * 8 + ax0_ax1_ax2_ax3_fused % 153 // 17)
                                    v3 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + ax0_ax1_ax2_ax3_fused % 17)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 112 and 0 <= v3 and v3 < 112, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(576):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 2, 1, 3, 1, 1, 2, 1, 1):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_4)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i2_3)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 1):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + ax1)
                                v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + ax2)
                                v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 16, 2, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 2, 1, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 8, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[01:02:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #4: "fused_nn_conv2d_add_clip_4"
[01:02:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 56, 56], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 56, 56):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 56, 56, 64, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[01:02:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:02:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 64, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(112, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1568):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + ax0_ax1_ax2_ax3_fused // 784)
                                    v2 = T.axis.spatial(56, ax0_ax1_ax2_ax3_fused % 784 // 14)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(128):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 64 + ax0_ax1_ax2_ax3_fused // 2)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + ax0_ax1_ax2_ax3_fused % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 1, 1, 2, 1, 1, 1, 1, 4, 7):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 28 * 16 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3)
                                    yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 28 // 2 * 4 + i2_4)
                                    xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i3_4)
                                    rc = T.axis.reduce(64, i4_0 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 28 * 16 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                                v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 28 // 2 * 4 + ax2)
                                v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 4, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 14, 1, 1, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 2, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[01:02:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #5: "fused_nn_conv2d_add_clip_5"
[01:02:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 128, 58, 58], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 58, 58):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 128, 56, 56, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[01:02:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:02:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            DepthwiseConv2d_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 128, 58, 58], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(13456):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 4 + ax0_ax1_ax2_ax3_fused // 3364)
                                    v2 = T.axis.spatial(58, ax0_ax1_ax2_ax3_fused % 3364 // 58)
                                    v3 = T.axis.spatial(58, ax0_ax1_ax2_ax3_fused % 58)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(36):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 4 + ax0_ax1_ax2_ax3_fused // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 2, 1, 1, 1, 2, 56, 2):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4)
                                    i = T.axis.spatial(56, i2_4)
                                    j = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_1, i5_1])
                                    T.reads(PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 56, 4):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                                v2 = T.axis.spatial(56, ax2)
                                v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 2, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 56])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 2, 2, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[01:03:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #6: "fused_nn_conv2d_add_clip_6"
[01:03:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 56, 56, 128, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[01:03:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:03:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 128, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(224, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(16, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(448):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 16 + ax0_ax1_ax2_ax3_fused // 28)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 112 // 28 * 14 + ax0_ax1_ax2_ax3_fused % 28 // 2)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 * 2 + ax0_ax1_ax2_ax3_fused % 2)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(1024):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 112 * 64 + ax0_ax1_ax2_ax3_fused // 16)
                                    v1 = T.axis.spatial(128, i4_0 * 16 + ax0_ax1_ax2_ax3_fused % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 4, 1, 2, 2, 1, 1, 1, 1, 7, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 112 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_3)
                                    yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 112 // 28 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i2_4)
                                    xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 * 2 + i3_3)
                                    rc = T.axis.reduce(128, i4_0 * 16 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 112 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + ax1)
                                v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 112 // 28 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax2)
                                v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 8, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 2, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[28, 1, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 8, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[01:03:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #7: "fused_nn_conv2d_add_clip_7"
[01:03:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 128, 57, 57], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 57, 57):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(0 <= i2_1 and i2_1 < 56 and 0 <= i3_1 and i3_1 < 56, placeholder[i0_1, i1_1, i2_1, i3_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 128, 28, 28, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i * 2 + di, j * 2 + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i * 2 + di, j * 2 + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[01:03:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:03:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            DepthwiseConv2d_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 128, 57, 57], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(32, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(14, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(51984):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 16 + ax0_ax1_ax2_ax3_fused // 3249)
                                    v2 = T.axis.spatial(57, ax0_ax1_ax2_ax3_fused % 3249 // 57)
                                    v3 = T.axis.spatial(57, ax0_ax1_ax2_ax3_fused % 57)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 56 and 0 <= v3 and v3 < 56, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(144):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 16 + ax0_ax1_ax2_ax3_fused // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 14, 2):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    i = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 14 + i2_4)
                                    j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_1, i5_2])
                                    T.reads(PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 14, 2):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax1)
                                v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 14 + ax2)
                                v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 8, 2, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 14])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 7, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[01:03:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #8: "fused_nn_conv2d_add_clip_8"
[01:03:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 256, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 28, 28, 128, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[01:03:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:03:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([256, 128, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(896, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(896):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 8 + ax0_ax1_ax2_ax3_fused // 112)
                                    v2 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 112 // 4)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + ax0_ax1_ax2_ax3_fused % 4)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(2048):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, ax0_ax1_ax2_ax3_fused // 8)
                                    v1 = T.axis.spatial(128, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 2, 4, 2, 2, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3)
                                    yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 4 + i2_3)
                                    xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3)
                                    rc = T.axis.reduce(128, i4_0 * 8 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 4, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax1)
                                v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 4 + ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 64, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 2, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 4, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[01:03:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #9: "fused_nn_conv2d_add_clip_9"
[01:03:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 256, 30, 30], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 256, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 30, 30):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 256, 28, 28, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[01:03:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:03:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            DepthwiseConv2d_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 256, 30, 30], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([256, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(61440):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + ax0_ax1_ax2_ax3_fused % 61440 // 480)
                                    v2 = T.axis.spatial(30, ax0_ax1_ax2_ax3_fused % 480 // 16)
                                    v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ax0_ax1_ax2_ax3_fused % 16)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(1152):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + ax0_ax1_ax2_ax3_fused // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 32, 2, 1, 3, 1, 1, 2, 7, 14):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i1_3 * 2 + i1_4)
                                    i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused * 14 + i2_3 * 7 + i2_4)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 64, 14, 14):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 64 + ax1)
                                v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused * 14 + ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 1, 32, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 2, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 14])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[01:03:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #10: "fused_nn_conv2d_add_clip_10"
[01:03:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 256, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 28, 28, 256, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[01:03:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:03:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([256, 256, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(448, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(16, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(56):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 8 + ax0_ax1_ax2_ax3_fused // 7)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 112 // 28 * 7 + ax0_ax1_ax2_ax3_fused % 7)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(512):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 112 * 64 + ax0_ax1_ax2_ax3_fused // 8)
                                    v1 = T.axis.spatial(256, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 112 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_4)
                                    yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 112 // 28 * 7 + i0_1_i1_1_i2_1_i3_1_fused)
                                    xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28)
                                    rc = T.axis.reduce(256, i4_0 * 8 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 112 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                                v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 112 // 28 * 7 + i0_1_i1_1_i2_1_i3_1_fused + ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 16, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[28, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[01:03:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #11: "fused_nn_conv2d_add_clip_11"
[01:03:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 256, 29, 29], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 29, 29):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(0 <= i2_1 and i2_1 < 28 and 0 <= i3_1 and i3_1 < 28, placeholder[i0_1, i1_1, i2_1, i3_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 256, 14, 14, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i * 2 + di, j * 2 + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i * 2 + di, j * 2 + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[01:03:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:03:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            DepthwiseConv2d_local = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 256, 29, 29], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([256, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(3, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(25056):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 32 + ax0_ax1_ax2_ax3_fused % 25056 // 783)
                                    v2 = T.axis.spatial(29, i4_0 + ax0_ax1_ax2_ax3_fused % 783 // 29)
                                    v3 = T.axis.spatial(29, ax0_ax1_ax2_ax3_fused % 29)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 28 and 0 <= v3 and v3 < 28, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(96):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 32 + ax0_ax1_ax2_ax3_fused // 3)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, i4_0)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 2, 1, 1, 1, 1, 1, 2, 1, 7):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_3 * 2 + i1_4)
                                    i = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 14)
                                    j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                    T.reads(PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 7):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + ax1)
                                v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 14 + ax2)
                                v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 4, 2, 2, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 14, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[01:03:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #12: "fused_nn_conv2d_add_clip_12"
[01:03:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 512, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 512, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 512, 14, 14, 256, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 512, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 14, 14):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[01:03:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:03:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([512, 256, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(8, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(448):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 16 + ax0_ax1_ax2_ax3_fused // 28)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 28 // 2)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 2 + ax0_ax1_ax2_ax3_fused % 2)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(8192):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, ax0_ax1_ax2_ax3_fused // 16)
                                    v1 = T.axis.spatial(256, i4_0 * 16 + ax0_ax1_ax2_ax3_fused % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 64, 7, 2, 2, 1, 1, 1, 2, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused // 2 * 128 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i2_3)
                                    xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 2 + i3_3)
                                    rc = T.axis.reduce(256, i4_0 * 16 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 128, 7, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused // 2 * 128 + ax1)
                                v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 64, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 8, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[01:03:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #13: "fused_nn_conv2d_add_clip_13"
[01:03:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 512, 16, 16], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 512, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 512, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 16, 16):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 512, 14, 14, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 512, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 14, 14):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[01:03:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:03:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            DepthwiseConv2d_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 512, 16, 16], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(224, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1152):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + ax0_ax1_ax2_ax3_fused % 1152 // 36)
                                    v2 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 2 + ax0_ax1_ax2_ax3_fused % 36 // 9)
                                    v3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + ax0_ax1_ax2_ax3_fused % 9)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(288):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + ax0_ax1_ax2_ax3_fused // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4)
                                    i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    di, dj = T.axis.remap("RR", [i4_1, i5_2])
                                    T.reads(PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                                v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 8, 2, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[01:03:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #14: "fused_nn_conv2d_add_clip_14"
[01:03:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 512, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 512, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 512, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 512, 14, 14, 512, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 512, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 14, 14):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[01:03:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:03:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([512, 512, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1568):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 8 + ax0_ax1_ax2_ax3_fused // 196)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 196 // 14)
                                    v3 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(4096):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, ax0_ax1_ax2_ax3_fused // 8)
                                    v1 = T.axis.spatial(512, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 32, 2, 14, 4, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused // 7 * 32 + i1_3)
                                    yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_3)
                                    xx = T.axis.spatial(14, i3_3)
                                    rc = T.axis.reduce(512, i4_0 * 8 + i4_1 * 4 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 2, 14):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused // 7 * 32 + ax1)
                                v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax2)
                                v3 = T.axis.spatial(14, ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 16, 32, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 14, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[01:03:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #15: "fused_nn_conv2d_add_clip_15"
[01:03:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 512, 15, 15], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 15, 15):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(0 <= i2_1 and i2_1 < 14 and 0 <= i3_1 and i3_1 < 14, placeholder[i0_1, i1_1, i2_1, i3_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 512, 7, 7, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i * 2 + di, j * 2 + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i * 2 + di, j * 2 + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[01:03:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:03:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            DepthwiseConv2d_local = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 512, 15, 15], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(7, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(3, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(3120):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 16 + ax0_ax1_ax2_ax3_fused % 3120 // 195)
                                    v2 = T.axis.spatial(15, i4_0 + ax0_ax1_ax2_ax3_fused % 195 // 15)
                                    v3 = T.axis.spatial(15, ax0_ax1_ax2_ax3_fused % 15)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 14 and 0 <= v3 and v3 < 14, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(48):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 16 + ax0_ax1_ax2_ax3_fused // 3)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, i4_0)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 3, 1, 16, 1, 1):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 16 + i1_4)
                                    i, j, di, dj = T.axis.remap("SSRR", [i0_1_i1_1_i2_1_i3_1_fused, i0_2_i1_2_i2_2_i3_2_fused, i4_0, i5_2])
                                    T.reads(PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 1):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 16 + ax1)
                                v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused + ax2)
                                v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 1, 1, 16])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[01:03:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #16: "fused_nn_conv2d_add_clip_16"
[01:03:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(1024, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], compute: T.Buffer[(1, 1024, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 1024, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 1024, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 1024, 7, 7, 512, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 1024, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 1024, 7, 7):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[01:03:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:03:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(1024, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], compute: T.Buffer[(1, 1024, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 1024, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([1024, 512, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(98, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(12544):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 256 + ax0_ax1_ax2_ax3_fused // 49)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(16384):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + ax0_ax1_ax2_ax3_fused // 256)
                                    v1 = T.axis.spatial(512, i4_0 * 256 + ax0_ax1_ax2_ax3_fused % 256)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 2, 1, 1, 16, 1, 1, 1, 16, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused // 49 * 32 + i1_3 * 16 + i1_4)
                                    yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 49 // 7)
                                    xx = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    rc = T.axis.reduce(512, i4_0 * 256 + i4_1 * 16 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused // 49 * 32 + ax1)
                                v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 49 // 7 + ax2)
                                v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 2, 1, 2, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 16, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[01:03:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #17: "fused_nn_conv2d_add_clip_17"
[01:03:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 7, 7), "float32"], placeholder_1: T.Buffer[(1024, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], compute: T.Buffer[(1, 1024, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 1024, 9, 9], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 1024, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 1024, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1024, 9, 9):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3_1 and i3_1 < 8, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 1024, 7, 7, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 1024, 7, 7], "float32"], ["TENSOR", [1024, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 1024, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 1024, 7, 7):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[01:03:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:03:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 7, 7), "float32"], placeholder_1: T.Buffer[(1024, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], compute: T.Buffer[(1, 1024, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            DepthwiseConv2d_local = T.alloc_buffer([1, 1024, 7, 7], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 1024, 9, 9], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([1024, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(3, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(50176):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, ax0_ax1_ax2_ax3_fused % 50176 // 49)
                                    v2 = T.axis.spatial(9, i4_0 + ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(9, i5_0 + ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 8 and 1 <= v3 and v3 < 8, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(1024):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, ax0_ax1_ax2_ax3_fused)
                                    v1 = T.axis.spatial(1, 0)
                                    v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 4, 7, 1, 1, 1, 1, 2, 1, 7):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused * 512 + i0_2_i1_2_i2_2_i3_2_fused * 8 + i1_3 * 2 + i1_4)
                                    i, j, di, dj = T.axis.remap("SSRR", [i2_3, i3_4, i4_0, i5_0])
                                    T.reads(PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 1024, 7, 7], "float32"], ["TENSOR", [1024, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 7, 7):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused * 512 + i0_2_i1_2_i2_2_i3_2_fused * 8 + ax1)
                                v2, v3 = T.axis.remap("SS", [ax2, ax3])
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 64, 4, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[01:03:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #18: "fused_nn_conv2d_add_clip_18"
[01:03:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 7, 7), "float32"], placeholder_1: T.Buffer[(1024, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], compute: T.Buffer[(1, 1024, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 1024, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 1024, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 1024, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1024, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 1024, 7, 7, 1024, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 7, 7], "float32"], ["TENSOR", [1024, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 1024, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 1024, 7, 7):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[01:03:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:03:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 7, 7), "float32"], placeholder_1: T.Buffer[(1024, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], compute: T.Buffer[(1, 1024, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 1024, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 1024, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([1024, 1024, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(224, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(6272):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i4_0 * 128 + ax0_ax1_ax2_ax3_fused // 49)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(131072):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, ax0_ax1_ax2_ax3_fused // 128)
                                    v1 = T.axis.spatial(1024, i4_0 * 128 + ax0_ax1_ax2_ax3_fused % 128)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 2, 1, 1, 16, 1, 1, 1, 16, 7, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i1_3 * 16 + i1_4)
                                    yy = T.axis.spatial(7, i2_4)
                                    xx = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    rc = T.axis.reduce(1024, i4_0 * 128 + i4_1 * 16 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 7, 7], "float32"], ["TENSOR", [1024, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 7, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + ax1)
                                v2 = T.axis.spatial(7, ax2)
                                v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 32, 1, 2, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 8, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[01:03:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #19: "fused_nn_avg_pool2d"
[01:03:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 7, 7), "float32"], tensor: T.Buffer[(1, 1024, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 1024, 1, 1], dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 1024, 1, 1, 7, 7):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor_1[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1]
        for i0, i1, i2, i3 in T.grid(1, 1024, 1, 1):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(tensor_1[ax0, ax1, ax2, ax3])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2, 0) * 2 + T.min(ax2 * 2 + 6, 6) + 1 - ax2 * 2) * (T.min(ax3, 0) * 2 + T.min(ax3 * 2 + 6, 6) + 1 - ax3 * 2), 1), "float32")
    

[01:03:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 2 design space(s) generated
[01:03:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 7, 7), "float32"], tensor: T.Buffer[(1, 1024, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            tensor_shared = T.alloc_buffer([1, 1024, 1, 1], dtype="float32", scope="shared")
            for i0, i1, i2, i3_0 in T.grid(1, 1024, 1, 1):
                for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(1, 1, 1, 1, 7):
                    for ax4_ax5_fused_1 in T.thread_binding(8, thread="threadIdx.x"):
                        with T.block("tensor"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(1024, i1)
                            ax2_1 = T.axis.spatial(1, 0)
                            ax3_1 = T.axis.spatial(1, 0)
                            rv0 = T.axis.reduce(7, (ax4_ax5_fused_0 * 8 + ax4_ax5_fused_1) // 7)
                            rv1 = T.axis.reduce(7, (ax4_ax5_fused_0 * 8 + ax4_ax5_fused_1) % 7)
                            T.where(ax4_ax5_fused_0 * 8 + ax4_ax5_fused_1 < 49)
                            T.reads(placeholder[ax0_1, ax1_1, ax2_1 * 2 + rv0, ax3_1 * 2 + rv1])
                            T.writes(tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1])
                            with T.init():
                                tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1] = T.float32(0)
                            tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1] = tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1] + placeholder[ax0_1, ax1_1, ax2_1 * 2 + rv0, ax3_1 * 2 + rv1]
                for i3_1 in T.thread_binding(8, thread="threadIdx.x"):
                    with T.block("tensor_1"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(1024, i1)
                        ax2 = T.axis.spatial(1, 0)
                        ax3 = T.axis.spatial(1, 0)
                        T.where(i3_1 < 1)
                        T.reads(tensor_shared[ax0, ax1, ax2, ax3])
                        T.writes(tensor[ax0, ax1, ax2, ax3])
                        tensor[ax0, ax1, ax2, ax3] = tensor_shared[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2, 0) * 2 + T.min(ax2 * 2 + 6, 6) + 1 - ax2 * 2) * (T.min(ax3, 0) * 2 + T.min(ax3 * 2 + 6, 6) + 1 - ax3 * 2), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
b2, = sch.get_consumers(block=b0)
l3, l4, l5, l6 = sch.get_loops(block=b2)
v7 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=1)
l8, l9 = sch.split(loop=l6, factors=[None, v7])
sch.bind(loop=l9, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l8, preserve_unit_loops=True)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l10, l11, l12, l13, l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b0)
l20 = sch.fuse(l18, l19)
l21, l22 = sch.split(loop=l20, factors=[None, v7])
sch.bind(loop=l22, thread_axis="threadIdx.x")
v23 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v23)
[01:03:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 7, 7), "float32"], tensor: T.Buffer[(1, 1024, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            tensor_1 = T.alloc_buffer([1, 1024, 1, 1], dtype="float32")
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 1024, 1, 1, 7, 7):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                    T.writes(tensor_1[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                    tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1]
            for i0, i1, i2, i3 in T.grid(1, 1024, 1, 1):
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2, 0) * 2 + T.min(ax2 * 2 + 6, 6) + 1 - ax2 * 2) * (T.min(ax3, 0) * 2 + T.min(ax3 * 2 + 6, 6) + 1 - ax3 * 2), 1), "float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[01:03:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #20: "fused_nn_conv2d_add"
[01:03:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 1, 1), "float32"], placeholder_1: T.Buffer[(1001, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1001, 1, 1), "float32"], T_add: T.Buffer[(1, 1001, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 1024, 1, 1], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 1001, 1, 1], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1024, 1, 1):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 1001, 1, 1, 1024, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 1, 1], "float32"], ["TENSOR", [1001, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 1001, 1, 1):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, ax2, ax3]
    

[01:04:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 1, 1), "float32"], placeholder_1: T.Buffer[(1001, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1001, 1, 1), "float32"], T_add: T.Buffer[(1, 1001, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 1001, 1, 1], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 1024, 1, 1], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([1001, 1024, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(13, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(11, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(512):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i4_0 * 512 + ax0_ax1_ax2_ax3_fused)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(39424):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1001, i0_0_i1_0_i2_0_i3_0_fused * 77 + ax0_ax1_ax2_ax3_fused // 512)
                                    v1 = T.axis.spatial(1024, i4_0 * 512 + ax0_ax1_ax2_ax3_fused % 512)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 1, 1, 1, 64, 1, 1, 1, 7, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(1001, i0_0_i1_0_i2_0_i3_0_fused * 77 + i0_2_i1_2_i2_2_i3_2_fused * 7 + i1_4)
                                    yy = T.axis.spatial(1, 0)
                                    xx = T.axis.spatial(1, 0)
                                    rc = T.axis.reduce(1024, i4_0 * 512 + i4_1 * 64 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 1, 1], "float32"], ["TENSOR", [1001, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 7, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(1001, i0_0_i1_0_i2_0_i3_0_fused * 77 + i0_2_i1_2_i2_2_i3_2_fused * 7 + ax1)
                                v2, v3 = T.axis.remap("SS", [ax2, ax3])
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[13, 1, 11, 1, 7])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[2, 8, 64])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[01:04:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #21: "fused_reshape_squeeze_reshape"
[01:04:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001, 1, 1), "float32"], T_reshape: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape_1 = T.alloc_buffer([1, 1, 1, 1001], dtype="float32")
        T_squeeze = T.alloc_buffer([1, 1001], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1, 1, 1001):
            with T.block("T_reshape"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[0, ax3 % 1001, 0, 0])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3])
                T_reshape_1[ax0, ax1, ax2, ax3] = placeholder[0, ax3 % 1001, 0, 0]
        for i0, i1 in T.grid(1, 1001):
            with T.block("T_squeeze"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_reshape_1[ax0, 0, 0, ax1])
                T.writes(T_squeeze[ax0, ax1])
                T_squeeze[ax0, ax1] = T_reshape_1[ax0, 0, 0, ax1]
        for i0, i1 in T.grid(1, 1001):
            with T.block("T_reshape_1"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_squeeze[0, ax1 % 1001])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = T_squeeze[0, ax1 % 1001]
    

[01:04:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001, 1, 1), "float32"], T_reshape: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1 in T.grid(1, 1001):
                with T.block("T_reshape_1"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder[0, ax1 % 1001, 0, 0])
                    T.writes(T_reshape[ax0, ax1])
                    T_reshape[ax0, ax1] = placeholder[0, ax1 % 1001 % 1001, 0, 0]
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_squeeze", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[01:04:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #22: "fused_nn_softmax"
[01:04:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_softmax_norm: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
        T_softmax_exp = T.alloc_buffer([1, 1001], dtype="float32")
        T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
        for i0, i1 in T.grid(1, 1001):
            with T.block("T_softmax_maxelem"):
                i0_1, k = T.axis.remap("SR", [i0, i1])
                T.reads(placeholder[i0_1, k])
                T.writes(T_softmax_maxelem[i0_1])
                with T.init():
                    T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
        for i0, i1 in T.grid(1, 1001):
            with T.block("T_softmax_exp"):
                i0_2, i1_1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder[i0_2, i1_1], T_softmax_maxelem[i0_2])
                T.writes(T_softmax_exp[i0_2, i1_1])
                T_softmax_exp[i0_2, i1_1] = T.exp(placeholder[i0_2, i1_1] - T_softmax_maxelem[i0_2], dtype="float32")
        for i0_3, i1 in T.grid(1, 1001):
            with T.block("T_softmax_expsum"):
                i0_4, k = T.axis.remap("SR", [i0_3, i1])
                T.reads(T_softmax_exp[i0_4, k])
                T.writes(T_softmax_expsum[i0_4])
                with T.init():
                    T_softmax_expsum[i0_4] = T.float32(0)
                T_softmax_expsum[i0_4] = T_softmax_expsum[i0_4] + T_softmax_exp[i0_4, k]
        for i0_5, i1 in T.grid(1, 1001):
            with T.block("T_softmax_norm"):
                i0_6, i1_2 = T.axis.remap("SS", [i0_5, i1])
                T.reads(T_softmax_exp[i0_6, i1_2], T_softmax_expsum[i0_6])
                T.writes(T_softmax_norm[i0_6, i1_2])
                T.block_attr({"axis":1})
                T_softmax_norm[i0_6, i1_2] = T_softmax_exp[i0_6, i1_2] / T_softmax_expsum[i0_6]
    

[01:04:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 4 design space(s) generated
[01:04:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_softmax_norm: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            T_softmax_maxelem_shared = T.alloc_buffer([1], dtype="float32", scope="shared")
            T_softmax_expsum_shared = T.alloc_buffer([1], dtype="float32", scope="shared")
            for i0 in T.serial(1):
                for ax0, ax1_0 in T.grid(1, 63):
                    for ax1_1 in T.thread_binding(16, thread="threadIdx.x"):
                        with T.block("T_softmax_maxelem"):
                            i0_1 = T.axis.spatial(1, 0)
                            k = T.axis.reduce(1001, ax1_0 * 16 + ax1_1)
                            T.where(ax1_0 * 16 + ax1_1 < 1001)
                            T.reads(placeholder[i0_1, k])
                            T.writes(T_softmax_maxelem_shared[i0_1])
                            with T.init():
                                T_softmax_maxelem_shared[i0_1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[i0_1] = T.max(T_softmax_maxelem_shared[i0_1], placeholder[i0_1, k])
                for ax0, ax1_0 in T.grid(1, 63):
                    for ax1_1 in T.thread_binding(16, thread="threadIdx.x"):
                        with T.block("T_softmax_expsum"):
                            i0_2 = T.axis.spatial(1, 0)
                            k = T.axis.reduce(1001, ax1_0 * 16 + ax1_1)
                            T.where(ax1_0 * 16 + ax1_1 < 1001)
                            T.reads(placeholder[i0_2, k], T_softmax_maxelem_shared[i0_2])
                            T.writes(T_softmax_expsum_shared[i0_2])
                            with T.init():
                                T_softmax_expsum_shared[i0_2] = T.float32(0)
                            T_softmax_expsum_shared[i0_2] = T_softmax_expsum_shared[i0_2] + T.exp(placeholder[i0_2, k] - T_softmax_maxelem_shared[i0_2], dtype="float32")
                for i1_0 in T.serial(63):
                    for i1_1 in T.thread_binding(16, thread="threadIdx.x"):
                        with T.block("T_softmax_norm"):
                            i0_3 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(1001, i1_0 * 16 + i1_1)
                            T.where(i1_0 * 16 + i1_1 < 1001)
                            T.reads(placeholder[i0_3, i1], T_softmax_maxelem_shared[i0_3], T_softmax_expsum_shared[i0_3])
                            T.writes(T_softmax_norm[i0_3, i1])
                            T.block_attr({"axis":1})
                            T_softmax_norm[i0_3, i1] = T.exp(placeholder[i0_3, i1] - T_softmax_maxelem_shared[i0_3], dtype="float32") / T_softmax_expsum_shared[i0_3]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
b4, = sch.get_consumers(block=b2)
l5, l6 = sch.get_loops(block=b4)
v7 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=2)
l8, l9 = sch.split(loop=l6, factors=[None, v7])
sch.bind(loop=l9, thread_axis="threadIdx.x")
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
sch.set_scope(block=b2, buffer_index=0, storage_scope="shared")
l10, l11, l12 = sch.get_loops(block=b2)
l13, l14 = sch.split(loop=l12, factors=[None, v7])
sch.bind(loop=l14, thread_axis="threadIdx.x")
b15, b16 = sch.get_consumers(block=b0)
l17, l18, l19, l20 = sch.get_loops(block=b15)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l21, l22, l23 = sch.get_loops(block=b0)
l24, l25 = sch.split(loop=l23, factors=[None, v7])
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
[01:04:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_softmax_norm: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum_shared = T.alloc_buffer([1], dtype="float32", scope="shared")
            for i0, i1 in T.grid(1, 1001):
                with T.block("T_softmax_maxelem"):
                    i0_1, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_1, k])
                    T.writes(T_softmax_maxelem[i0_1])
                    with T.init():
                        T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
            for i0 in T.serial(1):
                for ax0, ax1_0 in T.grid(1, 63):
                    for ax1_1 in T.thread_binding(16, thread="threadIdx.x"):
                        with T.block("T_softmax_expsum"):
                            i0_2 = T.axis.spatial(1, 0)
                            k = T.axis.reduce(1001, ax1_0 * 16 + ax1_1)
                            T.where(ax1_0 * 16 + ax1_1 < 1001)
                            T.reads(placeholder[i0_2, k], T_softmax_maxelem[i0_2])
                            T.writes(T_softmax_expsum_shared[i0_2])
                            with T.init():
                                T_softmax_expsum_shared[i0_2] = T.float32(0)
                            T_softmax_expsum_shared[i0_2] = T_softmax_expsum_shared[i0_2] + T.exp(placeholder[i0_2, k] - T_softmax_maxelem[i0_2], dtype="float32")
                for i1_0 in T.serial(63):
                    for i1_1 in T.thread_binding(16, thread="threadIdx.x"):
                        with T.block("T_softmax_norm"):
                            i0_3 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(1001, i1_0 * 16 + i1_1)
                            T.where(i1_0 * 16 + i1_1 < 1001)
                            T.reads(placeholder[i0_3, i1], T_softmax_maxelem[i0_3], T_softmax_expsum_shared[i0_3])
                            T.writes(T_softmax_norm[i0_3, i1])
                            T.block_attr({"axis":1})
                            T_softmax_norm[i0_3, i1] = T.exp(placeholder[i0_3, i1] - T_softmax_maxelem[i0_3], dtype="float32") / T_softmax_expsum_shared[i0_3]
    

b0 = sch.get_block(name="T_softmax_exp", func_name="main")
b1 = sch.get_block(name="T_softmax_expsum", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b0)
b3, = sch.get_consumers(block=b1)
l4, l5 = sch.get_loops(block=b3)
v6 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=2)
l7, l8 = sch.split(loop=l5, factors=[None, v6])
sch.bind(loop=l8, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l4, preserve_unit_loops=True)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l9, l10, l11 = sch.get_loops(block=b1)
l12, l13 = sch.split(loop=l11, factors=[None, v6])
sch.bind(loop=l13, thread_axis="threadIdx.x")
v14 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v14)
[01:04:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_softmax_norm: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            for i0, i1_0 in T.grid(1, 2):
                for i1_1 in T.thread_binding(512, thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        i0_1 = T.axis.spatial(1, 0)
                        k = T.axis.reduce(1001, i1_0 * 512 + i1_1)
                        T.where(i1_0 * 512 + i1_1 < 1001)
                        T.reads(placeholder[i0_1, k])
                        T.writes(T_softmax_maxelem[i0_1])
                        with T.init():
                            T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
            for i0, i1 in T.grid(1, 1001):
                with T.block("T_softmax_expsum"):
                    i0_2, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_2, k], T_softmax_maxelem[i0_2])
                    T.writes(T_softmax_expsum[i0_2])
                    with T.init():
                        T_softmax_expsum[i0_2] = T.float32(0)
                    T_softmax_expsum[i0_2] = T_softmax_expsum[i0_2] + T.exp(placeholder[i0_2, k] - T_softmax_maxelem[i0_2], dtype="float32")
            for i0_3, i1 in T.grid(1, 1001):
                with T.block("T_softmax_norm"):
                    i0_4, i1_2 = T.axis.remap("SS", [i0_3, i1])
                    T.reads(placeholder[i0_4, i1_2], T_softmax_maxelem[i0_4], T_softmax_expsum[i0_4])
                    T.writes(T_softmax_norm[i0_4, i1_2])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_4, i1_2] = T.exp(placeholder[i0_4, i1_2] - T_softmax_maxelem[i0_4], dtype="float32") / T_softmax_expsum[i0_4]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=7)
l4, l5 = sch.get_loops(block=b0)
l6, l7 = sch.split(loop=l5, factors=[None, v3])
sch.bind(loop=l7, thread_axis="threadIdx.x")
v8 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v8)
[01:04:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #3:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_softmax_norm: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            for i0, i1 in T.grid(1, 1001):
                with T.block("T_softmax_maxelem"):
                    i0_1, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_1, k])
                    T.writes(T_softmax_maxelem[i0_1])
                    with T.init():
                        T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
            for i0, i1 in T.grid(1, 1001):
                with T.block("T_softmax_expsum"):
                    i0_2, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_2, k], T_softmax_maxelem[i0_2])
                    T.writes(T_softmax_expsum[i0_2])
                    with T.init():
                        T_softmax_expsum[i0_2] = T.float32(0)
                    T_softmax_expsum[i0_2] = T_softmax_expsum[i0_2] + T.exp(placeholder[i0_2, k] - T_softmax_maxelem[i0_2], dtype="float32")
            for i0_3, i1 in T.grid(1, 1001):
                with T.block("T_softmax_norm"):
                    i0_4, i1_1 = T.axis.remap("SS", [i0_3, i1])
                    T.reads(placeholder[i0_4, i1_1], T_softmax_maxelem[i0_4], T_softmax_expsum[i0_4])
                    T.writes(T_softmax_norm[i0_4, i1_1])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_4, i1_1] = T.exp(placeholder[i0_4, i1_1] - T_softmax_maxelem[i0_4], dtype="float32") / T_softmax_expsum[i0_4]
    

b0 = sch.get_block(name="T_softmax_exp", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b0)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
[01:04:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #23: "fused_reshape"
[01:04:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_reshape: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1 in T.grid(1, 1001):
            with T.block("T_reshape"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder[0, ax1 % 1001])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = placeholder[0, ax1 % 1001]
    

[01:04:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_reshape: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1 in T.grid(1, 1001):
                with T.block("T_reshape"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder[0, ax1 % 1001])
                    T.writes(T_reshape[ax0, ax1])
                    T_reshape[ax0, ax1] = placeholder[0, ax1 % 1001]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[01:04:05] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |            N/A |          N/A |                   N/A |      0 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[01:04:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_nn_conv2d_add_clip"
[01:04:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:04:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:04:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9264f8618)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d925c21a88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d925c84918)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d928437508)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9286994c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d92674e9e8)]: 1891 failure(s)
[01:04:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 157 candidate(s)
[01:06:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9264f8618)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d925c21a88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d925c84918)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d928437508)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9286994c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d92674e9e8)]: 268 failure(s)
[01:08:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9264f8618)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d925c21a88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d925c84918)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d928437508)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9286994c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d92674e9e8)]: 261 failure(s)
[01:11:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9264f8618)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d925c21a88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d925c84918)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d928437508)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9286994c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d92674e9e8)]: 250 failure(s)
[01:12:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9264f8618)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d925c21a88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d925c84918)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d928437508)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9286994c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d92674e9e8)]: 262 failure(s)
[01:13:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9999  0.9996  0.9996  0.9995  0.9994  0.9993  0.9993  0.9990  0.9985  0.9985  0.9982  0.9980  0.9979  0.9977
[17 : 32]:	0.9974  0.9972  0.9971  0.9970  0.9969  0.9963  0.9961  0.9957  0.9949  0.9949  0.9946  0.9945  0.9945  0.9942  0.9939  0.9935
[01:13:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:13:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:13:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:14:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:14:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_conv2d_add_clip_1"
[01:14:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:14:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:16:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926737198)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9264ff4f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926502788)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d0a8c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92702d048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926508c68)]: 1980 failure(s)
[01:16:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 68 candidate(s)
[01:19:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926737198)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9264ff4f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926502788)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d0a8c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92702d048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926508c68)]: 389 failure(s)
[01:22:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926737198)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9264ff4f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926502788)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d0a8c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92702d048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926508c68)]: 320 failure(s)
[01:26:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926737198)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9264ff4f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926502788)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d0a8c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92702d048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926508c68)]: 340 failure(s)
[01:28:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926737198)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9264ff4f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926502788)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d0a8c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92702d048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926508c68)]: 348 failure(s)
[01:29:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9996  0.9996  0.9994  0.9994  0.9993  0.9992  0.9992  0.9991  0.9991  0.9991  0.9990  0.9990  0.9990  0.9989  0.9988
[17 : 32]:	0.9984  0.9984  0.9982  0.9976  0.9975  0.9975  0.9968  0.9958  0.9954  0.9953  0.9950  0.9946  0.9941  0.9941  0.9937  0.9936
[01:29:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:29:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:29:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:30:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:30:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_conv2d_add_clip_2"
[01:30:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:30:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:31:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d78058)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d274a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9267b6ab8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d90978)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927764548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9267289e8)]: 1862 failure(s)
[01:31:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 186 candidate(s)
[01:32:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d78058)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d274a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9267b6ab8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d90978)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927764548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9267289e8)]: 439 failure(s)
[01:35:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d78058)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d274a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9267b6ab8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d90978)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927764548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9267289e8)]: 364 failure(s)
[01:37:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d78058)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d274a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9267b6ab8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d90978)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927764548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9267289e8)]: 338 failure(s)
[01:39:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d78058)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d274a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9267b6ab8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d90978)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927764548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9267289e8)]: 351 failure(s)
[01:40:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9998  0.9997  0.9997  0.9995  0.9995  0.9992  0.9991  0.9988  0.9986  0.9984  0.9984  0.9982  0.9981  0.9980  0.9978
[17 : 32]:	0.9968  0.9965  0.9957  0.9952  0.9950  0.9950  0.9947  0.9946  0.9945  0.9942  0.9942  0.9941  0.9939  0.9937  0.9937  0.9936
[01:40:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:40:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:40:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:41:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:41:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_conv2d_add_clip_3"
[01:41:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:41:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:42:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9270cba18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d776b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92771af88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926ddddd8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927060fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9276ef4e8)]: 2017 failure(s)
[01:43:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9270cba18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d776b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92771af88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926ddddd8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927060fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9276ef4e8)]: 4031 failure(s)
[01:43:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 65 candidate(s)
[01:45:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9270cba18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d776b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92771af88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926ddddd8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927060fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9276ef4e8)]: 531 failure(s)
[01:48:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9270cba18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d776b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92771af88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926ddddd8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927060fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9276ef4e8)]: 427 failure(s)
[01:50:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9270cba18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d776b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92771af88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926ddddd8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927060fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9276ef4e8)]: 477 failure(s)
[01:54:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9270cba18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d776b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92771af88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926ddddd8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927060fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9276ef4e8)]: 383 failure(s)
[01:54:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9993  0.9990  0.9989  0.9988  0.9985  0.9984  0.9983  0.9981  0.9979  0.9976  0.9971  0.9969  0.9965  0.9965  0.9962  0.9960
[17 : 32]:	0.9960  0.9958  0.9957  0.9952  0.9950  0.9945  0.9945  0.9941  0.9940  0.9938  0.9936  0.9934  0.9925  0.9925  0.9923  0.9918
[01:54:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:54:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:54:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:55:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:55:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_conv2d_add_clip_4"
[01:55:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:55:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:56:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d3f448)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d612e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d57ab8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d92728)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267dcef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d46608)]: 1818 failure(s)
[01:56:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 230 candidate(s)
[01:58:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d3f448)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d612e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d57ab8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d92728)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267dcef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d46608)]: 362 failure(s)
[01:59:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d3f448)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d612e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d57ab8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d92728)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267dcef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d46608)]: 372 failure(s)
[02:01:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d3f448)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d612e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d57ab8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d92728)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267dcef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d46608)]: 328 failure(s)
[02:03:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d3f448)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d612e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d57ab8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d92728)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267dcef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d46608)]: 306 failure(s)
[02:04:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9998  0.9996  0.9996  0.9996  0.9996  0.9994  0.9994  0.9993  0.9990  0.9988  0.9988  0.9986  0.9981  0.9974
[17 : 32]:	0.9973  0.9971  0.9967  0.9965  0.9964  0.9963  0.9961  0.9960  0.9957  0.9956  0.9956  0.9956  0.9955  0.9952  0.9951  0.9947
[02:04:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:04:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:04:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:05:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:05:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_conv2d_add_clip_5"
[02:05:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:05:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:06:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927795768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926cf84b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d68968)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d81b98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9270cb8b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9277461a8)]: 1975 failure(s)
[02:06:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 73 candidate(s)
[02:10:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927795768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926cf84b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d68968)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d81b98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9270cb8b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9277461a8)]: 426 failure(s)
[02:13:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927795768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926cf84b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d68968)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d81b98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9270cb8b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9277461a8)]: 343 failure(s)
[02:17:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927795768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926cf84b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d68968)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d81b98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9270cb8b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9277461a8)]: 391 failure(s)
[02:21:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927795768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926cf84b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d68968)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d81b98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9270cb8b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9277461a8)]: 344 failure(s)
[02:22:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9998  0.9994  0.9994  0.9993  0.9992  0.9991  0.9986  0.9983  0.9979  0.9979  0.9975  0.9974  0.9973  0.9972  0.9972
[17 : 32]:	0.9970  0.9969  0.9967  0.9965  0.9965  0.9964  0.9960  0.9960  0.9959  0.9959  0.9955  0.9955  0.9945  0.9945  0.9944  0.9941
[02:22:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:22:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:22:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:23:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:23:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_conv2d_add_clip_6"
[02:23:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:23:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:24:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9267e1c48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926ff6ff8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926dd4598)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9267f51a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926ff2a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e048)]: 1895 failure(s)
[02:24:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 153 candidate(s)
[02:25:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9267e1c48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926ff6ff8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926dd4598)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9267f51a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926ff2a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e048)]: 418 failure(s)
[02:27:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9267e1c48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926ff6ff8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926dd4598)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9267f51a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926ff2a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e048)]: 411 failure(s)
[02:29:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9267e1c48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926ff6ff8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926dd4598)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9267f51a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926ff2a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e048)]: 352 failure(s)
[02:31:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9267e1c48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926ff6ff8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926dd4598)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9267f51a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926ff2a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e048)]: 355 failure(s)
[02:31:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9997  0.9995  0.9993  0.9993  0.9990  0.9983  0.9979  0.9978  0.9976  0.9974  0.9973  0.9973  0.9970  0.9966  0.9965
[17 : 32]:	0.9959  0.9958  0.9955  0.9955  0.9945  0.9944  0.9944  0.9943  0.9943  0.9942  0.9942  0.9942  0.9941  0.9940  0.9934  0.9925
[02:32:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:32:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:32:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:32:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:33:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_conv2d_add_clip_7"
[02:33:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:33:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:34:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927783408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9267a8b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92650e648)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927018fd8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927757be8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9267dc748)]: 2003 failure(s)
[02:35:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927783408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9267a8b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92650e648)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927018fd8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927757be8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9267dc748)]: 4003 failure(s)
[02:35:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 93 candidate(s)
[02:37:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927783408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9267a8b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92650e648)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927018fd8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927757be8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9267dc748)]: 519 failure(s)
[02:40:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927783408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9267a8b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92650e648)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927018fd8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927757be8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9267dc748)]: 479 failure(s)
[02:43:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927783408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9267a8b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92650e648)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927018fd8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927757be8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9267dc748)]: 457 failure(s)
[02:46:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927783408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9267a8b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92650e648)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927018fd8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927757be8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9267dc748)]: 415 failure(s)
[02:47:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9993  0.9993  0.9991  0.9990  0.9984  0.9983  0.9978  0.9977  0.9975  0.9973  0.9973  0.9972  0.9972  0.9969  0.9966  0.9965
[17 : 32]:	0.9964  0.9962  0.9957  0.9956  0.9953  0.9948  0.9947  0.9945  0.9942  0.9941  0.9936  0.9932  0.9932  0.9931  0.9929  0.9929
[02:47:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:47:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:47:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:47:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:48:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_nn_conv2d_add_clip_8"
[02:48:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:48:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:49:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9267400f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926dd8918)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927009548)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d925c61c88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92671b6b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9267dce68)]: 1814 failure(s)
[02:49:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 234 candidate(s)
[02:50:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9267400f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926dd8918)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927009548)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d925c61c88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92671b6b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9267dce68)]: 343 failure(s)
[02:52:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9267400f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926dd8918)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927009548)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d925c61c88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92671b6b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9267dce68)]: 318 failure(s)
[02:54:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9267400f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926dd8918)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927009548)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d925c61c88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92671b6b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9267dce68)]: 300 failure(s)
[02:56:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9267400f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926dd8918)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927009548)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d925c61c88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92671b6b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9267dce68)]: 290 failure(s)
[02:57:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9999  0.9998  0.9998  0.9992  0.9992  0.9992  0.9985  0.9984  0.9982  0.9980  0.9979  0.9978  0.9978  0.9976
[17 : 32]:	0.9976  0.9976  0.9975  0.9973  0.9970  0.9968  0.9967  0.9967  0.9965  0.9964  0.9962  0.9960  0.9959  0.9958  0.9958  0.9957
[02:57:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:57:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:57:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:57:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:58:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #9: "fused_nn_conv2d_add_clip_9"
[02:58:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:58:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:59:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9270969b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d9d738)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92774aa98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d92709b1d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926509798)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927759b38)]: 1952 failure(s)
[02:59:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 96 candidate(s)
[03:02:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9270969b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d9d738)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92774aa98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d92709b1d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926509798)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927759b38)]: 395 failure(s)
[03:05:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9270969b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d9d738)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92774aa98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d92709b1d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926509798)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927759b38)]: 385 failure(s)
[03:08:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9270969b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d9d738)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92774aa98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d92709b1d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926509798)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927759b38)]: 361 failure(s)
[03:12:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9270969b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d9d738)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92774aa98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d92709b1d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926509798)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927759b38)]: 313 failure(s)
[03:13:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9999  0.9998  0.9995  0.9994  0.9993  0.9992  0.9992  0.9992  0.9987  0.9987  0.9987  0.9986  0.9982  0.9979
[17 : 32]:	0.9974  0.9972  0.9972  0.9967  0.9965  0.9963  0.9962  0.9962  0.9960  0.9959  0.9958  0.9958  0.9958  0.9954  0.9953  0.9952
[03:13:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[03:13:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[03:13:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[03:14:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[03:14:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_nn_conv2d_add_clip_10"
[03:14:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:14:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:15:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d92704a6c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9267cf608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927797f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d671c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92709e2d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927715f28)]: 1853 failure(s)
[03:15:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 195 candidate(s)
[03:16:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d92704a6c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9267cf608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927797f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d671c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92709e2d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927715f28)]: 351 failure(s)
[03:18:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d92704a6c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9267cf608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927797f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d671c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92709e2d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927715f28)]: 329 failure(s)
[03:21:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d92704a6c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9267cf608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927797f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d671c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92709e2d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927715f28)]: 299 failure(s)
[03:23:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d92704a6c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9267cf608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927797f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926d671c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92709e2d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927715f28)]: 292 failure(s)
[03:23:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  1.0000  1.0000  0.9998  0.9995  0.9992  0.9991  0.9988  0.9988  0.9987  0.9986  0.9983  0.9981  0.9979  0.9978  0.9976
[17 : 32]:	0.9976  0.9975  0.9975  0.9972  0.9971  0.9961  0.9959  0.9958  0.9958  0.9958  0.9957  0.9957  0.9956  0.9953  0.9950  0.9948
[03:23:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[03:23:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[03:23:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[03:24:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[03:24:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #11: "fused_nn_conv2d_add_clip_11"
[03:24:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:24:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:25:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9267e4c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9267dbbd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92709c538)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9267e7a48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d09f88)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8a478)]: 1990 failure(s)
[03:25:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 58 candidate(s)
[03:27:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9267e4c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9267dbbd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92709c538)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9267e7a48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d09f88)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8a478)]: 500 failure(s)
[03:30:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9267e4c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9267dbbd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92709c538)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9267e7a48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d09f88)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8a478)]: 452 failure(s)
[03:33:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9267e4c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9267dbbd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92709c538)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9267e7a48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d09f88)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8a478)]: 415 failure(s)
[03:35:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9267e4c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9267dbbd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92709c538)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9267e7a48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d09f88)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8a478)]: 395 failure(s)
[03:36:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9995  0.9990  0.9990  0.9989  0.9988  0.9988  0.9987  0.9986  0.9984  0.9982  0.9981  0.9980  0.9974  0.9974  0.9970
[17 : 32]:	0.9962  0.9961  0.9956  0.9956  0.9955  0.9952  0.9951  0.9945  0.9945  0.9938  0.9936  0.9935  0.9935  0.9933  0.9930  0.9929
[03:36:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[03:36:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[03:36:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[03:36:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[03:36:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #12: "fused_nn_conv2d_add_clip_12"
[03:36:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:36:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:37:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d925c536e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9270ca5c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9270a6168)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927756048)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927082ca8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9270e4758)]: 1818 failure(s)
[03:37:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 230 candidate(s)
[03:38:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d925c536e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9270ca5c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9270a6168)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927756048)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927082ca8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9270e4758)]: 326 failure(s)
[03:39:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d925c536e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9270ca5c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9270a6168)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927756048)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927082ca8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9270e4758)]: 296 failure(s)
[03:41:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d925c536e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9270ca5c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9270a6168)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927756048)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927082ca8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9270e4758)]: 319 failure(s)
[03:43:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d925c536e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9270ca5c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9270a6168)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927756048)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927082ca8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9270e4758)]: 299 failure(s)
[03:43:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  1.0000  0.9999  0.9999  0.9997  0.9995  0.9994  0.9993  0.9993  0.9991  0.9990  0.9990  0.9989  0.9989  0.9988  0.9987
[17 : 32]:	0.9987  0.9984  0.9982  0.9980  0.9980  0.9979  0.9978  0.9978  0.9977  0.9975  0.9973  0.9972  0.9972  0.9972  0.9970  0.9968
[03:44:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[03:44:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[03:44:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[03:44:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [03:45:08] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:816
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:1188
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/_ctypes.c:4025
  24: _PyObject_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:199
  25: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4619
  26: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3093
  27: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  28: function_code_fastcall
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:283
  29: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:322
  30: _PyObject_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:98
  31: call_unbound_noarg
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1515
  32: slot_tp_finalize
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:6684
  33: PyObject_CallFinalizer
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:286
  34: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:303
  35: subtype_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1207
  36: frame_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/frameobject.c:470
  37: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:168
  38: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:167
  39: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:1842
  40: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  41: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  42: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  43: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  44: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  45: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  46: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  47: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3959
  48: PyEval_EvalCode
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:524
  49: builtin_exec_impl.isra.12
        at /tmp/build/80754af9/python_1627392990942/work/Python/bltinmodule.c:1079
  50: builtin_exec
        at /tmp/build/80754af9/python_1627392990942/work/Python/clinic/bltinmodule.c.h:283
  51: _PyMethodDef_RawFastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:654
  52: _PyCFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:732
  53: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4568
  54: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  55: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  56: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  57: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  58: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  59: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  60: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  61: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  62: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:376
  63: pymain_run_module
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:355
  64: pymain_run_python
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:2910
  65: pymain_main
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3460
  66: _Py_UnixMain
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3495
  67: __libc_start_main
  68: 0x0000564ec4ddb554
  69: 0xffffffffffffffff


[03:45:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #13: "fused_nn_conv2d_add_clip_13"
[03:45:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:45:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:46:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926da7f78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d8d6d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92775d968)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926743c38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267df128)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d15f78)]: 1935 failure(s)
[03:46:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 113 candidate(s)
[03:48:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926da7f78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d8d6d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92775d968)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926743c38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267df128)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d15f78)]: 487 failure(s)
[03:51:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926da7f78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d8d6d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92775d968)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926743c38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267df128)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d15f78)]: 375 failure(s)
[03:53:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926da7f78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d8d6d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92775d968)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926743c38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267df128)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d15f78)]: 388 failure(s)
[03:56:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926da7f78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d8d6d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92775d968)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926743c38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267df128)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d15f78)]: 376 failure(s)
[03:57:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9997  0.9997  0.9989  0.9986  0.9984  0.9978  0.9969  0.9967  0.9966  0.9964  0.9963  0.9963  0.9960  0.9958  0.9957
[17 : 32]:	0.9956  0.9952  0.9949  0.9948  0.9946  0.9945  0.9944  0.9941  0.9939  0.9935  0.9933  0.9933  0.9931  0.9930  0.9927  0.9927
[03:57:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[03:57:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[03:57:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[03:57:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[03:58:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_nn_conv2d_add_clip_14"
[03:58:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:58:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:58:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926dd8098)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d70d88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9267e0878)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9270654a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d31498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9270d7d18)]: 1860 failure(s)
[03:58:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 188 candidate(s)
[03:59:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926dd8098)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d70d88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9267e0878)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9270654a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d31498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9270d7d18)]: 379 failure(s)
[04:01:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926dd8098)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d70d88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9267e0878)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9270654a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d31498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9270d7d18)]: 308 failure(s)
[04:03:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926dd8098)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d70d88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9267e0878)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9270654a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d31498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9270d7d18)]: 307 failure(s)
[04:05:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926dd8098)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d70d88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9267e0878)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9270654a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d31498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9270d7d18)]: 289 failure(s)
[04:05:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9997  0.9994  0.9991  0.9987  0.9986  0.9985  0.9982  0.9981  0.9981  0.9980  0.9973  0.9971  0.9969  0.9968  0.9968
[17 : 32]:	0.9965  0.9963  0.9963  0.9962  0.9958  0.9958  0.9953  0.9951  0.9951  0.9951  0.9951  0.9948  0.9947  0.9946  0.9945  0.9945
[04:06:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[04:06:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[04:06:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[04:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[04:06:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_nn_conv2d_add_clip_15"
[04:06:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:06:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[04:07:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9277836a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d64d38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9276fd958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9267ade18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926743e98)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9276fd248)]: 1943 failure(s)
[04:07:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 105 candidate(s)
[04:09:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9277836a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d64d38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9276fd958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9267ade18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926743e98)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9276fd248)]: 684 failure(s)
[04:11:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9277836a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d64d38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9276fd958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9267ade18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926743e98)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9276fd248)]: 583 failure(s)
[04:13:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9277836a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d64d38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9276fd958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9267ade18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926743e98)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9276fd248)]: 563 failure(s)
[04:15:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9277836a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d64d38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9276fd958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9267ade18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926743e98)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9276fd248)]: 544 failure(s)
[04:16:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9997  0.9992  0.9988  0.9986  0.9986  0.9985  0.9984  0.9983  0.9982  0.9981  0.9976  0.9976  0.9974  0.9974  0.9972
[17 : 32]:	0.9972  0.9969  0.9969  0.9968  0.9966  0.9965  0.9960  0.9959  0.9959  0.9958  0.9954  0.9949  0.9946  0.9945  0.9945  0.9943
[04:16:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[04:16:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[04:16:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[04:16:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[04:17:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #16: "fused_nn_conv2d_add_clip_16"
[04:17:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:17:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[04:17:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d3f5d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926da3f28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d1a178)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9267dbaf8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d5c908)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d1a1a8)]: 1876 failure(s)
[04:17:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 172 candidate(s)
[04:18:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d3f5d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926da3f28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d1a178)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9267dbaf8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d5c908)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d1a1a8)]: 444 failure(s)
[04:20:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d3f5d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926da3f28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d1a178)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9267dbaf8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d5c908)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d1a1a8)]: 364 failure(s)
[04:21:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d3f5d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926da3f28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d1a178)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9267dbaf8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d5c908)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d1a1a8)]: 361 failure(s)
[04:22:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d3f5d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926da3f28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d1a178)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9267dbaf8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d5c908)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d1a1a8)]: 336 failure(s)
[04:23:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9995  0.9995  0.9995  0.9993  0.9992  0.9987  0.9986  0.9985  0.9983  0.9982  0.9979  0.9971  0.9968  0.9968  0.9968
[17 : 32]:	0.9965  0.9964  0.9963  0.9958  0.9953  0.9953  0.9949  0.9946  0.9943  0.9942  0.9942  0.9941  0.9940  0.9936  0.9935  0.9934
[04:23:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[04:23:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[04:23:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[04:24:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[04:24:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #17: "fused_nn_conv2d_add_clip_17"
[04:24:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:24:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[04:25:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927044938)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d8f3d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9276fc8c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926fe69d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92771ede8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d7c2b8)]: 1915 failure(s)
[04:25:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 133 candidate(s)
[04:27:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927044938)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d8f3d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9276fc8c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926fe69d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92771ede8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d7c2b8)]: 516 failure(s)
[04:30:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927044938)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d8f3d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9276fc8c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926fe69d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92771ede8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d7c2b8)]: 421 failure(s)
[04:32:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927044938)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d8f3d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9276fc8c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926fe69d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92771ede8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d7c2b8)]: 375 failure(s)
[04:35:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927044938)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d8f3d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d9276fc8c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926fe69d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92771ede8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d7c2b8)]: 319 failure(s)
[04:35:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9997  0.9997  0.9994  0.9994  0.9990  0.9988  0.9988  0.9985  0.9983  0.9982  0.9977  0.9976  0.9974  0.9973  0.9973
[17 : 32]:	0.9967  0.9964  0.9964  0.9963  0.9963  0.9962  0.9962  0.9962  0.9960  0.9959  0.9957  0.9951  0.9947  0.9944  0.9944  0.9942
[04:35:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[04:35:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[04:35:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[04:36:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[04:36:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #18: "fused_nn_conv2d_add_clip_18"
[04:36:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:36:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[04:37:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9267e3a08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9277000e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d269e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d92771c248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9270a97c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d13ea8)]: 1916 failure(s)
[04:37:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 132 candidate(s)
[04:37:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9267e3a08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9277000e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d269e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d92771c248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9270a97c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d13ea8)]: 457 failure(s)
[04:39:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9267e3a08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9277000e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d269e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d92771c248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9270a97c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d13ea8)]: 383 failure(s)
[04:40:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9267e3a08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9277000e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d269e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d92771c248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9270a97c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d13ea8)]: 330 failure(s)
[04:41:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9267e3a08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9277000e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d269e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d92771c248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9270a97c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d13ea8)]: 332 failure(s)
[04:42:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9995  0.9992  0.9991  0.9989  0.9987  0.9986  0.9986  0.9986  0.9986  0.9985  0.9975  0.9974  0.9972  0.9971  0.9971  0.9963
[17 : 32]:	0.9963  0.9961  0.9961  0.9957  0.9955  0.9955  0.9954  0.9953  0.9949  0.9949  0.9948  0.9946  0.9944  0.9941  0.9939  0.9937
[04:42:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[04:42:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[04:42:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[04:42:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [04:43:01] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:816
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:1188
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/_ctypes.c:4025
  24: _PyObject_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:199
  25: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4619
  26: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3093
  27: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  28: function_code_fastcall
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:283
  29: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:322
  30: _PyObject_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:98
  31: call_unbound_noarg
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1515
  32: slot_tp_finalize
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:6684
  33: PyObject_CallFinalizer
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:286
  34: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:303
  35: subtype_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1207
  36: frame_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/frameobject.c:470
  37: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:168
  38: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:167
  39: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:1842
  40: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  41: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  42: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  43: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  44: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  45: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  46: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  47: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3959
  48: PyEval_EvalCode
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:524
  49: builtin_exec_impl.isra.12
        at /tmp/build/80754af9/python_1627392990942/work/Python/bltinmodule.c:1079
  50: builtin_exec
        at /tmp/build/80754af9/python_1627392990942/work/Python/clinic/bltinmodule.c.h:283
  51: _PyMethodDef_RawFastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:654
  52: _PyCFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:732
  53: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4568
  54: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  55: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  56: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  57: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  58: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  59: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  60: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  61: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  62: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:376
  63: pymain_run_module
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:355
  64: pymain_run_python
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:2910
  65: pymain_main
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3460
  66: _Py_UnixMain
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3495
  67: __libc_start_main
  68: 0x0000555ab860e554
  69: 0xffffffffffffffff


[04:43:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_nn_avg_pool2d"
[04:43:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:43:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[04:43:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9270a0678)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d92772cef8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d231e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926749578)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267c47b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9277483e8)]: 0 failure(s)
[04:43:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[04:43:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9270a0678)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d92772cef8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d231e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926749578)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267c47b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9277483e8)]: 0 failure(s)
[04:44:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9270a0678)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d92772cef8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d231e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926749578)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267c47b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9277483e8)]: 0 failure(s)
[04:44:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9270a0678)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d92772cef8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d231e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926749578)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267c47b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9277483e8)]: 0 failure(s)
[04:44:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9270a0678)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d92772cef8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d231e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926749578)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267c47b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9277483e8)]: 0 failure(s)
[04:45:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9988  0.9796  0.9560  0.9453  0.9317  0.9277  0.9143  0.8687  0.8642  0.8534  0.8330  0.7899  0.7710  0.7661  0.7592  0.7534
[17 : 32]:	0.7143  0.6862  0.6723  0.6295  0.6278  0.6174  0.5854  0.5785  0.4967  0.4655  0.4571  0.4501  0.4393  0.4027  0.3978  0.3498
[04:45:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[04:45:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 31 candidates(s) for measurement
[04:45:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 31 sample(s) to builder
[04:45:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 31 sample(s) to runner
[04:45:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #20: "fused_nn_conv2d_add"
[04:45:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:45:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[04:46:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9277794a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9277daf48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d590c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d92776ff58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927762918)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d59168)]: 1980 failure(s)
[04:46:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 68 candidate(s)
[04:46:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9277794a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9277daf48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d590c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d92776ff58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927762918)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d59168)]: 517 failure(s)
[04:48:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9277794a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9277daf48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d590c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d92776ff58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927762918)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d59168)]: 413 failure(s)
[04:49:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9277794a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9277daf48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d590c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d92776ff58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927762918)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d59168)]: 463 failure(s)
[04:51:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9277794a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d9277daf48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d590c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d92776ff58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d927762918)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d59168)]: 507 failure(s)
[04:51:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9990  0.9979  0.9978  0.9978  0.9973  0.9968  0.9967  0.9965  0.9965  0.9962  0.9961  0.9954  0.9941  0.9934  0.9921  0.9920
[17 : 32]:	0.9916  0.9915  0.9912  0.9909  0.9908  0.9905  0.9900  0.9897  0.9897  0.9897  0.9888  0.9878  0.9870  0.9867  0.9864  0.9857
[04:51:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[04:51:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[04:51:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[04:52:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[04:52:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #21: "fused_reshape_squeeze_reshape"
[04:52:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:52:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[04:52:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[04:52:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[04:52:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[04:52:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[04:53:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[04:53:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[04:53:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.8767  0.6755  0.2799  0.2516  0.1071
[04:53:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[04:53:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[04:53:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[04:53:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[04:53:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #22: "fused_nn_softmax"
[04:53:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:53:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[04:53:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d92777b818)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3a3a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92706dbb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9276f5908)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92679ca98)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d92706dc58)]: 0 failure(s)
[04:53:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[04:54:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d92777b818)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3a3a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92706dbb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9276f5908)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92679ca98)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d92706dc58)]: 0 failure(s)
[04:54:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d92777b818)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3a3a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92706dbb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9276f5908)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92679ca98)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d92706dc58)]: 0 failure(s)
[04:55:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d92777b818)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3a3a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92706dbb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9276f5908)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92679ca98)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d92706dc58)]: 0 failure(s)
[04:55:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d92777b818)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3a3a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d92706dbb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9276f5908)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d92679ca98)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d92706dc58)]: 0 failure(s)
[04:55:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9915  0.9739  0.9732  0.9650  0.9608  0.9489  0.9368  0.9340  0.9281  0.9262  0.9186  0.9146  0.9010  0.8966  0.8948  0.8942
[17 : 32]:	0.8773  0.8640  0.8621  0.8518  0.8484  0.8415  0.8398  0.8350  0.8346  0.8336  0.8178  0.8162  0.8125  0.8062  0.8058  0.8011
[04:55:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[04:55:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[04:55:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[04:56:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[04:56:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_reshape"
[04:56:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:56:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[04:56:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[04:56:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[04:56:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[04:57:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[04:57:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[04:57:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[04:57:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.9926  0.9315  0.8053  0.2961  0.0158
[04:57:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[04:57:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[04:57:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[04:57:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_clip"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 225, 225], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i2_3_init, i3_4_init in T.grid(2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + i2_3_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(30):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 3277 // 113)
                                    v3 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused % 2 * 112 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 113)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 < 3277)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 224 and 0 <= v3 and v3 < 224, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(3, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 1, 1, 3, 3, 1, 1, 1, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + i2_3)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 1, 7, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 2, 1, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_clip"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 225, 225], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init in T.grid(4, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_4_init)
                            yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 4 + i2_3_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(225, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 2025 // 9)
                                    v3 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused * 8 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 9)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 < 2025)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 224 and 0 <= v3 and v3 < 224, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(3, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 3, 1, 1, 4, 2, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_4)
                                yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 4 + i2_3)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 4, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[28, 1, 2, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 64])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 64, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_clip"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 225, 225], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(180):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 11475 // 3825)
                                    v2 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused % 14 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 3825 // 225)
                                    v3 = T.axis.spatial(225, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 225)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 < 11475)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 224 and 0 <= v3 and v3 < 224, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 27)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 27 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 432)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 1, 1, 1, 2):
                            for i2_4_init in T.serial(8):
                                with T.block("conv2d_nchw_init"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                                    yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i2_4_init)
                                    xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i3_3)
                                    T.reads()
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                            for i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 3, 1, 1, 8, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                                    yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i2_4)
                                    xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i3_3)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_2, i6_2])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 8, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 8 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + ax2)
                            v3 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 8])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 8, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 64])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 64, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l183)
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_clip"] Trial #3: GFLOPs: 737.5505. Time: 0.0310 ms. Best GFLOPs: 737.5505
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_clip"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 225, 225], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(224, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_3_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 28 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 16)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 28 // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(117):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3729 // 113)
                                    v3 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused % 2 * 112 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 113)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 3729)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 224 and 0 <= v3 and v3 < 224, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(3, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 144)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 28 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 16)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 28 // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_3)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 28 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 16 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 28 // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 8, 2, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 4, 4, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 4, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_clip"] Trial #5: GFLOPs: 656.5223. Time: 0.0349 ms. Best GFLOPs: 737.5505
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_clip"] Trial #6: GFLOPs: 798.5623. Time: 0.0287 ms. Best GFLOPs: 798.5623
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_clip"] Trial #7: GFLOPs: 201.0053. Time: 0.1138 ms. Best GFLOPs: 798.5623
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_clip"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 225, 225], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_4_init, i3_4_init in T.grid(16, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 16 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 4)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(61):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 1921 // 113)
                                    v3 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused % 2 * 112 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 113)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 1921)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 224 and 0 <= v3 and v3 < 224, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(3, i4_0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 16, 1, 14):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 16 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + i3_4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 16 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 8, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 4, 1, 14])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_clip"] Trial #9: GFLOPs: 640.5020. Time: 0.0357 ms. Best GFLOPs: 798.5623
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_clip"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 225, 225], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(224, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init in T.grid(2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 56 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 56 // 8 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_3_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused % 56 // 8 * 32 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 837 // 27)
                                    v3 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused % 8 * 28 + i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 27)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 837)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 224 and 0 <= v3 and v3 < 224, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 56 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2))
                                        v1, v2, v3 = T.axis.remap("SSS", [i4_0, i5_0, i6_0])
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 8)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 56 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 56 // 8 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_3)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_0, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 56 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 56 // 8 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 4, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 8, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[8, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_clip"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 225, 225], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i3_4_init in T.grid(7, 16, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 16 * 16 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 8 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 * 14 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(30):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused // 8 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 957 // 29)
                                    v3 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused % 8 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 29)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 957)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 224 and 0 <= v3 and v3 < 224, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(3, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 7, 1, 3, 3, 1, 16, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 16 * 16 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 8 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 * 14 + i3_3 * 2 + i3_4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 16 * 16 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 8 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 16, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[8, 1, 1, 7, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_clip"] Trial #12: GFLOPs: 507.0060. Time: 0.0451 ms. Best GFLOPs: 798.5623
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_clip"] Trial #13: GFLOPs: 1908.3323. Time: 0.0120 ms. Best GFLOPs: 1908.3323
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_clip"] Trial #14: GFLOPs: 283.7470. Time: 0.0806 ms. Best GFLOPs: 1908.3323
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_clip"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 225, 225], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(256, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init in T.grid(2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 8)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i2_3_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(256, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1) % 465 // 15)
                                    v3 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused % 14 * 16 + i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1) % 15)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 < 465)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 224 and 0 <= v3 and v3 < 224, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(256, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, ax0_ax1_ax2_ax3_fused_1)
                                    v1, v2, v3 = T.axis.remap("SSS", [i4_0, i5_0, i6_0])
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 32)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 8)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i2_3)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_3)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_0, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 8 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 8, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 4, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 256])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 256])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_clip"] Trial #16: GFLOPs: 176.9098. Time: 0.1293 ms. Best GFLOPs: 1908.3323
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_clip"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 225, 225], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(26):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 5763 // 1921)
                                    v2 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 1921 // 113)
                                    v3 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused % 2 * 112 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 113)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 < 5763)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 224 and 0 <= v3 and v3 < 224, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) // 27)
                                    v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 27 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 < 864)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1 in T.grid(1, 1):
                            for i3_3_init, i1_4_init in T.grid(2, 16):
                                with T.block("conv2d_nchw_init"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 16 + i1_4_init)
                                    yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + i3_3_init)
                                    T.reads()
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                            for i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 2, 3, 3, 1, 1, 16, 1, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 16 + i1_4)
                                    yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + i3_3)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_2, i6_1])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 16 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 8, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 28, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 224])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 224])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l176)
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_clip"] Trial #18: GFLOPs: 445.5216. Time: 0.0514 ms. Best GFLOPs: 1908.3323
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_clip"] Trial #19: GFLOPs: 148.2541. Time: 0.1543 ms. Best GFLOPs: 1908.3323
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_clip"] Trial #20: GFLOPs: 32.3460. Time: 0.7074 ms. Best GFLOPs: 1908.3323
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_clip"] Trial #21: GFLOPs: 242.9385. Time: 0.0942 ms. Best GFLOPs: 1908.3323
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_clip"] Trial #22: GFLOPs: 32.0639. Time: 0.7136 ms. Best GFLOPs: 1908.3323
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_clip"] Trial #23: GFLOPs: 184.7523. Time: 0.1238 ms. Best GFLOPs: 1908.3323
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_clip"] Trial #24: GFLOPs: 295.1143. Time: 0.0775 ms. Best GFLOPs: 1908.3323
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_clip"] Trial #25: GFLOPs: 356.5932. Time: 0.0642 ms. Best GFLOPs: 1908.3323
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_clip"] Trial #26: GFLOPs: 316.7586. Time: 0.0722 ms. Best GFLOPs: 1908.3323
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_clip"] Trial #27: GFLOPs: 477.0868. Time: 0.0480 ms. Best GFLOPs: 1908.3323
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_clip"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 225, 225], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i2_3_init, i1_4_init, i3_4_init in T.grid(4, 4, 2):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i1_4_init)
                                yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i2_3_init)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_4_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i5_0, i6_0 in T.grid(3, 3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(180):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 10035 // 3345)
                                        v2 = T.axis.spatial(225, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 3345 // 15)
                                        v3 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused % 14 * 16 + i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 15)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 10035)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 224 and 0 <= v3 and v3 < 224, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 24)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 4, 1, 3, 1, 1, 1, 4, 1, 2):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i1_4)
                                    yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i2_3)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_4)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_0, i6_0])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + ax1)
                            v2 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 1, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 14, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 4, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l174)
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_clip"] Trial #29: GFLOPs: 963.0050. Time: 0.0238 ms. Best GFLOPs: 1908.3323
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_clip"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 225, 225], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(256, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i2_3_init, i3_3_init in T.grid(2, 2):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 8)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i2_3_init)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_3_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i5_0, i6_0 in T.grid(3, 3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(256, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("pad_temp_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(3, ((ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1395 // 465)
                                            v2 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 465 // 15)
                                            v3 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused % 14 * 16 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1395)
                                            T.reads(placeholder[v0, v1, v2, v3])
                                            T.writes(pad_temp_shared[v0, v1, v2, v3])
                                            pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 224 and 0 <= v3 and v3 < 224, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(256, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 2, 3, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 8)
                                    yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i2_3)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_3)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_0, i6_0])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 8 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 8, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 4, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 256, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 256, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l176)
[04:57:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_clip"] Trial #31: GFLOPs: 1253.0830. Time: 0.0183 ms. Best GFLOPs: 1908.3323
[04:57:55] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_nn_conv2d_add_clip"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 32
Total latency (us): 11.9897

[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #0: GFLOPs: 298.2965. Time: 0.0283 ms. Best GFLOPs: 298.2965
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 4104 // 2052)
                                        v2 = T.axis.spatial(114, ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 2052 // 18)
                                        v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 18)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 4104)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 18)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            for i2_3_init, i2_4_init in T.grid(14, 2):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 64)
                                    i = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 64 // 16 * 28 + i2_3_init * 2 + i2_4_init)
                                    j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 14, 1, 3, 1, 1, 1, 2, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 64)
                                    i = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 64 // 16 * 28 + i2_3 * 2 + i2_4)
                                    j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 28, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 64 + ax1)
                            v2 = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 64 // 16 * 28 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 2, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 4, 14, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 16, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 128, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 128, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l162)
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init in T.grid(2, 4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 32 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3_init)
                            i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 32 // 16 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 4 + i2_3_init)
                            j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 16 * 7 + i0_1_i1_1_i2_1_i3_1_fused)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(36):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 32 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 8064 // 504)
                                        v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 32 // 16 * 56 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 504 // 9)
                                        v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 16 * 7 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 9)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 32 * 16 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 48)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 2, 4, 1, 1, 3, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 32 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 32 // 16 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 4 + i2_3)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 16 * 7 + i0_1_i1_1_i2_1_i3_1_fused)
                                di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 4, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 32 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 32 // 16 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 16 * 7 + i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 8, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 14, 4, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[16, 7, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 112, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 112, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_3_init, i2_4_init, i3_4_init in T.grid(2, 2, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 49 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                            i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 2 + i2_4_init)
                            j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(48):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 49 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 4608 // 288)
                                        v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 16 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 288 // 18)
                                        v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 18)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 49 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 3)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, i4_0)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 48)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 49 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 2 + i2_4)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 4):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 49 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 8 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 4, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 4, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 2, 2, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 32])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(38):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 9600 // 300)
                                        v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused // 14 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 300 // 10)
                                        v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 10)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 9600)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 < 288)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_3_init, i2_3_init, i3_3_init, i1_4_init in T.grid(2, 7, 2, 4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 16 * 8 + i1_3_init * 4 + i1_4_init)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4 * 7 + i2_3_init)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 2, 7, 2, 1, 1, 1, 4, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 16 * 8 + i1_3 * 4 + i1_4)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4 * 7 + i2_3)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_3)
                                di, dj = T.axis.remap("RR", [i4_1, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 7, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 16 * 8 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4 * 7 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 2, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 4, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 4, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 64, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 64])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l159)
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(784, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_3_init in T.serial(8):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 196 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                            i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 196 // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                            j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i3_3_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(20):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 196 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 640 // 80)
                                    v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 196 // 14 * 8 + i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 80 // 10)
                                    v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 10)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 196 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 24)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 196 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 196 // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i3_3)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 8):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 196 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 8 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 196 // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 4, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 8, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 1, 8, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 32])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 32, 4])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(60):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 32 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 7680 // 480)
                                        v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 32 // 4 * 14 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 480 // 30)
                                        v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 32 * 16 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 144)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_4_init, i3_4_init in T.grid(2, 7):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 32 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_4_init)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 32 // 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 1, 1, 1, 1, 2, 1, 7):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 32 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_4)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 32 // 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_1, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 32 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 32 // 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 8, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 7, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 4, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 64, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 64, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l161)
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(240):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 32 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 7680 // 480)
                                    v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 32 // 4 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 480 // 30)
                                    v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 30)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 32 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 144)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i3_3_init, i1_4_init, i2_4_init in T.grid(7, 2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 32 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_4_init)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 32 // 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_4_init)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 7, 1, 1, 1, 2, 2, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 32 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_4)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 32 // 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_4)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_3)
                                di, dj = T.axis.remap("RR", [i4_1, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 32 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 32 // 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 8, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 7, 1, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 4, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 32])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 32, 4])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l159)
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #8: GFLOPs: 26.0525. Time: 0.3236 ms. Best GFLOPs: 298.2965
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(67):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 7424 // 232)
                                    v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 232 // 58)
                                    v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 58)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 < 7424)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_3_init, i2_3_init, i3_3_init in T.grid(2, 2, 4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3_init)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i2_3_init)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 4 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 2, 2, 4, 1, 1, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i2_3)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 4 + i3_3)
                                di, dj = T.axis.remap("RR", [i4_1, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 4):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 8, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[56, 1, 1, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 14, 4, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 112])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 112, 4])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l159)
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #10: GFLOPs: 92.2640. Time: 0.0914 ms. Best GFLOPs: 298.2965
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #11: GFLOPs: 540.7018. Time: 0.0156 ms. Best GFLOPs: 540.7018
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #12: GFLOPs: 55.2233. Time: 0.1526 ms. Best GFLOPs: 540.7018
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #13: GFLOPs: 162.5091. Time: 0.0519 ms. Best GFLOPs: 540.7018
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #14: GFLOPs: 170.1824. Time: 0.0495 ms. Best GFLOPs: 540.7018
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #15: GFLOPs: 114.4957. Time: 0.0736 ms. Best GFLOPs: 540.7018
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #16: GFLOPs: 147.9936. Time: 0.0570 ms. Best GFLOPs: 540.7018
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init, i1_4_init, i2_4_init in T.grid(2, 2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + i1_3_init * 2 + i1_4_init)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i2_4_init)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(126):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 8064 // 1008)
                                        v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 1008 // 56)
                                        v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 56)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 24)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + i1_3 * 2 + i1_4)
                                    i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i2_4)
                                    j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                    di, dj = T.axis.remap("RR", [i4_1, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 1, 2, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 8, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 8, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 64])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 64, 4])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l158)
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #18: GFLOPs: 394.0559. Time: 0.0214 ms. Best GFLOPs: 540.7018
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #19: GFLOPs: 257.6520. Time: 0.0327 ms. Best GFLOPs: 540.7018
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #20: GFLOPs: 116.1787. Time: 0.0726 ms. Best GFLOPs: 540.7018
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #21: GFLOPs: 174.7189. Time: 0.0482 ms. Best GFLOPs: 540.7018
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #22: GFLOPs: 156.6129. Time: 0.0538 ms. Best GFLOPs: 540.7018
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #23: GFLOPs: 45.4833. Time: 0.1853 ms. Best GFLOPs: 540.7018
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #24: GFLOPs: 161.0667. Time: 0.0523 ms. Best GFLOPs: 540.7018
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #25: GFLOPs: 116.7517. Time: 0.0722 ms. Best GFLOPs: 540.7018
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(224, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i3_3_init, i3_4_init in T.grid(4, 7):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 28 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 16)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i3_3_init * 7 + i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 28 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 2016 // 504)
                                        v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 504 // 28)
                                        v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 < 2016)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 28 * 4 + ax0_ax1_ax2_ax3_fused_1 // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused_1 % 3)
                                        v3 = T.axis.spatial(3, i5_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 < 12)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 4, 3, 1, 1, 1, 1, 7):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 28 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 16)
                                    i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16)
                                    j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i3_3 * 7 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 28):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 28 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 16 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 4, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 16, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 1, 4, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 64])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 64])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l156)
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #27: GFLOPs: 367.9302. Time: 0.0229 ms. Best GFLOPs: 540.7018
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #28: GFLOPs: 554.0199. Time: 0.0152 ms. Best GFLOPs: 554.0199
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #29: GFLOPs: 221.0235. Time: 0.0381 ms. Best GFLOPs: 554.0199
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #30: GFLOPs: 168.6411. Time: 0.0500 ms. Best GFLOPs: 554.0199
[04:57:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip_1"] Trial #31: GFLOPs: 206.3014. Time: 0.0409 ms. Best GFLOPs: 554.0199
[04:58:15] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_nn_conv2d_add_clip_1"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 64
Total latency (us): 27.2049

[04:58:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #0: GFLOPs: 138.4917. Time: 0.3884 ms. Best GFLOPs: 138.4917
[04:58:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init, i2_4_init in T.grid(7, 32, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 112 * 32 + i1_4_init)
                            yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused // 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 112 // 28 * 14 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0)
                                        v2 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 56)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + (ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 56)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(32, i4_0)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 64)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 32, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 112 * 32 + i1_4)
                                yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused // 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 112 // 28 * 14 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28)
                                rc = T.axis.reduce(32, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 14, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 112 * 32 + ax1)
                            v2 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused // 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 112 // 28 * 14 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 32])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 4, 7, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 2, 28, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 224, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 224, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:58:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #2: GFLOPs: 127.3447. Time: 0.4224 ms. Best GFLOPs: 138.4917
[04:58:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #3: GFLOPs: 538.1290. Time: 0.1000 ms. Best GFLOPs: 538.1290
[04:58:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #4: GFLOPs: 1190.5526. Time: 0.0452 ms. Best GFLOPs: 1190.5526
[04:58:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(32, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 16 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 16 // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 28)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(32, i4_0)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 64)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 16 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 16 // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                rc = T.axis.reduce(32, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 16 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 16 // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 4, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 8, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 2, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 112])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:58:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #6: GFLOPs: 508.2732. Time: 0.1058 ms. Best GFLOPs: 1190.5526
[04:58:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init in T.grid(8, 8, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 8 + i2_3_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 448)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 448 // 56)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 56)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 256)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 8, 8, 2, 2, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 8 + i2_3)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_3)
                                rc = T.axis.reduce(32, i4_0 * 8 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 8, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 8 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 4, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 1, 8, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 2, 14, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 4, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:58:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #8: GFLOPs: 520.9556. Time: 0.1033 ms. Best GFLOPs: 1190.5526
[04:58:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init, i3_4_init in T.grid(4, 4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_4_init)
                            yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 4 + i2_3_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 224)
                                    v2 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 224 // 2)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 2 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 2)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 512)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 4, 1, 8, 1, 1, 1, 4, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_4)
                                yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 4 + i2_3)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 2 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + ax1)
                            v2 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 4, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 14, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[56, 1, 1, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 56])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:58:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #10: GFLOPs: 337.7134. Time: 0.1593 ms. Best GFLOPs: 1190.5526
[04:58:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #11: GFLOPs: 1144.9575. Time: 0.0470 ms. Best GFLOPs: 1190.5526
[04:58:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #12: GFLOPs: 170.9320. Time: 0.3147 ms. Best GFLOPs: 1190.5526
[04:58:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(128):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 224)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 56 // 28 * 56 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 224 // 4)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 56 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 32)
                                        v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 32)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1024)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 1, 4, 2, 1):
                            for i1_4_init, i3_4_init in T.grid(2, 4):
                                with T.block("conv2d_nchw_init"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 56 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3 * 2 + i1_4_init)
                                    yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 56 // 28 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i2_3)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + i3_4_init)
                                    T.reads()
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                            for i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(32, 1, 1, 1, 2, 1, 4):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 56 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 56 // 28 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i2_3)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + i3_4)
                                    rc = T.axis.reduce(32, i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 56 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 56 // 28 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 4, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 14, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[28, 1, 1, 1, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l183)
[04:58:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(112):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 224)
                                        v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 224 // 8)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 32)
                                        v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 32)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_3_init, i2_4_init in T.grid(8, 4):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + i1_3_init)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i2_4_init)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 8, 1, 1, 8, 1, 1, 1, 1, 4, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + i1_3)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                rc = T.axis.reduce(32, i4_1 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 4, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 4, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 7, 1, 1, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 8, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 4, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l178)
[04:58:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #15: GFLOPs: 312.4613. Time: 0.1721 ms. Best GFLOPs: 1190.5526
[04:58:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(4, 2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 896)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 896 // 56)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 56)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 4)
                                    v1 = T.axis.spatial(32, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 4, 2, 1, 1, 1, 2, 2, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 8):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 8, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 1, 4, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 64])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 64])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:58:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init, i3_4_init in T.grid(8, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 8 + i1_4_init)
                            yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 8 * 2 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 1792)
                                        v2 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 1792 // 16)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + ax0_ax1_ax2_ax3_fused_1 // 2)
                                    v1 = T.axis.spatial(32, i4_0 * 2 + ax0_ax1_ax2_ax3_fused_1 % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 32)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 2, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 8 + i1_4)
                                yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 8 * 2 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 8 + ax1)
                            v2 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 8 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 2, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 8, 7, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 8, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 112])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:58:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #18: GFLOPs: 357.5438. Time: 0.1504 ms. Best GFLOPs: 1190.5526
[04:58:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i2_4_init in T.grid(2, 2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 56 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_3_init)
                            yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 56 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(112):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 224)
                                    v2 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 224 // 2)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 56 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 2)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 56 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 2, 1, 2, 4, 1, 1, 1, 1, 7, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 56 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_3)
                                yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 56 * 2 + i3_3)
                                rc = T.axis.reduce(32, i4_0 * 16 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 56 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 56 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 8, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 4, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[56, 1, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 4, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:58:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #20: GFLOPs: 3159.3497. Time: 0.0170 ms. Best GFLOPs: 3159.3497
[04:58:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(32, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 16 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 16 // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 448)
                                        v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 448 // 28)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1024)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 16 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 16 // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                rc = T.axis.reduce(32, i4_0 * 16 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 16 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 16 // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 4, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 8, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 2, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 16, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:58:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #22: GFLOPs: 4.9860. Time: 10.7878 ms. Best GFLOPs: 3159.3497
[04:58:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init, i3_4_init in T.grid(2, 4, 4, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 8 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 28 * 4 + i2_3_init)
                            xx = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 28 * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) // 896)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 8 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 896 // 112)
                                    v3 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 112)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) // 4)
                                    v1 = T.axis.spatial(32, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 < 256)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 4, 1, 2, 1, 1, 1, 4, 1, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 8 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 28 * 4 + i2_3)
                                xx = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 28 * 4 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 4, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 8 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 28 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 28 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 4, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 2, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 28, 1, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 224])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 224])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:58:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #24: GFLOPs: 1022.0211. Time: 0.0526 ms. Best GFLOPs: 3159.3497
[04:58:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(196, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) // 64)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 64 // 8)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 32)
                                        v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 32)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i3_3_init, i1_4_init, i2_4_init in T.grid(4, 4, 2):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + i1_4_init)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 2 + i2_4_init)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 1, 1, 4, 4, 1, 1, 1, 4, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 2 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3)
                                rc = T.axis.reduce(32, i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 16, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 4, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 2, 4, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 8, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 128])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 128, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l176)
[04:58:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i2_4_init in T.grid(2, 2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_3_init)
                            yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 7 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(112):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 448)
                                    v2 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 448 // 4)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 2, 16, 1, 1, 1, 1, 7, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_3)
                                yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 7 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3)
                                rc = T.axis.reduce(32, i4_0 * 16 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 7 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 8, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 4, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[28, 1, 2, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 1, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 64])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 64])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:58:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #27: GFLOPs: 1047.2622. Time: 0.0514 ms. Best GFLOPs: 3159.3497
[04:58:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(32, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(2, 2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 4 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 448)
                                        v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 56 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 448 // 8)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 128)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 4 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3)
                                rc = T.axis.reduce(32, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [64, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 16, 2, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 14, 2, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 2, 2, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 56, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:58:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #29: GFLOPs: 169.8399. Time: 0.3167 ms. Best GFLOPs: 3159.3497
[04:58:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #30: GFLOPs: 1193.4269. Time: 0.0451 ms. Best GFLOPs: 3159.3497
[04:58:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_2"] Trial #31: GFLOPs: 117.9975. Time: 0.4558 ms. Best GFLOPs: 3159.3497
[04:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_conv2d_add_clip_2"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 96
Total latency (us): 44.2302

[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #0: GFLOPs: 205.8979. Time: 0.0205 ms. Best GFLOPs: 205.8979
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 64, 113, 113], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(392, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i2_3_init, i3_3_init, i1_4_init in T.grid(2, 2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 196 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_4_init)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 196 // 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + i2_3_init)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 196 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 2016 // 63)
                                            v2 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused % 196 // 14 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 63 // 7)
                                            v3 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 2016)
                                            T.reads(placeholder[v0, v1, v2, v3])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 112 and 0 <= v3 and v3 < 112, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 196 * 32 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 196 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_4)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 196 // 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + i2_3)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_1, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 196 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 196 // 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 16, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 2, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 2, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 64, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 64, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 64, 113, 113], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(196, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_4_init, i3_4_init in T.grid(4, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 49 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i1_4_init)
                            i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 4)
                            j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 49 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 4080 // 255)
                                        v2 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 16 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 255 // 17)
                                        v3 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 17)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 4080)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 112 and 0 <= v3 and v3 < 112, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 49 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 3)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, i4_0)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 48)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 3, 1, 4, 1, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 49 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i1_4)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 4)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 49 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 4 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 4, 1, 1, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 8, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 32])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 64, 113, 113], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(196, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(2, 2, 8):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 98 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 16 + i1_3_init * 8 + i1_4_init)
                            i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 98 // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 2 + i2_3_init)
                            j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(27):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 98 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 3360 // 105)
                                        v2 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused % 98 // 7 * 8 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 105 // 15)
                                        v3 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3360)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 112 and 0 <= v3 and v3 < 112, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 98 * 32 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2))
                                        v1 = T.axis.spatial(1, 0)
                                        v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 32)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 2, 2, 1, 1, 1, 1, 8, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 98 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 16 + i1_3 * 8 + i1_4)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 98 // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 2 + i2_3)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                di, dj = T.axis.remap("RR", [i4_0, i5_0])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 98 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 16 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 98 // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 2, 2, 8])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 2, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 8, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #4: GFLOPs: 88.1982. Time: 0.0478 ms. Best GFLOPs: 205.8979
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #5: GFLOPs: 41.5821. Time: 0.1014 ms. Best GFLOPs: 205.8979
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #6: GFLOPs: 46.3671. Time: 0.0909 ms. Best GFLOPs: 205.8979
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 64, 113, 113], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(224, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_4_init in T.serial(7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused // 2)
                            i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                            j = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 8 * 7 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(105):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3330 // 1665)
                                    v2 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 1665 // 111)
                                    v3 = T.axis.spatial(113, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 111)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 3330)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 112 and 0 <= v3 and v3 < 112, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(1, 0)
                                    v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 2)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused // 2)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                                j = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 8 * 7 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_0])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused // 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 8 + ax2)
                            v3 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 8 * 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 2, 1, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 4, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 32])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 32])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l155)
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 64, 113, 113], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(224, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(25):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 56 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 4080 // 255)
                                        v2 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused % 56 // 7 * 14 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 255 // 17)
                                        v3 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 17)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 4080)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 112 and 0 <= v3 and v3 < 112, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 56 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 144)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_4_init, i3_4_init in T.grid(2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 56 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_4_init)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 56 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 56 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_4)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 56 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_1, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 56 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 56 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 4, 2, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 1, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l161)
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #9: GFLOPs: 67.7089. Time: 0.0622 ms. Best GFLOPs: 205.8979
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #10: GFLOPs: 42.9010. Time: 0.0982 ms. Best GFLOPs: 205.8979
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 64, 113, 113], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(224, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i3_3_init in T.serial(7):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 32)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 32 // 8)
                                j = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 8 * 7 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(30):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 3774 // 1887)
                                            v2 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 1887 // 111)
                                            v3 = T.axis.spatial(113, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 111)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 3774)
                                            T.reads(placeholder[v0, v1, v2, v3])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 112 and 0 <= v3 and v3 < 112, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + ax0_ax1_ax2_ax3_fused_1 // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused_1 % 3)
                                        v3 = T.axis.spatial(3, i5_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 < 6)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 7, 3, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 32)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 32 // 8)
                                    j = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 8 * 7 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 32 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 32 // 8 + ax2)
                            v3 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 8 * 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 2, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 4, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 8, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 64, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 64])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l158)
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #12: GFLOPs: 10.5032. Time: 0.4013 ms. Best GFLOPs: 205.8979
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #13: GFLOPs: 129.6967. Time: 0.0325 ms. Best GFLOPs: 205.8979
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 64, 113, 113], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(75):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 56 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 8352 // 261)
                                        v2 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused % 56 // 4 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 261 // 29)
                                        v3 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 29)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 8352)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 112 and 0 <= v3 and v3 < 112, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 56 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 1, 1, 1):
                            for i3_4_init in T.serial(2):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 56 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 56 // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 7)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 2):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 56 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 56 // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 7)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 56 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 56 // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 7 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 16, 2, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 4, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l167)
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 64, 113, 113], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init in T.grid(4, 2, 7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 8 + i1_3_init * 2 + i1_4_init)
                            i = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 16 // 2 * 7 + i2_4_init)
                            j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 10656 // 333)
                                        v2 = T.axis.spatial(113, i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 333 // 3)
                                        v3 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 10656)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 112 and 0 <= v3 and v3 < 112, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(1, 0)
                                    v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 32)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 4, 1, 1, 1, 1, 1, 2, 7, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 8 + i1_3 * 2 + i1_4)
                                i = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 16 // 2 * 7 + i2_4)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                di, dj = T.axis.remap("RR", [i4_0, i5_0])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 7, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 8 + ax1)
                            v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 16 // 2 * 7 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 4, 4, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[28, 1, 2, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 64, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 64])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #16: GFLOPs: 40.2786. Time: 0.1046 ms. Best GFLOPs: 205.8979
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #17: GFLOPs: 143.2683. Time: 0.0294 ms. Best GFLOPs: 205.8979
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #18: GFLOPs: 17.7860. Time: 0.2370 ms. Best GFLOPs: 205.8979
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #19: GFLOPs: 102.5083. Time: 0.0411 ms. Best GFLOPs: 205.8979
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 64, 113, 113], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(448, thread="threadIdx.x"):
                    for i2_4_init in T.serial(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 28 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 56)
                            i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 28 * 2 + i2_4_init)
                            j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(448, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 28 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 6384 // 399)
                                        v2 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 8 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 399 // 57)
                                        v3 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + ((ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 57)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 6384)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 112 and 0 <= v3 and v3 < 112, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(448, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 28 * 16 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 48)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 28 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 56)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 28 * 2 + i2_4)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 28 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 56 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 28 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 8, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 2, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 28, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 448, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 448, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #21: GFLOPs: 82.4267. Time: 0.0511 ms. Best GFLOPs: 205.8979
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #22: GFLOPs: 140.4912. Time: 0.0300 ms. Best GFLOPs: 205.8979
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #23: GFLOPs: 65.7726. Time: 0.0641 ms. Best GFLOPs: 205.8979
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #24: GFLOPs: 48.7079. Time: 0.0865 ms. Best GFLOPs: 205.8979
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #25: GFLOPs: 221.4536. Time: 0.0190 ms. Best GFLOPs: 221.4536
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #26: GFLOPs: 35.0703. Time: 0.1202 ms. Best GFLOPs: 221.4536
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #27: GFLOPs: 12.1646. Time: 0.3465 ms. Best GFLOPs: 221.4536
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 64, 113, 113], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i3_3_init, i2_4_init in T.grid(8, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_4_init)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(34):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(64, ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 8640 // 135)
                                            v2 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused // 7 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 135 // 15)
                                            v3 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 8640)
                                            T.reads(placeholder[v0, v1, v2, v3])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 112 and 0 <= v3 and v3 < 112, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 192)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 8, 1, 1, 1, 1, 2, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_4)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_1, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 8):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 64, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 2, 1, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 8, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 64, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 64, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #29: GFLOPs: 268.2119. Time: 0.0157 ms. Best GFLOPs: 268.2119
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], compute: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 64, 113, 113], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_4_init, i2_4_init in T.grid(2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_4_init)
                                i = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + i2_4_init)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_1_i1_1_i2_1_i3_1_fused)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(57):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 6328 // 791)
                                        v2 = T.axis.spatial(113, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 791 // 7)
                                        v3 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 < 6328)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 112 and 0 <= v3 and v3 < 112, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + ax0_ax1_ax2_ax3_fused_1 // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused_1 % 3)
                                        v3 = T.axis.spatial(3, i5_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 < 24)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 2, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_4)
                                    i = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + i2_4)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_1_i1_1_i2_1_i3_1_fused)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 4, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 28, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 4, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 112])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 112])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l156)
[04:58:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_clip_3"] Trial #31: GFLOPs: 109.2260. Time: 0.0386 ms. Best GFLOPs: 268.2119
[04:59:03] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_conv2d_add_clip_3"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 128
Total latency (us): 59.9446

[04:59:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #0: GFLOPs: 381.0453. Time: 0.1380 ms. Best GFLOPs: 381.0453
[04:59:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #1: GFLOPs: 846.5533. Time: 0.0621 ms. Best GFLOPs: 846.5533
[04:59:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #2: GFLOPs: 579.4056. Time: 0.0908 ms. Best GFLOPs: 846.5533
[04:59:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i2_4_init in T.grid(2, 14, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_3_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i2_4_init)
                            xx = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(49):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 1568)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 1568 // 56)
                                    v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 56)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 128)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 1, 14, 1, 1, 1, 1, 1, 14, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_3)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i2_4)
                                xx = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 14, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + ax2)
                            v3 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 16, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 1, 1, 14])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 4, 14, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 64])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 64, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:59:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #4: GFLOPs: 36.4407. Time: 1.4430 ms. Best GFLOPs: 846.5533
[04:59:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1568, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(128):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 64)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 64 // 8)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 49 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 64)
                                    v1 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 64)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_4_init, i3_4_init in T.grid(2, 2):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 49 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 2 + i1_4_init)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_4_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 1, 1, 16, 1, 1, 1, 2, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 49 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 2 + i1_4)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_4)
                                rc = T.axis.reduce(64, i4_1 * 16 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 49 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 2, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 4, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 4, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l174)
[04:59:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #6: GFLOPs: 87.2592. Time: 0.6026 ms. Best GFLOPs: 846.5533
[04:59:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init in T.grid(8, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i1_3_init)
                            yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(112):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 784)
                                    v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 784 // 14)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 256)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 8, 1, 7, 2, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i1_3)
                                yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 8 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 4, 1, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 28, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 2, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 4, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 56])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:59:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #8: GFLOPs: 86.5045. Time: 0.6079 ms. Best GFLOPs: 846.5533
[04:59:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #9: GFLOPs: 482.1499. Time: 0.1091 ms. Best GFLOPs: 846.5533
[04:59:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init in T.grid(2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 14 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_4_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 14)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) // 392)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 4 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 392 // 14)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(64, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 2048)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 2, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 14 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_4)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 14)
                                rc = T.axis.reduce(64, i4_0 * 16 + i4_1 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 14 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 16, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 14, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 14, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 224])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 224, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:59:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(448, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i1_4_init in T.grid(2, 2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 28 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 28 // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 7 * 2 + i2_3_init)
                            xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3136)
                                        v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3136 // 56)
                                        v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 56)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 2)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 256)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 28 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 28 // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 7 * 2 + i2_3)
                                xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 28 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 28 // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 7 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 16, 2, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 4, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 4, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 56])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:59:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init, i3_4_init in T.grid(2, 2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 8 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(98):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 196)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 8 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 196 // 7)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(64, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 2, 2, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 8 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 16 + i4_1 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 8 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 16, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 7, 2, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[8, 1, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:59:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i2_4_init, i3_4_init in T.grid(2, 4, 4, 7, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3_init)
                            yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 2 * 28 + i2_3_init * 7 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0)
                                        v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 448)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0, v1 = T.axis.remap("SS", [ax0_ax1_ax2_ax3_fused_1, i4_0])
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 4, 4, 1, 1, 1, 1, 1, 7, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3)
                                yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 2 * 28 + i2_3 * 7 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(64, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 28, 8):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 2 * 28 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 64, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 4, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 4, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 128, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 128])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:59:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i2_4_init, i3_4_init in T.grid(2, 2, 2, 4, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3_init)
                            yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 8 + i2_3_init * 4 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_3_init * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 1568)
                                        v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 1568 // 28)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + ax0_ax1_ax2_ax3_fused_1 // 2)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + ax0_ax1_ax2_ax3_fused_1 % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 128)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 4, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3)
                                yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 8 + i2_3 * 4 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_3 * 7 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 8, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 8 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 16, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 2, 2, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 224, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 224])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:59:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i2_4_init in T.grid(32, 4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 32 + i1_3_init)
                            yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 2 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 448)
                                    v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 448 // 8)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 256)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 32, 1, 4, 2, 1, 1, 1, 1, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 32 + i1_3)
                                yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 2 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 2, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 32 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 2, 32, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 14, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 2, 4, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:59:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i2_4_init, i3_4_init in T.grid(4, 2, 7, 28, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3_init)
                            yy = T.axis.spatial(56, i2_3_init * 28 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 14 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(49):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 784)
                                        v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 784 // 14)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 14 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 2, 7, 4, 1, 1, 1, 1, 28, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3)
                                yy = T.axis.spatial(56, i2_3 * 28 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 14 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 56, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2 = T.axis.spatial(56, ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 28])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 1, 7, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:59:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #17: GFLOPs: 327.5358. Time: 0.1605 ms. Best GFLOPs: 846.5533
[04:59:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #18: GFLOPs: 890.4734. Time: 0.0591 ms. Best GFLOPs: 890.4734
[04:59:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #19: GFLOPs: 494.3192. Time: 0.1064 ms. Best GFLOPs: 890.4734
[04:59:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i2_4_init, i3_4_init in T.grid(2, 4, 4, 7, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3_init)
                            yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 2 * 28 + i2_3_init * 7 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0)
                                        v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 448)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0, v1 = T.axis.remap("SS", [ax0_ax1_ax2_ax3_fused_1, i4_0])
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 4, 4, 1, 1, 1, 1, 1, 7, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3)
                                yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 2 * 28 + i2_3 * 7 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(64, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 28, 8):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 2 * 28 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 64, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 4, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 4, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 128, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 128])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:59:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(2, 4, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 49 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i2_3_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 64)
                                        v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 64 // 8)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 49 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(64, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 2, 4, 1, 2, 1, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 49 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i2_3)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                rc = T.axis.reduce(64, i4_0 * 16 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 4, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 49 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 8, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 8, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 8, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 64])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:59:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #22: GFLOPs: 1422.0789. Time: 0.0370 ms. Best GFLOPs: 1422.0789
[04:59:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #23: GFLOPs: 203.6115. Time: 0.2583 ms. Best GFLOPs: 1422.0789
[04:59:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init in T.grid(2, 14, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_4_init)
                            yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i2_3_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 784)
                                    v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 784 // 14)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 16)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 14, 2, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_4)
                                yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i2_3)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 4, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 14, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 1, 14, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:59:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i1_4_init, i3_4_init in T.grid(2, 4, 2, 4, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 4 + i2_3_init)
                            xx = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 2 * 28 + i3_3_init * 14 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) // 1568)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 1568 // 56)
                                    v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 56)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) // 4)
                                    v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 < 512)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 4, 2, 4, 1, 1, 1, 4, 1, 14):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 4 + i2_3)
                                xx = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 2 * 28 + i3_3 * 14 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 4, 28):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 4 + ax2)
                            v3 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 2 * 28 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 16, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 2, 2, 14])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 224])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 224])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:59:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #26: GFLOPs: 247.6916. Time: 0.2123 ms. Best GFLOPs: 1422.0789
[04:59:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #27: GFLOPs: 247.8929. Time: 0.2121 ms. Best GFLOPs: 1422.0789
[04:59:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i3_3_init, i2_4_init in T.grid(2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 7 + i2_4_init)
                            xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 784)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 784 // 56)
                                    v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 56)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 64)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 7, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 7 + i2_4)
                                xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 7, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 7 + ax2)
                            v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 4, 4, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 2, 1, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 14, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:59:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i2_4_init in T.grid(4, 4, 7, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3_init)
                            yy = T.axis.spatial(56, i2_3_init * 14 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 7 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(49):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 392)
                                        v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 392 // 7)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 7 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 4, 7, 4, 1, 1, 1, 1, 14, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3)
                                yy = T.axis.spatial(56, i2_3 * 14 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 7 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 8 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 56, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2 = T.axis.spatial(56, ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 4, 14])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[8, 1, 1, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 32])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:59:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i2_3_init, i3_4_init in T.grid(14, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4)
                            yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + i2_3_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 1568)
                                        v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 1568 // 28)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3136)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + ax0_ax1_ax2_ax3_fused_1 // 2)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + ax0_ax1_ax2_ax3_fused_1 % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 14, 1, 1, 1, 1, 1, 1, 1, 14):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4)
                                yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + i2_3)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 14, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 + ax1)
                            v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 4, 14, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 2, 1, 1, 14])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 128, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 128])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:59:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_4"] Trial #31: GFLOPs: 9.5992. Time: 5.4780 ms. Best GFLOPs: 1422.0789
[04:59:29] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_nn_conv2d_add_clip_4"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 160
Total latency (us): 96.9217

[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 64 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 11520 // 180)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 180 // 30)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 11520)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 576)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            for i3_3_init, i1_4_init, i2_4_init in T.grid(2, 2, 2):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_4_init)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i2_4_init)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_3_init)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 2, 3, 1, 1, 2, 2, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_4)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i2_4)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 16, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 2, 1, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 14, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 224, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 224, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l162)
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #1: GFLOPs: 177.9642. Time: 0.0474 ms. Best GFLOPs: 177.9642
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #2: GFLOPs: 93.8554. Time: 0.0898 ms. Best GFLOPs: 177.9642
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #3: GFLOPs: 167.8310. Time: 0.0502 ms. Best GFLOPs: 177.9642
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #4: GFLOPs: 73.4840. Time: 0.1147 ms. Best GFLOPs: 177.9642
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(67):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 7424 // 232)
                                    v2 = T.axis.spatial(58, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 232 // 4)
                                    v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 28 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 < 7424)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 < 288)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 1, 2, 1):
                            for i3_4_init in T.serial(2):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    i = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_3)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 * 2 + i3_4_init)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 2):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    i = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_3)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 16, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 7, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[28, 1, 1, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 112])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 112])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l163)
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(58):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 9600 // 300)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 300 // 30)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 9600)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            for i1_3_init, i3_3_init, i1_4_init in T.grid(2, 2, 16):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + i1_3_init * 16 + i1_4_init)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3_init)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 2, 1, 2, 3, 1, 1, 16, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + i1_3 * 16 + i1_4)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 1, 2, 16])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 8, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 2, 7, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l162)
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #7: GFLOPs: 101.7691. Time: 0.0828 ms. Best GFLOPs: 177.9642
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #8: GFLOPs: 166.6815. Time: 0.0506 ms. Best GFLOPs: 177.9642
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(224, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 2400 // 300)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 300 // 30)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 2400)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 72)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 2, 1, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i1_3)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i1_3)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 4, 1, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 8, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 2, 7, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 56])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l165)
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #10: GFLOPs: 106.1045. Time: 0.0794 ms. Best GFLOPs: 177.9642
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #11: GFLOPs: 141.2002. Time: 0.0597 ms. Best GFLOPs: 177.9642
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_4_init, i3_4_init in T.grid(2, 14):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_4_init)
                            i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2)
                            j = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(116):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 6496 // 812)
                                    v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 812 // 58)
                                    v3 = T.axis.spatial(58, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 58)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 24)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 1, 1, 1, 1, 2, 1, 14):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_4)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2)
                                j = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 14):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 + ax2)
                            v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 4, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 2, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 2, 1, 14])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 56])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 56, 4])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i2_4_init in T.grid(7, 2, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_4_init)
                            i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 2 + i2_4_init)
                            j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 4096 // 128)
                                    v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 8 + i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 128 // 16)
                                    v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + ax0_ax1_ax2_ax3_fused_1 // 3)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, i4_0)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused_1 % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 96)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 7, 1, 1, 1, 2, 2, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_4)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 2 + i2_4)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 16, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 2, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 128])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 128])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l155)
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(224, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(31):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 112 * 64 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 3456 // 54)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 112 // 14 * 7 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 54 // 6)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 6)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 3456)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 112 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 576)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i3_3_init, i1_4_init in T.grid(2, 8):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 112 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_4_init)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 112 // 14 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 2, 1, 1, 1, 8, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 112 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_4)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 112 // 14 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3)
                                di, dj = T.axis.remap("RR", [i4_1, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 112 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 112 // 14 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 4, 1, 8])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 1, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 2, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l161)
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init, i1_4_init, i3_4_init in T.grid(4, 2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 4 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3_init * 2 + i1_4_init)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(128, ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 10752 // 84)
                                            v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 84 // 14)
                                            v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 384)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 4, 1, 1, 1, 1, 1, 2, 1, 2):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 4 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3 * 2 + i1_4)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_1, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 4 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 8, 4, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 4, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(224, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(96):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 6144 // 48)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused // 4 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 48 // 16)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i3_3_init in T.serial(14):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 4)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 14, 1, 3, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 4)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i3_3)
                                di, dj = T.axis.remap("RR", [i4_1, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 14):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 4 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 32, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[56, 1, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 1, 14, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l161)
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #17: GFLOPs: 568.1622. Time: 0.0148 ms. Best GFLOPs: 568.1622
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #18: GFLOPs: 45.4526. Time: 0.1855 ms. Best GFLOPs: 568.1622
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(224, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 112 * 64 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 4096 // 64)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 112 // 4 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 64 // 16)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 112 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 576)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i3_3_init in T.serial(14):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 112 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 112 // 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 14, 1, 3, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 112 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 112 // 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i3_3)
                                di, dj = T.axis.remap("RR", [i4_1, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 14):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 112 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 112 // 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 32, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[28, 1, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 1, 14, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 64, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 64, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l161)
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #20: GFLOPs: 133.3667. Time: 0.0632 ms. Best GFLOPs: 568.1622
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(392, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init, i3_3_init, i2_4_init, i3_4_init in T.grid(2, 2, 2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 196 * 2 + i1_3_init)
                                i = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 196 // 7 * 2 + i2_4_init)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_3_init * 2 + i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(17):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(392, thread="threadIdx.x"):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1) % 6496 // 1624)
                                        v2 = T.axis.spatial(58, (ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1) % 1624 // 28)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1 < 6496)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(392, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 12)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 196 * 2 + i1_3)
                                    i = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 196 // 7 * 2 + i2_4)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_3 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_1, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 4):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 196 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 196 // 7 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 2, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 28, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 7, 2, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 392])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 392, 3])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l158)
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(26):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 5760 // 180)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 28 // 14 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 180 // 6)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 6)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 5760)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i3_3_init, i1_4_init in T.grid(4, 4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_4_init)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 // 14 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 4, 1, 3, 1, 4, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_4)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 // 14 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i3_3)
                                di, dj = T.axis.remap("RR", [i4_1, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 4):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 // 14 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 4, 1, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 14, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 1, 4, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l161)
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #23: GFLOPs: 149.4118. Time: 0.0564 ms. Best GFLOPs: 568.1622
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #24: GFLOPs: 82.4722. Time: 0.1022 ms. Best GFLOPs: 568.1622
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(448, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init in T.serial(2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i1_3_init)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(20):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 1120 // 140)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 140 // 14)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 8 + ax0_ax1_ax2_ax3_fused_1 // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused_1 % 3)
                                        v3 = T.axis.spatial(3, i5_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 < 24)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i1_3)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 4, 1, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 8, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 2, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 56])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 56])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l156)
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #26: GFLOPs: 50.9038. Time: 0.1656 ms. Best GFLOPs: 568.1622
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init in T.serial(16):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 7 * 16 + i1_3_init)
                            i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 8 * 7 + i0_1_i1_1_i2_1_i3_1_fused)
                            j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(144):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 8064 // 63)
                                    v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused // 8 * 7 + i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 63 // 9)
                                    v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 9)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 384)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 16, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 7 * 16 + i1_3)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 8 * 7 + i0_1_i1_1_i2_1_i3_1_fused)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 7 * 16 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 8 * 7 + i0_1_i1_1_i2_1_i3_1_fused + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 16, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 7, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[8, 1, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 56])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 56, 3])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(196, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init, i3_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 2, 2, 2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i1_3_init * 2 + i1_4_init)
                                i = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i2_4_init)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_3_init * 2 + i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(34):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(196, thread="threadIdx.x"):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) % 6496 // 1624)
                                        v2 = T.axis.spatial(58, (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) % 1624 // 28)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1 < 6496)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(196, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 12)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i1_3 * 2 + i1_4)
                                    i = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i2_4)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_3 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_1, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 4):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 1, 2, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 28, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 7, 2, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 196])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 196, 2])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l158)
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #29: GFLOPs: 143.5124. Time: 0.0587 ms. Best GFLOPs: 568.1622
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #30: GFLOPs: 165.3144. Time: 0.0510 ms. Best GFLOPs: 568.1622
[04:59:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_clip_5"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i3_3_init in T.serial(2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused // 2)
                                j = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(30):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 6720 // 1680)
                                            v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 1680 // 56)
                                            v3 = T.axis.spatial(58, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 56)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 12)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused // 2)
                                    j = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_1, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused // 2 + ax2)
                            v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 4, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 28, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 14, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[04:59:59] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_conv2d_add_clip_5"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 192
Total latency (us): 111.758

[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 2, 14, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 56 // 14 * 14 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 1568)
                                        v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 1568 // 28)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 128)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 14, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 56 // 14 * 14 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_4)
                                rc = T.axis.reduce(128, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 14, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 56 // 14 * 14 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 4, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 4, 1, 14])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 14, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 224, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 224, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #1: GFLOPs: 1224.0665. Time: 0.0849 ms. Best GFLOPs: 1224.0665
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #2: GFLOPs: 162.3443. Time: 0.6404 ms. Best GFLOPs: 1224.0665
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(196, thread="threadIdx.x"):
                    for i1_3_init, i3_4_init in T.grid(2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 98 * 2 + i1_3_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 98 // 7)
                            xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(196, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) // 1568)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) % 1568 // 56)
                                    v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) % 56)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(196, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + ax0_ax1_ax2_ax3_fused_1 // 4)
                                    v1 = T.axis.spatial(128, i4_0 * 4 + ax0_ax1_ax2_ax3_fused_1 % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 64)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 4, 1, 1, 1, 1, 1, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 98 * 2 + i1_3)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 98 // 7)
                                xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_4)
                                rc = T.axis.reduce(128, i4_0 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 98 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 98 // 7 + ax2)
                            v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 4, 2, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 14, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 7, 1, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 196])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 196])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #4: GFLOPs: 153.9630. Time: 0.6753 ms. Best GFLOPs: 1224.0665
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i3_3_init, i2_4_init in T.grid(56, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i2_4_init)
                            xx = T.axis.spatial(56, i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0)
                                        v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 14 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 56)
                                        v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 56)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 784)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(128, i4_0)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 128)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 56, 1, 1, 1, 1, 1, 7, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i2_4)
                                xx, rc = T.axis.remap("SR", [i3_3, i4_0])
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 7, 56):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax2)
                            v3 = T.axis.spatial(56, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 2, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 56, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 64, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #6: GFLOPs: 221.0499. Time: 0.4703 ms. Best GFLOPs: 1224.0665
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #7: GFLOPs: 988.2926. Time: 0.1052 ms. Best GFLOPs: 1224.0665
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #8: GFLOPs: 4899.0273. Time: 0.0212 ms. Best GFLOPs: 4899.0273
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init in T.grid(4, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + i1_3_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4)
                            xx = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(25):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) // 1568)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 1568 // 56)
                                    v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 56)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 < 3136)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(128, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 128)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 1, 14, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + i1_3)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4)
                                xx = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + i3_3)
                                rc = T.axis.reduce(128, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4 + ax2)
                            v3 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 8, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 7, 4, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 4, 14, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 128])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 128, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #10: GFLOPs: 1.7521. Time: 59.3375 ms. Best GFLOPs: 4899.0273
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #11: GFLOPs: 673.2518. Time: 0.1544 ms. Best GFLOPs: 4899.0273
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i2_4_init in T.grid(7, 4, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_4_init)
                            yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 14 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(98):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 784)
                                        v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 784 // 14)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(128, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 7, 8, 1, 1, 1, 4, 14, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_4)
                                yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 14 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3)
                                rc = T.axis.reduce(128, i4_0 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 14, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 14 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 16, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 1, 1, 14])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 2, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 32])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #13: GFLOPs: 373.3548. Time: 0.2785 ms. Best GFLOPs: 4899.0273
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #14: GFLOPs: 2.9380. Time: 35.3863 ms. Best GFLOPs: 4899.0273
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i1_4_init in T.grid(2, 7, 8, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 8 * 7 + i2_3_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 448)
                                        v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 448 // 8)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(128, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 7, 8, 16, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 8 * 7 + i2_3)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i3_3)
                                rc = T.axis.reduce(128, i4_0 * 16 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 8):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 8 * 7 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 4, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 8, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 8, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 1, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 32])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i2_4_init in T.grid(2, 2, 7, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) // 56)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 56 // 14)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 < 448)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(128, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 2, 2, 7, 1, 1, 1, 1, 1, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3)
                                rc = T.axis.reduce(128, i4_0 * 8 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 4, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 64, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 1, 2, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 2, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 128])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 128, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(49, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init in T.grid(16, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i1_3_init)
                            yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + i2_3_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 392)
                                        v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 392 // 7)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 7 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) // 4)
                                    v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1 < 512)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 16, 4, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i1_3)
                                yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + i2_3)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(128, i4_0 * 4 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 4, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 1, 16, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 7, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[8, 1, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 49, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 49])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #18: GFLOPs: 64.9941. Time: 1.5996 ms. Best GFLOPs: 4899.0273
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init, i2_4_init in T.grid(2, 2, 4, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 14 + i2_3_init * 7 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 112)
                                        v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 112 // 2)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 2 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(128, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 4, 7, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 14 + i2_3 * 7 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                rc = T.axis.reduce(128, i4_0 * 16 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 14, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 14 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 16, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 1, 2, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[28, 1, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 8, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 32])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(4, 14, 4, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_4_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i2_3_init * 7 + i2_4_init)
                            xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(25):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 56)
                                    v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 56)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 < 1568)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(128, i4_0)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 4, 14, 1, 1, 1, 1, 4, 7, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_4)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i2_3 * 7 + i2_4)
                                xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_3)
                                rc = T.axis.reduce(128, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 28, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + ax2)
                            v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1, 4, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 2, 14, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 64])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 64])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #21: GFLOPs: 1234.4702. Time: 0.0842 ms. Best GFLOPs: 4899.0273
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #22: GFLOPs: 198.7360. Time: 0.5231 ms. Best GFLOPs: 4899.0273
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_4_init in T.grid(4, 2, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_3_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i2_3_init)
                            xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 448)
                                        v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 448 // 56)
                                        v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 56)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(128, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 2, 1, 8, 1, 1, 1, 1, 1, 14):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_3)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i2_3)
                                xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i3_4)
                                rc = T.axis.reduce(128, i4_0 * 16 + i4_1 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 2, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 4, 1, 1, 14])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 64, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #24: GFLOPs: 570.9757. Time: 0.1821 ms. Best GFLOPs: 4899.0273
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #25: GFLOPs: 121.6350. Time: 0.8547 ms. Best GFLOPs: 4899.0273
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #26: GFLOPs: 1038.5213. Time: 0.1001 ms. Best GFLOPs: 4899.0273
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #27: GFLOPs: 166.5135. Time: 0.6244 ms. Best GFLOPs: 4899.0273
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #28: GFLOPs: 197.7205. Time: 0.5258 ms. Best GFLOPs: 4899.0273
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i1_4_init, i3_4_init in T.grid(2, 4, 7, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 4 + i2_3_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 1568)
                                    v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 1568 // 28)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(128, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 256)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 4, 7, 2, 1, 1, 1, 2, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 4 + i2_3)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(128, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 4 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 16, 2, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 14, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 2, 7, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #30: GFLOPs: 2115.7491. Time: 0.0491 ms. Best GFLOPs: 4899.0273
[05:00:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_clip_6"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_4_init, i3_4_init in T.grid(2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 8 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_4_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 7)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 392)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 392 // 14)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 8 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(128, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 512)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 2, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 8 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_4)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 7)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                rc = T.axis.reduce(128, i4_0 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 8 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 7 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 8, 4, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 7, 4, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:00:22] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_nn_conv2d_add_clip_6"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 224
Total latency (us): 132.98

[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #0: GFLOPs: 14.5305. Time: 0.1450 ms. Best GFLOPs: 14.5305
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(128, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init, i3_3_init in T.grid(2, 7):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_3_init)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4)
                                j = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 3300 // 825)
                                            v2 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 825 // 55)
                                            v3 = T.axis.spatial(57, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 55)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3300)
                                            T.reads(placeholder[v0, v1, v2, v3])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 56 and 0 <= v3 and v3 < 56, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 12)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 2, 1, 7, 3, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_3)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4)
                                    j = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 2, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 4, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #2: GFLOPs: 42.2551. Time: 0.0499 ms. Best GFLOPs: 42.2551
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i3_4_init in T.serial(7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4)
                            i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28)
                            j = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 64 + ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 3648 // 57)
                                        v2 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused % 28 * 2 + i4_0 + 0)
                                        v3 = T.axis.spatial(57, ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 57)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3648)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 56 and 0 <= v3 and v3 < 56, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 64 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 192)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28)
                                j = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 32, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[28, 1, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 4, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 128, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 128, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #4: GFLOPs: 126.4542. Time: 0.0167 ms. Best GFLOPs: 126.4542
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #5: GFLOPs: 47.9196. Time: 0.0440 ms. Best GFLOPs: 126.4542
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i2_3_init in T.serial(14):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i2_3_init)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(114):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 7296 // 57)
                                        v2 = T.axis.spatial(57, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 57)
                                        v3 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused * 2 + i5_0 + 0)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 56 and 0 <= v3 and v3 < 56, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 384)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 14, 1, 1, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                    i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i2_3)
                                    j, di, dj = T.axis.remap("SRR", [i0_0_i1_0_i2_0_i3_0_fused, i4_1, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 14, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 32, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 14, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[28, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 64])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 64, 4])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l158)
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(2, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_3_init * 2 + i1_4_init)
                            i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused)
                            j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 3456 // 27)
                                        v2 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 27 // 9)
                                        v3 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 9)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3456)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 56 and 0 <= v3 and v3 < 56, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) // 3)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, i4_0)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_3 * 2 + i1_4)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 2, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 2, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 128, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 128])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #8: GFLOPs: 26.1381. Time: 0.0806 ms. Best GFLOPs: 126.4542
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #9: GFLOPs: 96.4035. Time: 0.0219 ms. Best GFLOPs: 126.4542
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(128, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init, i3_3_init in T.grid(2, 7):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_3_init)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4)
                                j = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 3300 // 825)
                                            v2 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 825 // 55)
                                            v3 = T.axis.spatial(57, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 55)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3300)
                                            T.reads(placeholder[v0, v1, v2, v3])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 56 and 0 <= v3 and v3 < 56, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 12)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 2, 1, 7, 1, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_3)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4)
                                    j = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_1, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 2, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 4, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #11: GFLOPs: 49.6015. Time: 0.0425 ms. Best GFLOPs: 126.4542
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(20):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 64 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 8640 // 135)
                                        v2 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 135 // 15)
                                        v3 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 8640)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 56 and 0 <= v3 and v3 < 56, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 576)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                            i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7)
                            j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                di, dj = T.axis.remap("RR", [i4_1, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 16, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 224, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 224, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l161)
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #13: GFLOPs: 51.8192. Time: 0.0407 ms. Best GFLOPs: 126.4542
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(196, thread="threadIdx.x"):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 98)
                        i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 98 // 14)
                        j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                        T.reads()
                        T.writes(DepthwiseConv2d_local[b, c, i, j])
                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(21):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(196, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 11856 // 741)
                                        v2 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 741 // 57)
                                        v3 = T.axis.spatial(57, ((ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 57)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 11856)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 56 and 0 <= v3 and v3 < 56, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(196, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 48)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 98)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 98 // 14)
                                j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 98 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 98 // 14 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 8, 2, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 14, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 196, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 196, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #15: GFLOPs: 10.6656. Time: 0.1976 ms. Best GFLOPs: 126.4542
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i3_3_init in T.serial(14):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 2)
                                j = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(29):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 6380 // 1595)
                                            v2 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 1595 // 55)
                                            v3 = T.axis.spatial(57, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 55)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 6380)
                                            T.reads(placeholder[v0, v1, v2, v3])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 56 and 0 <= v3 and v3 < 56, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 12)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 14, 1, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 2)
                                    j = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_1, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 14):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 28 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 4, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 14, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 2, 14, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 112, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 112, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #17: GFLOPs: 71.6144. Time: 0.0294 ms. Best GFLOPs: 126.4542
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #18: GFLOPs: 189.1382. Time: 0.0111 ms. Best GFLOPs: 189.1382
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #19: GFLOPs: 96.8220. Time: 0.0218 ms. Best GFLOPs: 189.1382
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #20: GFLOPs: 74.1607. Time: 0.0284 ms. Best GFLOPs: 189.1382
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #21: GFLOPs: 21.1832. Time: 0.0995 ms. Best GFLOPs: 189.1382
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i2_3_init in T.serial(4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i2_3_init)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(98):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 10944 // 171)
                                        v2 = T.axis.spatial(57, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 171 // 3)
                                        v3 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 < 10944)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 56 and 0 <= v3 and v3 < 56, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 192)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 1, 3, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i2_3)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 4, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 16, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 4, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 2, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 112])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 112, 2])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l158)
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #23: GFLOPs: 98.7229. Time: 0.0213 ms. Best GFLOPs: 189.1382
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(392, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(23):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 98 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 1440 // 45)
                                        v2 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused % 98 // 7 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 45 // 9)
                                        v3 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 9)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1440)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 56 and 0 <= v3 and v3 < 56, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 98 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_3_init, i3_4_init in T.grid(2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 98 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3_init)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 98 // 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 98 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 98 // 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_1, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 98 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 98 // 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 16, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 2, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 32])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l159)
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #25: GFLOPs: 145.6849. Time: 0.0145 ms. Best GFLOPs: 189.1382
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], compute: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 128, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i2_3_init, i3_4_init in T.grid(2, 7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                            i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + i2_3_init)
                            j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(54):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 5940 // 1485)
                                        v2 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 1485 // 55)
                                        v3 = T.axis.spatial(57, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 55)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 5940)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 56 and 0 <= v3 and v3 < 56, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2))
                                        v1 = T.axis.spatial(1, 0)
                                        v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 4)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 7):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + i2_3)
                                j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_0])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 4, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 2, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #27: GFLOPs: 46.0726. Time: 0.0457 ms. Best GFLOPs: 189.1382
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #28: GFLOPs: 58.4860. Time: 0.0360 ms. Best GFLOPs: 189.1382
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #29: GFLOPs: 20.6561. Time: 0.1020 ms. Best GFLOPs: 189.1382
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #30: GFLOPs: 32.6609. Time: 0.0645 ms. Best GFLOPs: 189.1382
[05:00:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_7"] Trial #31: GFLOPs: 6.2114. Time: 0.3393 ms. Best GFLOPs: 189.1382
[05:00:51] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_conv2d_add_clip_7"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 256
Total latency (us): 144.122

[05:00:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #0: GFLOPs: 1850.1554. Time: 0.0281 ms. Best GFLOPs: 1850.1554
[05:00:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(112, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init in T.grid(4, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 28 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 28 // 14 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 14 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 784)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 784 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(128, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 2048)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 1, 1, 8, 1, 1, 1, 2, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 28 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 28 // 14 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 14 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                rc = T.axis.reduce(128, i4_0 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 28 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 28 // 14 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 14 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 8, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 7, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 14, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[05:00:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #2: GFLOPs: 796.3339. Time: 0.0653 ms. Best GFLOPs: 1850.1554
[05:00:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #3: GFLOPs: 899.9824. Time: 0.0578 ms. Best GFLOPs: 1850.1554
[05:00:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(2, 2, 2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 112)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 112 // 28)
                                    v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(128, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1024)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 2, 2, 16, 1, 1, 1, 2, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                rc = T.axis.reduce(128, i4_0 * 16 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 8, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 1, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:00:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i2_4_init in T.grid(8, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 112)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 112 // 28)
                                    v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 2)
                                    v1 = T.axis.spatial(128, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 256)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 8, 1, 2, 1, 1, 1, 1, 1, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                rc = T.axis.reduce(128, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 8, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 56])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[05:00:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 14 * 2 + i2_3_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 112)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 112 // 4)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 64)
                                        v1 = T.axis.spatial(128, i4_0 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 64)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 2, 2, 1, 4, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 14 * 2 + i2_3)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                rc = T.axis.reduce(128, i4_0 * 64 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 14 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 8, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 14, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 16, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[05:00:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #7: GFLOPs: 1.3928. Time: 37.3213 ms. Best GFLOPs: 1850.1554
[05:00:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i2_4_init in T.grid(7, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(49):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) // 392)
                                    v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 392 // 14)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(128, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 2, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i3_3)
                                rc = T.axis.reduce(128, i4_0 * 16 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 128, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 14, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 2, 1, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 16, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 128])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 128])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[05:00:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #9: GFLOPs: 668.8885. Time: 0.0777 ms. Best GFLOPs: 1850.1554
[05:00:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #10: GFLOPs: 1613.4792. Time: 0.0322 ms. Best GFLOPs: 1850.1554
[05:00:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #11: GFLOPs: 432.0101. Time: 0.1203 ms. Best GFLOPs: 1850.1554
[05:00:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(4, 4, 2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 4 * 16 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 112)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 112 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 448)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 4, 2, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 4 * 16 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_4)
                                rc = T.axis.reduce(128, i4_0 * 4 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 2, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 4 * 16 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 16, 4, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 4, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 64, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[05:00:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i2_4_init, i3_4_init in T.grid(2, 2, 7, 2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i3_3_init * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 112)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 112 // 28)
                                    v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 4)
                                    v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 2, 7, 2, 1, 1, 1, 1, 2, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(28, i3_3 * 4 + i3_4)
                                rc = T.axis.reduce(128, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 4, 28):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + ax2)
                            v3 = T.axis.spatial(28, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 64, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 64])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 64])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[05:00:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #14: GFLOPs: 676.7471. Time: 0.0768 ms. Best GFLOPs: 1850.1554
[05:00:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(112, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_4_init in T.grid(2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_3_init)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 2 + i2_3_init)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) // 784)
                                    v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 784 // 28)
                                    v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(128, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 < 2048)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_3)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 2 + i2_3)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_4)
                                rc = T.axis.reduce(128, i4_0 * 8 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 16, 8, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 14, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 2, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 4, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 224])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 224])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[05:00:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #16: GFLOPs: 1732.4149. Time: 0.0300 ms. Best GFLOPs: 1850.1554
[05:00:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i2_4_init in T.grid(2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 8 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 7 + i2_4_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 98)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 98 // 7)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(37):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 8 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 32)
                                    v1 = T.axis.spatial(128, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 32)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 2048)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 7, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 8 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 7 + i2_4)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(128, i4_0 * 32 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 8 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 7 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 8, 4, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 16, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 56])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[05:00:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(4, 4, 7, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 16 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 7 + i2_4_init)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(25):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 784)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 784 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 3136)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 4)
                                    v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 4, 7, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 16 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 7 + i2_4)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4)
                                rc = T.axis.reduce(128, i4_0 * 4 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 7, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 16 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 7 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 8, 4, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 4, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 2, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 64])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:00:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init in T.grid(4, 7, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i2_3_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 56)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 56 // 14)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1792)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) // 32)
                                    v1 = T.axis.spatial(128, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 32)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 4, 7, 32, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i2_3)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3)
                                rc = T.axis.reduce(128, i4_0 * 32 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 4, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 64, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 2, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 1, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 128, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 128])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:00:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #20: GFLOPs: 27.6578. Time: 1.8795 ms. Best GFLOPs: 1850.1554
[05:00:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(896, thread="threadIdx.x"):
                    with T.block("conv2d_nchw_init"):
                        nn = T.axis.spatial(1, 0)
                        ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                        yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4)
                        xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                        T.reads()
                        T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(896, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 2 + ax0_ax1_ax2_ax3_fused_1 // 196)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + ax0_ax1_ax2_ax3_fused_1 % 196 // 28)
                                    v3 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused_1 % 28)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 392)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(896, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + ax0_ax1_ax2_ax3_fused_1 // 2)
                                    v1 = T.axis.spatial(128, i4_0 * 2 + ax0_ax1_ax2_ax3_fused_1 % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 64)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                rc = T.axis.reduce(128, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 28 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 4, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 896])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 896])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[05:00:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #22: GFLOPs: 969.9677. Time: 0.0536 ms. Best GFLOPs: 1850.1554
[05:00:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #23: GFLOPs: 67.3857. Time: 0.7714 ms. Best GFLOPs: 1850.1554
[05:00:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(49, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(8, 2, 2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 16 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(128):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) // 392)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) % 392 // 28)
                                    v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(42):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(128, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 4096)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 8, 1, 1, 16, 1, 1, 1, 2, 2, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 16 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_4)
                                rc = T.axis.reduce(128, i4_0 * 16 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 2, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 16 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 16, 1, 8, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 1, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 49])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 49, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:00:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(256, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init in T.grid(4, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_3_init)
                            yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2)
                            xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(256, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 784)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 784 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 6272)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(256, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(128, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 4, 1, 14, 2, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_3)
                                yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2)
                                xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_3)
                                rc = T.axis.reduce(128, i4_0 * 8 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 64, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 14, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 2, 14, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 4, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 256, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 256, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[05:00:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #26: GFLOPs: 1090.1361. Time: 0.0477 ms. Best GFLOPs: 1850.1554
[05:00:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #27: GFLOPs: 514.2995. Time: 0.1011 ms. Best GFLOPs: 1850.1554
[05:00:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(32, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i2_3_init, i2_4_init in T.grid(7, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                            yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 14 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 392)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 392 // 14)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 784)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(128, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 512)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 7, 1, 2, 1, 1, 1, 1, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 14 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(128, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 14, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 14 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 7, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 2, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 224, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 224, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[05:00:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #29: GFLOPs: 20.2406. Time: 2.5682 ms. Best GFLOPs: 1850.1554
[05:00:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i2_4_init in T.grid(8, 7, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + i1_3_init)
                            yy = T.axis.spatial(28, i2_3_init * 4 + i2_4_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 112)
                                    v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 112 // 4)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 8, 7, 1, 4, 1, 1, 1, 1, 4, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + i1_3)
                                yy = T.axis.spatial(28, i2_3 * 4 + i2_4)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                rc = T.axis.reduce(128, i4_0 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 28, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + ax1)
                            v2 = T.axis.spatial(28, ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 8, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:00:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_clip_8"] Trial #31: GFLOPs: 10.9998. Time: 4.7258 ms. Best GFLOPs: 1850.1554
[05:01:23] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_nn_conv2d_add_clip_8"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 288
Total latency (us): 172.218

[05:01:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #0: GFLOPs: 30.9409. Time: 0.1362 ms. Best GFLOPs: 30.9409
[05:01:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 256, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init in T.grid(4, 4, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3_init * 2 + i1_4_init)
                            i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7)
                            j = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_3_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(35):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 3840 // 120)
                                        v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 120 // 30)
                                        v3 = T.axis.spatial(30, ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 3840)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 4, 1, 4, 1, 1, 1, 2, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3 * 2 + i1_4)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7)
                                j = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_3)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 4):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 4, 4, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 4, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #2: GFLOPs: 152.1685. Time: 0.0277 ms. Best GFLOPs: 152.1685
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #3: GFLOPs: 134.4931. Time: 0.0313 ms. Best GFLOPs: 152.1685
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #4: GFLOPs: 47.7075. Time: 0.0883 ms. Best GFLOPs: 152.1685
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 256, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(49, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i2_4_init, i3_4_init in T.grid(4, 2, 2, 4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i1_3_init)
                            i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + i2_3_init * 2 + i2_4_init)
                            j = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(46):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 6720 // 840)
                                        v2 = T.axis.spatial(30, i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 840 // 30)
                                        v3 = T.axis.spatial(30, ((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 6720)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 8 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 24)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 4, 2, 1, 1, 3, 1, 1, 2, 4):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i1_3)
                                i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + i2_3 * 2 + i2_4)
                                j = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 4):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 2, 1, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 4])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 49, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 49, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 256, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init in T.grid(4, 7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 4 + i1_4_init)
                            i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 28 * 7 + i2_4_init)
                            j = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 6272 // 392)
                                        v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 392 // 28)
                                        v3 = T.axis.spatial(30, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(1, 0)
                                    v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 16)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 7, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 4 + i1_4)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 28 * 7 + i2_4)
                                j = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28)
                                di, dj = T.axis.remap("RR", [i4_0, i5_0])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 4 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 28 * 7 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 4, 1, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 28, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 224, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 224])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 256, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(392, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 98 * 64 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1536 // 24)
                                        v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 98 // 14 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 24 // 4)
                                        v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 98 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 1, 2, 1):
                            for i2_4_init, i3_4_init in T.grid(2, 2):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 98 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 98 // 14 * 4 + i2_3 * 2 + i2_4_init)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i3_4_init)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 2, 2):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 98 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 98 // 14 * 4 + i2_3 * 2 + i2_4)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 4, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 98 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 98 // 14 * 4 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 32, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 32])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l165)
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 256, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_3_init in T.serial(14):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                            i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i3_3_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(30):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 1920 // 60)
                                        v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 60 // 30)
                                        v3 = T.axis.spatial(30, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 14, 1, 1, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i3_3)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 14):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 16, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 1, 14, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 256, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(49, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(168):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) % 8192 // 256)
                                    v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) % 256 // 16)
                                    v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1 < 8192)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 147 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 147 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 147 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 1, 2, 1):
                            for i1_4_init in T.serial(16):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i1_4_init)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i2_3)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 16, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i1_4)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i2_3)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 1, 1, 16])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 2, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 49])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 49, 3])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l165)
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #10: GFLOPs: 583.3417. Time: 0.0072 ms. Best GFLOPs: 583.3417
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #11: GFLOPs: 151.7276. Time: 0.0278 ms. Best GFLOPs: 583.3417
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 256, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(128):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 28 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 4096 // 64)
                                    v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 64 // 16)
                                    v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 28 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 1, 2, 14):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 28 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 2 + i2_3)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i3_3)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 28 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 2 + i2_3)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 14):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 28 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 32, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 1, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 14, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 32])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 32, 2])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l165)
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #13: GFLOPs: 103.2845. Time: 0.0408 ms. Best GFLOPs: 583.3417
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #14: GFLOPs: 111.4826. Time: 0.0378 ms. Best GFLOPs: 583.3417
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #15: GFLOPs: 93.9920. Time: 0.0448 ms. Best GFLOPs: 583.3417
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 256, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init, i3_3_init, i1_4_init in T.grid(4, 2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3_init * 2 + i1_4_init)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7)
                                j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(48):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 10752 // 168)
                                            v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 168 // 28)
                                            v3 = T.axis.spatial(30, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                        v3 = T.axis.spatial(3, i5_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 192)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 4, 1, 2, 1, 1, 1, 2, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3 * 2 + i1_4)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7)
                                    j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_1, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 4, 4, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 7, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 56])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l158)
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #17: GFLOPs: 41.4108. Time: 0.1018 ms. Best GFLOPs: 583.3417
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #18: GFLOPs: 258.8522. Time: 0.0163 ms. Best GFLOPs: 583.3417
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #19: GFLOPs: 113.1153. Time: 0.0373 ms. Best GFLOPs: 583.3417
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 256, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(49, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(42):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 8192 // 256)
                                        v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 14 + ((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 256 // 16)
                                        v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 8192)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 147 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 147 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 147 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_4_init, i2_4_init in T.grid(16, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i1_4_init)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i2_4_init)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 1, 3, 1, 16, 2, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i1_4)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i2_4)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                di, dj = T.axis.remap("RR", [i4_1, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 1, 1, 16])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 2, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 49, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 49, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l161)
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 256, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(45):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 5760 // 180)
                                        v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 180 // 30)
                                        v3 = T.axis.spatial(30, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_3_init, i3_4_init in T.grid(2, 4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_3_init)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 4):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_3)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_1, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 4):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 4])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l161)
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #22: GFLOPs: 109.1694. Time: 0.0386 ms. Best GFLOPs: 583.3417
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #23: GFLOPs: 62.1461. Time: 0.0678 ms. Best GFLOPs: 583.3417
[05:01:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 256, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 6144 // 96)
                                        v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 96 // 16)
                                        v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 6144)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 336 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 336 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 336 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 576)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            for i1_3_init, i2_3_init, i1_4_init in T.grid(2, 4, 2):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_3_init * 2 + i1_4_init)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + i2_3_init)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 2, 4, 1, 3, 1, 1, 2, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_3 * 2 + i1_4)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + i2_3)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 8, 2, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 4, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 14, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 112, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 112, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l162)
[05:01:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #25: GFLOPs: 96.8418. Time: 0.0435 ms. Best GFLOPs: 583.3417
[05:01:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #26: GFLOPs: 318.7894. Time: 0.0132 ms. Best GFLOPs: 583.3417
[05:01:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #27: GFLOPs: 90.2661. Time: 0.0467 ms. Best GFLOPs: 583.3417
[05:01:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 256, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(784, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 196 * 64 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 1152 // 18)
                                        v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 196 // 28 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 18 // 3)
                                        v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 28 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 196 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 1, 1, 1):
                            for i1_4_init, i2_4_init in T.grid(2, 2):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 196 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 196 // 28 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_4_init)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 2, 2, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 196 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 196 // 28 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_4)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 196 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 196 // 28 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 32, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[28, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l167)
[05:01:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #29: GFLOPs: 464.3459. Time: 0.0091 ms. Best GFLOPs: 583.3417
[05:01:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #30: GFLOPs: 249.0988. Time: 0.0169 ms. Best GFLOPs: 583.3417
[05:01:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_clip_9"] Trial #31: GFLOPs: 22.7993. Time: 0.1849 ms. Best GFLOPs: 583.3417
[05:01:59] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #9: "fused_nn_conv2d_add_clip_9"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 320
Total latency (us): 179.443

[05:02:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #0: GFLOPs: 543.7770. Time: 0.1901 ms. Best GFLOPs: 543.7770
[05:02:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #1: GFLOPs: 837.6098. Time: 0.1234 ms. Best GFLOPs: 837.6098
[05:02:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #2: GFLOPs: 83.4386. Time: 1.2388 ms. Best GFLOPs: 837.6098
[05:02:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(448, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(4, 7, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 7 + i2_3_init)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(448, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1) // 392)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1) % 392 // 28)
                                    v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 < 784)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(448, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(256, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 512)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 7, 1, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 7 + i2_3)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                rc = T.axis.reduce(256, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 7, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 7 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 1, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 448])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 448, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:02:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #4: GFLOPs: 663.3892. Time: 0.1558 ms. Best GFLOPs: 837.6098
[05:02:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #5: GFLOPs: 926.1910. Time: 0.1116 ms. Best GFLOPs: 926.1910
[05:02:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(64, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i2_4_init in T.grid(2, 7, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 // 4 * 4 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 784)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 784 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 2)
                                    v1 = T.axis.spatial(256, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 256)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 2, 7, 1, 1, 1, 1, 1, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 // 4 * 4 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_3)
                                rc = T.axis.reduce(256, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 4, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 // 4 * 4 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 64, 2, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 4, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 56])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:02:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i2_3_init, i2_4_init, i3_4_init in T.grid(2, 2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 128 + i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 4 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(49):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) // 196)
                                    v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 196 // 7)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 32)
                                        v1 = T.axis.spatial(256, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 32)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 1, 2, 1, 4, 1, 1, 1, 1, 2, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 128 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 4 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i3_4)
                                rc = T.axis.reduce(256, i4_0 * 32 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 4, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 128 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 4 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 128, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 2, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 8, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 128])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 128, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:02:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #8: GFLOPs: 2.7023. Time: 38.2493 ms. Best GFLOPs: 926.1910
[05:02:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #9: GFLOPs: 276.1167. Time: 0.3743 ms. Best GFLOPs: 926.1910
[05:02:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #10: GFLOPs: 1452.9478. Time: 0.0711 ms. Best GFLOPs: 1452.9478
[05:02:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #11: GFLOPs: 1616.9306. Time: 0.0639 ms. Best GFLOPs: 1616.9306
[05:02:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init in T.grid(4, 2, 32):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 7 * 32 + i1_4_init)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i2_3_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 392)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 392 // 14)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 2)
                                    v1 = T.axis.spatial(256, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 512)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 4, 2, 2, 1, 1, 1, 32, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 7 * 32 + i1_4)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i2_3)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_3)
                                rc = T.axis.reduce(256, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 7 * 32 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 32])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 56])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:02:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i3_4_init in T.grid(2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 14 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 7 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 14 // 2)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 28)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 7 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 28 // 4)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 448)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(256, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 16, 1, 1, 1, 1, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 14 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 7 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 14 // 2)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_4)
                                rc = T.axis.reduce(256, i4_0 * 16 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 14 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 7 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 14 // 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 64, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 64, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[05:02:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init in T.serial(16):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 16 + i1_3_init)
                            yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 56)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 56 // 2)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 112)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 2)
                                    v1 = T.axis.spatial(256, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 16, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 16 + i1_3)
                                yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2)
                                rc = T.axis.reduce(256, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 16 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 8, 16, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 4, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 2, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 32])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:02:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #15: GFLOPs: 2253.4127. Time: 0.0459 ms. Best GFLOPs: 2253.4127
[05:02:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #16: GFLOPs: 322.4662. Time: 0.3205 ms. Best GFLOPs: 2253.4127
[05:02:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #17: GFLOPs: 237.7087. Time: 0.4348 ms. Best GFLOPs: 2253.4127
[05:02:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #18: GFLOPs: 1911.2416. Time: 0.0541 ms. Best GFLOPs: 2253.4127
[05:02:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #19: GFLOPs: 17.3979. Time: 5.9411 ms. Best GFLOPs: 2253.4127
[05:02:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(392, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init in T.grid(4, 32):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 49 * 32 + i1_4_init)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 4 + i2_3_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(392, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1) // 196)
                                    v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1) % 196 // 7)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 7 + (ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(392, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1) // 4)
                                    v1 = T.axis.spatial(256, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1 < 1024)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 4, 1, 2, 1, 1, 1, 32, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 49 * 32 + i1_4)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 4 + i2_3)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(256, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 4, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 49 * 32 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 4 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 32])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 392])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 392])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[05:02:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #21: GFLOPs: 703.3436. Time: 0.1470 ms. Best GFLOPs: 2253.4127
[05:02:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #22: GFLOPs: 20.0678. Time: 5.1507 ms. Best GFLOPs: 2253.4127
[05:02:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(256, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init, i3_4_init in T.grid(2, 7, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i2_4_init)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(256, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 1024 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 784)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 1024 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 784 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 1024 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 6272)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(256, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(256, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 7, 14):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i2_4)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i3_4)
                                rc = T.axis.reduce(256, i4_0 * 8 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 64, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 4, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 14])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 256, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 256, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[05:02:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init in T.grid(2, 32):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 7 * 32 + i1_4_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 98)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 98 // 14)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 392)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(256, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1024)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 32, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 7 * 32 + i1_4)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_3)
                                rc = T.axis.reduce(256, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 7 * 32 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 32])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 56, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[05:02:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(448, thread="threadIdx.x"):
                    for i3_3_init, i2_4_init in T.grid(14, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(448, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 784)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 784 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1568)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(448, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(256, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 512)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 14, 1, 1, 1, 1, 1, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i3_3)
                                rc = T.axis.reduce(256, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 1, 14, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 448, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 448, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[05:02:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #26: GFLOPs: 1960.8355. Time: 0.0527 ms. Best GFLOPs: 2253.4127
[05:02:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(2, 7, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 7 + i2_3_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(49):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 392)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 392 // 14)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(128):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(256, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 2, 7, 1, 1, 1, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 7 + i2_3)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                rc = T.axis.reduce(256, i4_0 * 16 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 7, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 7 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 4, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 4, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 16, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 32])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:02:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i2_4_init, i3_4_init in T.grid(8, 2, 2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3_init)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 112)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 112 // 4)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(37):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(256, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 4096)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 8, 2, 2, 16, 1, 1, 1, 1, 2, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(256, i4_0 * 16 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 4, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 8, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 56, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[05:02:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #29: GFLOPs: 3791.6946. Time: 0.0273 ms. Best GFLOPs: 3791.6946
[05:02:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #30: GFLOPs: 846.1179. Time: 0.1222 ms. Best GFLOPs: 3791.6946
[05:02:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_clip_10"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_4_init, i3_4_init in T.grid(2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused // 2)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 112)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 112 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(256, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 2, 1, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused // 2)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4)
                                rc = T.axis.reduce(256, i4_0 * 16 + i4_1 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused // 2 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 16, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 4, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 2, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[05:02:40] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_nn_conv2d_add_clip_10"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 352
Total latency (us): 206.704

[05:02:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #0: GFLOPs: 33.7568. Time: 0.0312 ms. Best GFLOPs: 33.7568
[05:02:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #1: GFLOPs: 101.2207. Time: 0.0104 ms. Best GFLOPs: 101.2207
[05:02:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #2: GFLOPs: 44.6668. Time: 0.0236 ms. Best GFLOPs: 101.2207
[05:02:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #3: GFLOPs: 30.9716. Time: 0.0340 ms. Best GFLOPs: 101.2207
[05:02:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 256, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(290):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 9280 // 145)
                                    v2 = T.axis.spatial(29, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 145 // 5)
                                    v3 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 5)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 28 and 0 <= v3 and v3 < 28, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i2_3_init, i3_4_init in T.grid(14, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                i = T.axis.spatial(14, i2_3_init)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 14, 1, 1, 3, 1, 1, 1, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                i = T.axis.spatial(14, i2_3)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_1, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 14, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(14, ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 32, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 14, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 32])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 32])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l157)
[05:02:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #5: GFLOPs: 16.9002. Time: 0.0623 ms. Best GFLOPs: 101.2207
[05:02:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #6: GFLOPs: 27.7633. Time: 0.0380 ms. Best GFLOPs: 101.2207
[05:02:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #7: GFLOPs: 42.5370. Time: 0.0248 ms. Best GFLOPs: 101.2207
[05:02:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #8: GFLOPs: 11.2217. Time: 0.0939 ms. Best GFLOPs: 101.2207
[05:02:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 256, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init in T.grid(4, 7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3_init)
                            i = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + i2_3_init)
                            j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(108):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 10368 // 81)
                                        v2 = T.axis.spatial(29, i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 81 // 3)
                                        v3 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 28 and 0 <= v3 and v3 < 28, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 4, 7, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3)
                                i = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + i2_3)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 32, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[05:02:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 256, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i2_3_init, i1_4_init, i2_4_init in T.grid(2, 2, 7):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                                i = T.axis.spatial(14, i2_3_init * 7 + i2_4_init)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 3712 // 29)
                                            v2 = T.axis.spatial(29, ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 29)
                                            v3 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i5_0 + 0)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3712)
                                            T.reads(placeholder[v0, v1, v2, v3])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 28 and 0 <= v3 and v3 < 28, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 3, 1, 1, 2, 7, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                    i = T.axis.spatial(14, i2_3 * 7 + i2_4)
                                    j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 14, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(14, ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 64, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 64, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 64, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[05:02:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #11: GFLOPs: 8.6537. Time: 0.1218 ms. Best GFLOPs: 101.2207
[05:02:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #12: GFLOPs: 28.3116. Time: 0.0372 ms. Best GFLOPs: 101.2207
[05:02:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #13: GFLOPs: 5.9185. Time: 0.1780 ms. Best GFLOPs: 101.2207
[05:02:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #14: GFLOPs: 28.5452. Time: 0.0369 ms. Best GFLOPs: 101.2207
[05:02:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #15: GFLOPs: 107.2557. Time: 0.0098 ms. Best GFLOPs: 107.2557
[05:02:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #16: GFLOPs: 47.8027. Time: 0.0220 ms. Best GFLOPs: 107.2557
[05:02:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #17: GFLOPs: 31.0253. Time: 0.0340 ms. Best GFLOPs: 107.2557
[05:02:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 256, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i3_3_init, i1_4_init in T.grid(7, 4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_4_init)
                                i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2)
                                j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(68):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 8640 // 135)
                                            v2 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 135 // 27)
                                            v3 = T.axis.spatial(29, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 27)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 8640)
                                            T.reads(placeholder[v0, v1, v2, v3])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 28 and 0 <= v3 and v3 < 28, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                        v3 = T.axis.spatial(3, i5_0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 7, 3, 1, 1, 4, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_4)
                                    i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2)
                                    j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 8, 1, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 2, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 32])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l158)
[05:02:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 256, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_4_init, i3_4_init in T.grid(7, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                            i = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + i2_4_init)
                            j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(34):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 4320 // 135)
                                        v2 = T.axis.spatial(29, i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 135 // 5)
                                        v3 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 5)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 4320)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 28 and 0 <= v3 and v3 < 28, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 7, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                i = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + i2_4)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 7, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 32, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[05:02:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #20: GFLOPs: 31.2460. Time: 0.0337 ms. Best GFLOPs: 107.2557
[05:02:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #21: GFLOPs: 10.9851. Time: 0.0959 ms. Best GFLOPs: 107.2557
[05:02:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #22: GFLOPs: 44.2720. Time: 0.0238 ms. Best GFLOPs: 107.2557
[05:02:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #23: GFLOPs: 16.2101. Time: 0.0650 ms. Best GFLOPs: 107.2557
[05:02:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #24: GFLOPs: 16.8469. Time: 0.0625 ms. Best GFLOPs: 107.2557
[05:02:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #25: GFLOPs: 96.9623. Time: 0.0109 ms. Best GFLOPs: 107.2557
[05:02:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #26: GFLOPs: 40.1786. Time: 0.0262 ms. Best GFLOPs: 107.2557
[05:02:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 256, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init in T.serial(4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3_init)
                                i = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(29):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 3712 // 29)
                                            v2 = T.axis.spatial(29, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 29)
                                            v3 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i5_0 + 0)
                                            T.reads(placeholder[v0, v1, v2, v3])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 28 and 0 <= v3 and v3 < 28, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 4, 1, 1, 3, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3)
                                    i = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused)
                                    j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 32, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 14, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[05:02:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 256, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_3_init in T.serial(4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_3_init)
                            i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2)
                            j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(17):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 5568 // 87)
                                        v2 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 87 // 29)
                                        v3 = T.axis.spatial(29, ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 29)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 5568)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 28 and 0 <= v3 and v3 < 28, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 192)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_3)
                                i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2)
                                j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 8, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 112, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 112, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[05:02:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #29: GFLOPs: 7.3787. Time: 0.1428 ms. Best GFLOPs: 107.2557
[05:02:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], compute: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 256, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(37):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 9280 // 145)
                                        v2 = T.axis.spatial(29, ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 145 // 5)
                                        v3 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 5)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 9280)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 28 and 0 <= v3 and v3 < 28, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i2_4_init, i3_4_init in T.grid(14, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused)
                                i = T.axis.spatial(14, i2_4_init)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 14, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused)
                                i = T.axis.spatial(14, i2_4)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_1, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 14, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(14, ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 64, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 14])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 64, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 64, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l161)
[05:02:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_clip_11"] Trial #31: GFLOPs: 24.8843. Time: 0.0423 ms. Best GFLOPs: 107.2557
[05:03:05] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #11: "fused_nn_conv2d_add_clip_11"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |       107.2557 |       9.8242 |                9.8242 |     32 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 384
Total latency (us): 216.528

[05:03:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #0: GFLOPs: 1008.9442. Time: 0.0512 ms. Best GFLOPs: 1008.9442
[05:03:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_3_init, i2_4_init in T.grid(7, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                            yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i2_4_init)
                            xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(49):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 98)
                                        v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 98 // 7)
                                        v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 32)
                                    v1 = T.axis.spatial(256, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 32)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 7, 32, 1, 1, 1, 1, 7, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i2_4)
                                xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i3_3)
                                rc = T.axis.reduce(256, i4_0 * 32 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 7, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                            v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 2, 16, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 1, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 32])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:03:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i2_4_init in T.grid(2, 2, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_3_init)
                            yy = T.axis.spatial(14, i2_4_init)
                            xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 196)
                                        v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 196 // 14)
                                        v3 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 128 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(256, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1024)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 14, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_3)
                                yy = T.axis.spatial(14, i2_4)
                                xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                rc = T.axis.reduce(256, i4_0 * 8 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 14, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax1)
                            v2 = T.axis.spatial(14, ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 8, 8, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 14])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 4, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 56, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[05:03:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #3: GFLOPs: 61.5569. Time: 0.8396 ms. Best GFLOPs: 1008.9442
[05:03:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #4: GFLOPs: 11.6887. Time: 4.4215 ms. Best GFLOPs: 1008.9442
[05:03:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #5: GFLOPs: 701.6894. Time: 0.0737 ms. Best GFLOPs: 1008.9442
[05:03:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #6: GFLOPs: 1437.9136. Time: 0.0359 ms. Best GFLOPs: 1437.9136
[05:03:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #7: GFLOPs: 111.1724. Time: 0.4649 ms. Best GFLOPs: 1437.9136
[05:03:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #8: GFLOPs: 1429.5399. Time: 0.0362 ms. Best GFLOPs: 1437.9136
[05:03:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #9: GFLOPs: 705.4473. Time: 0.0733 ms. Best GFLOPs: 1437.9136
[05:03:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #10: GFLOPs: 324.8523. Time: 0.1591 ms. Best GFLOPs: 1437.9136
[05:03:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #11: GFLOPs: 186.2549. Time: 0.2775 ms. Best GFLOPs: 1437.9136
[05:03:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #12: GFLOPs: 269.0502. Time: 0.1921 ms. Best GFLOPs: 1437.9136
[05:03:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_3_init, i3_4_init in T.grid(2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                            yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            xx = T.axis.spatial(14, i3_3_init * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(98):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 196)
                                    v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 196 // 14)
                                    v3 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(256, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 2, 8, 1, 1, 1, 1, 1, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                xx = T.axis.spatial(14, i3_3 * 7 + i3_4)
                                rc = T.axis.reduce(256, i4_0 * 16 + i4_1 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                            v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax2)
                            v3 = T.axis.spatial(14, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 4, 16, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[05:03:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(196, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init in T.grid(4, 64, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused // 98 * 256 + i1_3_init * 64 + i1_4_init)
                            yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 98 // 14 * 2 + i2_4_init)
                            xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(196, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) // 196)
                                    v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) % 196 // 14)
                                    v3 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(21):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(196, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, (ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(256, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 8192)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 4, 1, 1, 4, 1, 1, 1, 64, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused // 98 * 256 + i1_3 * 64 + i1_4)
                                yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 98 // 14 * 2 + i2_4)
                                xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14)
                                rc = T.axis.reduce(256, i4_0 * 16 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 256, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused // 98 * 256 + ax1)
                            v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 98 // 14 * 2 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 2, 4, 64])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 4, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 196])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 196, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:03:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #15: GFLOPs: 594.0577. Time: 0.0870 ms. Best GFLOPs: 1437.9136
[05:03:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(49, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i3_4_init in T.grid(16, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                            xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 98)
                                        v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 98 // 14)
                                        v3 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(256, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1 < 512)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 16, 1, 1, 16, 1, 1, 1, 2, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                rc = T.axis.reduce(256, i4_0 * 16 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 1, 16, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 49, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 49])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:03:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #17: GFLOPs: 546.6955. Time: 0.0945 ms. Best GFLOPs: 1437.9136
[05:03:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #18: GFLOPs: 548.3456. Time: 0.0942 ms. Best GFLOPs: 1437.9136
[05:03:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #19: GFLOPs: 2087.3584. Time: 0.0248 ms. Best GFLOPs: 2087.3584
[05:03:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #20: GFLOPs: 3298.4290. Time: 0.0157 ms. Best GFLOPs: 3298.4290
[05:03:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #21: GFLOPs: 665.4576. Time: 0.0777 ms. Best GFLOPs: 3298.4290
[05:03:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #22: GFLOPs: 2111.3354. Time: 0.0245 ms. Best GFLOPs: 3298.4290
[05:03:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #23: GFLOPs: 932.7858. Time: 0.0554 ms. Best GFLOPs: 3298.4290
[05:03:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #24: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init in T.grid(2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 7 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i2_4_init)
                            xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(256, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0)
                                        v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2)
                                        v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 14)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(256, i4_0)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 7 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i2_4)
                                xx, rc = T.axis.remap("SR", [i0_0_i1_0_i2_0_i3_0_fused, i4_0])
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 7 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 32, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[256, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[05:03:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #25: GFLOPs: 230.4906. Time: 0.2242 ms. Best GFLOPs: 3298.4290
[05:03:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #26: GFLOPs: 1252.5092. Time: 0.0413 ms. Best GFLOPs: 3298.4290
[05:03:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #27: GFLOPs: 743.6271. Time: 0.0695 ms. Best GFLOPs: 3298.4290
[05:03:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #28: GFLOPs: 81.7318. Time: 0.6323 ms. Best GFLOPs: 3298.4290
[05:03:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(32, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init in T.grid(7, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                            yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i2_3_init)
                            xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 196)
                                        v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 196 // 14)
                                        v3 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1568)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(256, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 7, 7, 8, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i2_3)
                                xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3)
                                rc = T.axis.reduce(256, i4_0 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 7, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                            v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 16, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 2, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 64])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:03:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #30: GFLOPs: 753.9453. Time: 0.0685 ms. Best GFLOPs: 3298.4290
[05:03:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_clip_12"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(32, 2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused // 14 * 128 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_3_init)
                            xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 196)
                                    v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 196 // 14)
                                    v3 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(19):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(256, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 4096)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 32, 2, 1, 8, 1, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused // 14 * 128 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_3)
                                xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14)
                                rc = T.axis.reduce(256, i4_0 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 128, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused // 14 * 128 + ax1)
                            v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 2 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 32, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:03:32] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #12: "fused_nn_conv2d_add_clip_12"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |       107.2557 |       9.8242 |                9.8242 |     32 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |      3298.4290 |      15.6685 |               15.6685 |     32 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 416
Total latency (us): 232.196

[05:03:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_4_init in T.serial(16):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 16 + i1_4_init)
                            i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                            j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(37):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 4096 // 16)
                                    v2 = T.axis.spatial(16, i4_0 + i0_0_i1_0_i2_0_i3_0_fused % 14 + 0)
                                    v3 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 < 4096)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 768)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 1, 1, 1, 1, 16, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 16 + i1_4)
                                i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                                j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 16 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 8, 1, 16])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 112])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 112, 4])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[05:03:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #1: GFLOPs: 173.4604. Time: 0.0121 ms. Best GFLOPs: 173.4604
[05:03:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 2, 7, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                            i = T.axis.spatial(14, i2_3_init * 7 + i2_4_init)
                            j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(224):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 7168 // 56)
                                    v2 = T.axis.spatial(16, i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 56 // 4)
                                    v3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 3, 1, 2, 7, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                i = T.axis.spatial(14, i2_3 * 7 + i2_4)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 14, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(14, ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 32, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 32])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 32, 3])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[05:03:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i3_4_init in T.serial(2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                                j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 5376 // 42)
                                            v2 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 14 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 42 // 14)
                                            v3 = T.axis.spatial(16, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                        v3 = T.axis.spatial(3, i5_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 < 384)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 2):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                                    j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 8, 16, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 112, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 112])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l158)
[05:03:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #4: GFLOPs: 63.5684. Time: 0.0332 ms. Best GFLOPs: 173.4604
[05:03:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(196, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(42):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(196, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) // 256)
                                    v2 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) % 256 // 16)
                                    v3 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1 < 8192)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(196, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            for i2_4_init in T.serial(2):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 49)
                                    i = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 2 + i2_4_init)
                                    j = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 49)
                                    i = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 2 + i2_4)
                                    j = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 49 + ax1)
                            v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 2 + ax2)
                            v3 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 8, 4, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 196])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 196, 2])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l160)
[05:03:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_4_init in T.grid(8, 2, 7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3_init)
                            i = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + i2_3_init)
                            j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 6272 // 196)
                                        v2 = T.axis.spatial(16, i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 196 // 14)
                                        v3 = T.axis.spatial(16, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2))
                                        v1 = T.axis.spatial(1, 0)
                                        v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 32)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 8, 2, 1, 1, 1, 1, 1, 1, 7):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3)
                                i = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + i2_3)
                                j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_0])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + ax1)
                            v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 4, 8, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i1_3_init in T.serial(4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_3_init)
                            i = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7)
                            j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(36):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 8064 // 126)
                                    v2 = T.axis.spatial(16, i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 126 // 9)
                                    v3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 9)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 192)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_3)
                                i = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + ax1)
                            v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 16, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 224])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 224, 2])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(162):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 4 * 128 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 10368 // 81)
                                        v2 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 7 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 81 // 9)
                                        v3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 9)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 4 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            for i1_3_init, i2_4_init, i3_4_init in T.grid(4, 7, 7):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 4 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3_init)
                                    i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 7 + i2_4_init)
                                    j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i3_4_init)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 4, 1, 1, 3, 1, 1, 1, 7, 7):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 4 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3)
                                    i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 7 + i2_4)
                                    j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 4 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 7 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 32, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l162)
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init in T.grid(7, 2, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_4_init)
                            i = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + i2_3_init)
                            j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 7168 // 224)
                                    v2 = T.axis.spatial(16, i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 224 // 16)
                                    v3 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 7, 2, 1, 3, 1, 2, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_4)
                                i = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + i2_3)
                                j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax1)
                            v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 16, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 112])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 112, 2])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(48):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 9216 // 36)
                                        v2 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 7 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 36 // 4)
                                        v3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            for i1_3_init, i2_3_init in T.grid(2, 7):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3_init)
                                    i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 7 + i2_3_init)
                                    j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 2, 7, 1, 3, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3)
                                    i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 7 + i2_3)
                                    j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 7 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 32, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 64, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 64, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l162)
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(36):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 9216 // 36)
                                        v2 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 7 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 36 // 4)
                                        v3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(36):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 2, 7, 1):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3)
                                i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 7 + i2_3)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3)
                                    i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 7 + i2_3)
                                    j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 7 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 32, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 64, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 64])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l165)
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #12: GFLOPs: 214.8867. Time: 0.0098 ms. Best GFLOPs: 214.8867
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #13: GFLOPs: 106.0501. Time: 0.0199 ms. Best GFLOPs: 214.8867
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #14: GFLOPs: 63.5002. Time: 0.0332 ms. Best GFLOPs: 214.8867
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #15: GFLOPs: 60.6634. Time: 0.0347 ms. Best GFLOPs: 214.8867
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 49 * 256 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 4096 // 16)
                                        v2 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 16 // 4)
                                        v3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(36):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 49 * 256 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_3_init, i2_3_init, i1_4_init in T.grid(2, 2, 4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 49 * 256 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + i1_3_init * 4 + i1_4_init)
                                i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 2 + i2_3_init)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 2, 2, 1, 1, 1, 1, 4, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 49 * 256 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + i1_3 * 4 + i1_4)
                                i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 2 + i2_3)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                di, dj = T.axis.remap("RR", [i4_1, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 49 * 256 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 2 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 32, 2, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 64, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 64])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l159)
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #17: GFLOPs: 50.7710. Time: 0.0415 ms. Best GFLOPs: 214.8867
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #18: GFLOPs: 46.0144. Time: 0.0458 ms. Best GFLOPs: 214.8867
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(98, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(24):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(98, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) % 2304 // 144)
                                    v2 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) % 144 // 9)
                                    v3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) % 9)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 < 2304)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(98, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 144)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_4_init in T.serial(4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i1_4_init)
                                i = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused // 7)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i1_4)
                                i = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused // 7)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                di, dj = T.axis.remap("RR", [i4_1, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + ax1)
                            v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused // 7 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 4, 1, 1, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 98])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 98, 2])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l159)
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(2, 8):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused * 16 + i1_3_init * 8 + i1_4_init)
                            i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused // 14 * 2 + i0_1_i1_1_i2_1_i3_1_fused)
                            j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(24):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 3072 // 6)
                                        v2 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 14 * 2 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 6 // 3)
                                        v3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 14 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 2, 1, 1, 1, 3, 1, 8, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused * 16 + i1_3 * 8 + i1_4)
                                i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused // 14 * 2 + i0_1_i1_1_i2_1_i3_1_fused)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                                di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused * 16 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused // 14 * 2 + i0_1_i1_1_i2_1_i3_1_fused + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 2, 8])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #21: GFLOPs: 40.3224. Time: 0.0523 ms. Best GFLOPs: 214.8867
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #22: GFLOPs: 78.8904. Time: 0.0267 ms. Best GFLOPs: 214.8867
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #23: GFLOPs: 49.8000. Time: 0.0423 ms. Best GFLOPs: 214.8867
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #24: GFLOPs: 125.1402. Time: 0.0168 ms. Best GFLOPs: 214.8867
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init in T.grid(2, 7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3_init)
                            i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 7 + i2_3_init)
                            j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 3584 // 14)
                                        v2 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 7 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 14 // 2)
                                        v3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(1, 0)
                                        v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 2, 7, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3)
                                i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 7 + i2_3)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                di, dj = T.axis.remap("RR", [i4_0, i5_0])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 7 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 32, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 64, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 64, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(19):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 3072 // 48)
                                        v2 = T.axis.spatial(16, ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 48 // 3)
                                        v3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 14 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 3072)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 576)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_4_init, i2_4_init in T.grid(2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_4_init)
                                i = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_4_init)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 1, 3, 1, 2, 2, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_4)
                                i = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_4)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                                di, dj = T.axis.remap("RR", [i4_1, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax1)
                            v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 4, 8, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 56])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l159)
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i2_3_init, i1_4_init in T.grid(7, 4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_4_init)
                                i = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + i2_3_init)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 4096 // 16)
                                            v2 = T.axis.spatial(16, ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                            v3 = T.axis.spatial(16, i5_0 + i0_0_i1_0_i2_0_i3_0_fused % 14 + 0)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 7, 1, 3, 1, 1, 4, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_4)
                                    i = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + i2_3)
                                    j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 14 * 256 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 64, 1, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 64, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 64, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #28: GFLOPs: 39.0363. Time: 0.0540 ms. Best GFLOPs: 214.8867
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 4 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                        i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7)
                        j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                        T.reads()
                        T.writes(DepthwiseConv2d_local[b, c, i, j])
                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(36):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 4 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 4032 // 63)
                                    v2 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 7 + i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 63 // 9)
                                    v3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 9)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 4 * 64 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 192)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 4 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 4 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 4, 16, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 7, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 112])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 112, 3])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #30: GFLOPs: 59.8392. Time: 0.0352 ms. Best GFLOPs: 214.8867
[05:03:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_clip_13"] Trial #31: GFLOPs: 19.7864. Time: 0.1065 ms. Best GFLOPs: 214.8867
[05:04:16] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #13: "fused_nn_conv2d_add_clip_13"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |       107.2557 |       9.8242 |                9.8242 |     32 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |      3298.4290 |      15.6685 |               15.6685 |     32 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |       214.8867 |       9.8070 |               49.0350 |     32 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 448
Total latency (us): 281.231

[05:04:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init in T.grid(2, 8):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + i0_2_i1_2_i2_2_i3_2_fused * 8 + i1_4_init)
                            yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i2_3_init)
                            xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 28)
                                    v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 28 // 2)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 2)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(512, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 1, 16, 1, 1, 1, 8, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + i0_2_i1_2_i2_2_i3_2_fused * 8 + i1_4)
                                yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i2_3)
                                xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2)
                                rc = T.axis.reduce(512, i4_0 * 16 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + i0_2_i1_2_i2_2_i3_2_fused * 8 + ax1)
                            v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 32, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:04:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #1: GFLOPs: 35.6581. Time: 2.8903 ms. Best GFLOPs: 35.6581
[05:04:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(196, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(16, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 4 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 32 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 49 // 7)
                            xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(196, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) // 196)
                                    v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) % 196 // 14)
                                    v3 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(42):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(196, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(512, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1 < 8192)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 16, 1, 1, 2, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 4 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 32 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 49 // 7)
                                xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(512, i4_0 * 16 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 4 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 32 + ax1)
                            v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 + ax2)
                            v3 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 4, 16, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 8, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 196])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 196])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #3: GFLOPs: 114.5037. Time: 0.9001 ms. Best GFLOPs: 114.5037
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(49, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(25):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 98)
                                        v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 98 // 7)
                                        v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1568)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(512, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 1, 1, 8, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7)
                                rc = T.axis.reduce(512, i4_0 * 16 + i4_1 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + ax1)
                            v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 16, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #5: GFLOPs: 628.7952. Time: 0.1639 ms. Best GFLOPs: 628.7952
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #6: GFLOPs: 668.9516. Time: 0.1541 ms. Best GFLOPs: 668.9516
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #7: GFLOPs: 156.2495. Time: 0.6596 ms. Best GFLOPs: 668.9516
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #8: GFLOPs: 1068.1454. Time: 0.0965 ms. Best GFLOPs: 1068.1454
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #9: GFLOPs: 350.1839. Time: 0.2943 ms. Best GFLOPs: 1068.1454
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(32, 2, 7, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused // 7 * 64 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + i2_4_init)
                            xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 196)
                                    v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 196 // 14)
                                    v3 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(147):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(512, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 8192)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 32, 1, 1, 2, 1, 1, 1, 2, 7, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused // 7 * 64 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + i2_4)
                                xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                rc = T.axis.reduce(512, i4_0 * 16 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 64, 7, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused // 7 * 64 + ax1)
                            v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 32, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 8, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 56])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #11: GFLOPs: 1927.6609. Time: 0.0535 ms. Best GFLOPs: 1927.6609
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #12: GFLOPs: 11.0251. Time: 9.3479 ms. Best GFLOPs: 1927.6609
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #13: GFLOPs: 768.1193. Time: 0.1342 ms. Best GFLOPs: 1927.6609
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #14: GFLOPs: 15.6454. Time: 6.5873 ms. Best GFLOPs: 1927.6609
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #15: GFLOPs: 218.7296. Time: 0.4712 ms. Best GFLOPs: 1927.6609
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init, i3_4_init in T.grid(4, 2, 4, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 16 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_3_init)
                            xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 98)
                                        v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 98 // 7)
                                        v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(37):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(512, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 2048)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 2, 1, 8, 1, 1, 1, 4, 1, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 16 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_3)
                                xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i3_4)
                                rc = T.axis.reduce(512, i4_0 * 16 + i4_1 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 2, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 16 + ax1)
                            v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 8, 4, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 56])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #17: GFLOPs: 1922.2228. Time: 0.0536 ms. Best GFLOPs: 1927.6609
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #18: GFLOPs: 1228.2619. Time: 0.0839 ms. Best GFLOPs: 1927.6609
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #19: GFLOPs: 91.4152. Time: 1.1274 ms. Best GFLOPs: 1927.6609
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #20: GFLOPs: 707.1043. Time: 0.1458 ms. Best GFLOPs: 1927.6609
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #21: GFLOPs: 354.3760. Time: 0.2908 ms. Best GFLOPs: 1927.6609
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(112, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init in T.grid(2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3_init)
                            yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7)
                            xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 196)
                                        v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 196 // 14)
                                        v3 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1568)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, (ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 4096)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3)
                                yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7)
                                xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                rc = T.axis.reduce(512, i4_0 * 8 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax1)
                            v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 16, 16, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 4, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 224, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 224, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #23: GFLOPs: 1029.4664. Time: 0.1001 ms. Best GFLOPs: 1927.6609
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #24: GFLOPs: 17.6446. Time: 5.8410 ms. Best GFLOPs: 1927.6609
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(2, 7, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i2_3_init)
                            xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 98)
                                        v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 98 // 14)
                                        v3 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 784)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 < 1024)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 7, 1, 4, 1, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i2_3)
                                xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(512, i4_0 * 8 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 7, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + ax2)
                            v3 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 16, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 112])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #26: GFLOPs: 912.7874. Time: 0.1129 ms. Best GFLOPs: 1927.6609
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #27: GFLOPs: 260.8499. Time: 0.3951 ms. Best GFLOPs: 1927.6609
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(196, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init in T.grid(4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 49 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_4_init)
                            yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 49 // 7 * 2 + i2_4_init)
                            xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 98)
                                        v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 98 // 7)
                                        v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 784)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(128):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 4, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 49 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_4)
                                yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 49 // 7 * 2 + i2_4)
                                xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7)
                                rc = T.axis.reduce(512, i4_0 * 8 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 49 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 49 // 7 * 2 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 32, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 32])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #29: GFLOPs: 974.6984. Time: 0.1057 ms. Best GFLOPs: 1927.6609
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #30: GFLOPs: 2288.2636. Time: 0.0450 ms. Best GFLOPs: 2288.2636
[05:04:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_clip_14"] Trial #31: GFLOPs: 81.0718. Time: 1.2712 ms. Best GFLOPs: 2288.2636
[05:04:49] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_nn_conv2d_add_clip_14"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |       107.2557 |       9.8242 |                9.8242 |     32 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |      3298.4290 |      15.6685 |               15.6685 |     32 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |       214.8867 |       9.8070 |               49.0350 |     32 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |      2288.2636 |      45.0392 |              225.1959 |     32 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 480
Total latency (us): 506.427

[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #0: GFLOPs: 13.4923. Time: 0.0390 ms. Best GFLOPs: 13.4923
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #1: GFLOPs: 65.9724. Time: 0.0080 ms. Best GFLOPs: 65.9724
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #2: GFLOPs: 52.6808. Time: 0.0100 ms. Best GFLOPs: 65.9724
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #3: GFLOPs: 82.8494. Time: 0.0064 ms. Best GFLOPs: 82.8494
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #4: GFLOPs: 12.0302. Time: 0.0438 ms. Best GFLOPs: 82.8494
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #5: GFLOPs: 94.8017. Time: 0.0056 ms. Best GFLOPs: 94.8017
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #6: GFLOPs: 29.1313. Time: 0.0181 ms. Best GFLOPs: 94.8017
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(2, 7, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3_init * 2 + i1_4_init)
                            i = T.axis.spatial(7, i2_3_init)
                            j = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(312):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 9984 // 39)
                                    v2 = T.axis.spatial(15, i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 39 // 3)
                                    v3 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 14 and 0 <= v3 and v3 < 14, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 2, 7, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3 * 2 + i1_4)
                                i = T.axis.spatial(7, i2_3)
                                j = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2 = T.axis.spatial(7, ax2)
                            v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 32, 2, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 32])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 32, 2])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #8: GFLOPs: 22.1127. Time: 0.0238 ms. Best GFLOPs: 94.8017
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(129):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 225)
                                    v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 225 // 15)
                                    v3 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 15)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 7200)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 14 and 0 <= v3 and v3 < 14, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 288)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_4_init in T.serial(4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + i1_4_init)
                                i = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                j = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 1, 3, 1, 4, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + i1_4)
                                i = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                j, di, dj = T.axis.remap("SRR", [i0_1_i1_1_i2_1_i3_1_fused, i4_1, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + ax1)
                            v2 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 8, 1, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 56])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 56])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l157)
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init in T.serial(4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3_init)
                            i = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused)
                            j = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(52):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 3328 // 13)
                                    v2 = T.axis.spatial(15, i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    v3 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i5_0 + 0)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 14 and 0 <= v3 and v3 < 14, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(1, 0)
                                    v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3)
                                i = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused)
                                j = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                di, dj = T.axis.remap("RR", [i4_0, i5_0])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused + ax2)
                            v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 64, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 64])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 64])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l155)
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #11: GFLOPs: 29.5716. Time: 0.0178 ms. Best GFLOPs: 94.8017
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #12: GFLOPs: 36.6061. Time: 0.0144 ms. Best GFLOPs: 94.8017
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init, i1_4_init in T.grid(2, 4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3_init * 4 + i1_4_init)
                                i = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                j = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(35):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(512, ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 7680 // 15)
                                            v2 = T.axis.spatial(15, ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                            v3 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_fused * 2 + i5_0 + 0)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 7680)
                                            T.reads(placeholder[v0, v1, v2, v3])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 14 and 0 <= v3 and v3 < 14, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(512, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1536)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 2, 1, 1, 1, 1, 1, 4, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3 * 4 + i1_4)
                                    i = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    j, di, dj = T.axis.remap("SRR", [i0_0_i1_0_i2_0_i3_0_fused, i4_1, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + ax1)
                            v2 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 16, 2, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 112, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 112, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i3_4_init in T.serial(7):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                i = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                j = T.axis.spatial(7, i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(19):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 6240 // 195)
                                            v2 = T.axis.spatial(15, ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 195 // 13)
                                            v3 = T.axis.spatial(15, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 6240)
                                            T.reads(placeholder[v0, v1, v2, v3])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 14 and 0 <= v3 and v3 < 14, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    i = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    j, di, dj = T.axis.remap("SRR", [i3_4, i4_1, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax1)
                            v2 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 2, 16, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 112, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 112, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #15: GFLOPs: 12.7094. Time: 0.0415 ms. Best GFLOPs: 94.8017
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #16: GFLOPs: 31.4760. Time: 0.0167 ms. Best GFLOPs: 94.8017
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #17: GFLOPs: 7.7020. Time: 0.0684 ms. Best GFLOPs: 94.8017
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #18: GFLOPs: 18.9871. Time: 0.0277 ms. Best GFLOPs: 94.8017
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], compute: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 512, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(45):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 5760 // 45)
                                        v2 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 45 // 15)
                                        v3 = T.axis.spatial(15, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 14 and 0 <= v3 and v3 < 14, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 1, 1, 7):
                            for i1_4_init in T.serial(2):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                                    i = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    j = T.axis.spatial(7, i3_3)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 2, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                    i = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    j, di, dj = T.axis.remap("SRR", [i3_3, i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 32, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l167)
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #20: GFLOPs: 7.5116. Time: 0.0701 ms. Best GFLOPs: 94.8017
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #21: GFLOPs: 13.0086. Time: 0.0405 ms. Best GFLOPs: 94.8017
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #22: GFLOPs: 10.7540. Time: 0.0490 ms. Best GFLOPs: 94.8017
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #23: GFLOPs: 15.8765. Time: 0.0332 ms. Best GFLOPs: 94.8017
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #24: GFLOPs: 14.5803. Time: 0.0361 ms. Best GFLOPs: 94.8017
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #25: GFLOPs: 31.9486. Time: 0.0165 ms. Best GFLOPs: 94.8017
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #26: GFLOPs: 75.5904. Time: 0.0070 ms. Best GFLOPs: 94.8017
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #27: GFLOPs: 29.9128. Time: 0.0176 ms. Best GFLOPs: 94.8017
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #28: GFLOPs: 15.7441. Time: 0.0335 ms. Best GFLOPs: 94.8017
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #29: GFLOPs: 22.9725. Time: 0.0229 ms. Best GFLOPs: 94.8017
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #30: GFLOPs: 5.3010. Time: 0.0994 ms. Best GFLOPs: 94.8017
[05:04:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_clip_15"] Trial #31: GFLOPs: 43.9620. Time: 0.0120 ms. Best GFLOPs: 94.8017
[05:05:29] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_nn_conv2d_add_clip_15"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |       107.2557 |       9.8242 |                9.8242 |     32 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |      3298.4290 |      15.6685 |               15.6685 |     32 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |       214.8867 |       9.8070 |               49.0350 |     32 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |      2288.2636 |      45.0392 |              225.1959 |     32 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |        94.8017 |       5.5574 |                5.5574 |     32 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 512
Total latency (us): 511.984

[05:05:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #0: GFLOPs: 118.9307. Time: 0.4333 ms. Best GFLOPs: 118.9307
[05:05:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #1: GFLOPs: 21.4065. Time: 2.4073 ms. Best GFLOPs: 118.9307
[05:05:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #2: GFLOPs: 108.1950. Time: 0.4763 ms. Best GFLOPs: 118.9307
[05:05:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #3: GFLOPs: 1081.0051. Time: 0.0477 ms. Best GFLOPs: 1081.0051
[05:05:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #4: GFLOPs: 416.6051. Time: 0.1237 ms. Best GFLOPs: 1081.0051
[05:05:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(1024, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], compute: T.Buffer[(1, 1024, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i3_4_init in T.grid(4, 2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 256 + i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                            xx = T.axis.spatial(7, i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 49)
                                        v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 49 // 7)
                                        v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 196)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 256 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(512, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1024)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 1, 1, 4, 1, 1, 1, 2, 1, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 256 + i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                xx = T.axis.spatial(7, i3_4)
                                rc = T.axis.reduce(512, i4_0 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 256 + i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + ax1)
                            v2 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 16, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[05:05:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #6: GFLOPs: 1126.6151. Time: 0.0457 ms. Best GFLOPs: 1126.6151
[05:05:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #7: GFLOPs: 218.3269. Time: 0.2360 ms. Best GFLOPs: 1126.6151
[05:05:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #8: GFLOPs: 115.3270. Time: 0.4468 ms. Best GFLOPs: 1126.6151
[05:05:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #9: GFLOPs: 66.6413. Time: 0.7733 ms. Best GFLOPs: 1126.6151
[05:05:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(1024, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], compute: T.Buffer[(1, 1024, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(2, 7, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3_init * 2 + i1_4_init)
                            yy, xx = T.axis.remap("SS", [i2_3_init, i0_1_i1_1_i2_1_i3_1_fused])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(49):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i4_0 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 49)
                                        v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 49 // 7)
                                        v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(256):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 128 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 64)
                                    v1 = T.axis.spatial(512, i4_0 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 64)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 7, 1, 64, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3 * 2 + i1_4)
                                yy, xx = T.axis.remap("SS", [i2_3, i0_1_i1_1_i2_1_i3_1_fused])
                                rc = T.axis.reduce(512, i4_0 * 64 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2 = T.axis.spatial(7, ax2)
                            v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 32, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 1, 64])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 32])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:05:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #11: GFLOPs: 240.6309. Time: 0.2141 ms. Best GFLOPs: 1126.6151
[05:05:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(1024, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], compute: T.Buffer[(1, 1024, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused // 7 * 256 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                            xx = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 49)
                                        v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 49 // 7)
                                        v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 392)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(37):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 < 8192)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 1, 1, 4, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused // 7 * 256 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                xx = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                rc = T.axis.reduce(512, i4_0 * 8 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused // 7 * 256 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + ax1)
                            v2 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 32, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 224, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 224])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:05:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #13: GFLOPs: 66.9696. Time: 0.7695 ms. Best GFLOPs: 1126.6151
[05:05:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #14: GFLOPs: 627.9867. Time: 0.0821 ms. Best GFLOPs: 1126.6151
[05:05:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #15: GFLOPs: 636.2240. Time: 0.0810 ms. Best GFLOPs: 1126.6151
[05:05:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(1024, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], compute: T.Buffer[(1, 1024, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(32, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(392, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 49 // 7)
                            xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(392, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 49)
                                        v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 49 // 7)
                                        v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 196)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(392, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, (ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1) // 4)
                                    v1 = T.axis.spatial(512, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1 < 4096)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 49 // 7)
                                xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(512, i4_0 * 4 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 4 + ax1)
                            v2 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 + ax2)
                            v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 32, 8, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 392, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 392])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:05:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #17: GFLOPs: 234.6447. Time: 0.2196 ms. Best GFLOPs: 1126.6151
[05:05:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #18: GFLOPs: 371.7769. Time: 0.1386 ms. Best GFLOPs: 1126.6151
[05:05:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #19: GFLOPs: 94.0970. Time: 0.5476 ms. Best GFLOPs: 1126.6151
[05:05:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(1024, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], compute: T.Buffer[(1, 1024, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init in T.grid(4, 8, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_2_i1_2_i2_2_i3_2_fused * 32 + i1_3_init * 8 + i1_4_init)
                            yy, xx = T.axis.remap("SS", [i2_4_init, i0_1_i1_1_i2_1_i3_1_fused])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 49)
                                    v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 49 // 7)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 392)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(256):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 1, 1, 4, 1, 1, 1, 8, 7, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_2_i1_2_i2_2_i3_2_fused * 32 + i1_3 * 8 + i1_4)
                                yy, xx = T.axis.remap("SS", [i2_4, i0_1_i1_1_i2_1_i3_1_fused])
                                rc = T.axis.reduce(512, i4_0 * 8 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [1024, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 7, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_2_i1_2_i2_2_i3_2_fused * 32 + ax1)
                            v2 = T.axis.spatial(7, ax2)
                            v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 4, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[05:05:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #21: GFLOPs: 170.5108. Time: 0.3022 ms. Best GFLOPs: 1126.6151
[05:05:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #22: GFLOPs: 519.6673. Time: 0.0992 ms. Best GFLOPs: 1126.6151
[05:05:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #23: GFLOPs: 670.9340. Time: 0.0768 ms. Best GFLOPs: 1126.6151
[05:05:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #24: GFLOPs: 98.3832. Time: 0.5238 ms. Best GFLOPs: 1126.6151
[05:05:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #25: GFLOPs: 420.7559. Time: 0.1225 ms. Best GFLOPs: 1126.6151
[05:05:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #26: GFLOPs: 294.0670. Time: 0.1752 ms. Best GFLOPs: 1126.6151
[05:05:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #27: GFLOPs: 816.1363. Time: 0.0631 ms. Best GFLOPs: 1126.6151
[05:05:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #28: GFLOPs: 180.4566. Time: 0.2856 ms. Best GFLOPs: 1126.6151
[05:05:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #29: GFLOPs: 968.9518. Time: 0.0532 ms. Best GFLOPs: 1126.6151
[05:05:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #30: GFLOPs: 169.5518. Time: 0.3039 ms. Best GFLOPs: 1126.6151
[05:05:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_clip_16"] Trial #31: GFLOPs: 184.9592. Time: 0.2786 ms. Best GFLOPs: 1126.6151
[05:05:54] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #16: "fused_nn_conv2d_add_clip_16"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |       107.2557 |       9.8242 |                9.8242 |     32 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |      3298.4290 |      15.6685 |               15.6685 |     32 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |       214.8867 |       9.8070 |               49.0350 |     32 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |      2288.2636 |      45.0392 |              225.1959 |     32 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |        94.8017 |       5.5574 |                5.5574 |     32 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |      1126.6151 |      45.7394 |               45.7394 |     32 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 544
Total latency (us): 557.724

[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #0: GFLOPs: 48.0002. Time: 0.0220 ms. Best GFLOPs: 48.0002
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #1: GFLOPs: 222.8638. Time: 0.0047 ms. Best GFLOPs: 222.8638
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #2: GFLOPs: 108.7260. Time: 0.0097 ms. Best GFLOPs: 222.8638
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #3: GFLOPs: 20.1298. Time: 0.0523 ms. Best GFLOPs: 222.8638
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #4: GFLOPs: 252.9632. Time: 0.0042 ms. Best GFLOPs: 252.9632
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #5: GFLOPs: 15.7263. Time: 0.0670 ms. Best GFLOPs: 252.9632
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #6: GFLOPs: 23.8530. Time: 0.0442 ms. Best GFLOPs: 252.9632
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #7: GFLOPs: 41.0497. Time: 0.0257 ms. Best GFLOPs: 252.9632
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #8: GFLOPs: 44.8385. Time: 0.0235 ms. Best GFLOPs: 252.9632
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #9: GFLOPs: 28.0226. Time: 0.0376 ms. Best GFLOPs: 252.9632
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #10: GFLOPs: 40.3055. Time: 0.0261 ms. Best GFLOPs: 252.9632
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #11: GFLOPs: 102.8127. Time: 0.0102 ms. Best GFLOPs: 252.9632
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #12: GFLOPs: 27.2432. Time: 0.0387 ms. Best GFLOPs: 252.9632
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #13: GFLOPs: 38.5749. Time: 0.0273 ms. Best GFLOPs: 252.9632
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #14: GFLOPs: 48.0412. Time: 0.0219 ms. Best GFLOPs: 252.9632
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #15: GFLOPs: 42.8455. Time: 0.0246 ms. Best GFLOPs: 252.9632
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 7, 7), "float32"], placeholder_1: T.Buffer[(1024, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], compute: T.Buffer[(1, 1024, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 1024, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 1024, 9, 9], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(47):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 81)
                                    v2 = T.axis.spatial(9, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 81 // 9)
                                    v3 = T.axis.spatial(9, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 9)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 < 5184)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 8 and 1 <= v3 and v3 < 8, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 < 576)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i2_3_init, i1_4_init in T.grid(7, 4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + i1_4_init)
                                i = T.axis.spatial(7, i2_3_init)
                                j = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 1024, 7, 7], "float32"], ["TENSOR", [1024, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 7, 1, 1, 3, 1, 4, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + i1_4)
                                i = T.axis.spatial(7, i2_3)
                                j = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                di, dj = T.axis.remap("RR", [i4_1, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 1024, 7, 7], "float32"], ["TENSOR", [1024, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + ax1)
                            v2 = T.axis.spatial(7, ax2)
                            v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 16, 1, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 112])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 112])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l157)
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #17: GFLOPs: 61.7346. Time: 0.0171 ms. Best GFLOPs: 252.9632
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #18: GFLOPs: 113.3868. Time: 0.0093 ms. Best GFLOPs: 252.9632
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #19: GFLOPs: 72.6410. Time: 0.0145 ms. Best GFLOPs: 252.9632
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 7, 7), "float32"], placeholder_1: T.Buffer[(1024, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], compute: T.Buffer[(1, 1024, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 1024, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 1024, 9, 9], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(62):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 3456 // 27)
                                    v2 = T.axis.spatial(9, i0_0_i1_0_i2_0_i3_0_fused % 7 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 27 // 9)
                                    v3 = T.axis.spatial(9, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 9)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 3456)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 8 and 1 <= v3 and v3 < 8, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(21):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 1152)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            for i1_3_init, i1_4_init in T.grid(4, 4):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 16 + i1_3_init * 4 + i1_4_init)
                                    i = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    j = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 1024, 7, 7], "float32"], ["TENSOR", [1024, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 4, 1, 1, 3, 1, 1, 4, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 16 + i1_3 * 4 + i1_4)
                                    i = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    j = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 1024, 7, 7], "float32"], ["TENSOR", [1024, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 16 + ax1)
                            v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 8, 4, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 56])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 56])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l158)
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #21: GFLOPs: 194.3638. Time: 0.0054 ms. Best GFLOPs: 252.9632
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #22: GFLOPs: 21.7810. Time: 0.0484 ms. Best GFLOPs: 252.9632
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #23: GFLOPs: 67.1343. Time: 0.0157 ms. Best GFLOPs: 252.9632
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #24: GFLOPs: 11.3431. Time: 0.0929 ms. Best GFLOPs: 252.9632
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 7, 7), "float32"], placeholder_1: T.Buffer[(1024, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], compute: T.Buffer[(1, 1024, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 1024, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 1024, 9, 9], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init in T.serial(7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                            i, j = T.axis.remap("SS", [i2_3_init, i0_1_i1_1_i2_1_i3_1_fused])
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 1024, 7, 7], "float32"], ["TENSOR", [1024, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(21):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 2016 // 63)
                                        v2 = T.axis.spatial(9, i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 63 // 9)
                                        v3 = T.axis.spatial(9, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 9)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 8 and 1 <= v3 and v3 < 8, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 32 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 7, 1, 1, 3, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                i, j, di, dj = T.axis.remap("SSRR", [i2_3, i0_1_i1_1_i2_1_i3_1_fused, i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 1024, 7, 7], "float32"], ["TENSOR", [1024, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 7, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(7, ax2)
                            v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 32, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 7, 7), "float32"], placeholder_1: T.Buffer[(1024, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], compute: T.Buffer[(1, 1024, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 1024, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 1024, 9, 9], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i3_4_init in T.grid(2, 2, 7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + i1_3_init * 2 + i1_4_init)
                            i = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                            j = T.axis.spatial(7, i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 1024, 7, 7], "float32"], ["TENSOR", [1024, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(112):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 128 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 6272 // 49)
                                    v2 = T.axis.spatial(9, i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 49 // 7)
                                    v3 = T.axis.spatial(9, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 8 and 1 <= v3 and v3 < 8, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 128 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1))
                                    v1 = T.axis.spatial(1, 0)
                                    v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 128)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 7):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + i1_3 * 2 + i1_4)
                                i = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                j, di, dj = T.axis.remap("SRR", [i3_4, i4_0, i5_0])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 1024, 7, 7], "float32"], ["TENSOR", [1024, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + ax1)
                            v2 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 4, 8, 2, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 56])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 56])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l155)
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #27: GFLOPs: 57.0134. Time: 0.0185 ms. Best GFLOPs: 252.9632
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #28: GFLOPs: 42.9007. Time: 0.0246 ms. Best GFLOPs: 252.9632
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #29: GFLOPs: 50.8173. Time: 0.0207 ms. Best GFLOPs: 252.9632
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 7, 7), "float32"], placeholder_1: T.Buffer[(1024, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], compute: T.Buffer[(1, 1024, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 1024, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 1024, 9, 9], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused)
                            i = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                            j = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 1024, 7, 7], "float32"], ["TENSOR", [1024, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1344 // 21)
                                            v2 = T.axis.spatial(9, i0_0_i1_0_i2_0_i3_0_fused % 7 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 21 // 7)
                                            v3 = T.axis.spatial(9, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 8 and 1 <= v3 and v3 < 8, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 192)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused)
                                    i = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    j, di, dj = T.axis.remap("SRR", [i0_1_i1_1_i2_1_i3_1_fused, i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 1024, 7, 7], "float32"], ["TENSOR", [1024, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 64, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 64, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 64, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[05:05:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_clip_17"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 7, 7), "float32"], placeholder_1: T.Buffer[(1024, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], compute: T.Buffer[(1, 1024, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 1024, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 1024, 9, 9], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(49, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(53):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 81)
                                        v2 = T.axis.spatial(9, (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 81 // 9)
                                        v3 = T.axis.spatial(9, (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 5184)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 8 and 1 <= v3 and v3 < 8, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 576)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 2, 1, 1):
                            for i1_4_init in T.serial(16):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i1_3 * 16 + i1_4_init)
                                    i = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    j = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 1024, 7, 7], "float32"], ["TENSOR", [1024, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 16, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i1_3 * 16 + i1_4)
                                    i = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    j = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 1024, 7, 7], "float32"], ["TENSOR", [1024, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + ax1)
                            v2 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused // 7 + ax2)
                            v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 2, 1, 2, 16])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 49, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 49, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l167)
[05:06:22] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #17: "fused_nn_conv2d_add_clip_17"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |       107.2557 |       9.8242 |                9.8242 |     32 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |      3298.4290 |      15.6685 |               15.6685 |     32 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |       214.8867 |       9.8070 |               49.0350 |     32 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |      2288.2636 |      45.0392 |              225.1959 |     32 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |        94.8017 |       5.5574 |                5.5574 |     32 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |      1126.6151 |      45.7394 |               45.7394 |     32 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |       252.9632 |       4.1654 |                4.1654 |     32 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 576
Total latency (us): 561.889

[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #0: GFLOPs: 1698.0961. Time: 0.0606 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 7, 7), "float32"], placeholder_1: T.Buffer[(1024, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], compute: T.Buffer[(1, 1024, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 1024, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 512 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                            xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 7, 7], "float32"], ["TENSOR", [1024, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(1024, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i4_0)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused_1 // 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused_1 % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 49)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 512 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1))
                                    v1 = T.axis.spatial(1024, i4_0)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 512)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 512 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(1024, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 7, 7], "float32"], ["TENSOR", [1024, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 512 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + ax1)
                            v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 8, 8, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1024, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 56])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #2: GFLOPs: 98.4235. Time: 1.0456 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #3: GFLOPs: 229.1129. Time: 0.4492 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #4: GFLOPs: 974.2388. Time: 0.1056 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #5: GFLOPs: 17.6903. Time: 5.8174 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #6: GFLOPs: 329.0683. Time: 0.3127 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #7: GFLOPs: 826.8677. Time: 0.1245 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #8: GFLOPs: 449.5133. Time: 0.2289 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #9: GFLOPs: 149.1375. Time: 0.6900 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #10: GFLOPs: 307.4909. Time: 0.3347 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #11: GFLOPs: 46.4293. Time: 2.2165 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #12: GFLOPs: 151.9779. Time: 0.6771 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 7, 7), "float32"], placeholder_1: T.Buffer[(1024, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], compute: T.Buffer[(1, 1024, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 1024, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(392, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    with T.block("conv2d_nchw_init"):
                        nn = T.axis.spatial(1, 0)
                        ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 256 + i0_1_i1_1_i2_1_i3_1_fused // 49 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                        yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 49 // 7)
                        xx = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                        T.reads()
                        T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 7, 7], "float32"], ["TENSOR", [1024, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(49):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 49)
                                    v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 49 // 7)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 256 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 32)
                                        v1 = T.axis.spatial(1024, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 32)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 16, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 256 + i0_1_i1_1_i2_1_i3_1_fused // 49 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 49 // 7)
                                xx = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                rc = T.axis.reduce(1024, i4_0 * 32 + i4_1 * 16 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 7, 7], "float32"], ["TENSOR", [1024, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 256 + i0_1_i1_1_i2_1_i3_1_fused // 49 * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 49 // 7 + ax2)
                            v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 8, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #14: GFLOPs: 158.5795. Time: 0.6490 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #15: GFLOPs: 199.0006. Time: 0.5171 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #16: GFLOPs: 681.4442. Time: 0.1510 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #17: GFLOPs: 92.7121. Time: 1.1100 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #18: GFLOPs: 649.8420. Time: 0.1584 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #19: GFLOPs: 20.3622. Time: 5.0540 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #20: GFLOPs: 511.9384. Time: 0.2010 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #21: GFLOPs: 588.4538. Time: 0.1749 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #22: GFLOPs: 125.0679. Time: 0.8228 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #23: GFLOPs: 171.8290. Time: 0.5989 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #24: GFLOPs: 44.8258. Time: 2.2958 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #25: GFLOPs: 224.9296. Time: 0.4575 ms. Best GFLOPs: 1698.0961
[05:06:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #26: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 7, 7), "float32"], placeholder_1: T.Buffer[(1024, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], compute: T.Buffer[(1, 1024, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 1024, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(2, 7, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused * 256 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3_init * 2 + i1_4_init)
                            yy, xx = T.axis.remap("SS", [i2_3_init, i0_0_i1_0_i2_0_i3_0_fused])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 7, 7], "float32"], ["TENSOR", [1024, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(512, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i4_0 * 2 + ax0_ax1_ax2_ax3_fused_1 // 7)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused_1 % 7)
                                    v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(1024, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 7, 1, 2, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused * 256 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3 * 2 + i1_4)
                                yy, xx = T.axis.remap("SS", [i2_3, i0_0_i1_0_i2_0_i3_0_fused])
                                rc = T.axis.reduce(1024, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 7, 7], "float32"], ["TENSOR", [1024, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused * 256 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2 = T.axis.spatial(7, ax2)
                            v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 64, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[512, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 64])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 64, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[05:06:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #27: GFLOPs: 420.2456. Time: 0.2449 ms. Best GFLOPs: 1698.0961
[05:06:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #28: GFLOPs: 139.8203. Time: 0.7360 ms. Best GFLOPs: 1698.0961
[05:06:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #29: GFLOPs: 7.6861. Time: 13.3892 ms. Best GFLOPs: 1698.0961
[05:06:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #30: GFLOPs: 928.3083. Time: 0.1109 ms. Best GFLOPs: 1698.0961
[05:06:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_clip_18"] Trial #31: GFLOPs: 242.0403. Time: 0.4252 ms. Best GFLOPs: 1698.0961
[05:07:00] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #18: "fused_nn_conv2d_add_clip_18"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |       107.2557 |       9.8242 |                9.8242 |     32 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |      3298.4290 |      15.6685 |               15.6685 |     32 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |       214.8867 |       9.8070 |               49.0350 |     32 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |      2288.2636 |      45.0392 |              225.1959 |     32 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |        94.8017 |       5.5574 |                5.5574 |     32 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |      1126.6151 |      45.7394 |               45.7394 |     32 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |       252.9632 |       4.1654 |                4.1654 |     32 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |      1698.0961 |      60.6037 |               60.6037 |     32 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 608
Total latency (us): 622.493

[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #0: GFLOPs: 3.4886. Time: 0.0205 ms. Best GFLOPs: 3.4886
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #1: GFLOPs: 2.4123. Time: 0.0297 ms. Best GFLOPs: 3.4886
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #2: GFLOPs: 2.2881. Time: 0.0313 ms. Best GFLOPs: 3.4886
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #3: GFLOPs: 4.3929. Time: 0.0163 ms. Best GFLOPs: 4.3929
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #4: GFLOPs: 20.9290. Time: 0.0034 ms. Best GFLOPs: 20.9290
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #5: GFLOPs: 2.5881. Time: 0.0277 ms. Best GFLOPs: 20.9290
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #6: GFLOPs: 7.3912. Time: 0.0097 ms. Best GFLOPs: 20.9290
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #7: GFLOPs: 2.9202. Time: 0.0245 ms. Best GFLOPs: 20.9290
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #8: GFLOPs: 20.1114. Time: 0.0036 ms. Best GFLOPs: 20.9290
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #9: GFLOPs: 14.4260. Time: 0.0050 ms. Best GFLOPs: 20.9290
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #10: GFLOPs: 4.3775. Time: 0.0164 ms. Best GFLOPs: 20.9290
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #11: GFLOPs: 3.9738. Time: 0.0180 ms. Best GFLOPs: 20.9290
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #12: GFLOPs: 20.9144. Time: 0.0034 ms. Best GFLOPs: 20.9290
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #13: GFLOPs: 10.2138. Time: 0.0070 ms. Best GFLOPs: 20.9290
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #14: GFLOPs: 3.6800. Time: 0.0195 ms. Best GFLOPs: 20.9290
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #15: GFLOPs: 14.4219. Time: 0.0050 ms. Best GFLOPs: 20.9290
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #16: GFLOPs: 9.3828. Time: 0.0076 ms. Best GFLOPs: 20.9290
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #17: GFLOPs: 14.8736. Time: 0.0048 ms. Best GFLOPs: 20.9290
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #18: GFLOPs: 1.8576. Time: 0.0386 ms. Best GFLOPs: 20.9290
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #19: GFLOPs: 21.1321. Time: 0.0034 ms. Best GFLOPs: 21.1321
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #20: GFLOPs: 4.6702. Time: 0.0153 ms. Best GFLOPs: 21.1321
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #21: GFLOPs: 4.4073. Time: 0.0163 ms. Best GFLOPs: 21.1321
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #22: GFLOPs: 4.4773. Time: 0.0160 ms. Best GFLOPs: 21.1321
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #23: GFLOPs: 3.2944. Time: 0.0218 ms. Best GFLOPs: 21.1321
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #24: GFLOPs: 20.9982. Time: 0.0034 ms. Best GFLOPs: 21.1321
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #25: GFLOPs: 19.4935. Time: 0.0037 ms. Best GFLOPs: 21.1321
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #26: GFLOPs: 20.9624. Time: 0.0034 ms. Best GFLOPs: 21.1321
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #27: GFLOPs: 2.1896. Time: 0.0327 ms. Best GFLOPs: 21.1321
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #28: GFLOPs: 2.7533. Time: 0.0260 ms. Best GFLOPs: 21.1321
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #29: GFLOPs: 3.1673. Time: 0.0226 ms. Best GFLOPs: 21.1321
[05:07:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #30: GFLOPs: 2.2329. Time: 0.0321 ms. Best GFLOPs: 21.1321
[05:07:45] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #19: "fused_nn_avg_pool2d"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |       107.2557 |       9.8242 |                9.8242 |     32 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |      3298.4290 |      15.6685 |               15.6685 |     32 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |       214.8867 |       9.8070 |               49.0350 |     32 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |      2288.2636 |      45.0392 |              225.1959 |     32 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |        94.8017 |       5.5574 |                5.5574 |     32 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |      1126.6151 |      45.7394 |               45.7394 |     32 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |       252.9632 |       4.1654 |                4.1654 |     32 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |      1698.0961 |      60.6037 |               60.6037 |     32 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |        21.1321 |       3.3920 |                3.3920 |     31 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 639
Total latency (us): 625.885

[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #0: GFLOPs: 21.5494. Time: 0.0952 ms. Best GFLOPs: 21.5494
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #1: GFLOPs: 3.7836. Time: 0.5421 ms. Best GFLOPs: 21.5494
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #2: GFLOPs: 31.7340. Time: 0.0646 ms. Best GFLOPs: 31.7340
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #3: GFLOPs: 4.0874. Time: 0.5018 ms. Best GFLOPs: 31.7340
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #4: GFLOPs: 5.6977. Time: 0.3600 ms. Best GFLOPs: 31.7340
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #5: GFLOPs: 21.8568. Time: 0.0938 ms. Best GFLOPs: 31.7340
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #6: GFLOPs: 29.5404. Time: 0.0694 ms. Best GFLOPs: 31.7340
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #7: GFLOPs: 5.0410. Time: 0.4069 ms. Best GFLOPs: 31.7340
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #8: GFLOPs: 38.8476. Time: 0.0528 ms. Best GFLOPs: 38.8476
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #9: GFLOPs: 30.6290. Time: 0.0670 ms. Best GFLOPs: 38.8476
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #10: GFLOPs: 2.3854. Time: 0.8598 ms. Best GFLOPs: 38.8476
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #11: GFLOPs: 2.3689. Time: 0.8658 ms. Best GFLOPs: 38.8476
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #12: GFLOPs: 1.9499. Time: 1.0519 ms. Best GFLOPs: 38.8476
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #13: GFLOPs: 21.7215. Time: 0.0944 ms. Best GFLOPs: 38.8476
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #14: GFLOPs: 4.9505. Time: 0.4143 ms. Best GFLOPs: 38.8476
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #15: GFLOPs: 4.1979. Time: 0.4886 ms. Best GFLOPs: 38.8476
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #16: GFLOPs: 3.4924. Time: 0.5873 ms. Best GFLOPs: 38.8476
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #17: GFLOPs: 6.0186. Time: 0.3408 ms. Best GFLOPs: 38.8476
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #18: GFLOPs: 5.3989. Time: 0.3799 ms. Best GFLOPs: 38.8476
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #19: GFLOPs: 4.7706. Time: 0.4299 ms. Best GFLOPs: 38.8476
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #20: GFLOPs: 34.6597. Time: 0.0592 ms. Best GFLOPs: 38.8476
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #21: GFLOPs: 52.7398. Time: 0.0389 ms. Best GFLOPs: 52.7398
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #22: GFLOPs: 33.8414. Time: 0.0606 ms. Best GFLOPs: 52.7398
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #23: GFLOPs: 52.8778. Time: 0.0388 ms. Best GFLOPs: 52.8778
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #24: GFLOPs: 6.5581. Time: 0.3128 ms. Best GFLOPs: 52.8778
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #25: GFLOPs: 42.4715. Time: 0.0483 ms. Best GFLOPs: 52.8778
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #26: GFLOPs: 32.2590. Time: 0.0636 ms. Best GFLOPs: 52.8778
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #27: GFLOPs: 37.5295. Time: 0.0547 ms. Best GFLOPs: 52.8778
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #28: GFLOPs: 25.3265. Time: 0.0810 ms. Best GFLOPs: 52.8778
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #29: GFLOPs: 38.5435. Time: 0.0532 ms. Best GFLOPs: 52.8778
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #30: GFLOPs: 61.7245. Time: 0.0332 ms. Best GFLOPs: 61.7245
[05:07:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add"] Trial #31: GFLOPs: 4.5108. Time: 0.4547 ms. Best GFLOPs: 61.7245
[05:08:28] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #20: "fused_nn_conv2d_add"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |       107.2557 |       9.8242 |                9.8242 |     32 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |      3298.4290 |      15.6685 |               15.6685 |     32 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |       214.8867 |       9.8070 |               49.0350 |     32 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |      2288.2636 |      45.0392 |              225.1959 |     32 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |        94.8017 |       5.5574 |                5.5574 |     32 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |      1126.6151 |      45.7394 |               45.7394 |     32 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |       252.9632 |       4.1654 |                4.1654 |     32 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |      1698.0961 |      60.6037 |               60.6037 |     32 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |        21.1321 |       3.3920 |                3.3920 |     31 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |        61.7245 |      33.2291 |               33.2291 |     32 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 671
Total latency (us): 659.114

[05:08:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_reshape_squeeze_reshape"] Trial #0: GFLOPs: 0.0000. Time: 0.0248 ms. Best GFLOPs: 0.0000
[05:08:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_reshape_squeeze_reshape"] Trial #1: GFLOPs: 0.0000. Time: 0.0074 ms. Best GFLOPs: 0.0000
[05:08:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_reshape_squeeze_reshape"] Trial #2: GFLOPs: 0.0000. Time: 0.0033 ms. Best GFLOPs: 0.0000
[05:08:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_reshape_squeeze_reshape"] Trial #3: GFLOPs: 0.0000. Time: 0.0087 ms. Best GFLOPs: 0.0000
[05:08:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_reshape_squeeze_reshape"] Trial #4: GFLOPs: 0.0000. Time: 0.0219 ms. Best GFLOPs: 0.0000
[05:09:04] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #21: "fused_reshape_squeeze_reshape"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |       107.2557 |       9.8242 |                9.8242 |     32 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |      3298.4290 |      15.6685 |               15.6685 |     32 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |       214.8867 |       9.8070 |               49.0350 |     32 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |      2288.2636 |      45.0392 |              225.1959 |     32 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |        94.8017 |       5.5574 |                5.5574 |     32 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |      1126.6151 |      45.7394 |               45.7394 |     32 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |       252.9632 |       4.1654 |                4.1654 |     32 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |      1698.0961 |      60.6037 |               60.6037 |     32 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |        21.1321 |       3.3920 |                3.3920 |     31 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |        61.7245 |      33.2291 |               33.2291 |     32 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |         0.0003 |       3.3414 |                3.3414 |      5 |            
 22 |              fused_nn_softmax |      4004 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 676
Total latency (us): 662.455

[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #0: GFLOPs: 0.7427. Time: 0.0054 ms. Best GFLOPs: 0.7427
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #1: GFLOPs: 0.0944. Time: 0.0424 ms. Best GFLOPs: 0.7427
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #2: GFLOPs: 0.2594. Time: 0.0154 ms. Best GFLOPs: 0.7427
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_softmax"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_softmax_norm: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_softmax_maxelem_shared = T.alloc_buffer([1], dtype="float32", scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer([1], dtype="float32", scope="shared")
        for i0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1_0 in T.grid(1, 251):
                for ax1_1 in T.thread_binding(4, thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        i0 = T.axis.spatial(1, 0)
                        k = T.axis.reduce(1001, ax1_0 * 4 + ax1_1)
                        T.where(ax1_0 * 4 + ax1_1 < 1001)
                        T.reads(placeholder[i0, k])
                        T.writes(T_softmax_maxelem_shared[i0])
                        with T.init():
                            T_softmax_maxelem_shared[i0] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[i0] = T.max(T_softmax_maxelem_shared[i0], placeholder[i0, k])
            for ax0, ax1_0 in T.grid(1, 251):
                for ax1_1 in T.thread_binding(4, thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        i0 = T.axis.spatial(1, 0)
                        k = T.axis.reduce(1001, ax1_0 * 4 + ax1_1)
                        T.where(ax1_0 * 4 + ax1_1 < 1001)
                        T.reads(placeholder[i0, k], T_softmax_maxelem_shared[i0])
                        T.writes(T_softmax_expsum_shared[i0])
                        with T.init():
                            T_softmax_expsum_shared[i0] = T.float32(0)
                        T_softmax_expsum_shared[i0] = T_softmax_expsum_shared[i0] + T.exp(placeholder[i0, k] - T_softmax_maxelem_shared[i0], dtype="float32")
            for i1_0 in T.serial(251):
                for i1_1 in T.thread_binding(4, thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(1001, i1_0 * 4 + i1_1)
                        T.where(i1_0 * 4 + i1_1 < 1001)
                        T.reads(placeholder[i0, i1], T_softmax_maxelem_shared[i0], T_softmax_expsum_shared[i0])
                        T.writes(T_softmax_norm[i0, i1])
                        T.block_attr({"axis":1})
                        T_softmax_norm[i0, i1] = T.exp(placeholder[i0, i1] - T_softmax_maxelem_shared[i0], dtype="float32") / T_softmax_expsum_shared[i0]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
b4, = sch.get_consumers(block=b2)
l5, l6 = sch.get_loops(block=b4)
v7 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=0)
l8, l9 = sch.split(loop=l6, factors=[None, v7])
sch.bind(loop=l9, thread_axis="threadIdx.x")
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
sch.set_scope(block=b2, buffer_index=0, storage_scope="shared")
l10, l11, l12 = sch.get_loops(block=b2)
l13, l14 = sch.split(loop=l12, factors=[None, v7])
sch.bind(loop=l14, thread_axis="threadIdx.x")
b15, b16 = sch.get_consumers(block=b0)
l17, l18, l19, l20 = sch.get_loops(block=b15)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l21, l22, l23 = sch.get_loops(block=b0)
l24, l25 = sch.split(loop=l23, factors=[None, v7])
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
sch.enter_postproc()
b27 = sch.get_block(name="T_softmax_maxelem", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
l32 = sch.fuse(l28)
sch.bind(loop=l32, thread_axis="blockIdx.x")
b33 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b33, ann_key="meta_schedule.unroll_explicit")
b34, b35, b36 = sch.get_child_blocks(b33)
l37, l38, l39, l40 = sch.get_loops(block=b34)
sch.annotate(block_or_loop=l37, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l37, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42, l43, l44 = sch.get_loops(block=b35)
sch.annotate(block_or_loop=l41, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l41, ann_key="pragma_unroll_explicit", ann_val=1)
l45, l46, l47 = sch.get_loops(block=b36)
sch.annotate(block_or_loop=l45, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l45, ann_key="pragma_unroll_explicit", ann_val=1)
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #4: GFLOPs: 0.1966. Time: 0.0204 ms. Best GFLOPs: 0.7427
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #5: GFLOPs: 0.0710. Time: 0.0564 ms. Best GFLOPs: 0.7427
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #6: GFLOPs: 0.0888. Time: 0.0451 ms. Best GFLOPs: 0.7427
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #7: GFLOPs: 0.1945. Time: 0.0206 ms. Best GFLOPs: 0.7427
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #8: GFLOPs: 0.0476. Time: 0.0841 ms. Best GFLOPs: 0.7427
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #9: GFLOPs: 0.0530. Time: 0.0756 ms. Best GFLOPs: 0.7427
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #10: GFLOPs: 0.0841. Time: 0.0476 ms. Best GFLOPs: 0.7427
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #11: GFLOPs: 0.0949. Time: 0.0422 ms. Best GFLOPs: 0.7427
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #12: GFLOPs: 0.2286. Time: 0.0175 ms. Best GFLOPs: 0.7427
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #13: GFLOPs: 0.2869. Time: 0.0140 ms. Best GFLOPs: 0.7427
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #14: GFLOPs: 1.1695. Time: 0.0034 ms. Best GFLOPs: 1.1695
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #15: GFLOPs: 0.0624. Time: 0.0641 ms. Best GFLOPs: 1.1695
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #16: GFLOPs: 0.1316. Time: 0.0304 ms. Best GFLOPs: 1.1695
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #17: GFLOPs: 0.3877. Time: 0.0103 ms. Best GFLOPs: 1.1695
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #18: GFLOPs: 0.4273. Time: 0.0094 ms. Best GFLOPs: 1.1695
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #19: GFLOPs: 1.1682. Time: 0.0034 ms. Best GFLOPs: 1.1695
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #20: GFLOPs: 0.0683. Time: 0.0586 ms. Best GFLOPs: 1.1695
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_softmax"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1001), "float32"], T_softmax_norm: T.Buffer[(1, 1001), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
        T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
        for i0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i1_0 in T.serial(63):
                for i1_1 in T.thread_binding(16, thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        i0 = T.axis.spatial(1, 0)
                        k = T.axis.reduce(1001, i1_0 * 16 + i1_1)
                        T.where(i1_0 * 16 + i1_1 < 1001)
                        T.reads(placeholder[i0, k])
                        T.writes(T_softmax_maxelem[i0])
                        with T.init():
                            T_softmax_maxelem[i0] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem[i0] = T.max(T_softmax_maxelem[i0], placeholder[i0, k])
        for i0_fused_0 in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_fused_1 in T.thread_binding(1, thread="threadIdx.x"):
                with T.block("T_softmax_expsum_init"):
                    i0 = T.axis.spatial(1, 0)
                    T.reads()
                    T.writes(T_softmax_expsum[i0])
                    T_softmax_expsum[i0] = T.float32(0)
                for i1 in T.serial(1001):
                    with T.block("T_softmax_expsum_update"):
                        i0 = T.axis.spatial(1, 0)
                        k = T.axis.reduce(1001, i1)
                        T.reads(T_softmax_expsum[i0], placeholder[i0, k], T_softmax_maxelem[i0])
                        T.writes(T_softmax_expsum[i0])
                        T_softmax_expsum[i0] = T_softmax_expsum[i0] + T.exp(placeholder[i0, k] - T_softmax_maxelem[i0], dtype="float32")
        for i0_i1_fused_0 in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_i1_fused_1 in T.thread_binding(1001, thread="threadIdx.x"):
                with T.block("T_softmax_norm"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(1001, i0_i1_fused_1)
                    T.reads(placeholder[i0, i1], T_softmax_maxelem[i0], T_softmax_expsum[i0])
                    T.writes(T_softmax_norm[i0, i1])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0, i1] = T.exp(placeholder[i0, i1] - T_softmax_maxelem[i0], dtype="float32") / T_softmax_expsum[i0]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=2)
l4, l5 = sch.get_loops(block=b0)
l6, l7 = sch.split(loop=l5, factors=[None, v3])
sch.bind(loop=l7, thread_axis="threadIdx.x")
v8 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v8)
sch.enter_postproc()
b9 = sch.get_block(name="T_softmax_maxelem", func_name="main")
l10, l11, l12 = sch.get_loops(block=b9)
l13 = sch.fuse(l10)
sch.bind(loop=l13, thread_axis="blockIdx.x")
b14 = sch.get_block(name="T_softmax_expsum", func_name="main")
l15, l16 = sch.get_loops(block=b14)
l17 = sch.fuse(l15)
l18, l19 = sch.split(loop=l17, factors=[None, 1])
sch.bind(loop=l18, thread_axis="blockIdx.x")
sch.bind(loop=l19, thread_axis="threadIdx.x")
b20 = sch.get_block(name="T_softmax_norm", func_name="main")
l21, l22 = sch.get_loops(block=b20)
l23 = sch.fuse(l21, l22)
l24, l25 = sch.split(loop=l23, factors=[None, 1001])
sch.bind(loop=l24, thread_axis="blockIdx.x")
sch.bind(loop=l25, thread_axis="threadIdx.x")
b26 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b26, ann_key="meta_schedule.unroll_explicit")
b27, b28, b29 = sch.get_child_blocks(b26)
l30, l31, l32 = sch.get_loops(block=b27)
sch.annotate(block_or_loop=l30, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l30, ann_key="pragma_unroll_explicit", ann_val=1)
l33, l34, l35 = sch.get_loops(block=b28)
sch.annotate(block_or_loop=l33, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l33, ann_key="pragma_unroll_explicit", ann_val=1)
l36, l37 = sch.get_loops(block=b29)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
b38 = sch.get_block(name="T_softmax_expsum", func_name="main")
l39, l40, l41 = sch.get_loops(block=b38)
b42 = sch.decompose_reduction(block=b38, loop=l41)
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #22: GFLOPs: 0.1061. Time: 0.0377 ms. Best GFLOPs: 1.1695
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #23: GFLOPs: 0.6383. Time: 0.0063 ms. Best GFLOPs: 1.1695
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #24: GFLOPs: 0.6923. Time: 0.0058 ms. Best GFLOPs: 1.1695
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #25: GFLOPs: 0.2470. Time: 0.0162 ms. Best GFLOPs: 1.1695
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #26: GFLOPs: 0.1381. Time: 0.0290 ms. Best GFLOPs: 1.1695
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #27: GFLOPs: 0.8065. Time: 0.0050 ms. Best GFLOPs: 1.1695
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #28: GFLOPs: 0.0632. Time: 0.0633 ms. Best GFLOPs: 1.1695
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #29: GFLOPs: 0.1879. Time: 0.0213 ms. Best GFLOPs: 1.1695
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #30: GFLOPs: 0.1638. Time: 0.0244 ms. Best GFLOPs: 1.1695
[05:09:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_softmax"] Trial #31: GFLOPs: 0.1638. Time: 0.0244 ms. Best GFLOPs: 1.1695
[05:09:42] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #22: "fused_nn_softmax"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |       107.2557 |       9.8242 |                9.8242 |     32 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |      3298.4290 |      15.6685 |               15.6685 |     32 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |       214.8867 |       9.8070 |               49.0350 |     32 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |      2288.2636 |      45.0392 |              225.1959 |     32 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |        94.8017 |       5.5574 |                5.5574 |     32 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |      1126.6151 |      45.7394 |               45.7394 |     32 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |       252.9632 |       4.1654 |                4.1654 |     32 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |      1698.0961 |      60.6037 |               60.6037 |     32 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |        21.1321 |       3.3920 |                3.3920 |     31 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |        61.7245 |      33.2291 |               33.2291 |     32 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |         0.0003 |       3.3414 |                3.3414 |      5 |            
 22 |              fused_nn_softmax |      4004 |      1 |         1.1695 |       3.4236 |                3.4236 |     32 |            
 23 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 708
Total latency (us): 665.879

[05:09:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_reshape"] Trial #0: GFLOPs: 0.0000. Time: 0.0128 ms. Best GFLOPs: 0.0000
[05:09:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_reshape"] Trial #1: GFLOPs: 0.0000. Time: 0.0117 ms. Best GFLOPs: 0.0000
[05:09:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_reshape"] Trial #2: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[05:09:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_reshape"] Trial #3: GFLOPs: 0.0000. Time: 0.0035 ms. Best GFLOPs: 0.0000
[05:09:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_reshape"] Trial #4: GFLOPs: 0.0000. Time: 0.0051 ms. Best GFLOPs: 0.0000
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_reshape"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |            
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |            
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |            
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |            
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |            
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |            
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |            
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |            
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |            
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |            
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |            
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |       107.2557 |       9.8242 |                9.8242 |     32 |            
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |      3298.4290 |      15.6685 |               15.6685 |     32 |            
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |       214.8867 |       9.8070 |               49.0350 |     32 |            
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |      2288.2636 |      45.0392 |              225.1959 |     32 |            
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |        94.8017 |       5.5574 |                5.5574 |     32 |            
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |      1126.6151 |      45.7394 |               45.7394 |     32 |            
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |       252.9632 |       4.1654 |                4.1654 |     32 |            
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |      1698.0961 |      60.6037 |               60.6037 |     32 |            
 19 |           fused_nn_avg_pool2d |     71680 |      1 |        21.1321 |       3.3920 |                3.3920 |     31 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |        61.7245 |      33.2291 |               33.2291 |     32 |            
 21 | fused_reshape_squeeze_reshape |         1 |      1 |         0.0003 |       3.3414 |                3.3414 |      5 |            
 22 |              fused_nn_softmax |      4004 |      1 |         1.1695 |       3.4236 |                3.4236 |     32 |            
 23 |                 fused_reshape |         1 |      1 |         0.0003 |       3.5155 |                3.5155 |      5 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 713
Total latency (us): 669.395

[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_nn_conv2d_add_clip_14"
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #14 has finished. Remaining task(s): 23
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #18: "fused_nn_conv2d_add_clip_18"
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #18 has finished. Remaining task(s): 22
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #13: "fused_nn_conv2d_add_clip_13"
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #13 has finished. Remaining task(s): 21
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #16: "fused_nn_conv2d_add_clip_16"
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #16 has finished. Remaining task(s): 20
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_conv2d_add_clip_4"
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #4 has finished. Remaining task(s): 19
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #20: "fused_nn_conv2d_add"
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #20 has finished. Remaining task(s): 18
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_nn_conv2d_add_clip_8"
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #8 has finished. Remaining task(s): 17
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_nn_conv2d_add_clip_10"
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #10 has finished. Remaining task(s): 16
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_conv2d_add_clip_6"
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #6 has finished. Remaining task(s): 15
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_conv2d_add_clip_2"
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #2 has finished. Remaining task(s): 14
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_conv2d_add_clip_3"
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #3 has finished. Remaining task(s): 13
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #12: "fused_nn_conv2d_add_clip_12"
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #12 has finished. Remaining task(s): 12
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_conv2d_add_clip_1"
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #1 has finished. Remaining task(s): 11
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_conv2d_add_clip_5"
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #5 has finished. Remaining task(s): 10
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_nn_conv2d_add_clip"
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #0 has finished. Remaining task(s): 9
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_conv2d_add_clip_7"
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #7 has finished. Remaining task(s): 8
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #11: "fused_nn_conv2d_add_clip_11"
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #11 has finished. Remaining task(s): 7
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #9: "fused_nn_conv2d_add_clip_9"
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #9 has finished. Remaining task(s): 6
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_nn_conv2d_add_clip_15"
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #15 has finished. Remaining task(s): 5
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #17: "fused_nn_conv2d_add_clip_17"
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #17 has finished. Remaining task(s): 4
[05:10:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_reshape"
[05:10:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:10:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:10:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:10:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:10:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:10:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:10:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:11:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:11:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:11:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:11:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:11:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[05:11:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[05:11:18] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_reshape"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |          Y 
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |          Y 
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |          Y 
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |          Y 
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |          Y 
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |          Y 
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |          Y 
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |          Y 
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |          Y 
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |          Y 
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |          Y 
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |       107.2557 |       9.8242 |                9.8242 |     32 |          Y 
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |      3298.4290 |      15.6685 |               15.6685 |     32 |          Y 
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |       214.8867 |       9.8070 |               49.0350 |     32 |          Y 
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |      2288.2636 |      45.0392 |              225.1959 |     32 |          Y 
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |        94.8017 |       5.5574 |                5.5574 |     32 |          Y 
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |      1126.6151 |      45.7394 |               45.7394 |     32 |          Y 
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |       252.9632 |       4.1654 |                4.1654 |     32 |          Y 
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |      1698.0961 |      60.6037 |               60.6037 |     32 |          Y 
 19 |           fused_nn_avg_pool2d |     71680 |      1 |        21.1321 |       3.3920 |                3.3920 |     31 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |        61.7245 |      33.2291 |               33.2291 |     32 |          Y 
 21 | fused_reshape_squeeze_reshape |         1 |      1 |         0.0003 |       3.3414 |                3.3414 |      5 |            
 22 |              fused_nn_softmax |      4004 |      1 |         1.1695 |       3.4236 |                3.4236 |     32 |            
 23 |                 fused_reshape |         1 |      1 |         0.0003 |       3.5155 |                3.5155 |      5 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 713
Total latency (us): 669.395

[05:11:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #22: "fused_nn_softmax"
[05:11:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #22 has finished. Remaining task(s): 3
[05:11:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_nn_avg_pool2d"
[05:11:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:11:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 31 candidate(s) from database
[05:11:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9270a0678)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d92772cef8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d231e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926749578)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267c47b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9277483e8)]: 0 failure(s)
[05:11:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2017 candidate(s)
[05:11:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9270a0678)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d92772cef8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d231e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926749578)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267c47b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9277483e8)]: 0 failure(s)
[05:12:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9270a0678)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d92772cef8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d231e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926749578)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267c47b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9277483e8)]: 0 failure(s)
[05:13:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9270a0678)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d92772cef8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d231e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926749578)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267c47b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9277483e8)]: 0 failure(s)
[05:13:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d9270a0678)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d92772cef8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d231e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d926749578)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267c47b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d9277483e8)]: 0 failure(s)
[05:14:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 1 candidates:
[1 : 1]:	1.0034
[05:14:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 1 candidate(s) with evolutionary search
[05:14:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 1 candidates(s) for measurement
[05:14:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 1 sample(s) to builder
[05:14:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 1 sample(s) to runner
[05:14:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_avg_pool2d"] Trial #31: GFLOPs: 13.8145. Time: 0.0052 ms. Best GFLOPs: 21.1321
[05:14:40] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #19: "fused_nn_avg_pool2d"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |          Y 
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |          Y 
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |          Y 
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |          Y 
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |          Y 
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |          Y 
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |          Y 
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |          Y 
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |          Y 
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |          Y 
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |          Y 
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |       107.2557 |       9.8242 |                9.8242 |     32 |          Y 
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |      3298.4290 |      15.6685 |               15.6685 |     32 |          Y 
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |       214.8867 |       9.8070 |               49.0350 |     32 |          Y 
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |      2288.2636 |      45.0392 |              225.1959 |     32 |          Y 
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |        94.8017 |       5.5574 |                5.5574 |     32 |          Y 
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |      1126.6151 |      45.7394 |               45.7394 |     32 |          Y 
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |       252.9632 |       4.1654 |                4.1654 |     32 |          Y 
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |      1698.0961 |      60.6037 |               60.6037 |     32 |          Y 
 19 |           fused_nn_avg_pool2d |     71680 |      1 |        21.1321 |       3.3920 |                3.3920 |     32 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |        61.7245 |      33.2291 |               33.2291 |     32 |          Y 
 21 | fused_reshape_squeeze_reshape |         1 |      1 |         0.0003 |       3.3414 |                3.3414 |      5 |            
 22 |              fused_nn_softmax |      4004 |      1 |         1.1695 |       3.4236 |                3.4236 |     32 |          Y 
 23 |                 fused_reshape |         1 |      1 |         0.0003 |       3.5155 |                3.5155 |      5 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 714
Total latency (us): 669.395

[05:14:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #21: "fused_reshape_squeeze_reshape"
[05:14:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:14:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:14:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:14:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:15:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:15:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:15:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:16:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:16:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:16:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:16:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:16:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[05:16:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[05:16:13] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #21: "fused_reshape_squeeze_reshape"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |          Y 
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |          Y 
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |          Y 
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |          Y 
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |          Y 
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |          Y 
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |          Y 
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |          Y 
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |          Y 
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |          Y 
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |          Y 
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |       107.2557 |       9.8242 |                9.8242 |     32 |          Y 
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |      3298.4290 |      15.6685 |               15.6685 |     32 |          Y 
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |       214.8867 |       9.8070 |               49.0350 |     32 |          Y 
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |      2288.2636 |      45.0392 |              225.1959 |     32 |          Y 
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |        94.8017 |       5.5574 |                5.5574 |     32 |          Y 
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |      1126.6151 |      45.7394 |               45.7394 |     32 |          Y 
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |       252.9632 |       4.1654 |                4.1654 |     32 |          Y 
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |      1698.0961 |      60.6037 |               60.6037 |     32 |          Y 
 19 |           fused_nn_avg_pool2d |     71680 |      1 |        21.1321 |       3.3920 |                3.3920 |     32 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |        61.7245 |      33.2291 |               33.2291 |     32 |          Y 
 21 | fused_reshape_squeeze_reshape |         1 |      1 |         0.0003 |       3.3414 |                3.3414 |      5 |            
 22 |              fused_nn_softmax |      4004 |      1 |         1.1695 |       3.4236 |                3.4236 |     32 |          Y 
 23 |                 fused_reshape |         1 |      1 |         0.0003 |       3.5155 |                3.5155 |      5 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 714
Total latency (us): 669.395

[05:16:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_reshape"
[05:16:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:16:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:16:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:16:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:16:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:16:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:16:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:16:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:17:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:17:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:17:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:17:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[05:17:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[05:17:02] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_reshape"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |          Y 
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |          Y 
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |          Y 
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |          Y 
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |          Y 
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |          Y 
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |          Y 
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |          Y 
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |          Y 
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |          Y 
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |          Y 
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |       107.2557 |       9.8242 |                9.8242 |     32 |          Y 
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |      3298.4290 |      15.6685 |               15.6685 |     32 |          Y 
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |       214.8867 |       9.8070 |               49.0350 |     32 |          Y 
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |      2288.2636 |      45.0392 |              225.1959 |     32 |          Y 
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |        94.8017 |       5.5574 |                5.5574 |     32 |          Y 
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |      1126.6151 |      45.7394 |               45.7394 |     32 |          Y 
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |       252.9632 |       4.1654 |                4.1654 |     32 |          Y 
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |      1698.0961 |      60.6037 |               60.6037 |     32 |          Y 
 19 |           fused_nn_avg_pool2d |     71680 |      1 |        21.1321 |       3.3920 |                3.3920 |     32 |            
 20 |           fused_nn_conv2d_add |   2051049 |      1 |        61.7245 |      33.2291 |               33.2291 |     32 |          Y 
 21 | fused_reshape_squeeze_reshape |         1 |      1 |         0.0003 |       3.3414 |                3.3414 |      5 |            
 22 |              fused_nn_softmax |      4004 |      1 |         1.1695 |       3.4236 |                3.4236 |     32 |          Y 
 23 |                 fused_reshape |         1 |      1 |         0.0003 |       3.5155 |                3.5155 |      5 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 714
Total latency (us): 669.395

[05:17:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_nn_avg_pool2d"
[05:17:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #19 has finished. Remaining task(s): 2
[05:17:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #21: "fused_reshape_squeeze_reshape"
[05:17:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:17:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:17:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:17:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:17:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:17:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:17:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:17:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:17:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:17:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:17:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:17:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[05:17:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[05:17:59] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #21: "fused_reshape_squeeze_reshape"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |          Y 
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |          Y 
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |          Y 
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |          Y 
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |          Y 
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |          Y 
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |          Y 
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |          Y 
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |          Y 
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |          Y 
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |          Y 
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |       107.2557 |       9.8242 |                9.8242 |     32 |          Y 
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |      3298.4290 |      15.6685 |               15.6685 |     32 |          Y 
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |       214.8867 |       9.8070 |               49.0350 |     32 |          Y 
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |      2288.2636 |      45.0392 |              225.1959 |     32 |          Y 
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |        94.8017 |       5.5574 |                5.5574 |     32 |          Y 
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |      1126.6151 |      45.7394 |               45.7394 |     32 |          Y 
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |       252.9632 |       4.1654 |                4.1654 |     32 |          Y 
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |      1698.0961 |      60.6037 |               60.6037 |     32 |          Y 
 19 |           fused_nn_avg_pool2d |     71680 |      1 |        21.1321 |       3.3920 |                3.3920 |     32 |          Y 
 20 |           fused_nn_conv2d_add |   2051049 |      1 |        61.7245 |      33.2291 |               33.2291 |     32 |          Y 
 21 | fused_reshape_squeeze_reshape |         1 |      1 |         0.0003 |       3.3414 |                3.3414 |      5 |            
 22 |              fused_nn_softmax |      4004 |      1 |         1.1695 |       3.4236 |                3.4236 |     32 |          Y 
 23 |                 fused_reshape |         1 |      1 |         0.0003 |       3.5155 |                3.5155 |      5 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 714
Total latency (us): 669.395

[05:17:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_reshape"
[05:17:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:17:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:18:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:18:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:18:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:18:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:18:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:19:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:19:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:19:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:19:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:19:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[05:19:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[05:19:10] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_reshape"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |          Y 
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |          Y 
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |          Y 
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |          Y 
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |          Y 
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |          Y 
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |          Y 
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |          Y 
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |          Y 
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |          Y 
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |          Y 
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |       107.2557 |       9.8242 |                9.8242 |     32 |          Y 
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |      3298.4290 |      15.6685 |               15.6685 |     32 |          Y 
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |       214.8867 |       9.8070 |               49.0350 |     32 |          Y 
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |      2288.2636 |      45.0392 |              225.1959 |     32 |          Y 
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |        94.8017 |       5.5574 |                5.5574 |     32 |          Y 
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |      1126.6151 |      45.7394 |               45.7394 |     32 |          Y 
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |       252.9632 |       4.1654 |                4.1654 |     32 |          Y 
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |      1698.0961 |      60.6037 |               60.6037 |     32 |          Y 
 19 |           fused_nn_avg_pool2d |     71680 |      1 |        21.1321 |       3.3920 |                3.3920 |     32 |          Y 
 20 |           fused_nn_conv2d_add |   2051049 |      1 |        61.7245 |      33.2291 |               33.2291 |     32 |          Y 
 21 | fused_reshape_squeeze_reshape |         1 |      1 |         0.0003 |       3.3414 |                3.3414 |      5 |            
 22 |              fused_nn_softmax |      4004 |      1 |         1.1695 |       3.4236 |                3.4236 |     32 |          Y 
 23 |                 fused_reshape |         1 |      1 |         0.0003 |       3.5155 |                3.5155 |      5 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 714
Total latency (us): 669.395

[05:19:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #21: "fused_reshape_squeeze_reshape"
[05:19:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:19:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:19:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:19:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:19:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:19:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:19:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:20:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:20:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:20:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:20:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:20:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[05:20:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[05:20:24] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #21: "fused_reshape_squeeze_reshape"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |          Y 
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |          Y 
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |          Y 
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |          Y 
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |          Y 
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |          Y 
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |          Y 
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |          Y 
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |          Y 
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |          Y 
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |          Y 
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |       107.2557 |       9.8242 |                9.8242 |     32 |          Y 
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |      3298.4290 |      15.6685 |               15.6685 |     32 |          Y 
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |       214.8867 |       9.8070 |               49.0350 |     32 |          Y 
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |      2288.2636 |      45.0392 |              225.1959 |     32 |          Y 
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |        94.8017 |       5.5574 |                5.5574 |     32 |          Y 
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |      1126.6151 |      45.7394 |               45.7394 |     32 |          Y 
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |       252.9632 |       4.1654 |                4.1654 |     32 |          Y 
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |      1698.0961 |      60.6037 |               60.6037 |     32 |          Y 
 19 |           fused_nn_avg_pool2d |     71680 |      1 |        21.1321 |       3.3920 |                3.3920 |     32 |          Y 
 20 |           fused_nn_conv2d_add |   2051049 |      1 |        61.7245 |      33.2291 |               33.2291 |     32 |          Y 
 21 | fused_reshape_squeeze_reshape |         1 |      1 |         0.0003 |       3.3414 |                3.3414 |      5 |            
 22 |              fused_nn_softmax |      4004 |      1 |         1.1695 |       3.4236 |                3.4236 |     32 |          Y 
 23 |                 fused_reshape |         1 |      1 |         0.0003 |       3.5155 |                3.5155 |      5 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 714
Total latency (us): 669.395

[05:20:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_reshape"
[05:20:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:20:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:20:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:20:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:20:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:20:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:21:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:21:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:21:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:21:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:21:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:21:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[05:21:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[05:21:33] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_reshape"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |          Y 
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |          Y 
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |          Y 
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |          Y 
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |          Y 
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |          Y 
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |          Y 
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |          Y 
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |          Y 
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |          Y 
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |          Y 
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |       107.2557 |       9.8242 |                9.8242 |     32 |          Y 
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |      3298.4290 |      15.6685 |               15.6685 |     32 |          Y 
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |       214.8867 |       9.8070 |               49.0350 |     32 |          Y 
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |      2288.2636 |      45.0392 |              225.1959 |     32 |          Y 
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |        94.8017 |       5.5574 |                5.5574 |     32 |          Y 
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |      1126.6151 |      45.7394 |               45.7394 |     32 |          Y 
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |       252.9632 |       4.1654 |                4.1654 |     32 |          Y 
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |      1698.0961 |      60.6037 |               60.6037 |     32 |          Y 
 19 |           fused_nn_avg_pool2d |     71680 |      1 |        21.1321 |       3.3920 |                3.3920 |     32 |          Y 
 20 |           fused_nn_conv2d_add |   2051049 |      1 |        61.7245 |      33.2291 |               33.2291 |     32 |          Y 
 21 | fused_reshape_squeeze_reshape |         1 |      1 |         0.0003 |       3.3414 |                3.3414 |      5 |            
 22 |              fused_nn_softmax |      4004 |      1 |         1.1695 |       3.4236 |                3.4236 |     32 |          Y 
 23 |                 fused_reshape |         1 |      1 |         0.0003 |       3.5155 |                3.5155 |      5 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 714
Total latency (us): 669.395

[05:21:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #21: "fused_reshape_squeeze_reshape"
[05:21:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:21:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:21:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:21:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:21:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:22:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:22:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:22:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:22:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:22:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:22:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:22:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[05:22:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[05:22:52] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #21: "fused_reshape_squeeze_reshape"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |      fused_nn_conv2d_add_clip |  22880256 |      1 |      1908.3323 |      11.9897 |               11.9897 |     32 |          Y 
  1 |    fused_nn_conv2d_add_clip_1 |   8429568 |      1 |       554.0199 |      15.2153 |               15.2153 |     32 |          Y 
  2 |    fused_nn_conv2d_add_clip_2 |  53788672 |      1 |      3159.3497 |      17.0252 |               17.0252 |     32 |          Y 
  3 |    fused_nn_conv2d_add_clip_3 |   4214784 |      1 |       268.2119 |      15.7144 |               15.7144 |     32 |          Y 
  4 |    fused_nn_conv2d_add_clip_4 |  52584448 |      1 |      1422.0789 |      36.9772 |               36.9772 |     32 |          Y 
  5 |    fused_nn_conv2d_add_clip_5 |   8429568 |      1 |       568.1622 |      14.8366 |               14.8366 |     32 |          Y 
  6 |    fused_nn_conv2d_add_clip_6 | 103964672 |      1 |      4899.0273 |      21.2215 |               21.2215 |     32 |          Y 
  7 |    fused_nn_conv2d_add_clip_7 |   2107392 |      1 |       189.1382 |      11.1421 |               11.1421 |     32 |          Y 
  8 |    fused_nn_conv2d_add_clip_8 |  51982336 |      1 |      1850.1554 |      28.0962 |               28.0962 |     32 |          Y 
  9 |    fused_nn_conv2d_add_clip_9 |   4214784 |      1 |       583.3417 |       7.2252 |                7.2252 |     32 |          Y 
 10 |   fused_nn_conv2d_add_clip_10 | 103362560 |      1 |      3791.6946 |      27.2603 |               27.2603 |     32 |          Y 
 11 |   fused_nn_conv2d_add_clip_11 |   1053696 |      1 |       107.2557 |       9.8242 |                9.8242 |     32 |          Y 
 12 |   fused_nn_conv2d_add_clip_12 |  51681280 |      1 |      3298.4290 |      15.6685 |               15.6685 |     32 |          Y 
 13 |   fused_nn_conv2d_add_clip_13 |   2107392 |      5 |       214.8867 |       9.8070 |               49.0350 |     32 |          Y 
 14 |   fused_nn_conv2d_add_clip_14 | 103061504 |      5 |      2288.2636 |      45.0392 |              225.1959 |     32 |          Y 
 15 |   fused_nn_conv2d_add_clip_15 |    526848 |      1 |        94.8017 |       5.5574 |                5.5574 |     32 |          Y 
 16 |   fused_nn_conv2d_add_clip_16 |  51530752 |      1 |      1126.6151 |      45.7394 |               45.7394 |     32 |          Y 
 17 |   fused_nn_conv2d_add_clip_17 |   1053696 |      1 |       252.9632 |       4.1654 |                4.1654 |     32 |          Y 
 18 |   fused_nn_conv2d_add_clip_18 | 102910976 |      1 |      1698.0961 |      60.6037 |               60.6037 |     32 |          Y 
 19 |           fused_nn_avg_pool2d |     71680 |      1 |        21.1321 |       3.3920 |                3.3920 |     32 |          Y 
 20 |           fused_nn_conv2d_add |   2051049 |      1 |        61.7245 |      33.2291 |               33.2291 |     32 |          Y 
 21 | fused_reshape_squeeze_reshape |         1 |      1 |         0.0003 |       3.3414 |                3.3414 |      5 |            
 22 |              fused_nn_softmax |      4004 |      1 |         1.1695 |       3.4236 |                3.4236 |     32 |          Y 
 23 |                 fused_reshape |         1 |      1 |         0.0003 |       3.5155 |                3.5155 |      5 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 714
Total latency (us): 669.395

[05:22:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_reshape"
[05:22:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:22:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:22:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:22:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:23:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:23:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:23:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:23:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d927710ce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926797038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d927769d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d927741648)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d9267f0678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d927769dd8)]: 0 failure(s)
[05:24:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:24:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:24:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:24:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #23 has finished. Remaining task(s): 1
[05:24:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #21: "fused_reshape_squeeze_reshape"
[05:24:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:24:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:24:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:24:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:24:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:24:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:24:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:25:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55d926d7bf58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55d926d3e4d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55d926d8e508)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55d9277024c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55d926d06198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55d926d8e5a8)]: 0 failure(s)
[05:25:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:25:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:25:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:25:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #21 has finished. Remaining task(s): 0
Starting to build with relay.
/home/yj/anaconda3/lib/python3.7/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
[[1.5198668e-07 1.0118400e-06 1.3571358e-05 ... 6.8713526e-08
  4.4986226e-07 9.0587247e-07]]
[[1.5198647e-07 1.0118399e-06 1.3571313e-05 ... 6.8713518e-08
  4.4986223e-07 9.0587122e-07]]
